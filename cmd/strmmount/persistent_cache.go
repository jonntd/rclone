package strmmount

import (
	"context"
	"crypto/md5"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"time"

	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/hash"
)

// STRMPersistentCache æŒä¹…åŒ–ç¼“å­˜ç®¡ç†å™¨
type STRMPersistentCache struct {
	mu           sync.RWMutex
	backend      string        // åç«¯ç±»å‹ (123/115)
	remotePath   string        // è¿œç¨‹è·¯å¾„
	configHash   string        // é…ç½®å“ˆå¸Œ
	cacheDir     string        // ç¼“å­˜ç›®å½•
	cacheFile    string        // ç¼“å­˜æ–‡ä»¶è·¯å¾„
	metaFile     string        // å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
	lockFile     string        // é”æ–‡ä»¶è·¯å¾„
	ttl          time.Duration // ç¼“å­˜è¿‡æœŸæ—¶é—´
	enabled      bool          // æ˜¯å¦å¯ç”¨
	lastSync     time.Time     // ä¸Šæ¬¡åŒæ­¥æ—¶é—´
	syncInterval time.Duration // åŒæ­¥é—´éš”
	maxCacheSize int64         // æœ€å¤§ç¼“å­˜å¤§å°
	compression  bool          // æ˜¯å¦å¯ç”¨å‹ç¼©
}

// CacheData ç¼“å­˜æ•°æ®ç»“æ„
type CacheData struct {
	Version     string            `json:"version"`
	Backend     string            `json:"backend"`
	RemotePath  string            `json:"remote_path"`
	ConfigHash  string            `json:"config_hash"`
	CreatedAt   time.Time         `json:"created_at"`
	UpdatedAt   time.Time         `json:"updated_at"`
	ExpiresAt   time.Time         `json:"expires_at"`
	FileCount   int               `json:"file_count"`
	TotalSize   int64             `json:"total_size"`
	Directories []CachedDirectory `json:"directories"`
	Metadata    CacheMetadata     `json:"metadata"`
}

// CachedDirectory ç¼“å­˜çš„ç›®å½•ä¿¡æ¯
type CachedDirectory struct {
	Path      string       `json:"path"`
	DirID     string       `json:"dir_id"`
	ModTime   time.Time    `json:"mod_time"`
	FileCount int          `json:"file_count"`
	TotalSize int64        `json:"total_size"`
	Files     []CachedFile `json:"files"`
}

// CachedFile ç¼“å­˜çš„æ–‡ä»¶ä¿¡æ¯
type CachedFile struct {
	Name     string    `json:"name"`
	Size     int64     `json:"size"`
	ModTime  time.Time `json:"mod_time"`
	FileID   string    `json:"file_id"`
	PickCode string    `json:"pick_code"`
	Hash     string    `json:"hash"`
	MimeType string    `json:"mime_type"`
}

// CacheMetadata ç¼“å­˜å…ƒæ•°æ®
type CacheMetadata struct {
	APICallsCount    int          `json:"api_calls_count"`
	CacheHitRate     float64      `json:"cache_hit_rate"`
	LastSyncDuration string       `json:"last_sync_duration"` // å­˜å‚¨ä¸ºå­—ç¬¦ä¸²æ ¼å¼
	SyncHistory      []SyncRecord `json:"sync_history"`
}

// SyncRecord åŒæ­¥è®°å½•
type SyncRecord struct {
	Timestamp time.Time `json:"timestamp"`
	Duration  string    `json:"duration"` // å­˜å‚¨ä¸ºå­—ç¬¦ä¸²æ ¼å¼
	Added     int       `json:"added"`
	Modified  int       `json:"modified"`
	Deleted   int       `json:"deleted"`
	APICount  int       `json:"api_count"`
}

// SyncChanges åŒæ­¥å˜æ›´
type SyncChanges struct {
	Added         int
	Deleted       int
	Modified      int
	AddedFiles    []CachedFile
	DeletedFiles  []string
	ModifiedFiles []CachedFile
}

// NewSTRMPersistentCache åˆ›å»ºæ–°çš„æŒä¹…åŒ–ç¼“å­˜å®ä¾‹
func NewSTRMPersistentCache(backend, remotePath string, minFileSize int64, videoExts []string, urlFormat string) (*STRMPersistentCache, error) {
	// ç”Ÿæˆé…ç½®å“ˆå¸Œ
	configHash := generateConfigHash(backend, remotePath, minFileSize, videoExts, urlFormat)

	// è·å–ç¼“å­˜ç›®å½•
	cacheDir := getSTRMCacheDir()
	if err := os.MkdirAll(cacheDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create cache directory: %w", err)
	}

	// ç”Ÿæˆæ–‡ä»¶è·¯å¾„
	baseName := fmt.Sprintf("%s_%s_%s", backend, sanitizePath(remotePath), configHash[:8])
	cacheFile := filepath.Join(cacheDir, baseName+".json")
	metaFile := filepath.Join(cacheDir, "metadata", baseName+".meta")
	lockFile := filepath.Join(cacheDir, "locks", baseName+".lock")

	// åˆ›å»ºå­ç›®å½•
	os.MkdirAll(filepath.Dir(metaFile), 0755)
	os.MkdirAll(filepath.Dir(lockFile), 0755)

	spc := &STRMPersistentCache{
		backend:      backend,
		remotePath:   remotePath,
		configHash:   configHash,
		cacheDir:     cacheDir,
		cacheFile:    cacheFile,
		metaFile:     metaFile,
		lockFile:     lockFile,
		ttl:          5 * time.Minute, // é»˜è®¤5åˆ†é’Ÿ
		enabled:      true,
		syncInterval: 1 * time.Minute,
		maxCacheSize: 100 * 1024 * 1024, // 100MB
		compression:  true,
	}

	fs.Infof(nil, "ğŸ—ï¸ [CACHE] åˆå§‹åŒ–æŒä¹…åŒ–ç¼“å­˜: %s", baseName)
	return spc, nil
}

// getSTRMCacheDir è·å– STRM ç¼“å­˜ç›®å½•
func getSTRMCacheDir() string {
	cacheDir := config.GetCacheDir()
	return filepath.Join(cacheDir, "strm-cache")
}

// generateConfigHash ç”Ÿæˆé…ç½®å“ˆå¸Œ
func generateConfigHash(backend, remotePath string, minFileSize int64, videoExts []string, urlFormat string) string {
	data := fmt.Sprintf("%s:%s:%v:%v:%s",
		backend, remotePath, minFileSize, videoExts, urlFormat)
	hash := md5.Sum([]byte(data))
	return fmt.Sprintf("%x", hash)
}

// sanitizePath æ¸…ç†è·¯å¾„ç”¨äºæ–‡ä»¶å
func sanitizePath(path string) string {
	// æ›¿æ¢ä¸å®‰å…¨çš„å­—ç¬¦
	safe := strings.ReplaceAll(path, "/", "_")
	safe = strings.ReplaceAll(safe, "\\", "_")
	safe = strings.ReplaceAll(safe, ":", "_")
	safe = strings.ReplaceAll(safe, "*", "_")
	safe = strings.ReplaceAll(safe, "?", "_")
	safe = strings.ReplaceAll(safe, "\"", "_")
	safe = strings.ReplaceAll(safe, "<", "_")
	safe = strings.ReplaceAll(safe, ">", "_")
	safe = strings.ReplaceAll(safe, "|", "_")

	// é™åˆ¶é•¿åº¦
	if len(safe) > 50 {
		safe = safe[:50]
	}

	return safe
}

// LoadOrCreate åŠ è½½æˆ–åˆ›å»ºç¼“å­˜
func (spc *STRMPersistentCache) LoadOrCreate(ctx context.Context, fsys fs.Fs) (*CacheData, error) {
	if !spc.enabled {
		return spc.createFreshCache(ctx, fsys)
	}

	// è·å–æ–‡ä»¶é”
	if err := spc.acquireLock(); err != nil {
		fs.Logf(nil, "âš ï¸ [CACHE] è·å–é”å¤±è´¥ï¼Œä½¿ç”¨æ— ç¼“å­˜æ¨¡å¼: %v", err)
		return spc.createFreshCache(ctx, fsys)
	}
	defer spc.releaseLock()

	// æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	if !spc.cacheFileExists() {
		fs.Infof(nil, "ğŸ†• [CACHE] é¦–æ¬¡å¯åŠ¨ï¼Œåˆ›å»ºæ–°ç¼“å­˜")
		return spc.createFreshCache(ctx, fsys)
	}

	// åŠ è½½ç°æœ‰ç¼“å­˜
	cacheData, err := spc.loadFromDisk()
	if err != nil {
		fs.Logf(nil, "âš ï¸ [CACHE] åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºæ–°ç¼“å­˜: %v", err)
		return spc.createFreshCache(ctx, fsys)
	}

	// éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
	if !spc.validateCache(cacheData) {
		fs.Infof(nil, "âŒ [CACHE] ç¼“å­˜æ— æ•ˆï¼Œåˆ›å»ºæ–°ç¼“å­˜")
		return spc.createFreshCache(ctx, fsys)
	}

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
	if time.Now().After(cacheData.ExpiresAt) {
		fs.Infof(nil, "â° [CACHE] ç¼“å­˜å·²è¿‡æœŸï¼Œæ‰§è¡Œå¢é‡åŒæ­¥")
		return spc.incrementalSync(ctx, fsys, cacheData)
	}

	fs.Infof(nil, "ğŸ’¾ [CACHE] ä½¿ç”¨æœ‰æ•ˆç¼“å­˜ (%d ä¸ªæ–‡ä»¶, è¿‡æœŸæ—¶é—´: %v)",
		cacheData.FileCount, cacheData.ExpiresAt.Format("15:04:05"))
	return cacheData, nil
}

// cacheFileExists æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
func (spc *STRMPersistentCache) cacheFileExists() bool {
	_, err := os.Stat(spc.cacheFile)
	return err == nil
}

// validateCache éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
func (spc *STRMPersistentCache) validateCache(cacheData *CacheData) bool {
	// æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
	if cacheData.Version != "1.0" {
		return false
	}

	// æ£€æŸ¥åç«¯åŒ¹é…
	if cacheData.Backend != spc.backend {
		return false
	}

	// æ£€æŸ¥è·¯å¾„åŒ¹é…
	if cacheData.RemotePath != spc.remotePath {
		return false
	}

	// æ£€æŸ¥é…ç½®å“ˆå¸Œ
	if cacheData.ConfigHash != spc.configHash {
		return false
	}

	return true
}

// acquireLock è·å–æ–‡ä»¶é”
func (spc *STRMPersistentCache) acquireLock() error {
	// ç®€å•çš„æ–‡ä»¶é”å®ç°
	lockFile, err := os.OpenFile(spc.lockFile, os.O_CREATE|os.O_EXCL|os.O_WRONLY, 0644)
	if err != nil {
		return fmt.Errorf("failed to acquire lock: %w", err)
	}

	// å†™å…¥è¿›ç¨‹ä¿¡æ¯
	fmt.Fprintf(lockFile, "%d\n%s\n", os.Getpid(), time.Now().Format(time.RFC3339))
	lockFile.Close()

	return nil
}

// releaseLock é‡Šæ”¾æ–‡ä»¶é”
func (spc *STRMPersistentCache) releaseLock() {
	os.Remove(spc.lockFile)
}

// createCachedFile ä» fs.Object åˆ›å»º CachedFile
func (spc *STRMPersistentCache) createCachedFile(obj fs.Object) CachedFile {
	cached := CachedFile{
		Name:    filepath.Base(obj.Remote()),
		Size:    obj.Size(),
		ModTime: obj.ModTime(context.Background()),
	}

	// è·å–æ–‡ä»¶IDå’ŒPickCode (æ ¹æ®åç«¯ç±»å‹)
	switch spc.backend {
	case "123":
		if obj123, ok := obj.(interface{ GetID() string }); ok {
			cached.FileID = obj123.GetID()
		}
	case "115":
		if obj115, ok := obj.(interface{ GetPickCode() string }); ok {
			cached.PickCode = obj115.GetPickCode()
		}
	}

	// è·å–å“ˆå¸Œå€¼
	if hashValue, err := obj.Hash(context.Background(), hash.SHA1); err == nil && hashValue != "" {
		cached.Hash = "sha1:" + hashValue
	}

	return cached
}

// SetEnabled å¯ç”¨æˆ–ç¦ç”¨æŒä¹…åŒ–ç¼“å­˜
func (spc *STRMPersistentCache) SetEnabled(enabled bool) {
	spc.mu.Lock()
	defer spc.mu.Unlock()
	spc.enabled = enabled
}

// SetTTL è®¾ç½®ç¼“å­˜è¿‡æœŸæ—¶é—´
func (spc *STRMPersistentCache) SetTTL(ttl time.Duration) {
	spc.mu.Lock()
	defer spc.mu.Unlock()
	spc.ttl = ttl
}

// createFreshCache åˆ›å»ºæ–°çš„ç¼“å­˜
func (spc *STRMPersistentCache) createFreshCache(ctx context.Context, fsys fs.Fs) (*CacheData, error) {
	startTime := time.Now()
	fs.Infof(nil, "ğŸ†• [CACHE] å¼€å§‹åˆ›å»ºæ–°ç¼“å­˜...")

	// è·å–è¿œç¨‹æ–‡ä»¶åˆ—è¡¨
	remoteFiles, err := spc.fetchRemoteFiles(ctx, fsys)
	if err != nil {
		return nil, fmt.Errorf("è·å–è¿œç¨‹æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åˆ›å»ºç¼“å­˜æ•°æ®
	now := time.Now()
	cacheData := &CacheData{
		Version:     "1.0",
		Backend:     spc.backend,
		RemotePath:  spc.remotePath,
		ConfigHash:  spc.configHash,
		CreatedAt:   now,
		UpdatedAt:   now,
		ExpiresAt:   now.Add(spc.ttl),
		FileCount:   len(remoteFiles),
		Directories: spc.organizeFilesByDirectory(remoteFiles),
		Metadata: CacheMetadata{
			APICallsCount:    1, // ä¸€æ¬¡ List è°ƒç”¨
			CacheHitRate:     0.0,
			LastSyncDuration: time.Since(startTime).String(),
			SyncHistory: []SyncRecord{
				{
					Timestamp: now,
					Duration:  time.Since(startTime).String(),
					Added:     len(remoteFiles),
					Modified:  0,
					Deleted:   0,
					APICount:  1,
				},
			},
		},
	}

	// è®¡ç®—æ€»å¤§å°
	for _, dir := range cacheData.Directories {
		cacheData.TotalSize += dir.TotalSize
	}

	// ä¿å­˜åˆ°ç£ç›˜
	if err := spc.saveToDisk(cacheData); err != nil {
		return nil, fmt.Errorf("ä¿å­˜ç¼“å­˜å¤±è´¥: %w", err)
	}

	duration := time.Since(startTime)
	fs.Infof(nil, "âœ… [CACHE] æ–°ç¼“å­˜åˆ›å»ºå®Œæˆ: %d ä¸ªæ–‡ä»¶, è€—æ—¶ %v",
		cacheData.FileCount, duration)

	return cacheData, nil
}

// fetchRemoteFiles è·å–è¿œç¨‹æ–‡ä»¶åˆ—è¡¨ï¼ˆæ™ºèƒ½é™åˆ¶æ‰«æèŒƒå›´ï¼‰
func (spc *STRMPersistentCache) fetchRemoteFiles(ctx context.Context, fsys fs.Fs) ([]fs.Object, error) {
	// ğŸ›¡ï¸ æ™ºèƒ½QPSä¿æŠ¤ï¼šåªæ‰«ææ ¹ç›®å½•ï¼Œé¿å…æ·±åº¦é€’å½’
	fs.Infof(nil, "ğŸ” [CACHE] æ™ºèƒ½æ‰«ææ¨¡å¼ï¼šä»…æ‰«ææ ¹ç›®å½•ï¼Œé¿å…æ·±åº¦é€’å½’")

	var files []fs.Object

	// åªåˆ—å‡ºæ ¹ç›®å½•ï¼Œä¸é€’å½’å­ç›®å½•
	entries, err := fsys.List(ctx, "")
	if err != nil {
		return nil, fmt.Errorf("åˆ—å‡ºæ ¹ç›®å½•å¤±è´¥: %w", err)
	}

	// åªå¤„ç†æ ¹ç›®å½•ä¸­çš„è§†é¢‘æ–‡ä»¶
	for _, entry := range entries {
		if obj, ok := entry.(fs.Object); ok {
			if spc.isVideoFile(obj) {
				files = append(files, obj)
			}
		}
	}

	fs.Infof(nil, "ğŸ“ [CACHE] æ ¹ç›®å½•æ‰«æå®Œæˆ: %d ä¸ªè§†é¢‘æ–‡ä»¶", len(files))
	return files, nil
}

// isVideoFile æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
func (spc *STRMPersistentCache) isVideoFile(obj fs.Object) bool {
	name := obj.Remote()
	size := obj.Size()

	// æ£€æŸ¥æ–‡ä»¶å¤§å° (è¿™é‡Œéœ€è¦ä»é…ç½®è·å–ï¼Œæš‚æ—¶ç¡¬ç¼–ç )
	minSize := int64(100 * 1024 * 1024) // 100MB
	if size < minSize {
		return false
	}

	// æ£€æŸ¥æ‰©å±•å
	ext := strings.ToLower(filepath.Ext(name))
	videoExts := []string{".iso", ".mp4", ".mkv", ".avi", ".mov", ".wmv", ".flv", ".webm", ".m4v", ".3gp", ".ts", ".m2ts"}

	for _, videoExt := range videoExts {
		if ext == videoExt {
			return true
		}
	}

	return false
}

// organizeFilesByDirectory æŒ‰ç›®å½•ç»„ç»‡æ–‡ä»¶
func (spc *STRMPersistentCache) organizeFilesByDirectory(files []fs.Object) []CachedDirectory {
	dirMap := make(map[string]*CachedDirectory)

	for _, obj := range files {
		dirPath := filepath.Dir(obj.Remote())
		if dirPath == "." {
			dirPath = ""
		}

		// è·å–æˆ–åˆ›å»ºç›®å½•æ¡ç›®
		dir, exists := dirMap[dirPath]
		if !exists {
			dir = &CachedDirectory{
				Path:      dirPath,
				ModTime:   obj.ModTime(context.Background()),
				FileCount: 0,
				TotalSize: 0,
				Files:     []CachedFile{},
			}
			dirMap[dirPath] = dir
		}

		// æ·»åŠ æ–‡ä»¶
		cachedFile := spc.createCachedFile(obj)
		dir.Files = append(dir.Files, cachedFile)
		dir.FileCount++
		dir.TotalSize += cachedFile.Size

		// æ›´æ–°ç›®å½•ä¿®æ”¹æ—¶é—´ä¸ºæœ€æ–°æ–‡ä»¶çš„æ—¶é—´
		if cachedFile.ModTime.After(dir.ModTime) {
			dir.ModTime = cachedFile.ModTime
		}
	}

	// è½¬æ¢ä¸ºåˆ‡ç‰‡
	var directories []CachedDirectory
	for _, dir := range dirMap {
		directories = append(directories, *dir)
	}

	return directories
}

// GetStats è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
func (spc *STRMPersistentCache) GetStats() map[string]interface{} {
	spc.mu.RLock()
	defer spc.mu.RUnlock()

	stats := map[string]interface{}{
		"enabled":       spc.enabled,
		"backend":       spc.backend,
		"remote_path":   spc.remotePath,
		"cache_file":    spc.cacheFile,
		"ttl":           spc.ttl.String(),
		"last_sync":     spc.lastSync.Format(time.RFC3339),
		"sync_interval": spc.syncInterval.String(),
	}

	// æ·»åŠ æ–‡ä»¶å¤§å°ä¿¡æ¯
	if info, err := os.Stat(spc.cacheFile); err == nil {
		stats["cache_size"] = info.Size()
		stats["cache_mod_time"] = info.ModTime().Format(time.RFC3339)
	}

	return stats
}

// loadFromDisk ä»ç£ç›˜åŠ è½½ç¼“å­˜æ•°æ®
func (spc *STRMPersistentCache) loadFromDisk() (*CacheData, error) {
	data, err := os.ReadFile(spc.cacheFile)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: %w", err)
	}

	var cacheData CacheData
	if err := json.Unmarshal(data, &cacheData); err != nil {
		return nil, fmt.Errorf("è§£æç¼“å­˜æ•°æ®å¤±è´¥: %w", err)
	}

	return &cacheData, nil
}

// saveToDisk ä¿å­˜ç¼“å­˜æ•°æ®åˆ°ç£ç›˜
func (spc *STRMPersistentCache) saveToDisk(cacheData *CacheData) error {
	// æ›´æ–°æ—¶é—´æˆ³
	cacheData.UpdatedAt = time.Now()

	// åºåˆ—åŒ–ä¸º JSON
	data, err := json.MarshalIndent(cacheData, "", "  ")
	if err != nil {
		return fmt.Errorf("åºåˆ—åŒ–ç¼“å­˜æ•°æ®å¤±è´¥: %w", err)
	}

	// å†™å…¥ä¸´æ—¶æ–‡ä»¶
	tempFile := spc.cacheFile + ".tmp"
	if err := os.WriteFile(tempFile, data, 0644); err != nil {
		return fmt.Errorf("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// åŸå­æ€§é‡å‘½å
	if err := os.Rename(tempFile, spc.cacheFile); err != nil {
		os.Remove(tempFile) // æ¸…ç†ä¸´æ—¶æ–‡ä»¶
		return fmt.Errorf("é‡å‘½åç¼“å­˜æ–‡ä»¶å¤±è´¥: %w", err)
	}

	fs.Debugf(nil, "ğŸ’¾ [CACHE] ç¼“å­˜å·²ä¿å­˜: %d ä¸ªæ–‡ä»¶, å¤§å°: %d å­—èŠ‚",
		cacheData.FileCount, len(data))
	return nil
}

// incrementalSync æ‰§è¡Œå¢é‡åŒæ­¥ï¼ˆæ™ºèƒ½é™åˆ¶æ‰«æèŒƒå›´ï¼‰
func (spc *STRMPersistentCache) incrementalSync(ctx context.Context, fsys fs.Fs, oldCache *CacheData) (*CacheData, error) {
	startTime := time.Now()
	fs.Infof(nil, "ğŸ”„ [SYNC] å¼€å§‹æ™ºèƒ½å¢é‡åŒæ­¥ï¼ˆä»…æ ¹ç›®å½•ï¼‰...")

	// ğŸ›¡ï¸ æ™ºèƒ½QPSä¿æŠ¤ï¼šåªè·å–æ ¹ç›®å½•æ–‡ä»¶ï¼Œé¿å…æ·±åº¦é€’å½’
	remoteFiles, err := spc.fetchRemoteFiles(ctx, fsys)
	if err != nil {
		return nil, fmt.Errorf("è·å–è¿œç¨‹æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// æ£€æµ‹å˜æ›´
	changes := spc.detectChanges(oldCache, remoteFiles)

	// åº”ç”¨å˜æ›´
	newCache := spc.applyChanges(oldCache, changes)
	newCache.UpdatedAt = time.Now()
	newCache.ExpiresAt = time.Now().Add(spc.ttl)

	// æ›´æ–°å…ƒæ•°æ®
	syncRecord := SyncRecord{
		Timestamp: time.Now(),
		Duration:  time.Since(startTime).String(),
		Added:     changes.Added,
		Modified:  changes.Modified,
		Deleted:   changes.Deleted,
		APICount:  1, // ä¸€æ¬¡æ ¹ç›®å½•Listè°ƒç”¨
	}

	newCache.Metadata.SyncHistory = append(newCache.Metadata.SyncHistory, syncRecord)
	if len(newCache.Metadata.SyncHistory) > 10 {
		newCache.Metadata.SyncHistory = newCache.Metadata.SyncHistory[1:]
	}

	newCache.Metadata.LastSyncDuration = syncRecord.Duration
	newCache.Metadata.APICallsCount += syncRecord.APICount

	// ä¿å­˜æ–°ç¼“å­˜
	if err := spc.saveToDisk(newCache); err != nil {
		return nil, fmt.Errorf("ä¿å­˜ç¼“å­˜å¤±è´¥: %w", err)
	}

	duration := time.Since(startTime)
	fs.Infof(nil, "âœ… [SYNC] æ™ºèƒ½åŒæ­¥å®Œæˆ: +%d -%d ~%d (è€—æ—¶ %v, ä»…æ ¹ç›®å½•)",
		changes.Added, changes.Deleted, changes.Modified, duration)

	return newCache, nil
}

// detectChanges æ£€æµ‹æ–‡ä»¶å˜æ›´
func (spc *STRMPersistentCache) detectChanges(oldCache *CacheData, remoteFiles []fs.Object) *SyncChanges {
	changes := &SyncChanges{}

	// åˆ›å»ºæœ¬åœ°æ–‡ä»¶æ˜ å°„ (path -> CachedFile)
	localFiles := make(map[string]CachedFile)
	for _, dir := range oldCache.Directories {
		for _, file := range dir.Files {
			fullPath := filepath.Join(dir.Path, file.Name)
			localFiles[fullPath] = file
		}
	}

	// åˆ›å»ºè¿œç¨‹æ–‡ä»¶æ˜ å°„
	remoteFileMap := make(map[string]fs.Object)
	for _, obj := range remoteFiles {
		remoteFileMap[obj.Remote()] = obj
	}

	// æ£€æµ‹æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
	for remotePath, remoteObj := range remoteFileMap {
		if localFile, exists := localFiles[remotePath]; exists {
			// æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦ä¿®æ”¹
			if spc.isFileModified(localFile, remoteObj) {
				changes.ModifiedFiles = append(changes.ModifiedFiles,
					spc.createCachedFile(remoteObj))
				changes.Modified++
			}
		} else {
			// æ–°æ–‡ä»¶
			changes.AddedFiles = append(changes.AddedFiles,
				spc.createCachedFile(remoteObj))
			changes.Added++
		}
	}

	// æ£€æµ‹åˆ é™¤çš„æ–‡ä»¶
	for localPath := range localFiles {
		if _, exists := remoteFileMap[localPath]; !exists {
			changes.DeletedFiles = append(changes.DeletedFiles, localPath)
			changes.Deleted++
		}
	}

	return changes
}

// isFileModified æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
func (spc *STRMPersistentCache) isFileModified(local CachedFile, remote fs.Object) bool {
	// æ¯”è¾ƒæ–‡ä»¶å¤§å°
	if local.Size != remote.Size() {
		return true
	}

	// æ¯”è¾ƒä¿®æ”¹æ—¶é—´
	if !local.ModTime.Equal(remote.ModTime(context.Background())) {
		return true
	}

	// æ¯”è¾ƒå“ˆå¸Œå€¼ (å¦‚æœå¯ç”¨)
	if hashValue, err := remote.Hash(context.Background(), hash.SHA1); err == nil && hashValue != "" {
		expectedHash := "sha1:" + hashValue
		if local.Hash != expectedHash {
			return true
		}
	}

	return false
}

// applyChanges åº”ç”¨å˜æ›´åˆ°ç¼“å­˜
func (spc *STRMPersistentCache) applyChanges(oldCache *CacheData, changes *SyncChanges) *CacheData {
	// åˆ›å»ºæ–°çš„ç¼“å­˜æ•°æ®
	newCache := &CacheData{
		Version:    oldCache.Version,
		Backend:    oldCache.Backend,
		RemotePath: oldCache.RemotePath,
		ConfigHash: oldCache.ConfigHash,
		CreatedAt:  oldCache.CreatedAt,
		Metadata:   oldCache.Metadata,
	}

	// åˆå¹¶æ‰€æœ‰æ–‡ä»¶
	allFiles := []fs.Object{}

	// æ·»åŠ æ–°å¢çš„æ–‡ä»¶
	for range changes.AddedFiles {
		// è¿™é‡Œéœ€è¦ä» CachedFile é‡å»º fs.Objectï¼Œæš‚æ—¶è·³è¿‡
		// å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„é€»è¾‘
	}

	// é‡æ–°ç»„ç»‡ç›®å½•ç»“æ„
	newCache.Directories = spc.organizeFilesByDirectory(allFiles)

	// æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	newCache.FileCount = 0
	newCache.TotalSize = 0
	for _, dir := range newCache.Directories {
		newCache.FileCount += dir.FileCount
		newCache.TotalSize += dir.TotalSize
	}

	return newCache
}
