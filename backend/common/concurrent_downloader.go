// Package common provides unified components for cloud storage backends
package common

import (
	"context"
	"fmt"
	"io"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/accounting"
)

// ConcurrentDownloader ç»Ÿä¸€çš„å¹¶å‘ä¸‹è½½å™¨æ¥å£
// ğŸš€ è®¾è®¡ç›®æ ‡ï¼šæ¶ˆé™¤123ç½‘ç›˜å’Œ115ç½‘ç›˜85%çš„é‡å¤ä»£ç 
type ConcurrentDownloader interface {
	// ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
	DownloadToFile(ctx context.Context, obj fs.Object, tempFile *os.File, options ...fs.OpenOption) (int64, error)

	// æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
	ShouldUseConcurrentDownload(ctx context.Context, obj fs.Object, options []fs.OpenOption) bool

	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å‚æ•°
	CalculateChunkParams(fileSize int64) ChunkParams

	// è·å–ä¸‹è½½URLï¼ˆåç«¯ç‰¹å®šå®ç°ï¼‰
	GetDownloadURL(ctx context.Context, obj fs.Object, start, end int64) (string, error)

	// ä¸‹è½½å•ä¸ªåˆ†ç‰‡ï¼ˆåç«¯ç‰¹å®šå®ç°ï¼‰
	DownloadChunk(ctx context.Context, url string, tempFile *os.File, start, end int64, account *accounting.Account) error
}

// ChunkParams åˆ†ç‰‡å‚æ•°
type ChunkParams struct {
	ChunkSize      int64 // åˆ†ç‰‡å¤§å°
	NumChunks      int64 // åˆ†ç‰‡æ•°é‡
	MaxConcurrency int64 // æœ€å¤§å¹¶å‘æ•°
}

// DownloadProgress ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
type DownloadProgress struct {
	mu              sync.RWMutex
	totalChunks     int64
	completedChunks int64
	totalBytes      int64
	downloadedBytes int64
	startTime       time.Time
	chunkProgress   map[int64]ChunkProgress
}

// ChunkProgress åˆ†ç‰‡è¿›åº¦
type ChunkProgress struct {
	Index       int64
	Size        int64
	Downloaded  int64
	StartTime   time.Time
	CompletedAt time.Time
	Speed       int64 // bytes/second
}

// UnifiedConcurrentDownloader ç»Ÿä¸€çš„å¹¶å‘ä¸‹è½½å™¨å®ç°
type UnifiedConcurrentDownloader struct {
	fs            fs.Fs
	backendType   string // "123" æˆ– "115"
	adapter       DownloadAdapter
	resumeManager UnifiedResumeManager

	// é…ç½®å‚æ•°
	minFileSize      int64 // å¯ç”¨å¹¶å‘ä¸‹è½½çš„æœ€å°æ–‡ä»¶å¤§å°
	maxConcurrency   int64 // æœ€å¤§å¹¶å‘æ•°
	defaultChunkSize int64 // é»˜è®¤åˆ†ç‰‡å¤§å°
}

// DownloadAdapter åç«¯ç‰¹å®šçš„ä¸‹è½½é€‚é…å™¨æ¥å£
type DownloadAdapter interface {
	// è·å–ä¸‹è½½URL
	GetDownloadURL(ctx context.Context, obj fs.Object, start, end int64) (string, error)

	// ä¸‹è½½å•ä¸ªåˆ†ç‰‡
	DownloadChunk(ctx context.Context, url string, tempFile *os.File, start, end int64, account *accounting.Account) error

	// éªŒè¯ä¸‹è½½å®Œæ•´æ€§
	VerifyDownload(ctx context.Context, obj fs.Object, tempFile *os.File) error

	// è·å–åç«¯ç‰¹å®šçš„é…ç½®
	GetConfig() DownloadConfig
}

// DownloadConfig ä¸‹è½½é…ç½®
type DownloadConfig struct {
	MinFileSize      int64         // å¯ç”¨å¹¶å‘ä¸‹è½½çš„æœ€å°æ–‡ä»¶å¤§å°
	MaxConcurrency   int64         // æœ€å¤§å¹¶å‘æ•°
	DefaultChunkSize int64         // é»˜è®¤åˆ†ç‰‡å¤§å°
	TimeoutPerChunk  time.Duration // æ¯ä¸ªåˆ†ç‰‡çš„è¶…æ—¶æ—¶é—´
}

// NewUnifiedConcurrentDownloader åˆ›å»ºç»Ÿä¸€çš„å¹¶å‘ä¸‹è½½å™¨
func NewUnifiedConcurrentDownloader(filesystem fs.Fs, backendType string, adapter DownloadAdapter, resumeManager UnifiedResumeManager) *UnifiedConcurrentDownloader {
	config := adapter.GetConfig()

	return &UnifiedConcurrentDownloader{
		fs:               filesystem,
		backendType:      backendType,
		adapter:          adapter,
		resumeManager:    resumeManager,
		minFileSize:      config.MinFileSize,
		maxConcurrency:   config.MaxConcurrency,
		defaultChunkSize: config.DefaultChunkSize,
	}
}

// ShouldUseConcurrentDownload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
func (d *UnifiedConcurrentDownloader) ShouldUseConcurrentDownload(ctx context.Context, obj fs.Object, options []fs.OpenOption) bool {
	// ğŸ” è°ƒè¯•ï¼šæ·»åŠ è¯¦ç»†çš„åˆ¤æ–­æ—¥å¿—
	fs.Debugf(d.fs, "ğŸ” ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨åˆ¤æ–­å¼€å§‹: åç«¯=%s, æ–‡ä»¶=%s, å¤§å°=%s",
		d.backendType, obj.Remote(), fs.SizeSuffix(obj.Size()))

	// æ£€æŸ¥æ˜¯å¦æœ‰ç¦ç”¨å¹¶å‘ä¸‹è½½çš„é€‰é¡¹
	for _, option := range options {
		if option.String() == "DisableConcurrentDownload" {
			fs.Debugf(d.fs, "ğŸ”§ æ£€æµ‹åˆ°ç¦ç”¨å¹¶å‘ä¸‹è½½é€‰é¡¹ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½")
			return false
		}
	}

	// æ£€æŸ¥æ–‡ä»¶å¤§å°
	fs.Debugf(d.fs, "ğŸ” æ–‡ä»¶å¤§å°æ£€æŸ¥: %s >= %s ?", fs.SizeSuffix(obj.Size()), fs.SizeSuffix(d.minFileSize))
	if obj.Size() < d.minFileSize {
		fs.Debugf(d.fs, "ğŸ” æ–‡ä»¶å¤ªå°ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½")
		return false
	}

	// æ£€æŸ¥Rangeé€‰é¡¹ï¼Œé¿å…å¤šé‡å¹¶å‘ä¸‹è½½å†²çª
	for _, option := range options {
		if rangeOpt, ok := option.(*fs.RangeOption); ok {
			// å¦‚æœRangeè¦†ç›–äº†æ•´ä¸ªæ–‡ä»¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
			if rangeOpt.Start == 0 && (rangeOpt.End == -1 || rangeOpt.End >= obj.Size()-1) {
				fs.Debugf(d.fs, "æ£€æµ‹åˆ°å…¨æ–‡ä»¶Rangeé€‰é¡¹ï¼Œå…è®¸å¹¶å‘ä¸‹è½½")
				break
			} else {
				// è®¡ç®—Rangeå¤§å°
				rangeSize := rangeOpt.End - rangeOpt.Start + 1

				// ğŸš€ ä¼˜åŒ–ï¼š123ç½‘ç›˜ç‰¹æ®Šå¤„ç† - ä¸rcloneå¤šçº¿ç¨‹å¤åˆ¶ååŒå·¥ä½œ
				if d.backendType == "123" {
					// ğŸ¯ å…³é”®ä¼˜åŒ–ï¼šæ£€æµ‹rcloneå¤šçº¿ç¨‹å¤åˆ¶åœºæ™¯
					// rcloneå¤šçº¿ç¨‹å¤åˆ¶é€šå¸¸ä½¿ç”¨64MBåˆ†ç‰‡ï¼Œæˆ‘ä»¬åº”è¯¥é…åˆè€Œä¸æ˜¯å†²çª
					if rangeSize >= 32*1024*1024 && rangeSize <= 128*1024*1024 {
						// è¿™å¾ˆå¯èƒ½æ˜¯rcloneå¤šçº¿ç¨‹å¤åˆ¶çš„åˆ†ç‰‡ï¼Œä¸éœ€è¦é¢å¤–çš„å¹¶å‘ä¸‹è½½
						fs.Debugf(d.fs, "ğŸ¯ 123ç½‘ç›˜æ£€æµ‹åˆ°rcloneå¤šçº¿ç¨‹å¤åˆ¶åˆ†ç‰‡ (%d-%d, %s)ï¼Œä½¿ç”¨å•çº¿ç¨‹ä¸‹è½½é…åˆrcloneå¹¶å‘",
							rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
						return false
					} else if rangeSize >= 200*1024*1024 {
						// è¶…å¤§Rangeï¼Œå¯èƒ½æ˜¯è·¨äº‘ä¼ è¾“ï¼Œç¦ç”¨å¹¶å‘é¿å…èµ„æºç«äº‰
						fs.Infof(d.fs, "ğŸŒ 123ç½‘ç›˜æ£€æµ‹åˆ°è¶…å¤§Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œä¸ºé¿å…èµ„æºç«äº‰ç¦ç”¨å¹¶å‘ä¸‹è½½",
							rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
						return false
					} else {
						// å°Rangeæˆ–å…¶ä»–æƒ…å†µï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½
						fs.Debugf(d.fs, "123ç½‘ç›˜æ£€æµ‹åˆ°Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½",
							rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
						return false
					}
				} else {
					// ğŸ”§ ä¿®å¤115ç½‘ç›˜Rangeé€‰é¡¹å¤„ç†ï¼šRangeé€‰é¡¹ä¸åº”è¯¥ç¦ç”¨å¹¶å‘ä¸‹è½½
					// Rangeé€‰é¡¹ç”¨äºrcloneå¤šçº¿ç¨‹å¤åˆ¶çš„åˆ†ç‰‡ä¸‹è½½ï¼Œä¸æˆ‘ä»¬çš„å¹¶å‘ä¸‹è½½æ˜¯ä¸åŒå±‚é¢çš„ä¼˜åŒ–
					// 115ç½‘ç›˜åº”è¯¥æ”¯æŒRangeè¯·æ±‚çš„å¹¶å‘ä¸‹è½½ï¼Œé™¤éæ–‡ä»¶å¤ªå°
					if rangeSize < 10*1024*1024 { // å°äº10MBçš„Rangeï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½
						fs.Debugf(d.fs, "115ç½‘ç›˜æ£€æµ‹åˆ°å°Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½",
							rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
						return false
					} else {
						// å¤§Rangeè¯·æ±‚ï¼Œå…è®¸å¹¶å‘ä¸‹è½½ä»¥æé«˜æ•ˆç‡
						fs.Debugf(d.fs, "115ç½‘ç›˜æ£€æµ‹åˆ°å¤§Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œå…è®¸å¹¶å‘ä¸‹è½½",
							rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
						// ç»§ç»­æ£€æŸ¥å…¶ä»–æ¡ä»¶ï¼Œä¸ç›´æ¥è¿”å›false
					}
				}
			}
		}
	}

	return true
}

// CalculateChunkParams è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å‚æ•°
func (d *UnifiedConcurrentDownloader) CalculateChunkParams(fileSize int64) ChunkParams {
	// åŠ¨æ€è®¡ç®—åˆ†ç‰‡å¤§å°
	chunkSize := d.calculateOptimalChunkSize(fileSize)

	// è®¡ç®—åˆ†ç‰‡æ•°é‡
	numChunks := (fileSize + chunkSize - 1) / chunkSize

	// è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	maxConcurrency := d.calculateOptimalConcurrency(fileSize)
	maxConcurrency = min(numChunks, maxConcurrency)

	return ChunkParams{
		ChunkSize:      chunkSize,
		NumChunks:      numChunks,
		MaxConcurrency: maxConcurrency,
	}
}

// DownloadToFile ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶æ–‡ä»¶
func (d *UnifiedConcurrentDownloader) DownloadToFile(ctx context.Context, obj fs.Object, tempFile *os.File, options ...fs.OpenOption) (int64, error) {
	fileSize := obj.Size()

	// è®¡ç®—åˆ†ç‰‡å‚æ•°
	params := d.CalculateChunkParams(fileSize)

	fs.Infof(d.fs, "ğŸ“Š %sç½‘ç›˜å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		d.backendType, fs.SizeSuffix(params.ChunkSize), params.NumChunks, params.MaxConcurrency)

	fs.Infof(d.fs, "ğŸš€ %sç½‘ç›˜å¼€å§‹å¹¶å‘ä¸‹è½½: %s (%s)",
		d.backendType, obj.Remote(), fs.SizeSuffix(fileSize))

	// ç”Ÿæˆä»»åŠ¡IDç”¨äºæ–­ç‚¹ç»­ä¼ 
	taskID := d.generateTaskID(obj)

	// å°è¯•åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	resumeInfo, err := d.resumeManager.LoadResumeInfo(taskID)
	if err != nil {
		fs.Debugf(d.fs, "ğŸ“Š åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v (TaskID: %s)", err, taskID)
	} else if resumeInfo != nil {
		fs.Infof(d.fs, "ğŸ“Š å‘ç°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: å·²å®Œæˆ %d/%d åˆ†ç‰‡ (TaskID: %s)",
			resumeInfo.GetCompletedChunkCount(), resumeInfo.GetTotalChunks(), taskID)
	} else {
		fs.Debugf(d.fs, "ğŸ“Š æœªæ‰¾åˆ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œå°†åˆ›å»ºæ–°çš„ (TaskID: %s)", taskID)
	}

	// ğŸ”§ ä¼˜åŒ–1ï¼šæ–­ç‚¹ç»­ä¼ ä¿¡æ¯é¢„éªŒè¯
	if resumeInfo != nil {
		if !d.validateResumeInfo(resumeInfo, tempFile, params, fileSize) {
			fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯éªŒè¯å¤±è´¥ï¼Œæ¸…ç†å¹¶é‡æ–°å¼€å§‹", d.backendType)
			d.resumeManager.DeleteResumeInfo(taskID)
			resumeInfo = nil
		} else {
			fs.Debugf(d.fs, "âœ… %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯éªŒè¯é€šè¿‡", d.backendType)
		}
	}

	// å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ–­ç‚¹ä¿¡æ¯ï¼Œåˆ›å»ºæ–°çš„
	if resumeInfo == nil {
		resumeInfo = d.createResumeInfo(taskID, obj, params, tempFile)
	}

	// åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
	progress := NewDownloadProgress(params.NumChunks, fileSize)

	// ğŸ”§ ä¿®å¤è¿›åº¦åˆå§‹åŒ–ï¼šå¦‚æœæœ‰å·²å®Œæˆçš„åˆ†ç‰‡ï¼Œä½¿ç”¨æ­£ç¡®çš„åˆ†ç‰‡å¤§å°æ›´æ–°è¿›åº¦
	if resumeInfo != nil {
		for chunkIndex := range resumeInfo.GetCompletedChunks() {
			// ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¡ç®—å®é™…çš„åˆ†ç‰‡å¤§å°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„params.ChunkSize
			chunkStart := chunkIndex * params.ChunkSize
			chunkEnd := chunkStart + params.ChunkSize - 1
			if chunkEnd >= fileSize {
				chunkEnd = fileSize - 1
			}
			actualChunkSize := chunkEnd - chunkStart + 1
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, 0)
		}
	}

	// æ‰§è¡Œå¹¶å‘ä¸‹è½½
	startTime := time.Now()
	err = d.downloadChunksConcurrently(ctx, obj, tempFile, params, resumeInfo, progress)
	downloadDuration := time.Since(startTime)

	if err != nil {
		return 0, fmt.Errorf("%sç½‘ç›˜å¹¶å‘ä¸‹è½½å¤±è´¥: %w", d.backendType, err)
	}

	// ğŸ”§ ä¿®å¤ï¼šå¢å¼ºä¸‹è½½å®Œæ•´æ€§éªŒè¯
	stat, err := tempFile.Stat()
	if err != nil {
		return 0, fmt.Errorf("è·å–ä¸‹è½½æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	actualSize := stat.Size()
	if actualSize != fileSize {
		// ğŸ”§ æ–‡ä»¶å¤§å°ä¸åŒ¹é…æ—¶ï¼Œæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¹¶æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
		if d.resumeManager != nil {
			d.resumeManager.DeleteResumeInfo(taskID)
			fs.Debugf(d.fs, "æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œå·²æ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", taskID)
		}

		// æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
		completedChunks := 0
		if resumeInfo != nil {
			completedChunks = len(resumeInfo.GetCompletedChunks())
		}

		return 0, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s (å·²å®Œæˆåˆ†ç‰‡: %d/%d)",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(actualSize), completedChunks, params.NumChunks)
	}

	// ä½¿ç”¨åç«¯ç‰¹å®šçš„éªŒè¯
	if err := d.adapter.VerifyDownload(ctx, obj, tempFile); err != nil {
		return 0, fmt.Errorf("ä¸‹è½½å®Œæ•´æ€§éªŒè¯å¤±è´¥: %w", err)
	}

	fs.Infof(d.fs, "âœ… %sç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		d.backendType, fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// ğŸ”§ ä¿®å¤ï¼šå»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“åœºæ™¯
	if d.resumeManager != nil {
		// æ·»åŠ å»¶è¿Ÿæ¸…ç†ï¼Œç»™è·¨äº‘ä¼ è¾“ç­‰åç»­æ“ä½œç•™å‡ºæ—¶é—´
		go func() {
			time.Sleep(30 * time.Second) // å»¶è¿Ÿ30ç§’æ¸…ç†
			err := d.resumeManager.DeleteResumeInfo(taskID)
			if err != nil {
				fs.Debugf(d.fs, "å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v (TaskID: %s)", err, taskID)
			} else {
				fs.Debugf(d.fs, "âœ… å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æˆåŠŸ (TaskID: %s)", taskID)
			}
		}()
		fs.Debugf(d.fs, "ğŸ”§ å·²å®‰æ’å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", taskID)
	}

	return actualSize, nil
}

// DownloadToFileWithAccount æ”¯æŒAccountå¯¹è±¡çš„å¹¶å‘ä¸‹è½½ï¼ˆå‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (d *UnifiedConcurrentDownloader) DownloadToFileWithAccount(ctx context.Context, obj fs.Object, tempFile *os.File, transfer *accounting.Transfer, options ...fs.OpenOption) (int64, error) {
	fileSize := obj.Size()
	if fileSize <= 0 {
		return 0, fmt.Errorf("æ–‡ä»¶å¤§å°æ— æ•ˆ: %d", fileSize)
	}

	// è®¡ç®—åˆ†ç‰‡å‚æ•°
	params := d.CalculateChunkParams(fileSize)

	fs.Infof(d.fs, "ğŸ“Š %sç½‘ç›˜å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		d.backendType, fs.SizeSuffix(params.ChunkSize), params.NumChunks, params.MaxConcurrency)

	fs.Infof(d.fs, "ğŸš€ %sç½‘ç›˜å¼€å§‹å¹¶å‘ä¸‹è½½: %s (%s)",
		d.backendType, obj.Remote(), fs.SizeSuffix(fileSize))

	// ç”Ÿæˆä»»åŠ¡IDç”¨äºæ–­ç‚¹ç»­ä¼ 
	taskID := d.generateTaskID(obj)

	// å°è¯•åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	resumeInfo, err := d.resumeManager.LoadResumeInfo(taskID)
	if err != nil {
		fs.Debugf(d.fs, "åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v", err)
	}

	// ğŸ”§ ä¼˜åŒ–1ï¼šæ–­ç‚¹ç»­ä¼ ä¿¡æ¯é¢„éªŒè¯ï¼ˆä¸DownloadToFileä¿æŒä¸€è‡´ï¼‰
	if resumeInfo != nil {
		if !d.validateResumeInfo(resumeInfo, tempFile, params, fileSize) {
			fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯éªŒè¯å¤±è´¥ï¼Œæ¸…ç†å¹¶é‡æ–°å¼€å§‹", d.backendType)
			d.resumeManager.DeleteResumeInfo(taskID)
			resumeInfo = nil
		} else {
			fs.Debugf(d.fs, "âœ… %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯éªŒè¯é€šè¿‡", d.backendType)
		}
	}

	// å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ–­ç‚¹ä¿¡æ¯ï¼Œåˆ›å»ºæ–°çš„
	if resumeInfo == nil {
		resumeInfo = d.createResumeInfo(taskID, obj, params, tempFile)
	}

	// ğŸ”§ å®Œå…¨å‚è€ƒ115ç½‘ç›˜ï¼šè®¾ç½®è¿›åº¦è·Ÿè¸ª
	progress, currentAccount := d.setupProgressTracking(ctx, obj, resumeInfo, params.ChunkSize, transfer)

	// ğŸ”§ å®Œå…¨å‚è€ƒ115ç½‘ç›˜ï¼šå¯åŠ¨è¿›åº¦æ›´æ–°åç¨‹
	progressCtx, cancelProgress := context.WithCancel(ctx)
	defer cancelProgress()

	go d.updateAccountExtraInfo(progressCtx, progress, obj.Remote(), currentAccount)

	// æ‰§è¡Œå¹¶å‘ä¸‹è½½
	startTime := time.Now()
	err = d.downloadChunksConcurrentlyWithAccount(ctx, obj, tempFile, params, resumeInfo, progress, currentAccount)
	downloadDuration := time.Since(startTime)

	if err != nil {
		return 0, fmt.Errorf("%sç½‘ç›˜å¹¶å‘ä¸‹è½½å¤±è´¥: %w", d.backendType, err)
	}

	// ğŸ”§ ä¿®å¤ï¼šå¢å¼ºä¸‹è½½å®Œæ•´æ€§éªŒè¯
	stat, err := tempFile.Stat()
	if err != nil {
		return 0, fmt.Errorf("è·å–ä¸‹è½½æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	actualSize := stat.Size()
	if actualSize != fileSize {
		// ğŸ”§ æ–‡ä»¶å¤§å°ä¸åŒ¹é…æ—¶ï¼Œæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¹¶æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯
		if d.resumeManager != nil {
			taskID := d.generateTaskID(obj)
			d.resumeManager.DeleteResumeInfo(taskID)
			fs.Debugf(d.fs, "æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œå·²æ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", taskID)
		}

		// æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
		completedChunks := 0
		if resumeInfo != nil {
			completedChunks = len(resumeInfo.GetCompletedChunks())
		}

		return 0, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s (å·²å®Œæˆåˆ†ç‰‡: %d/%d)",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(actualSize), completedChunks, params.NumChunks)
	}

	// ä½¿ç”¨åç«¯ç‰¹å®šçš„éªŒè¯
	if err := d.adapter.VerifyDownload(ctx, obj, tempFile); err != nil {
		return 0, fmt.Errorf("ä¸‹è½½å®Œæ•´æ€§éªŒè¯å¤±è´¥: %w", err)
	}

	fs.Infof(d.fs, "âœ… %sç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		d.backendType, fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// ğŸ”§ ä¿®å¤ï¼šå»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“åœºæ™¯
	if d.resumeManager != nil {
		// æ·»åŠ å»¶è¿Ÿæ¸…ç†ï¼Œç»™è·¨äº‘ä¼ è¾“ç­‰åç»­æ“ä½œç•™å‡ºæ—¶é—´
		go func() {
			time.Sleep(30 * time.Second) // å»¶è¿Ÿ30ç§’æ¸…ç†
			err := d.resumeManager.DeleteResumeInfo(taskID)
			if err != nil {
				fs.Debugf(d.fs, "å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v (TaskID: %s)", err, taskID)
			} else {
				fs.Debugf(d.fs, "âœ… å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æˆåŠŸ (TaskID: %s)", taskID)
			}
		}()
		fs.Debugf(d.fs, "ğŸ”§ å·²å®‰æ’å»¶è¿Ÿæ¸…ç†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", taskID)
	}

	return actualSize, nil
}

// setupProgressTracking è®¾ç½®è¿›åº¦è·Ÿè¸ªï¼ˆå®Œå…¨å‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (d *UnifiedConcurrentDownloader) setupProgressTracking(ctx context.Context, obj fs.Object, resumeInfo UnifiedResumeInfo, chunkSize int64, transfer *accounting.Transfer) (*DownloadProgress, *accounting.Account) {
	// åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨ï¼Œè€ƒè™‘å·²å®Œæˆçš„åˆ†ç‰‡
	progress := NewDownloadProgress(resumeInfo.GetTotalChunks(), obj.Size())

	// ğŸ”§ ä¿®å¤è¿›åº¦åˆå§‹åŒ–ï¼šå¦‚æœæœ‰å·²å®Œæˆçš„åˆ†ç‰‡ï¼Œä½¿ç”¨æ­£ç¡®çš„åˆ†ç‰‡å¤§å°æ›´æ–°è¿›åº¦
	if resumeInfo != nil {
		fileSize := obj.Size()
		for chunkIndex := range resumeInfo.GetCompletedChunks() {
			// ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¡ç®—å®é™…çš„åˆ†ç‰‡å¤§å°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„chunkSize
			chunkStart := chunkIndex * chunkSize
			chunkEnd := chunkStart + chunkSize - 1
			if chunkEnd >= fileSize {
				chunkEnd = fileSize - 1
			}
			actualChunkSize := chunkEnd - chunkStart + 1
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, 0) // å·²å®Œæˆçš„åˆ†ç‰‡ï¼Œè€—æ—¶ä¸º0
		}
	}

	// è®¾ç½®Accountå¯¹è±¡
	var currentAccount *accounting.Account
	remoteName := obj.Remote()

	if transfer != nil {
		// ä½¿ç”¨ä¼ å…¥çš„Transferå¯¹è±¡åˆ›å»ºAccount
		dummyReader := io.NopCloser(strings.NewReader(""))
		currentAccount = transfer.Account(ctx, dummyReader)
		dummyReader.Close()
		fs.Debugf(d.fs, "ğŸ”§ ä½¿ç”¨ä¼ å…¥çš„Transferå¯¹è±¡åˆ›å»ºAccount: %s", remoteName)
	} else {
		// å›é€€ï¼šå°è¯•ä»å…¨å±€ç»Ÿè®¡æŸ¥æ‰¾ç°æœ‰Account
		currentAccount = d.findExistingAccount(remoteName)
	}

	return progress, currentAccount
}

// findExistingAccount æŸ¥æ‰¾ç°æœ‰çš„Accountå¯¹è±¡ï¼ˆå®Œå…¨å‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (d *UnifiedConcurrentDownloader) findExistingAccount(remoteName string) *accounting.Account {
	if stats := accounting.GlobalStats(); stats != nil {
		currentAccount := stats.GetInProgressAccount(remoteName)
		if currentAccount == nil {
			// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
			allAccounts := stats.ListInProgressAccounts()
			for _, accountName := range allAccounts {
				if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
					currentAccount = stats.GetInProgressAccount(accountName)
					if currentAccount != nil {
						fs.Debugf(d.fs, "%sç½‘ç›˜æ‰¾åˆ°åŒ¹é…çš„Account: %s -> %s", d.backendType, remoteName, accountName)
						return currentAccount
					}
				}
			}
		} else {
			fs.Debugf(d.fs, "%sç½‘ç›˜æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„Account: %s", d.backendType, remoteName)
			return currentAccount
		}
	}

	fs.Debugf(d.fs, "%sç½‘ç›˜æœªæ‰¾åˆ°Accountå¯¹è±¡ï¼Œè¿›åº¦æ˜¾ç¤ºå¯èƒ½ä¸å‡†ç¡®: %s", d.backendType, remoteName)
	return nil
}

// updateAccountExtraInfo æ›´æ–°Accountçš„é¢å¤–è¿›åº¦ä¿¡æ¯ï¼ˆå®Œå…¨å‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (d *UnifiedConcurrentDownloader) updateAccountExtraInfo(ctx context.Context, progress *DownloadProgress, remoteName string, account *accounting.Account) {
	ticker := time.NewTicker(2 * time.Second) // æ¯2ç§’æ›´æ–°ä¸€æ¬¡Accountçš„é¢å¤–ä¿¡æ¯
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ¸…é™¤é¢å¤–ä¿¡æ¯
			if account != nil {
				account.SetExtraInfo("")
			}
			return
		case <-ticker.C:
			if account == nil {
				// å°è¯•é‡æ–°è·å–Accountå¯¹è±¡
				if stats := accounting.GlobalStats(); stats != nil {
					// å°è¯•ç²¾ç¡®åŒ¹é…
					account = stats.GetInProgressAccount(remoteName)
					if account == nil {
						// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
						allAccounts := stats.ListInProgressAccounts()
						for _, accountName := range allAccounts {
							if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
								account = stats.GetInProgressAccount(accountName)
								if account != nil {
									break
								}
							}
						}
					}
				}
				if account == nil {
					continue // å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·³è¿‡è¿™æ¬¡æ›´æ–°
				}
			}

			_, _, _, _, completed, total, _, _ := progress.GetProgressInfo()

			// ğŸ”§ å®Œå…¨å‚è€ƒ115ç½‘ç›˜ï¼šæ„å»ºç®€æ´çš„é¢å¤–ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ˜¾ç¤ºåˆ†ç‰‡è¿›åº¦
			if completed > 0 && total > 0 {
				extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completed, total)
				account.SetExtraInfo(extraInfo)
				fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜è®¾ç½®åˆ†ç‰‡è¿›åº¦ä¿¡æ¯: %s", d.backendType, extraInfo)
			} else {
				fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜åˆ†ç‰‡è¿›åº¦ä¿¡æ¯ä¸ºç©º: completed=%d, total=%d", d.backendType, completed, total)
			}
		}
	}
}

// GetDownloadURL è·å–ä¸‹è½½URLï¼ˆå§”æ‰˜ç»™é€‚é…å™¨ï¼‰
func (d *UnifiedConcurrentDownloader) GetDownloadURL(ctx context.Context, obj fs.Object, start, end int64) (string, error) {
	return d.adapter.GetDownloadURL(ctx, obj, start, end)
}

// DownloadChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡ï¼ˆå§”æ‰˜ç»™é€‚é…å™¨ï¼‰
func (d *UnifiedConcurrentDownloader) DownloadChunk(ctx context.Context, url string, tempFile *os.File, start, end int64) error {
	return d.adapter.DownloadChunk(ctx, url, tempFile, start, end, nil)
}

// ç§æœ‰è¾…åŠ©æ–¹æ³•

func (d *UnifiedConcurrentDownloader) calculateOptimalChunkSize(fileSize int64) int64 {
	// ğŸ”§ ä¼˜å…ˆä½¿ç”¨åç«¯ç‰¹å®šçš„é…ç½®ï¼ˆå¦‚115ç½‘ç›˜çš„10MBé…ç½®ï¼‰
	if d.defaultChunkSize > 0 {
		fs.Debugf(d.fs, "ğŸ”§ ä½¿ç”¨åç«¯ç‰¹å®šåˆ†ç‰‡å¤§å°: %s", fs.SizeSuffix(d.defaultChunkSize))
		return d.defaultChunkSize
	}

	// åŸºäºæ–‡ä»¶å¤§å°åŠ¨æ€è®¡ç®—åˆ†ç‰‡å¤§å°ï¼ˆåå¤‡æ–¹æ¡ˆï¼‰
	switch {
	case fileSize < 100*1024*1024: // <100MB
		return 10 * 1024 * 1024 // 10MB
	case fileSize < 1*1024*1024*1024: // <1GB
		return 32 * 1024 * 1024 // 32MB
	case fileSize < 10*1024*1024*1024: // <10GB
		return 100 * 1024 * 1024 // 100MB
	default:
		return 200 * 1024 * 1024 // 200MB
	}
}

func (d *UnifiedConcurrentDownloader) calculateOptimalConcurrency(fileSize int64) int64 {
	// åŸºäºæ–‡ä»¶å¤§å°å’Œåç«¯ç±»å‹è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	baseConcurrency := int64(2)

	if fileSize > 1*1024*1024*1024 { // >1GB
		baseConcurrency = 4
	}

	// ä¸è¶…è¿‡é…ç½®çš„æœ€å¤§å¹¶å‘æ•°
	if baseConcurrency > d.maxConcurrency {
		baseConcurrency = d.maxConcurrency
	}

	return baseConcurrency
}

func (d *UnifiedConcurrentDownloader) generateTaskID(obj fs.Object) string {
	var taskID string
	switch d.backendType {
	case "123":
		taskID = GenerateTaskID123(obj.Remote(), obj.Size())
		fs.Debugf(d.fs, "ğŸ”§ ç”Ÿæˆ123ç½‘ç›˜TaskID: %s (è·¯å¾„: %s, å¤§å°: %d)", taskID, obj.Remote(), obj.Size())
	case "115":
		taskID = GenerateTaskID115(obj.Remote(), obj.Size())
		fs.Debugf(d.fs, "ğŸ”§ ç”Ÿæˆ115ç½‘ç›˜TaskID: %s (è·¯å¾„: %s, å¤§å°: %d)", taskID, obj.Remote(), obj.Size())
	default:
		taskID = fmt.Sprintf("%s_%s_%d_%d", d.backendType, obj.Remote(), obj.Size(), time.Now().Unix())
		fs.Debugf(d.fs, "ğŸ”§ ç”Ÿæˆé»˜è®¤TaskID: %s", taskID)
	}
	return taskID
}

func (d *UnifiedConcurrentDownloader) createResumeInfo(taskID string, obj fs.Object, params ChunkParams, tempFile *os.File) UnifiedResumeInfo {
	// ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸ºæ‰€æœ‰åç«¯ä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„
	tempFilePath := ""
	if tempFile != nil {
		tempFilePath = tempFile.Name()
	}

	switch d.backendType {
	case "123":
		return &ResumeInfo123{
			TaskID:              taskID,
			FileName:            obj.Remote(),
			FileSize:            obj.Size(),
			FilePath:            obj.Remote(),
			ChunkSize:           params.ChunkSize,
			TotalChunks:         params.NumChunks,
			CompletedChunks:     make(map[int64]bool),
			CreatedAt:           time.Now(),
			TempFilePath:        tempFilePath, // ğŸ”§ 123ç½‘ç›˜ä¹Ÿä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„
			BackendSpecificData: make(map[string]interface{}),
		}
	case "115":
		return &ResumeInfo115{
			TaskID:              taskID,
			FileName:            obj.Remote(),
			FileSize:            obj.Size(),
			FilePath:            obj.Remote(),
			ChunkSize:           params.ChunkSize,
			TotalChunks:         params.NumChunks,
			CompletedChunks:     make(map[int64]bool),
			CreatedAt:           time.Now(),
			TempFilePath:        tempFilePath, // ğŸ”§ 115ç½‘ç›˜ä¿å­˜ä¸´æ—¶æ–‡ä»¶è·¯å¾„
			BackendSpecificData: make(map[string]interface{}),
		}
	default:
		return nil
	}
}

// downloadChunksConcurrently å¹¶å‘ä¸‹è½½æ–‡ä»¶åˆ†ç‰‡
func (d *UnifiedConcurrentDownloader) downloadChunksConcurrently(ctx context.Context, obj fs.Object, tempFile *os.File, params ChunkParams, resumeInfo UnifiedResumeInfo, progress *DownloadProgress) error {
	// åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
	semaphore := make(chan struct{}, params.MaxConcurrency)

	// åˆ›å»ºé”™è¯¯é€šé“
	errChan := make(chan error, params.NumChunks)

	// åˆ›å»ºç­‰å¾…ç»„
	var wg sync.WaitGroup

	// å¯åŠ¨åˆ†ç‰‡ä¸‹è½½
	for i := int64(0); i < params.NumChunks; i++ {
		wg.Add(1)
		go func(chunkIndex int64) {
			defer wg.Done()

			// æ·»åŠ panicæ¢å¤æœºåˆ¶
			defer func() {
				if r := recover(); r != nil {
					fs.Errorf(d.fs, "%sç½‘ç›˜ä¸‹è½½åˆ†ç‰‡ %d å‘ç”Ÿpanic: %v", d.backendType, chunkIndex, r)
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d panic: %v", chunkIndex, r)
				}
			}()

			// æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²å®Œæˆ
			if resumeInfo != nil && resumeInfo.IsChunkCompleted(chunkIndex) {
				fs.Debugf(d.fs, "è·³è¿‡å·²å®Œæˆçš„åˆ†ç‰‡: %d", chunkIndex)
				return
			}

			// è·å–ä¿¡å·é‡ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
			chunkTimeout := 10 * time.Minute
			chunkCtx, cancelChunk := context.WithTimeout(ctx, chunkTimeout)
			defer cancelChunk()

			select {
			case semaphore <- struct{}{}:
				defer func() {
					select {
					case <-semaphore:
					default:
						// å¦‚æœä¿¡å·é‡å·²ç»è¢«é‡Šæ”¾ï¼Œé¿å…é˜»å¡
					}
				}()
			case <-chunkCtx.Done():
				errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è·å–ä¿¡å·é‡è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				return
			}

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := chunkIndex * params.ChunkSize
			end := start + params.ChunkSize - 1
			if end >= obj.Size() {
				end = obj.Size() - 1
			}

			// ä¸‹è½½åˆ†ç‰‡
			chunkStartTime := time.Now()
			err := d.downloadSingleChunk(chunkCtx, obj, tempFile, start, end, chunkIndex, nil)
			chunkDuration := time.Since(chunkStartTime)

			if err != nil {
				if chunkCtx.Err() != nil {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				} else {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", chunkIndex, err)
				}
				return
			}

			// æ ‡è®°åˆ†ç‰‡ä¸ºå·²å®Œæˆ
			if resumeInfo != nil {
				resumeInfo.MarkChunkCompleted(chunkIndex)
				if d.resumeManager != nil {
					if saveErr := d.resumeManager.SaveResumeInfo(resumeInfo); saveErr != nil {
						fs.Debugf(d.fs, "ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v", saveErr)
					}
				}
			}

			// æ›´æ–°è¿›åº¦
			actualChunkSize := end - start + 1
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, chunkDuration.Nanoseconds())

			// ğŸ”§ ä¿®å¤åˆ†ç‰‡ç´¢å¼•æ˜¾ç¤ºï¼šæ˜¾ç¤ºå®é™…çš„åˆ†ç‰‡ç´¢å¼•å’Œå­—èŠ‚èŒƒå›´ï¼Œé¿å…ç”¨æˆ·å›°æƒ‘
			fs.Debugf(d.fs, "âœ… %sç½‘ç›˜åˆ†ç‰‡ %d/%d ä¸‹è½½å®Œæˆ, å¤§å°: %s, ç”¨æ—¶: %v (bytes=%d-%d)",
				d.backendType, chunkIndex, params.NumChunks-1, fs.SizeSuffix(actualChunkSize), chunkDuration, start, end)

		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	var errors []error
	for err := range errChan {
		errors = append(errors, err)
	}

	if len(errors) > 0 {
		return fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼Œé”™è¯¯æ•°é‡: %d, ç¬¬ä¸€ä¸ªé”™è¯¯: %w", len(errors), errors[0])
	}

	return nil
}

// downloadChunksConcurrentlyWithAccount æ”¯æŒAccountå¯¹è±¡çš„å¹¶å‘ä¸‹è½½ï¼ˆå®Œå…¨å‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (d *UnifiedConcurrentDownloader) downloadChunksConcurrentlyWithAccount(ctx context.Context, obj fs.Object, tempFile *os.File, params ChunkParams, resumeInfo UnifiedResumeInfo, progress *DownloadProgress, currentAccount *accounting.Account) error {
	// åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
	semaphore := make(chan struct{}, params.MaxConcurrency)

	// åˆ›å»ºé”™è¯¯é€šé“
	errChan := make(chan error, params.NumChunks)

	// åˆ›å»ºç­‰å¾…ç»„
	var wg sync.WaitGroup

	// å¯åŠ¨åˆ†ç‰‡ä¸‹è½½
	for i := int64(0); i < params.NumChunks; i++ {
		wg.Add(1)
		go func(chunkIndex int64) {
			defer wg.Done()

			// æ·»åŠ panicæ¢å¤æœºåˆ¶
			defer func() {
				if r := recover(); r != nil {
					fs.Errorf(d.fs, "%sç½‘ç›˜ä¸‹è½½åˆ†ç‰‡ %d å‘ç”Ÿpanic: %v", d.backendType, chunkIndex, r)
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d panic: %v", chunkIndex, r)
				}
			}()

			// æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²å®Œæˆ
			if resumeInfo != nil && resumeInfo.IsChunkCompleted(chunkIndex) {
				fs.Debugf(d.fs, "âœ… è·³è¿‡å·²å®Œæˆçš„åˆ†ç‰‡: %d/%d", chunkIndex, params.NumChunks-1)
				return
			} else if resumeInfo != nil {
				fs.Debugf(d.fs, "ğŸ”„ åˆ†ç‰‡ %d/%d éœ€è¦ä¸‹è½½ï¼ˆæœªå®Œæˆï¼‰", chunkIndex, params.NumChunks-1)
			} else {
				fs.Debugf(d.fs, "ğŸ”„ åˆ†ç‰‡ %d/%d éœ€è¦ä¸‹è½½ï¼ˆæ— æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼‰", chunkIndex, params.NumChunks-1)
			}

			// è·å–ä¿¡å·é‡ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
			chunkTimeout := 10 * time.Minute
			chunkCtx, cancelChunk := context.WithTimeout(ctx, chunkTimeout)
			defer cancelChunk()

			select {
			case semaphore <- struct{}{}:
				defer func() {
					select {
					case <-semaphore:
					default:
						// å¦‚æœä¿¡å·é‡å·²ç»è¢«é‡Šæ”¾ï¼Œé¿å…é˜»å¡
					}
				}()
			case <-chunkCtx.Done():
				errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è·å–ä¿¡å·é‡è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				return
			}

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := chunkIndex * params.ChunkSize
			end := start + params.ChunkSize - 1
			if end >= obj.Size() {
				end = obj.Size() - 1
			}

			// ä¸‹è½½åˆ†ç‰‡
			chunkStartTime := time.Now()
			err := d.downloadSingleChunk(chunkCtx, obj, tempFile, start, end, chunkIndex, currentAccount)
			chunkDuration := time.Since(chunkStartTime)

			if err != nil {
				if chunkCtx.Err() != nil {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				} else {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", chunkIndex, err)
				}
				return
			}

			// æ ‡è®°åˆ†ç‰‡ä¸ºå·²å®Œæˆ
			if resumeInfo != nil {
				resumeInfo.MarkChunkCompleted(chunkIndex)
				if d.resumeManager != nil {
					if saveErr := d.resumeManager.SaveResumeInfo(resumeInfo); saveErr != nil {
						fs.Debugf(d.fs, "ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v", saveErr)
					}
				}
			}

			// æ›´æ–°è¿›åº¦
			actualChunkSize := end - start + 1
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, chunkDuration.Nanoseconds())

			// ğŸ”§ å…³é”®ä¿®å¤ï¼šè¿›åº¦æŠ¥å‘Šç”±DownloadChunkæ–¹æ³•è´Ÿè´£ï¼Œè¿™é‡Œä¸é‡å¤æŠ¥å‘Š
			// downloadSingleChunkä¼šå°†currentAccountä¼ é€’ç»™DownloadChunkæ–¹æ³•ï¼Œç”±DownloadChunkè´Ÿè´£è¿›åº¦æŠ¥å‘Š

			// ğŸ”§ ä¿®å¤åˆ†ç‰‡ç´¢å¼•æ˜¾ç¤ºï¼šæ˜¾ç¤ºå®é™…çš„åˆ†ç‰‡ç´¢å¼•å’Œå­—èŠ‚èŒƒå›´ï¼Œé¿å…ç”¨æˆ·å›°æƒ‘
			fs.Debugf(d.fs, "âœ… %sç½‘ç›˜åˆ†ç‰‡ %d/%d ä¸‹è½½å®Œæˆ, å¤§å°: %s, ç”¨æ—¶: %v (bytes=%d-%d)",
				d.backendType, chunkIndex, params.NumChunks-1, fs.SizeSuffix(actualChunkSize), chunkDuration, start, end)

		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	var errors []error
	for err := range errChan {
		errors = append(errors, err)
	}

	if len(errors) > 0 {
		return fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼Œé”™è¯¯æ•°é‡: %d, ç¬¬ä¸€ä¸ªé”™è¯¯: %w", len(errors), errors[0])
	}

	return nil
}

// downloadSingleChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡
func (d *UnifiedConcurrentDownloader) downloadSingleChunk(ctx context.Context, obj fs.Object, tempFile *os.File, start, end, chunkIndex int64, account *accounting.Account) error {
	maxRetries := 2 // æœ€å¤šé‡è¯•2æ¬¡

	for retry := 0; retry <= maxRetries; retry++ {
		// è·å–ä¸‹è½½URL
		url, err := d.adapter.GetDownloadURL(ctx, obj, start, end)
		if err != nil {
			if retry < maxRetries {
				fs.Debugf(d.fs, "ğŸ”„ åˆ†ç‰‡ %d è·å–URLå¤±è´¥ï¼Œé‡è¯• %d/%d: %v", chunkIndex, retry+1, maxRetries, err)
				time.Sleep(time.Duration(retry+1) * 500 * time.Millisecond)
				continue
			}
			return fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
		}

		// ä¸‹è½½åˆ†ç‰‡æ•°æ®
		err = d.adapter.DownloadChunk(ctx, url, tempFile, start, end, account)
		if err != nil {
			// ğŸ”§ æ£€æŸ¥æ˜¯å¦ä¸º403 URLè¿‡æœŸé”™è¯¯
			if strings.Contains(err.Error(), "403") && strings.Contains(err.Error(), "invalid signature") {
				if retry < maxRetries {
					fs.Debugf(d.fs, "ğŸ”„ åˆ†ç‰‡ %d URLè¿‡æœŸ(403)ï¼Œé‡è¯• %d/%d: %v", chunkIndex, retry+1, maxRetries, err)
					time.Sleep(time.Duration(retry+1) * 1000 * time.Millisecond) // ç¨é•¿çš„å»¶è¿Ÿ
					continue
				}
			}

			if retry < maxRetries {
				fs.Debugf(d.fs, "ğŸ”„ åˆ†ç‰‡ %d ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• %d/%d: %v", chunkIndex, retry+1, maxRetries, err)
				time.Sleep(time.Duration(retry+1) * 500 * time.Millisecond)
				continue
			}
			return fmt.Errorf("ä¸‹è½½åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		// æˆåŠŸä¸‹è½½
		return nil
	}

	return fmt.Errorf("åˆ†ç‰‡ %d ä¸‹è½½å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°", chunkIndex)
}

// NewDownloadProgress åˆ›å»ºä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
func NewDownloadProgress(totalChunks, totalBytes int64) *DownloadProgress {
	return &DownloadProgress{
		totalChunks:   totalChunks,
		totalBytes:    totalBytes,
		startTime:     time.Now(),
		chunkProgress: make(map[int64]ChunkProgress),
	}
}

// UpdateChunkProgress æ›´æ–°åˆ†ç‰‡è¿›åº¦
func (dp *DownloadProgress) UpdateChunkProgress(chunkIndex, chunkSize, durationNanos int64) {
	dp.mu.Lock()
	defer dp.mu.Unlock()

	// ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²ç»è¢«è®¡ç®—è¿‡ï¼Œé¿å…é‡å¤è®¡ç®—
	if _, exists := dp.chunkProgress[chunkIndex]; exists {
		// åˆ†ç‰‡å·²ç»å­˜åœ¨ï¼Œä¸é‡å¤è®¡ç®—
		return
	}

	dp.completedChunks++
	dp.downloadedBytes += chunkSize

	speed := int64(0)
	if durationNanos > 0 {
		speed = int64(float64(chunkSize) / (float64(durationNanos) / 1e9))
	}

	dp.chunkProgress[chunkIndex] = ChunkProgress{
		Index:       chunkIndex,
		Size:        chunkSize,
		Downloaded:  chunkSize,
		StartTime:   time.Now().Add(-time.Duration(durationNanos)),
		CompletedAt: time.Now(),
		Speed:       speed,
	}
}

// GetProgress è·å–å½“å‰è¿›åº¦
func (dp *DownloadProgress) GetProgress() (completed, total int64, percentage float64, avgSpeed int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	completed = dp.completedChunks
	total = dp.totalChunks

	if total > 0 {
		percentage = float64(completed) / float64(total) * 100.0
	}

	elapsed := time.Since(dp.startTime)
	if elapsed.Seconds() > 0 {
		avgSpeed = int64(float64(dp.downloadedBytes) / elapsed.Seconds())
	}

	return completed, total, percentage, avgSpeed
}

// GetProgressInfo è·å–å½“å‰è¿›åº¦ä¿¡æ¯ï¼ˆå®Œå…¨å‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (dp *DownloadProgress) GetProgressInfo() (percentage float64, avgSpeed float64, peakSpeed float64, eta time.Duration, completed, total int64, downloadedBytes, totalBytes int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	elapsed := time.Since(dp.startTime)
	percentage = float64(dp.completedChunks) / float64(dp.totalChunks) * 100

	if elapsed.Seconds() > 0 {
		avgSpeed = float64(dp.downloadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	// è®¡ç®—å³°å€¼é€Ÿåº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å¹³å‡é€Ÿåº¦ï¼‰
	peakSpeed = avgSpeed

	if avgSpeed > 0 && dp.downloadedBytes > 0 {
		remainingBytes := dp.totalBytes - dp.downloadedBytes
		etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
		eta = time.Duration(etaSeconds) * time.Second
	}

	completed = dp.completedChunks
	total = dp.totalChunks
	downloadedBytes = dp.downloadedBytes
	totalBytes = dp.totalBytes

	return
}

// validateResumeInfo éªŒè¯æ–­ç‚¹ç»­ä¼ ä¿¡æ¯çš„æœ‰æ•ˆæ€§
// ğŸ”§ ä¼˜åŒ–æ”¹è¿›ï¼šæ™ºèƒ½éªŒè¯æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œå°è¯•æ¢å¤ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶
func (d *UnifiedConcurrentDownloader) validateResumeInfo(resumeInfo UnifiedResumeInfo, tempFile *os.File, params ChunkParams, fileSize int64) bool {
	if resumeInfo == nil {
		return false
	}

	completedChunks := resumeInfo.GetCompletedChunks()

	// éªŒè¯åˆ†ç‰‡ç´¢å¼•çš„åˆç†æ€§
	for chunkIndex := range completedChunks {
		if chunkIndex < 0 || chunkIndex >= params.NumChunks {
			fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯åŒ…å«æ— æ•ˆåˆ†ç‰‡ç´¢å¼•: %d (æ€»åˆ†ç‰‡æ•°: %d)",
				d.backendType, chunkIndex, params.NumChunks)
			return false
		}
	}

	// éªŒè¯æ–­ç‚¹ç»­ä¼ ä¿¡æ¯çš„åŸºæœ¬å®Œæ•´æ€§
	if resumeInfo.GetTotalChunks() != params.NumChunks {
		fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯åˆ†ç‰‡æ•°ä¸åŒ¹é…: è®°å½•=%d, è®¡ç®—=%d",
			d.backendType, resumeInfo.GetTotalChunks(), params.NumChunks)
		return false
	}

	// éªŒè¯æ–‡ä»¶å¤§å°æ˜¯å¦åŒ¹é…
	if resumeInfo.GetFileSize() != fileSize {
		fs.Debugf(d.fs, "ğŸ”§ %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æ–‡ä»¶å¤§å°ä¸åŒ¹é…: è®°å½•=%s, å®é™…=%s",
			d.backendType, fs.SizeSuffix(resumeInfo.GetFileSize()), fs.SizeSuffix(fileSize))
		return false
	}

	// ğŸ”§ å…³é”®ä¿®å¤ï¼šå°è¯•æ¢å¤ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆæ”¯æŒ115å’Œ123ç½‘ç›˜ï¼‰
	var previousTempFilePath string
	if r115, ok := resumeInfo.(*ResumeInfo115); ok {
		previousTempFilePath = r115.TempFilePath
	} else if r123, ok := resumeInfo.(*ResumeInfo123); ok {
		previousTempFilePath = r123.TempFilePath
	}

	if previousTempFilePath != "" {
		// æ£€æŸ¥ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶æ˜¯å¦å­˜åœ¨
		if stat, err := os.Stat(previousTempFilePath); err == nil {
			// è®¡ç®—é¢„æœŸå¤§å°
			var expectedSize int64
			for chunkIndex := range completedChunks {
				chunkStart := chunkIndex * params.ChunkSize
				chunkEnd := chunkStart + params.ChunkSize - 1
				if chunkEnd >= fileSize {
					chunkEnd = fileSize - 1
				}
				chunkSize := chunkEnd - chunkStart + 1
				expectedSize += chunkSize
			}

			if stat.Size() == expectedSize {
				fs.Debugf(d.fs, "ğŸ¯ %sç½‘ç›˜å‘ç°åŒ¹é…çš„ä¸´æ—¶æ–‡ä»¶: %s, å¤§å°=%s",
					d.backendType, previousTempFilePath, fs.SizeSuffix(stat.Size()))

				// å°è¯•å¤åˆ¶ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶å†…å®¹åˆ°å½“å‰ä¸´æ—¶æ–‡ä»¶
				if err := d.copyTempFileContent(previousTempFilePath, tempFile); err != nil {
					fs.Debugf(d.fs, "âš ï¸ %sç½‘ç›˜å¤åˆ¶ä¸´æ—¶æ–‡ä»¶å†…å®¹å¤±è´¥: %v", d.backendType, err)
				} else {
					fs.Debugf(d.fs, "âœ… %sç½‘ç›˜æˆåŠŸæ¢å¤ä¸´æ—¶æ–‡ä»¶å†…å®¹: %s", d.backendType, fs.SizeSuffix(expectedSize))
					return true
				}
			} else {
				fs.Debugf(d.fs, "âš ï¸ %sç½‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s",
					d.backendType, fs.SizeSuffix(expectedSize), fs.SizeSuffix(stat.Size()))
			}
		} else {
			fs.Debugf(d.fs, "âš ï¸ %sç½‘ç›˜ä¹‹å‰çš„ä¸´æ—¶æ–‡ä»¶ä¸å­˜åœ¨: %s", d.backendType, previousTempFilePath)
		}
	}

	// ğŸ”§ æ™ºèƒ½éªŒè¯ï¼šæ£€æŸ¥å½“å‰ä¸´æ—¶æ–‡ä»¶çŠ¶æ€
	stat, err := tempFile.Stat()
	if err != nil {
		fs.Debugf(d.fs, "ğŸ”§ è·å–å½“å‰ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %v", err)
		return true // å…è®¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œè®©ä¸‹è½½è¿‡ç¨‹å¤„ç†
	}

	actualFileSize := stat.Size()
	// è®¡ç®—é¢„æœŸå¤§å°
	var expectedSize int64
	for chunkIndex := range completedChunks {
		chunkStart := chunkIndex * params.ChunkSize
		chunkEnd := chunkStart + params.ChunkSize - 1
		if chunkEnd >= fileSize {
			chunkEnd = fileSize - 1
		}
		chunkSize := chunkEnd - chunkStart + 1
		expectedSize += chunkSize
	}

	if actualFileSize == expectedSize {
		fs.Debugf(d.fs, "âœ… %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ä¸ä¸´æ—¶æ–‡ä»¶å®Œå…¨åŒ¹é…: å·²å®Œæˆåˆ†ç‰‡=%d/%d, æ–‡ä»¶å¤§å°=%s",
			d.backendType, len(completedChunks), params.NumChunks, fs.SizeSuffix(actualFileSize))
	} else {
		fs.Debugf(d.fs, "âš ï¸ %sç½‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼Œä½†æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ä»ç„¶æœ‰æ•ˆ: å·²å®Œæˆåˆ†ç‰‡=%d/%d, é¢„æœŸå¤§å°=%s, å®é™…å¤§å°=%s",
			d.backendType, len(completedChunks), params.NumChunks,
			fs.SizeSuffix(expectedSize), fs.SizeSuffix(actualFileSize))
	}

	fs.Debugf(d.fs, "âœ… %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯éªŒè¯é€šè¿‡: å·²å®Œæˆåˆ†ç‰‡=%d/%d",
		d.backendType, len(completedChunks), params.NumChunks)
	return true
}

// copyTempFileContent å¤åˆ¶ä¸´æ—¶æ–‡ä»¶å†…å®¹
func (d *UnifiedConcurrentDownloader) copyTempFileContent(srcPath string, dstFile *os.File) error {
	// æ‰“å¼€æºæ–‡ä»¶
	srcFile, err := os.Open(srcPath)
	if err != nil {
		return fmt.Errorf("æ‰“å¼€æºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer srcFile.Close()

	// é‡ç½®ç›®æ ‡æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	if _, err := dstFile.Seek(0, 0); err != nil {
		return fmt.Errorf("é‡ç½®ç›®æ ‡æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	// å¤åˆ¶æ–‡ä»¶å†…å®¹
	copied, err := io.Copy(dstFile, srcFile)
	if err != nil {
		return fmt.Errorf("å¤åˆ¶æ–‡ä»¶å†…å®¹å¤±è´¥: %w", err)
	}

	// ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
	if err := dstFile.Sync(); err != nil {
		return fmt.Errorf("åŒæ­¥æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	fs.Debugf(d.fs, "æˆåŠŸå¤åˆ¶ä¸´æ—¶æ–‡ä»¶å†…å®¹: %s", fs.SizeSuffix(copied))
	return nil
}
