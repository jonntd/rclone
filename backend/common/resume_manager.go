// Package common provides unified components for cloud storage backends
package common

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"sync"
	"time"

	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/lib/cache"
)

// ResumeStats æ–­ç‚¹ç»­ä¼ ç»Ÿè®¡ä¿¡æ¯
type ResumeStats struct {
	mu                 sync.RWMutex
	TotalAttempts      int64              // æ€»å°è¯•æ¬¡æ•°
	SuccessfulResumes  int64              // æˆåŠŸæ¢å¤æ¬¡æ•°
	FailedResumes      int64              // å¤±è´¥æ¢å¤æ¬¡æ•°
	AverageResumeTime  float64            // å¹³å‡æ¢å¤æ—¶é—´(ç§’)
	LastSuccessTime    time.Time          // æœ€åŽæˆåŠŸæ—¶é—´
	LastFailureTime    time.Time          // æœ€åŽå¤±è´¥æ—¶é—´
	FailureReasons     map[string]int64   // å¤±è´¥åŽŸå› ç»Ÿè®¡
	PerformanceMetrics PerformanceMetrics // æ€§èƒ½æŒ‡æ ‡
}

// PerformanceMetrics æ€§èƒ½æŒ‡æ ‡
type PerformanceMetrics struct {
	AverageUploadSpeed   float64 // å¹³å‡ä¸Šä¼ é€Ÿåº¦ (MB/s)
	AverageDownloadSpeed float64 // å¹³å‡ä¸‹è½½é€Ÿåº¦ (MB/s)
	TotalBytesResumed    int64   // æ€»æ¢å¤å­—èŠ‚æ•°
	TotalTimesSaved      float64 // æ€»èŠ‚çœæ—¶é—´(ç§’)
}

// UnifiedResumeManager ç»Ÿä¸€çš„æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æŽ¥å£
// ðŸš€ è®¾è®¡ç›®æ ‡ï¼šæ¶ˆé™¤123ç½‘ç›˜å’Œ115ç½‘ç›˜90%çš„é‡å¤ä»£ç 
type UnifiedResumeManager interface {
	// åŸºç¡€æ“ä½œ
	SaveResumeInfo(info UnifiedResumeInfo) error
	LoadResumeInfo(taskID string) (UnifiedResumeInfo, error)
	DeleteResumeInfo(taskID string) error
	Close() error

	// åˆ†ç‰‡æ“ä½œ
	UpdateChunkInfo(taskID string, chunkIndex int64, metadata map[string]interface{}) error
	IsChunkCompleted(taskID string, chunkIndex int64) (bool, error)
	MarkChunkCompleted(taskID string, chunkIndex int64) error

	// è¿›åº¦æŸ¥è¯¢
	GetResumeProgress(taskID string) (uploaded, total int64, percentage float64, err error)
	GetCompletedChunkCount(taskID string) (int64, error)

	// æ¸…ç†æ“ä½œ
	CleanupExpiredInfo() error

	// ðŸ”§ ç›‘æŽ§å’Œç»Ÿè®¡
	GetResumeStats() ResumeStats
	GetResumeHealthReport() map[string]interface{}
}

// UnifiedResumeInfo ç»Ÿä¸€çš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æŽ¥å£
// ðŸš€ è®¾è®¡ç›®æ ‡ï¼šæ”¯æŒ123ç½‘ç›˜å’Œ115ç½‘ç›˜çš„ä¸åŒéœ€æ±‚
type UnifiedResumeInfo interface {
	// åŸºç¡€ä¿¡æ¯
	GetTaskID() string
	GetFileName() string
	GetFileSize() int64
	GetFilePath() string
	GetChunkSize() int64
	GetTotalChunks() int64
	GetCreatedAt() time.Time
	GetLastUpdated() time.Time

	// åˆ†ç‰‡çŠ¶æ€
	IsChunkCompleted(chunkIndex int64) bool
	MarkChunkCompleted(chunkIndex int64)
	GetCompletedChunkCount() int64
	GetCompletedChunks() map[int64]bool

	// åŽç«¯ç‰¹å®šä¿¡æ¯
	GetBackendSpecificData() map[string]interface{}
	SetBackendSpecificData(key string, value interface{})

	// åºåˆ—åŒ–
	ToJSON() ([]byte, error)
	FromJSON(data []byte) error
}

// BadgerResumeManager åŸºäºŽBadgerDBçš„ç»Ÿä¸€æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å®žçŽ°
// ðŸš€ åŠŸèƒ½ç‰¹ç‚¹ï¼šç»Ÿä¸€çš„ç¼“å­˜æœºåˆ¶ï¼Œæ”¯æŒå¤šåŽç«¯
// ðŸ”§ ä¼˜åŒ–ï¼šæ·»åŠ ç¼“å­˜å¥åº·ç›‘æŽ§å’Œæ¢å¤æœºåˆ¶
type BadgerResumeManager struct {
	mu          sync.RWMutex
	resumeCache *cache.BadgerCache
	fs          fs.Fs
	backendType string // "123" æˆ– "115"

	// æ¸…ç†å®šæ—¶å™¨
	cleanupTimer *time.Timer

	// ðŸ”§ æ–°å¢žï¼šç¼“å­˜å¥åº·ç›‘æŽ§
	cacheHealthTimer *time.Timer
	lastHealthCheck  time.Time
	cacheFailures    int

	// ðŸ”§ æ–°å¢žï¼šæ–­ç‚¹ç»­ä¼ ç›‘æŽ§ç»Ÿè®¡
	resumeStats ResumeStats
}

// NewBadgerResumeManager åˆ›å»ºåŸºäºŽBadgerDBçš„æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
func NewBadgerResumeManager(filesystem fs.Fs, backendType string, cacheDir string) (*BadgerResumeManager, error) {
	cacheKey := fmt.Sprintf("resume_%s", backendType)
	resumeCache, err := cache.NewBadgerCache(cacheKey, cacheDir)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»º%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ç¼“å­˜å¤±è´¥: %w", backendType, err)
	}

	rm := &BadgerResumeManager{
		resumeCache:     resumeCache,
		fs:              filesystem,
		backendType:     backendType,
		lastHealthCheck: time.Now(),
		cacheFailures:   0,
		resumeStats: ResumeStats{
			FailureReasons: make(map[string]int64),
		},
	}

	// å¯åŠ¨å®šæœŸæ¸…ç†
	rm.startCleanupTimer()

	// ðŸ”§ å¯åŠ¨ç¼“å­˜å¥åº·ç›‘æŽ§
	rm.startCacheHealthMonitor()

	return rm, nil
}

// SaveResumeInfo ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *BadgerResumeManager) SaveResumeInfo(info UnifiedResumeInfo) error {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	cacheKey := rm.generateCacheKey(info.GetTaskID())

	// åºåˆ—åŒ–ä¿¡æ¯
	data, err := info.ToJSON()
	if err != nil {
		return fmt.Errorf("åºåˆ—åŒ–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	err = rm.resumeCache.Set(cacheKey, data, 24*time.Hour)
	if err != nil {
		return fmt.Errorf("ä¿å­˜%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", rm.backendType, err)
	}

	// ðŸ”§ æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	rm.updateResumeStats(true, "", 0, 0)

	fs.Debugf(rm.fs, "ä¿å­˜%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s, å·²å®Œæˆåˆ†ç‰‡: %d/%d",
		rm.backendType, info.GetFileName(), info.GetCompletedChunkCount(), info.GetTotalChunks())

	return nil
}

// LoadResumeInfo åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *BadgerResumeManager) LoadResumeInfo(taskID string) (UnifiedResumeInfo, error) {
	rm.mu.RLock()
	defer rm.mu.RUnlock()

	cacheKey := rm.generateCacheKey(taskID)
	var data []byte

	found, err := rm.resumeCache.Get(cacheKey, &data)
	if err != nil {
		return nil, fmt.Errorf("åŠ è½½%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", rm.backendType, err)
	}

	if !found {
		return nil, nil // æ²¡æœ‰æ‰¾åˆ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	}

	// æ ¹æ®åŽç«¯ç±»åž‹åˆ›å»ºç›¸åº”çš„ä¿¡æ¯å¯¹è±¡
	var info UnifiedResumeInfo
	switch rm.backendType {
	case "123":
		info = &ResumeInfo123{}
	case "115":
		info = &ResumeInfo115{}
	default:
		return nil, fmt.Errorf("ä¸æ”¯æŒçš„åŽç«¯ç±»åž‹: %s", rm.backendType)
	}

	err = info.FromJSON(data)
	if err != nil {
		return nil, fmt.Errorf("ååºåˆ—åŒ–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	// æ£€æŸ¥ä¿¡æ¯æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
	if time.Since(info.GetCreatedAt()) > 24*time.Hour {
		rm.DeleteResumeInfo(taskID)
		return nil, nil
	}

	// ðŸ”§ è®¡ç®—æ¢å¤çš„å­—èŠ‚æ•°å’ŒèŠ‚çœçš„æ—¶é—´
	completedChunks := info.GetCompletedChunkCount()
	totalChunks := info.GetTotalChunks()
	if completedChunks > 0 && totalChunks > 0 {
		// ä¼°ç®—å·²å®Œæˆçš„å­—èŠ‚æ•°
		bytesResumed := int64(float64(info.GetFileSize()) * float64(completedChunks) / float64(totalChunks))
		// ä¼°ç®—èŠ‚çœçš„æ—¶é—´ï¼ˆå‡è®¾å¹³å‡ä¸Šä¼ é€Ÿåº¦1MB/sï¼‰
		timeSaved := float64(bytesResumed) / (1024 * 1024) // ç§’
		rm.updateResumeStats(true, "", bytesResumed, timeSaved)
	}

	fs.Debugf(rm.fs, "åŠ è½½%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s, å·²å®Œæˆåˆ†ç‰‡: %d/%d",
		rm.backendType, info.GetFileName(), info.GetCompletedChunkCount(), info.GetTotalChunks())

	return info, nil
}

// DeleteResumeInfo åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *BadgerResumeManager) DeleteResumeInfo(taskID string) error {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	cacheKey := rm.generateCacheKey(taskID)
	err := rm.resumeCache.Delete(cacheKey)
	if err != nil {
		return fmt.Errorf("åˆ é™¤%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", rm.backendType, err)
	}

	fs.Debugf(rm.fs, "åˆ é™¤%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", rm.backendType, taskID)
	return nil
}

// UpdateChunkInfo æ›´æ–°åˆ†ç‰‡ä¿¡æ¯
func (rm *BadgerResumeManager) UpdateChunkInfo(taskID string, chunkIndex int64, metadata map[string]interface{}) error {
	info, err := rm.LoadResumeInfo(taskID)
	if err != nil {
		return err
	}
	if info == nil {
		return fmt.Errorf("æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ä¸å­˜åœ¨: %s", taskID)
	}

	// æ ‡è®°åˆ†ç‰‡å®Œæˆ
	info.MarkChunkCompleted(chunkIndex)

	// æ›´æ–°åŽç«¯ç‰¹å®šæ•°æ®
	for key, value := range metadata {
		info.SetBackendSpecificData(key, value)
	}

	return rm.SaveResumeInfo(info)
}

// IsChunkCompleted æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²å®Œæˆ
func (rm *BadgerResumeManager) IsChunkCompleted(taskID string, chunkIndex int64) (bool, error) {
	info, err := rm.LoadResumeInfo(taskID)
	if err != nil {
		return false, err
	}
	if info == nil {
		return false, nil
	}

	return info.IsChunkCompleted(chunkIndex), nil
}

// MarkChunkCompleted æ ‡è®°åˆ†ç‰‡ä¸ºå·²å®Œæˆ
func (rm *BadgerResumeManager) MarkChunkCompleted(taskID string, chunkIndex int64) error {
	return rm.UpdateChunkInfo(taskID, chunkIndex, nil)
}

// GetResumeProgress èŽ·å–æ–­ç‚¹ç»­ä¼ è¿›åº¦
func (rm *BadgerResumeManager) GetResumeProgress(taskID string) (uploaded, total int64, percentage float64, err error) {
	info, err := rm.LoadResumeInfo(taskID)
	if err != nil {
		return 0, 0, 0, err
	}
	if info == nil {
		return 0, 0, 0, nil
	}

	uploaded = info.GetCompletedChunkCount()
	total = info.GetTotalChunks()

	if total > 0 {
		percentage = float64(uploaded) / float64(total) * 100.0
	}

	return uploaded, total, percentage, nil
}

// GetCompletedChunkCount èŽ·å–å·²å®Œæˆçš„åˆ†ç‰‡æ•°é‡
func (rm *BadgerResumeManager) GetCompletedChunkCount(taskID string) (int64, error) {
	info, err := rm.LoadResumeInfo(taskID)
	if err != nil {
		return 0, err
	}
	if info == nil {
		return 0, nil
	}

	return info.GetCompletedChunkCount(), nil
}

// CleanupExpiredInfo æ¸…ç†è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *BadgerResumeManager) CleanupExpiredInfo() error {
	fs.Debugf(rm.fs, "ðŸ§¹ å¼€å§‹æ¸…ç†%sç½‘ç›˜è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯", rm.backendType)

	// èŽ·å–æ‰€æœ‰ç¼“å­˜æ¡ç›®
	entries, err := rm.resumeCache.GetAllEntries()
	if err != nil {
		return fmt.Errorf("èŽ·å–ç¼“å­˜æ¡ç›®å¤±è´¥: %w", err)
	}

	cleanedCount := 0

	for key := range entries {
		// æ£€æŸ¥æ˜¯å¦ä¸ºæ–­ç‚¹ç»­ä¼ ä¿¡æ¯
		if !rm.isResumeKey(key) {
			continue
		}

		// å°è¯•åŠ è½½ä¿¡æ¯æ£€æŸ¥è¿‡æœŸæ—¶é—´
		taskID := rm.extractTaskIDFromKey(key)
		info, err := rm.LoadResumeInfo(taskID)
		if err != nil {
			// åŠ è½½å¤±è´¥ï¼Œåˆ é™¤æŸåçš„æ¡ç›®
			rm.resumeCache.Delete(key)
			cleanedCount++
			continue
		}

		if info != nil && time.Since(info.GetCreatedAt()) > 24*time.Hour {
			rm.DeleteResumeInfo(taskID)
			cleanedCount++
		}
	}

	fs.Debugf(rm.fs, "ðŸ§¹ æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº†%dä¸ªè¿‡æœŸçš„%sç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯", cleanedCount, rm.backendType)
	return nil
}

// Close å…³é—­æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
// ðŸ”§ ä¼˜åŒ–ï¼šæ¸…ç†æ‰€æœ‰å®šæ—¶å™¨
func (rm *BadgerResumeManager) Close() error {
	if rm.cleanupTimer != nil {
		rm.cleanupTimer.Stop()
	}

	// ðŸ”§ æ¸…ç†ç¼“å­˜å¥åº·ç›‘æŽ§å®šæ—¶å™¨
	if rm.cacheHealthTimer != nil {
		rm.cacheHealthTimer.Stop()
	}

	if rm.resumeCache != nil {
		return rm.resumeCache.Close()
	}

	return nil
}

// ç§æœ‰è¾…åŠ©æ–¹æ³•

func (rm *BadgerResumeManager) generateCacheKey(taskID string) string {
	return fmt.Sprintf("resume_%s_%s", rm.backendType, taskID)
}

func (rm *BadgerResumeManager) isResumeKey(key string) bool {
	prefix := fmt.Sprintf("resume_%s_", rm.backendType)
	return len(key) > len(prefix) && key[:len(prefix)] == prefix
}

func (rm *BadgerResumeManager) extractTaskIDFromKey(key string) string {
	prefix := fmt.Sprintf("resume_%s_", rm.backendType)
	if len(key) > len(prefix) {
		return key[len(prefix):]
	}
	return ""
}

func (rm *BadgerResumeManager) startCleanupTimer() {
	// æ¯6å°æ—¶æ¸…ç†ä¸€æ¬¡è¿‡æœŸä¿¡æ¯
	rm.cleanupTimer = time.AfterFunc(6*time.Hour, func() {
		rm.CleanupExpiredInfo()
		rm.startCleanupTimer() // é‡æ–°è®¾ç½®å®šæ—¶å™¨
	})
}

// ResumeInfo123 123ç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å®žçŽ°
type ResumeInfo123 struct {
	TaskID              string                 `json:"task_id"`
	PreuploadID         string                 `json:"preupload_id"`
	FileName            string                 `json:"file_name"`
	FileSize            int64                  `json:"file_size"`
	FilePath            string                 `json:"file_path"`
	ChunkSize           int64                  `json:"chunk_size"`
	TotalChunks         int64                  `json:"total_chunks"`
	CompletedChunks     map[int64]bool         `json:"completed_chunks"`
	CreatedAt           time.Time              `json:"created_at"`
	LastUpdated         time.Time              `json:"last_updated"`
	TempFilePath        string                 `json:"temp_file_path"` // ðŸ”§ æ–°å¢žï¼šä¸´æ—¶æ–‡ä»¶è·¯å¾„
	BackendSpecificData map[string]interface{} `json:"backend_specific_data"`
}

// ResumeInfo115 115ç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å®žçŽ°
type ResumeInfo115 struct {
	TaskID              string                 `json:"task_id"`
	FileName            string                 `json:"file_name"`
	FileSize            int64                  `json:"file_size"`
	FilePath            string                 `json:"file_path"`
	ChunkSize           int64                  `json:"chunk_size"`
	TotalChunks         int64                  `json:"total_chunks"`
	CompletedChunks     map[int64]bool         `json:"completed_chunks"`
	CreatedAt           time.Time              `json:"created_at"`
	LastUpdated         time.Time              `json:"last_updated"`
	TempFilePath        string                 `json:"temp_file_path"`
	BackendSpecificData map[string]interface{} `json:"backend_specific_data"`
}

// 123ç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å®žçŽ°

func (r *ResumeInfo123) GetTaskID() string         { return r.TaskID }
func (r *ResumeInfo123) GetFileName() string       { return r.FileName }
func (r *ResumeInfo123) GetFileSize() int64        { return r.FileSize }
func (r *ResumeInfo123) GetFilePath() string       { return r.FilePath }
func (r *ResumeInfo123) GetChunkSize() int64       { return r.ChunkSize }
func (r *ResumeInfo123) GetTotalChunks() int64     { return r.TotalChunks }
func (r *ResumeInfo123) GetCreatedAt() time.Time   { return r.CreatedAt }
func (r *ResumeInfo123) GetLastUpdated() time.Time { return r.LastUpdated }

func (r *ResumeInfo123) IsChunkCompleted(chunkIndex int64) bool {
	if r.CompletedChunks == nil {
		return false
	}
	return r.CompletedChunks[chunkIndex]
}

func (r *ResumeInfo123) MarkChunkCompleted(chunkIndex int64) {
	if r.CompletedChunks == nil {
		r.CompletedChunks = make(map[int64]bool)
	}
	r.CompletedChunks[chunkIndex] = true
	r.LastUpdated = time.Now()
}

func (r *ResumeInfo123) GetCompletedChunkCount() int64 {
	return int64(len(r.CompletedChunks))
}

func (r *ResumeInfo123) GetCompletedChunks() map[int64]bool {
	if r.CompletedChunks == nil {
		return make(map[int64]bool)
	}
	// è¿”å›žå‰¯æœ¬ä»¥é¿å…å¤–éƒ¨ä¿®æ”¹
	result := make(map[int64]bool)
	for k, v := range r.CompletedChunks {
		result[k] = v
	}
	return result
}

func (r *ResumeInfo123) GetBackendSpecificData() map[string]interface{} {
	if r.BackendSpecificData == nil {
		r.BackendSpecificData = make(map[string]interface{})
	}
	return r.BackendSpecificData
}

func (r *ResumeInfo123) SetBackendSpecificData(key string, value interface{}) {
	if r.BackendSpecificData == nil {
		r.BackendSpecificData = make(map[string]interface{})
	}
	r.BackendSpecificData[key] = value
}

func (r *ResumeInfo123) ToJSON() ([]byte, error) {
	return json.Marshal(r)
}

func (r *ResumeInfo123) FromJSON(data []byte) error {
	return json.Unmarshal(data, r)
}

// 115ç½‘ç›˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å®žçŽ°

func (r *ResumeInfo115) GetTaskID() string         { return r.TaskID }
func (r *ResumeInfo115) GetFileName() string       { return r.FileName }
func (r *ResumeInfo115) GetFileSize() int64        { return r.FileSize }
func (r *ResumeInfo115) GetFilePath() string       { return r.FilePath }
func (r *ResumeInfo115) GetChunkSize() int64       { return r.ChunkSize }
func (r *ResumeInfo115) GetTotalChunks() int64     { return r.TotalChunks }
func (r *ResumeInfo115) GetCreatedAt() time.Time   { return r.CreatedAt }
func (r *ResumeInfo115) GetLastUpdated() time.Time { return r.LastUpdated }

func (r *ResumeInfo115) IsChunkCompleted(chunkIndex int64) bool {
	if r.CompletedChunks == nil {
		return false
	}
	return r.CompletedChunks[chunkIndex]
}

func (r *ResumeInfo115) MarkChunkCompleted(chunkIndex int64) {
	if r.CompletedChunks == nil {
		r.CompletedChunks = make(map[int64]bool)
	}
	r.CompletedChunks[chunkIndex] = true
	r.LastUpdated = time.Now()
}

func (r *ResumeInfo115) GetCompletedChunkCount() int64 {
	return int64(len(r.CompletedChunks))
}

func (r *ResumeInfo115) GetCompletedChunks() map[int64]bool {
	if r.CompletedChunks == nil {
		return make(map[int64]bool)
	}
	// è¿”å›žå‰¯æœ¬ä»¥é¿å…å¤–éƒ¨ä¿®æ”¹
	result := make(map[int64]bool)
	for k, v := range r.CompletedChunks {
		result[k] = v
	}
	return result
}

func (r *ResumeInfo115) GetBackendSpecificData() map[string]interface{} {
	if r.BackendSpecificData == nil {
		r.BackendSpecificData = make(map[string]interface{})
	}
	return r.BackendSpecificData
}

func (r *ResumeInfo115) SetBackendSpecificData(key string, value interface{}) {
	if r.BackendSpecificData == nil {
		r.BackendSpecificData = make(map[string]interface{})
	}
	r.BackendSpecificData[key] = value
}

func (r *ResumeInfo115) ToJSON() ([]byte, error) {
	return json.Marshal(r)
}

func (r *ResumeInfo115) FromJSON(data []byte) error {
	return json.Unmarshal(data, r)
}

// å·¥å…·å‡½æ•°

// GenerateTaskID123 ç”Ÿæˆ123ç½‘ç›˜ä»»åŠ¡ID
// ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç¨³å®šçš„å“ˆå¸Œå€¼ï¼Œç¡®ä¿ä¸‹è½½å’Œä¸Šä¼ ä½¿ç”¨ä¸€è‡´çš„TaskID
func GenerateTaskID123(filePath string, fileSize int64) string {
	// ä½¿ç”¨æ–‡ä»¶è·¯å¾„å’Œå¤§å°ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œå€¼
	hasher := sha256.New()
	hasher.Write([]byte(fmt.Sprintf("123_%s_%d", filePath, fileSize)))
	hash := hex.EncodeToString(hasher.Sum(nil))[:16] // å–å‰16ä½ä½œä¸ºçŸ­å“ˆå¸Œ

	return fmt.Sprintf("123_%s_%d_%s",
		filePath,
		fileSize,
		hash)
}

// GenerateTaskID115 ç”Ÿæˆ115ç½‘ç›˜ä»»åŠ¡ID
// ðŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç¨³å®šçš„å“ˆå¸Œå€¼è€Œä¸æ˜¯æ—¶é—´æˆ³ï¼Œç¡®ä¿æ–­ç‚¹ç»­ä¼ èƒ½æ­£ç¡®å·¥ä½œ
func GenerateTaskID115(filePath string, fileSize int64) string {
	// ä½¿ç”¨æ–‡ä»¶è·¯å¾„å’Œå¤§å°ç”Ÿæˆç¨³å®šçš„å“ˆå¸Œå€¼
	hasher := sha256.New()
	hasher.Write([]byte(fmt.Sprintf("115_%s_%d", filePath, fileSize)))
	hash := hex.EncodeToString(hasher.Sum(nil))[:16] // å–å‰16ä½ä½œä¸ºçŸ­å“ˆå¸Œ

	return fmt.Sprintf("115_%s_%d_%s",
		filePath,
		fileSize,
		hash)
}

// ðŸ”§ ç¼“å­˜å¥åº·ç›‘æŽ§å’Œæ¢å¤æœºåˆ¶

// startCacheHealthMonitor å¯åŠ¨ç¼“å­˜å¥åº·ç›‘æŽ§
func (rm *BadgerResumeManager) startCacheHealthMonitor() {
	// æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ç¼“å­˜å¥åº·çŠ¶æ€
	healthCheckInterval := 5 * time.Minute

	fs.Debugf(rm.fs, "ðŸ”§ å¯åŠ¨%sç½‘ç›˜ç¼“å­˜å¥åº·ç›‘æŽ§ï¼Œæ£€æŸ¥é—´éš”: %v", rm.backendType, healthCheckInterval)

	rm.cacheHealthTimer = time.AfterFunc(healthCheckInterval, func() {
		rm.performHealthCheck()
		rm.startCacheHealthMonitor() // é‡æ–°å¯åŠ¨å®šæ—¶å™¨
	})
}

// performHealthCheck æ‰§è¡Œç¼“å­˜å¥åº·æ£€æŸ¥
func (rm *BadgerResumeManager) performHealthCheck() {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	// æµ‹è¯•ç¼“å­˜åŸºæœ¬æ“ä½œ
	testKey := fmt.Sprintf("health_check_%s_%d", rm.backendType, time.Now().Unix())
	testValue := map[string]interface{}{
		"test":      true,
		"timestamp": time.Now(),
	}

	// æµ‹è¯•å†™å…¥
	err := rm.resumeCache.Set(testKey, testValue, 1*time.Minute)
	if err != nil {
		rm.cacheFailures++
		fs.Debugf(rm.fs, "âš ï¸ %sç½‘ç›˜ç¼“å­˜å¥åº·æ£€æŸ¥å¤±è´¥(å†™å…¥): %v, å¤±è´¥æ¬¡æ•°: %d",
			rm.backendType, err, rm.cacheFailures)

		// å¦‚æžœè¿žç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå°è¯•æ¢å¤
		if rm.cacheFailures >= 3 {
			rm.attemptCacheRecovery()
		}
		return
	}

	// æµ‹è¯•è¯»å–
	var result map[string]interface{}
	found, err := rm.resumeCache.Get(testKey, &result)
	if err != nil || !found {
		rm.cacheFailures++
		fs.Debugf(rm.fs, "âš ï¸ %sç½‘ç›˜ç¼“å­˜å¥åº·æ£€æŸ¥å¤±è´¥(è¯»å–): found=%v, err=%v, å¤±è´¥æ¬¡æ•°: %d",
			rm.backendType, found, err, rm.cacheFailures)

		if rm.cacheFailures >= 3 {
			rm.attemptCacheRecovery()
		}
		return
	}

	// æµ‹è¯•åˆ é™¤
	err = rm.resumeCache.Delete(testKey)
	if err != nil {
		fs.Debugf(rm.fs, "âš ï¸ %sç½‘ç›˜ç¼“å­˜å¥åº·æ£€æŸ¥åˆ é™¤å¤±è´¥: %v", rm.backendType, err)
	}

	// å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œé‡ç½®å¤±è´¥è®¡æ•°
	if rm.cacheFailures > 0 {
		fs.Debugf(rm.fs, "âœ… %sç½‘ç›˜ç¼“å­˜å¥åº·æ£€æŸ¥æ¢å¤æ­£å¸¸ï¼Œé‡ç½®å¤±è´¥è®¡æ•°", rm.backendType)
		rm.cacheFailures = 0
	}

	rm.lastHealthCheck = time.Now()

	// ðŸ”§ æ–°å¢žï¼šæ–­ç‚¹ç»­ä¼ å¥åº·æ£€æŸ¥æŠ¥å‘Š
	rm.logResumeHealthReport()
}

// logResumeHealthReport è®°å½•æ–­ç‚¹ç»­ä¼ å¥åº·æŠ¥å‘Š
func (rm *BadgerResumeManager) logResumeHealthReport() {
	report := rm.GetResumeHealthReport()

	fs.Infof(rm.fs, "ðŸ“Š %sç½‘ç›˜æ–­ç‚¹ç»­ä¼ å¥åº·æŠ¥å‘Š:", rm.backendType)
	fs.Infof(rm.fs, "   æ€»å°è¯•æ¬¡æ•°: %d, æˆåŠŸ: %d, å¤±è´¥: %d, æˆåŠŸçŽ‡: %s",
		report["total_attempts"], report["successful_resumes"],
		report["failed_resumes"], report["success_rate"])

	if report["total_attempts"].(int64) > 0 {
		fs.Infof(rm.fs, "   å¹³å‡æ¢å¤æ—¶é—´: %s", report["average_resume_time"])

		// æ˜¾ç¤ºå¤±è´¥åŽŸå› ç»Ÿè®¡
		if failureReasons, ok := report["failure_reasons"].(map[string]int64); ok && len(failureReasons) > 0 {
			fs.Infof(rm.fs, "   å¤±è´¥åŽŸå› ç»Ÿè®¡:")
			for reason, count := range failureReasons {
				fs.Infof(rm.fs, "     - %s: %dæ¬¡", reason, count)
			}
		}
	}
}

// attemptCacheRecovery å°è¯•ç¼“å­˜æ¢å¤
func (rm *BadgerResumeManager) attemptCacheRecovery() {
	fs.Logf(rm.fs, "ðŸ”§ %sç½‘ç›˜ç¼“å­˜è¿žç»­å¤±è´¥%dæ¬¡ï¼Œå°è¯•æ¢å¤...", rm.backendType, rm.cacheFailures)

	// è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ¢å¤ç­–ç•¥ï¼Œæ¯”å¦‚ï¼š
	// 1. é‡æ–°åˆå§‹åŒ–ç¼“å­˜
	// 2. æ¸…ç†æŸåçš„ç¼“å­˜æ–‡ä»¶
	// 3. åˆ‡æ¢åˆ°å†…å­˜æ¨¡å¼

	// ç›®å‰ç®€å•åœ°é‡ç½®å¤±è´¥è®¡æ•°ï¼Œè®©ç¼“å­˜ç»§ç»­ä½¿ç”¨å†…å­˜å¤‡ä»½æ¨¡å¼
	rm.cacheFailures = 0
	fs.Logf(rm.fs, "ðŸ”§ %sç½‘ç›˜ç¼“å­˜æ¢å¤å®Œæˆï¼Œå°†ç»§ç»­ä½¿ç”¨å†…å­˜å¤‡ä»½æ¨¡å¼", rm.backendType)
}

// ðŸ”§ æ–­ç‚¹ç»­ä¼ ç›‘æŽ§å’Œç»Ÿè®¡æ–¹æ³•

// updateResumeStats æ›´æ–°æ–­ç‚¹ç»­ä¼ ç»Ÿè®¡ä¿¡æ¯
func (rm *BadgerResumeManager) updateResumeStats(success bool, failureReason string, bytesResumed int64, timeSaved float64) {
	rm.resumeStats.mu.Lock()
	defer rm.resumeStats.mu.Unlock()

	rm.resumeStats.TotalAttempts++

	if success {
		rm.resumeStats.SuccessfulResumes++
		rm.resumeStats.LastSuccessTime = time.Now()
		rm.resumeStats.PerformanceMetrics.TotalBytesResumed += bytesResumed
		rm.resumeStats.PerformanceMetrics.TotalTimesSaved += timeSaved
	} else {
		rm.resumeStats.FailedResumes++
		rm.resumeStats.LastFailureTime = time.Now()
		if failureReason != "" {
			rm.resumeStats.FailureReasons[failureReason]++
		}
	}

	// æ›´æ–°å¹³å‡æ¢å¤æ—¶é—´
	if rm.resumeStats.SuccessfulResumes > 0 {
		rm.resumeStats.AverageResumeTime = rm.resumeStats.PerformanceMetrics.TotalTimesSaved / float64(rm.resumeStats.SuccessfulResumes)
	}
}

// GetResumeStats èŽ·å–æ–­ç‚¹ç»­ä¼ ç»Ÿè®¡ä¿¡æ¯
func (rm *BadgerResumeManager) GetResumeStats() ResumeStats {
	rm.resumeStats.mu.RLock()
	defer rm.resumeStats.mu.RUnlock()

	// åˆ›å»ºå‰¯æœ¬é¿å…å¹¶å‘é—®é¢˜
	stats := rm.resumeStats
	stats.FailureReasons = make(map[string]int64)
	for k, v := range rm.resumeStats.FailureReasons {
		stats.FailureReasons[k] = v
	}

	return stats
}

// GetResumeHealthReport èŽ·å–æ–­ç‚¹ç»­ä¼ å¥åº·æŠ¥å‘Š
func (rm *BadgerResumeManager) GetResumeHealthReport() map[string]interface{} {
	stats := rm.GetResumeStats()

	successRate := float64(0)
	if stats.TotalAttempts > 0 {
		successRate = float64(stats.SuccessfulResumes) / float64(stats.TotalAttempts) * 100
	}

	return map[string]interface{}{
		"backend_type":        rm.backendType,
		"total_attempts":      stats.TotalAttempts,
		"successful_resumes":  stats.SuccessfulResumes,
		"failed_resumes":      stats.FailedResumes,
		"success_rate":        fmt.Sprintf("%.2f%%", successRate),
		"average_resume_time": fmt.Sprintf("%.2fs", stats.AverageResumeTime),
		"last_success_time":   stats.LastSuccessTime.Format("2006-01-02 15:04:05"),
		"last_failure_time":   stats.LastFailureTime.Format("2006-01-02 15:04:05"),
		"failure_reasons":     stats.FailureReasons,
		"performance_metrics": map[string]interface{}{
			"total_bytes_resumed": fmt.Sprintf("%.2f MB", float64(stats.PerformanceMetrics.TotalBytesResumed)/1024/1024),
			"total_time_saved":    fmt.Sprintf("%.2fs", stats.PerformanceMetrics.TotalTimesSaved),
			"avg_upload_speed":    fmt.Sprintf("%.2f MB/s", stats.PerformanceMetrics.AverageUploadSpeed),
			"avg_download_speed":  fmt.Sprintf("%.2f MB/s", stats.PerformanceMetrics.AverageDownloadSpeed),
		},
		"cache_health": map[string]interface{}{
			"last_health_check": rm.lastHealthCheck.Format("2006-01-02 15:04:05"),
			"cache_failures":    rm.cacheFailures,
		},
	}
}
