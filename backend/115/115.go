// Package _115 provides an interface to 115 cloud storage
package _115

import (
	"bytes"
	"context"
	"crypto/rand"
	"crypto/sha1"
	"crypto/sha256"
	"encoding/base64"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"math"
	"mime/multipart"
	"net/http"
	"net/url"
	"os"
	"path"
	"path/filepath"
	"reflect"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/aliyun/alibabacloud-oss-go-sdk-v2/oss"
	"github.com/aliyun/alibabacloud-oss-go-sdk-v2/oss/credentials"
	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/accounting"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	"github.com/rclone/rclone/fs/fshttp"
	"github.com/rclone/rclone/fs/hash"
	"github.com/rclone/rclone/lib/dircache"
	"github.com/rclone/rclone/lib/encoder"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/pool"
	"github.com/rclone/rclone/lib/rest"
	"golang.org/x/oauth2"
)

// API Types and Structures

// Time represents date and time information
type Time time.Time

// MarshalJSON turns a Time into JSON (in UTC)
func (t *Time) MarshalJSON() (out []byte, err error) {
	s := strconv.Itoa(int((*time.Time)(t).Unix()))
	return []byte(s), nil
}

// UnmarshalJSON turns JSON into a Time
func (t *Time) UnmarshalJSON(data []byte) error {
	s := strings.Trim(string(data), `"`)
	if s == "null" || s == "" {
		return nil
	}
	i, err := strconv.ParseInt(s, 10, 64)
	if err != nil {
		// Try parsing RFC3339 format for Expiration in OSSTokenData
		parsedTime, timeErr := time.Parse(time.RFC3339, s)
		if timeErr == nil {
			*t = Time(parsedTime)
			return nil
		}
		// Return original error if RFC3339 parsing also fails
		return err
	}
	newT := time.Unix(i, 0)
	*t = Time(newT)
	return nil
}

type Int int

func (e *Int) UnmarshalJSON(in []byte) (err error) {
	s := strings.Trim(string(in), `"`)
	if s == "" {
		s = "0"
	}
	if i, err := strconv.Atoi(s); err == nil {
		*e = Int(i)
	}
	return
}

type Int64 int64

func (e *Int64) UnmarshalJSON(in []byte) (err error) {
	s := strings.Trim(string(in), `"`)
	if s == "" {
		s = "0"
	}
	if i, err := strconv.ParseInt(s, 10, 64); err == nil {
		*e = Int64(i)
	}
	return
}

type BoolOrInt bool

func (b *BoolOrInt) UnmarshalJSON(in []byte) error {
	// if in is "true" or "false", unmarshal it as a bool
	// if in is 0 or 1, unmarshal it as an int
	// otherwise, return an error
	var boolVal bool
	err := json.Unmarshal(in, &boolVal)
	if err == nil {
		*b = BoolOrInt(boolVal)
		return nil
	}

	var intVal int
	err = json.Unmarshal(in, &intVal)
	if err == nil {
		*b = BoolOrInt(intVal == 1)
		return nil
	}

	return fmt.Errorf("cannot unmarshal %s into BoolOrInt", string(in))
}

// String ensures JSON unmarshals to a string, handling both quoted and unquoted inputs.
// Unquoted inputs are treated as raw bytes and converted directly to a string.
type String string

func (s *String) UnmarshalJSON(in []byte) error {
	if n := len(in); n > 1 && in[0] == '"' && in[n-1] == '"' {
		return json.Unmarshal(in, (*string)(s))
	}
	*s = String(in)
	return nil
}

// StringOrNumber handles API fields that can be either a string or a number
type StringOrNumber string

func (s *StringOrNumber) UnmarshalJSON(data []byte) error {
	// Try string first
	var str string
	if err := json.Unmarshal(data, &str); err == nil {
		*s = StringOrNumber(str)
		return nil
	}

	// Try number (int)
	var intVal int
	if err := json.Unmarshal(data, &intVal); err == nil {
		*s = StringOrNumber(strconv.Itoa(intVal))
		return nil
	}

	// Try number (float)
	var floatVal float64
	if err := json.Unmarshal(data, &floatVal); err == nil {
		*s = StringOrNumber(strconv.FormatFloat(floatVal, 'f', -1, 64))
		return nil
	}

	// If it's null or empty
	if string(data) == "null" || string(data) == "" {
		*s = ""
		return nil
	}

	// Last resort: use the raw value
	*s = StringOrNumber(strings.Trim(string(data), "\""))
	return nil
}

// 115ç½‘ç›˜APIå¸¸é‡
const (
	// APIåˆ†é¡µé™åˆ¶
	defaultListChunkSize = 1150
)

type TraditionalBase struct {
	Msg   string    `json:"msg,omitempty"`
	Errno Int       `json:"errno,omitempty"` // Base, NewDir, DirID, UploadBasicInfo, ShareSnap
	ErrNo Int       `json:"errNo,omitempty"` // FileList
	Error string    `json:"error,omitempty"` // Base, FileList, NewDir, DirID, UploadBasicInfo, ShareSnap
	State BoolOrInt `json:"state,omitempty"`
}

func (b *TraditionalBase) ErrCode() Int {
	if b.Errno != 0 {
		return b.Errno
	}
	return b.ErrNo
}

func (b *TraditionalBase) ErrMsg() string {
	if b.Error != "" {
		return b.Error
	}
	return b.Msg
}

// Err returns Error or Nil for TraditionalBase
func (b *TraditionalBase) Err() error {
	if b.State {
		return nil
	}
	out := fmt.Sprintf("Traditional API Error(%d)", b.ErrCode())
	if msg := b.ErrMsg(); msg != "" {
		out += fmt.Sprintf(": %q", msg)
	}
	return errors.New(out)
}

// OpenAPIBase is for the new Open API

type OpenAPIBase struct {
	State   BoolOrInt      `json:"state"` // Note: OpenAPI uses boolean state
	Code    Int            `json:"code,omitempty"`
	Message StringOrNumber `json:"message,omitempty"` // Can be either string or number
	Error   string         `json:"error,omitempty"`   // Some endpoints might still use this
	Errno   Int            `json:"errno,omitempty"`   // Some endpoints might still use this
}

func (b *OpenAPIBase) ErrCode() Int {
	if b.Code != 0 {
		return b.Code
	}
	return b.Errno
}

func (b *OpenAPIBase) ErrMsg() string {
	if string(b.Message) != "" {
		return string(b.Message)
	}
	return b.Error
}

// Err returns Error or Nil for OpenAPIBase
func (b *OpenAPIBase) Err() error {
	if b.State {
		return nil
	}
	out := fmt.Sprintf("OpenAPI Error(%d)", b.ErrCode())
	if msg := b.ErrMsg(); msg != "" {
		out += fmt.Sprintf(": %q", msg)
	}

	// Specific error codes to check based on API documentation
	code := b.ErrCode()
	// Check for rate limit error codes in multiple formats
	if code == 0 && (string(b.Message) == "770004" ||
		strings.Contains(b.ErrMsg(), "770004") ||
		strings.Contains(b.ErrMsg(), "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™")) {
		// This is a rate limit error
		return fmt.Errorf("%s: rate limit exceeded", out)
	}

	switch code {
	// Codes that require re-login
	case 40140116: // refresh_token invalid (authorization revoked)
		return NewTokenError(out, true)
	case 40140117: // access_token refreshed too frequently
		return NewTokenError(out, true)
	case 40140119: // refresh_token expired
		return NewTokenError(out, true)
	case 40140120: // refresh_token verification failed (anti-tampering)
		// Check if local refresh token is updated; if not, re-login
		return NewTokenError(out, true)
	case 40140121: // access_token refresh failed
		// This should allow a retry of the refresh token operation
		return NewTokenError(out, false)
	case 40140125: // access_token invalid (expired or authorization revoked)
		// Try refreshing token first
		return NewTokenError(out, false)
	}

	return errors.New(out)
}

// TokenError indicates an issue with the access or refresh token
type TokenError struct {
	msg                   string
	IsRefreshTokenExpired bool
}

// NewTokenError creates a new TokenError with the given message and refresh token status
func NewTokenError(msg string, isRefreshTokenExpired ...bool) *TokenError {
	expired := false
	if len(isRefreshTokenExpired) > 0 {
		expired = isRefreshTokenExpired[0]
	}
	return &TokenError{
		msg:                   msg,
		IsRefreshTokenExpired: expired,
	}
}

func (e *TokenError) Error() string {
	return e.msg
}

// ------------------------------------------------------------
// Authentication related structs
// ------------------------------------------------------------

type AuthDeviceCodeData struct {
	UID    string `json:"uid"`
	Time   int64  `json:"time"`
	Qrcode string `json:"qrcode"`
	Sign   string `json:"sign"`
}

type AuthDeviceCodeResp struct {
	OpenAPIBase
	Data *AuthDeviceCodeData `json:"data"`
}

type DeviceCodeTokenData struct {
	AccessToken  string `json:"access_token"`
	RefreshToken string `json:"refresh_token"`
	ExpiresIn    int64  `json:"expires_in"` // seconds
}

type DeviceCodeTokenResp struct {
	OpenAPIBase
	Data *DeviceCodeTokenData `json:"data"`
}

type RefreshTokenData struct {
	AccessToken  string `json:"access_token"`
	RefreshToken string `json:"refresh_token"`
	ExpiresIn    int64  `json:"expires_in"` // seconds
}

type RefreshTokenResp struct {
	OpenAPIBase
	Data *RefreshTokenData `json:"data"`
}

// ------------------------------------------------------------
// File and Directory related structs
// ------------------------------------------------------------

// File represents a file or folder object from either API
// Uses tags for both traditional and OpenAPI field names
type File struct {
	// Common fields (adapt names if needed)
	Name            string `json:"n,omitempty"`         // Traditional: n, OpenAPI: fn or file_name
	FileName        string `json:"fn,omitempty"`        // OpenAPI: fn (in list), file_name (in details)
	Size            Int64  `json:"size,omitempty"`      // Traditional: s, OpenAPI: file_size
	FileSize        Int64  `json:"fs,omitempty"`        // OpenAPI: file_size
	PickCode        string `json:"pc,omitempty"`        // Traditional: pc
	PickCodeOpenAPI string `json:"pick_code,omitempty"` // OpenAPI: pick_code
	Sha             string `json:"sha,omitempty"`       // Traditional: sha, OpenAPI: sha1
	Sha1            string `json:"sha1,omitempty"`      // OpenAPI: sha1

	// Identifiers
	FID string `json:"fid,omitempty"` // Traditional: fid (file), OpenAPI: fid (file), file_id (folder/file details)
	CID string `json:"cid,omitempty"` // Traditional: cid (folder), OpenAPI: cid (in list query param), file_id (folder details)
	PID string `json:"pid,omitempty"` // Traditional: pid (folder parent), OpenAPI: pid (folder parent)

	// Timestamps
	T   string `json:"t,omitempty"`    // Traditional: representative time? "2024-05-19 03:54" or "1715919337"
	Te  Time   `json:"te,omitempty"`   // Traditional: modify time
	Tp  Time   `json:"tp,omitempty"`   // Traditional: create time
	Tu  Time   `json:"tu,omitempty"`   // Traditional: update time?
	To  Time   `json:"to,omitempty"`   // Traditional: last opened 0 if never accessed or "1716165082"
	Upt Time   `json:"upt,omitempty"`  // OpenAPI: upt (update time)
	Uet Time   `json:"uet,omitempty"`  // OpenAPI: uet (update time alias?)
	Ppt Time   `json:"uppt,omitempty"` // OpenAPI: uppt (upload time)

	// Type/Category
	IsFolder Int    `json:"fc,omitempty"`    // OpenAPI: fc (0 folder, 1 file)
	Ico      string `json:"ico,omitempty"`   // Traditional icon
	Class    string `json:"class,omitempty"` // Traditional class

	// Status/Attributes
	IsMarked Int `json:"ism,omitempty"`  // OpenAPI: ism (starred, 1=yes)
	Star     Int `json:"star,omitempty"` // OpenAPI: star (in update response)
	IsHidden Int `json:"ih,omitempty"`   // OpenAPI: ih (hidden?)
	IsLocked Int `json:"lo,omitempty"`   // OpenAPI: lo (locked?)
	IsCrypt  Int `json:"isp,omitempty"`  // OpenAPI: isp (encrypted, 1=yes)
	Censored Int `json:"c,omitempty"`    // Traditional: censored flag

	// Other fields from OpenAPI list
	Aid   json.Number `json:"aid,omitempty"`   // OpenAPI: aid (area id?)
	Fco   string      `json:"fco,omitempty"`   // OpenAPI: fco (folder cover?)
	Cm    Int         `json:"cm,omitempty"`    // OpenAPI: cm (?)
	Fdesc string      `json:"fdesc,omitempty"` // OpenAPI: fdesc (description?)
	Ispl  Int         `json:"ispl,omitempty"`  // OpenAPI: ispl (play long related?)

	// Fields from traditional API (might be redundant or map differently)
	UID       json.Number `json:"uid,omitempty"` // Traditional user ID
	CheckCode int         `json:"check_code,omitempty"`
	CheckMsg  string      `json:"check_msg,omitempty"`
	Score     int         `json:"score,omitempty"`
	PlayLong  float64     `json:"play_long,omitempty"` // playback secs if media
}

// IsDir checks if the item represents a directory based on OpenAPI fields
func (f *File) IsDir() bool {
	// OpenAPI uses fc=0 for folder, file_category="0" in details
	// Traditional uses fid="" for folder
	// ğŸ”§ æ ‡å‡†rcloneåšæ³•ï¼šå®Œå…¨ä¿¡ä»»æœåŠ¡å™¨è¿”å›çš„ç±»å‹æ ‡è¯†ï¼Œä¸Google Driveã€OneDriveç­‰ä¸€è‡´
	return f.IsFolder == 0 || (f.FID == "" && f.CID != "")
}

// ID returns the best identifier (File ID or Category ID)
func (f *File) ID() string {
	if f.FID != "" { // Prefer FID if available (OpenAPI file, Traditional file)
		return f.FID
	}
	if f.CID != "" { // Use CID if FID is empty (OpenAPI folder, Traditional folder)
		return f.CID
	}
	// Fallback for folder details where file_id is used
	// This might need adjustment based on actual API responses for folder details via OpenAPI
	// if f.FileID != "" {
	// return f.FileID
	// }
	return "" // Should not happen for valid items
}

// ParentID returns the parent directory ID
func (f *File) ParentID() string {
	return f.PID // Both APIs seem to use 'pid'
}

// FileName returns the best name field
func (f *File) FileNameBest() string {
	if f.FileName != "" { // OpenAPI list 'fn', details 'file_name'
		return f.FileName
	}
	return f.Name // Traditional 'n'
}

// FileSizeBest returns the best size field
func (f *File) FileSizeBest() int64 {
	if f.FileSize > 0 { // OpenAPI 'file_size'
		return int64(f.FileSize)
	}
	return int64(f.Size) // Traditional 's'
}

// PickCodeBest returns the best pick code field
func (f *File) PickCodeBest() string {
	// Prefer OpenAPI pick_code field
	if f.PickCodeOpenAPI != "" {
		return f.PickCodeOpenAPI
	}
	return f.PickCode // å›é€€åˆ°ä¼ ç»ŸAPIçš„pcå­—æ®µ
}

// Sha1Best returns the best SHA1 field
func (f *File) Sha1Best() string {
	if f.Sha1 != "" { // OpenAPI 'sha1'
		return f.Sha1
	}
	return f.Sha // Traditional 'sha'
}

// ModTime returns the best modification time
func (f *File) ModTime() time.Time {
	// Prefer OpenAPI update times
	if t := time.Time(f.Upt); !t.IsZero() {
		return t
	}
	if t := time.Time(f.Uet); !t.IsZero() {
		return t
	}
	// Fallback to traditional times
	if t := time.Time(f.Te); !t.IsZero() {
		return t
	}
	if t := time.Time(f.Tu); !t.IsZero() {
		return t
	}
	// Fallback for ShareSnap list items
	if ts, err := strconv.ParseInt(f.T, 10, 64); err == nil {
		return time.Unix(ts, 0)
	}
	return time.Time{}
}

// FilePath represents an item in the path hierarchy (used in traditional list)
type FilePath struct {
	Name string      `json:"name,omitempty"`
	AID  json.Number `json:"aid,omitempty"` // area
	CID  json.Number `json:"cid,omitempty"` // category
	PID  json.Number `json:"pid,omitempty"` // parent
	Isp  json.Number `json:"isp,omitempty"`
	PCid string      `json:"p_cid,omitempty"`
	Iss  string      `json:"iss,omitempty"`
	Fv   string      `json:"fv,omitempty"`
	Fvs  string      `json:"fvs,omitempty"`
}

// FileList represents the response from listing files (adaptable for both APIs)
type FileList struct {
	// Use OpenAPIBase for state/code/message
	OpenAPIBase

	// Data payload
	Files []*File `json:"data,omitempty"` // OpenAPI uses 'data', Traditional uses 'data'

	// Pagination and Counts (check OpenAPI names)
	Count       int         `json:"count,omitempty"`        // Traditional total count
	TotalCount  int         `json:"total_count,omitempty"`  // OpenAPI might use a different name
	FileCount   int         `json:"file_count,omitempty"`   // Traditional
	FolderCount int         `json:"folder_count,omitempty"` // Traditional
	PageSize    int         `json:"page_size,omitempty"`    // Traditional
	Limit       json.Number `json:"limit,omitempty"`        // OpenAPI uses 'limit' param, response might confirm
	Offset      json.Number `json:"offset,omitempty"`       // OpenAPI uses 'offset' param, response might confirm

	// Context/Query Info (check OpenAPI names)
	DataSource     string      `json:"data_source,omitempty"`      // Traditional
	SysCount       int         `json:"sys_count,omitempty"`        // Traditional
	AID            json.Number `json:"aid,omitempty"`              // Traditional
	CID            json.Number `json:"cid,omitempty"`              // Traditional context CID
	IsAsc          json.Number `json:"is_asc,omitempty"`           // Traditional sort order
	Star           int         `json:"star,omitempty"`             // Traditional star filter
	IsShare        int         `json:"is_share,omitempty"`         // Traditional
	Type           int         `json:"type,omitempty"`             // Traditional type filter
	IsQ            int         `json:"is_q,omitempty"`             // Traditional
	RAll           int         `json:"r_all,omitempty"`            // Traditional
	Stdir          int         `json:"stdir,omitempty"`            // Traditional
	Cur            int         `json:"cur,omitempty"`              // Traditional
	MinSize        int         `json:"min_size,omitempty"`         // Traditional
	MaxSize        int         `json:"max_size,omitempty"`         // Traditional
	RecordOpenTime string      `json:"record_open_time,omitempty"` // Traditional
	Path           []*FilePath `json:"path,omitempty"`             // Traditional path breadcrumbs
	Fields         string      `json:"fields,omitempty"`           // Traditional
	Order          string      `json:"order,omitempty"`            // Traditional sort field
	FcMix          int         `json:"fc_mix,omitempty"`           // Traditional
	Natsort        int         `json:"natsort,omitempty"`          // Traditional
	UID            json.Number `json:"uid,omitempty"`              // Traditional user ID
	Suffix         string      `json:"suffix,omitempty"`           // Traditional suffix filter
}

// FileInfo represents the response for getting single file info (Traditional)
// OpenAPI might use a different structure or just return a File object in 'data'
type FileInfo struct {
	TraditionalBase
	Data []*File `json:"data,omitempty"`
}
type NewDirData struct {
	FileName string `json:"file_name,omitempty"`
	FileID   string `json:"file_id,omitempty"`
}

// NewDir represents the response for creating a directory
type NewDir struct {
	OpenAPIBase
	Data *NewDirData `json:"data,omitempty"`
}

// DirID represents the response for getting a directory ID by path (Traditional)
type DirID struct {
	TraditionalBase
	ID        json.Number `json:"id,omitempty"`
	IsPrivate json.Number `json:"is_private,omitempty"`
}

// FileStats represents the response for getting folder stats (Traditional /category/get)
// OpenAPI has /open/folder/get_info
type FileStats struct {
	OpenAPIBase
	Data *FolderInfoData `json:"data"`
}

type FolderInfoData struct {
	Count        String `json:"count"`          // OpenAPI: string
	Size         String `json:"size"`           // OpenAPI: string
	FolderCount  String `json:"folder_count"`   // OpenAPI: string
	PlayLong     Int64  `json:"play_long"`      // OpenAPI: number (seconds), -1=calculating
	ShowPlayLong Int    `json:"show_play_long"` // OpenAPI: number (bool 0/1)
	Ptime        String `json:"ptime"`          // OpenAPI: string timestamp?
	Utime        String `json:"utime"`          // OpenAPI: string timestamp?
	FileName     string `json:"file_name"`      // OpenAPI: string
	PickCode     string `json:"pick_code"`      // OpenAPI: string
	Sha1         string `json:"sha1"`           // OpenAPI: string
	FileID       string `json:"file_id"`        // OpenAPI: string
	IsMark       String `json:"is_mark"`        // OpenAPI: string (bool 0/1)
	OpenTime     Int64  `json:"open_time"`      // OpenAPI: number timestamp?
	FileCategory String `json:"file_category"`  // OpenAPI: string (0=folder)
	Paths        []struct {
		FileID   String `json:"file_id"`   // OpenAPI: number (as string?)
		FileName string `json:"file_name"` // OpenAPI: string
	} `json:"paths"`
}

// StringInfo is a generic response where data is just a string (Traditional)
type StringInfo struct {
	TraditionalBase
	Data String `json:"data,omitempty"`
}

// IndexInfo represents user quota info (Traditional)
type IndexInfo struct {
	TraditionalBase
	Data *IndexData `json:"data,omitempty"`
}

type IndexData struct {
	SpaceInfo map[string]*SizeInfo `json:"space_info"`
}

type SizeInfo struct {
	Size       float64 `json:"size"`
	SizeFormat string  `json:"size_format"`
}

// ------------------------------------------------------------
// Download related structs
// ------------------------------------------------------------

// DownloadURL represents the URL structure from both APIs
type DownloadURL struct {
	URL       string         `json:"url"`              // Present in both
	Client    Int            `json:"client,omitempty"` // Traditional
	Desc      string         `json:"desc,omitempty"`   // Traditional
	OssID     string         `json:"oss_id,omitempty"` // Traditional
	Cookies   []*http.Cookie // Added manually after request
	CreatedAt time.Time      `json:"created_at"` // åˆ›å»ºæ—¶é—´ï¼Œç”¨äºfallbackè¿‡æœŸæ£€æµ‹
}

func (u *DownloadURL) UnmarshalJSON(data []byte) error {
	if string(data) == "false" {
		*u = DownloadURL{}
		return nil
	}

	type Alias DownloadURL // Use type alias to avoid recursion
	aux := Alias{}
	if err := json.Unmarshal(data, &aux); err != nil {
		// Try unmarshalling just as a string if object fails (OpenAPI might simplify)
		var urlStr string
		if strErr := json.Unmarshal(data, &urlStr); strErr == nil {
			*u = DownloadURL{
				URL:       urlStr,
				CreatedAt: time.Now(), // è®¾ç½®åˆ›å»ºæ—¶é—´ç”¨äºfallbackè¿‡æœŸæ£€æµ‹
			}
			return nil
		}
		return err // Return original error if string unmarshal also fails
	}
	*u = DownloadURL(aux)
	// ç¡®ä¿ä»JSONè§£æçš„DownloadURLä¹Ÿæœ‰åˆ›å»ºæ—¶é—´
	if u.CreatedAt.IsZero() {
		u.CreatedAt = time.Now()
	}
	return nil
}

// expiry parses expiry from URL parameter t (Traditional URL format)
func (u *DownloadURL) expiry() time.Time {
	if p, err := url.Parse(u.URL); err == nil {
		if q, err := url.ParseQuery(p.RawQuery); err == nil {
			if t := q.Get("t"); t != "" {
				if i, err := strconv.ParseInt(t, 10, 64); err == nil {
					return time.Unix(i, 0)
				}
			}
			// Check for OSS expiry parameter (might be different)
			if exp := q.Get("Expires"); exp != "" {
				if i, err := strconv.ParseInt(exp, 10, 64); err == nil {
					return time.Unix(i, 0)
				}
			}
		}
	}
	return time.Time{}
}

// expired reports whether the token is expired.
func (u *DownloadURL) expired() bool {
	// ç­–ç•¥1ï¼šé¦–å…ˆå°è¯•ç°æœ‰çš„URLè§£æé€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰
	expiry := u.expiry()
	if !expiry.IsZero() {
		// ä¿®å¤ï¼š115ç½‘ç›˜URLæœ‰æ•ˆæœŸé€šå¸¸4-5åˆ†é’Ÿï¼Œä½¿ç”¨æ›´åˆç†çš„ç¼“å†²æ—¶é—´
		now := time.Now()
		timeUntilExpiry := expiry.Sub(now)

		// ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•è¿‡æœŸæ£€æµ‹è¯¦æƒ…
		fs.Debugf(nil, "ğŸ” 115ç½‘ç›˜URLè¿‡æœŸæ£€æŸ¥(URLè§£æ): å½“å‰æ—¶é—´=%v, è¿‡æœŸæ—¶é—´=%v, å‰©ä½™æ—¶é—´=%v",
			now.Format("15:04:05"), expiry.Format("15:04:05"), timeUntilExpiry)

		// å¦‚æœURLå·²ç»è¿‡æœŸï¼Œç›´æ¥è¿”å›true
		if timeUntilExpiry <= 0 {
			fs.Debugf(nil, "âš ï¸ 115ç½‘ç›˜URLå·²è¿‡æœŸ(URLè§£æ)")
			return true
		}

		// æ ¹æ®å‰©ä½™æ—¶é—´é€‰æ‹©ç¼“å†²ç­–ç•¥
		var expiryDelta time.Duration
		if timeUntilExpiry < 2*time.Minute {
			expiryDelta = 15 * time.Second // çŸ­æœŸURLä½¿ç”¨15ç§’ç¼“å†²
		} else {
			expiryDelta = 30 * time.Second // é•¿æœŸURLä½¿ç”¨30ç§’ç¼“å†²
		}

		isExpired := timeUntilExpiry <= expiryDelta
		fs.Debugf(nil, "ğŸ” 115ç½‘ç›˜URLè¿‡æœŸå†³ç­–(URLè§£æ): ç¼“å†²åŒº=%v, å·²è¿‡æœŸ=%v", expiryDelta, isExpired)

		return isExpired
	}

	// ç­–ç•¥2ï¼šURLè§£æå¤±è´¥æ—¶çš„fallback - ä½¿ç”¨åˆ›å»ºæ—¶é—´ + 4åˆ†é’Ÿ
	if !u.CreatedAt.IsZero() {
		timeSinceCreated := time.Since(u.CreatedAt)
		fallbackExpired := timeSinceCreated > 4*time.Minute

		fs.Debugf(nil, "ğŸ” 115ç½‘ç›˜URLè¿‡æœŸæ£€æŸ¥(fallback): åˆ›å»ºæ—¶é—´=%v, å·²å­˜åœ¨æ—¶é—´=%v, å·²è¿‡æœŸ=%v",
			u.CreatedAt.Format("15:04:05"), timeSinceCreated, fallbackExpired)

		return fallbackExpired
	}

	// ç­–ç•¥3ï¼šæœ€åçš„fallback - å‡è®¾ä¸è¿‡æœŸï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
	fs.Debugf(nil, "ğŸ” 115ç½‘ç›˜URLè¿‡æœŸæ£€æŸ¥(æœ€åfallback): å‡è®¾ä¸è¿‡æœŸ")
	return false
}

// Valid reports whether u is non-nil and is not expired.
func (u *DownloadURL) Valid() bool {
	return u != nil && u.URL != "" && !u.expired()
}

func (u *DownloadURL) Cookie() string {
	cookie := ""
	for _, ck := range u.Cookies {
		cookie += fmt.Sprintf("%s=%s;", ck.Name, ck.Value)
	}
	return cookie
}

// DownloadInfo represents the structure within the DownloadData map (Traditional)
type DownloadInfo struct {
	FileName string      `json:"file_name"`
	FileSize Int64       `json:"file_size"`
	PickCode string      `json:"pick_code"`
	URL      DownloadURL `json:"url"`
}

// DownloadData is the map returned by the traditional download URL endpoint
type DownloadData map[string]*DownloadInfo

// OpenAPI specific download response structure
type OpenAPIDownloadInfo struct {
	FileName string      `json:"file_name"`
	FileSize Int64       `json:"file_size"`
	PickCode string      `json:"pick_code"`
	Sha1     string      `json:"sha1"`
	URL      DownloadURL `json:"url"` // Assumes nested URL object like traditional
}

// OpenAPIDownloadResp represents the response from POST /open/ufile/downurl
// 115ç½‘ç›˜APIæœ‰æ—¶è¿”å›mapæ ¼å¼ï¼Œæœ‰æ—¶è¿”å›arrayæ ¼å¼ï¼Œéœ€è¦çµæ´»å¤„ç†
type OpenAPIDownloadResp struct {
	OpenAPIBase
	// Data can be either a map or an array, we'll handle both formats
	Data json.RawMessage `json:"data"`
}

// GetDownloadInfo ä»å“åº”ä¸­æå–ä¸‹è½½ä¿¡æ¯ï¼Œå¤„ç†mapå’Œarrayä¸¤ç§æ ¼å¼
func (r *OpenAPIDownloadResp) GetDownloadInfo() (*OpenAPIDownloadInfo, error) {
	if len(r.Data) == 0 {
		return nil, errors.New("empty data field in download response")
	}

	// å°è¯•è§£æä¸ºmapæ ¼å¼
	var mapData map[string]*OpenAPIDownloadInfo
	if err := json.Unmarshal(r.Data, &mapData); err == nil {
		// æˆåŠŸè§£æä¸ºmapï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‹è½½ä¿¡æ¯
		for _, info := range mapData {
			if info != nil {
				return info, nil
			}
		}
		return nil, errors.New("no valid download info found in map data")
	}

	// å°è¯•è§£æä¸ºarrayæ ¼å¼
	var arrayData []*OpenAPIDownloadInfo
	if err := json.Unmarshal(r.Data, &arrayData); err == nil {
		// æˆåŠŸè§£æä¸ºarrayï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„ä¸‹è½½ä¿¡æ¯
		for _, info := range arrayData {
			if info != nil {
				return info, nil
			}
		}
		return nil, errors.New("no valid download info found in array data")
	}

	// ä¸¤ç§æ ¼å¼éƒ½è§£æå¤±è´¥
	return nil, fmt.Errorf("unable to parse data field as either map or array: %s", string(r.Data))
}

// ------------------------------------------------------------
// Upload related structs
// ------------------------------------------------------------

// UploadBasicInfo (Traditional - /app/uploadinfo) - May become obsolete
type UploadBasicInfo struct {
	TraditionalBase
	Uploadinfo       string      `json:"uploadinfo,omitempty"`
	UserID           json.Number `json:"user_id,omitempty"`
	AppVersion       int         `json:"app_version,omitempty"`
	AppID            int         `json:"app_id,omitempty"`
	Userkey          string      `json:"userkey,omitempty"`
	SizeLimit        int64       `json:"size_limit,omitempty"`
	SizeLimitYun     int64       `json:"size_limit_yun,omitempty"`
	MaxDirLevel      int64       `json:"max_dir_level,omitempty"`
	MaxDirLevelYun   int64       `json:"max_dir_level_yun,omitempty"`
	MaxFileNum       int64       `json:"max_file_num,omitempty"`
	MaxFileNumYun    int64       `json:"max_file_num_yun,omitempty"`
	UploadAllowed    bool        `json:"upload_allowed,omitempty"`
	UploadAllowedMsg string      `json:"upload_allowed_msg,omitempty"`
}

// UploadInitInfo represents the response from upload init/resume (adaptable for both)
type UploadInitInfo struct {
	// Common Base - OpenAPI uses state/code/message, Traditional uses statuscode/statusmsg
	OpenAPIBase
	Request   string `json:"request,omitempty"`    // Traditional
	ErrorCode int    `json:"statuscode,omitempty"` // Traditional
	ErrorMsg  string `json:"statusmsg,omitempty"`  // Traditional

	// Data payload (nested under 'data' in OpenAPI)
	Data *UploadInitData `json:"data,omitempty"` // OpenAPI nests the main info

	// Traditional top-level fields (might be moved into Data for OpenAPI)
	Status   Int    `json:"status,omitempty"`   // Traditional: 1=need upload, 2=ç§’ä¼ ; OpenAPI: 1=non-ç§’ä¼ , 2=ç§’ä¼ , 6/7/8=auth
	PickCode string `json:"pickcode,omitempty"` // Traditional: pickcode; OpenAPI: pick_code
	Target   string `json:"target,omitempty"`   // Both
	Version  string `json:"version,omitempty"`  // Both?

	// OSS upload fields (Traditional top-level, OpenAPI in 'data')
	Bucket   string          `json:"bucket,omitempty"`   // Both
	Object   string          `json:"object,omitempty"`   // Both
	Callback json.RawMessage `json:"callback,omitempty"` // Both (structure might differ)

	// Useless fields (Traditional)
	FileID   int    `json:"fileid,omitempty"`
	FileInfo string `json:"fileinfo,omitempty"`

	// New fields in upload v4.0 / OpenAPI
	SignKey   string `json:"sign_key,omitempty"`   // Both
	SignCheck string `json:"sign_check,omitempty"` // Both
	FileIDStr string `json:"file_id,omitempty"`    // OpenAPI: file_id (for ç§’ä¼  success)

	// Raw data for custom UnmarshalJSON
	rawData json.RawMessage
}

// UnmarshalJSON handles custom unmarshaling for UploadInitInfo
func (ui *UploadInitInfo) UnmarshalJSON(data []byte) error {
	// Define an alias type to avoid infinite recursion
	type Alias UploadInitInfo
	aux := &struct {
		Data json.RawMessage `json:"data"`
		*Alias
	}{
		Alias: (*Alias)(ui),
	}

	// Unmarshal into the auxiliary struct
	if err := json.Unmarshal(data, aux); err != nil {
		return err
	}

	// Store raw data for later processing
	ui.rawData = aux.Data

	// Handle different formats for the data field
	if len(aux.Data) > 0 {
		// Try to unmarshal as an object first
		var objData UploadInitData
		if err := json.Unmarshal(aux.Data, &objData); err != nil {
			// If that fails, try as an array
			var arrData []map[string]any
			if err := json.Unmarshal(aux.Data, &arrData); err != nil {
				return fmt.Errorf("data field is neither a valid object nor an array: %w", err)
			}

			// If it's an array and has at least one element, use the first element
			if len(arrData) > 0 {
				// Convert the first array element back to JSON
				firstElem, err := json.Marshal(arrData[0])
				if err != nil {
					return fmt.Errorf("failed to marshal first array element: %w", err)
				}

				// Then unmarshal it into the objData
				if err := json.Unmarshal(firstElem, &objData); err != nil {
					return fmt.Errorf("failed to unmarshal first array element: %w", err)
				}

				ui.Data = &objData
			}
		} else {
			// It was a valid object
			ui.Data = &objData
		}
	}

	return nil
}

// UploadInitData holds the nested data part of the OpenAPI upload init/resume response
type UploadInitData struct {
	PickCode  string          `json:"pick_code"`         // Upload task ID
	Status    Int             `json:"status"`            // 1: non-ç§’ä¼ , 2: ç§’ä¼ , 6/7/8: auth needed
	SignKey   string          `json:"sign_key"`          // SHA1 ID for secondary auth
	SignCheck string          `json:"sign_check"`        // SHA1 range for secondary auth
	FileID    string          `json:"file_id"`           // File ID if ç§’ä¼  success (status=2)
	Target    string          `json:"target"`            // Upload target string
	Bucket    string          `json:"bucket"`            // OSS Bucket
	Object    string          `json:"object"`            // OSS Object ID
	Callback  json.RawMessage `json:"callback"`          // Can be either struct or array
	Version   string          `json:"version,omitempty"` // Optional version info
}

// GetCallback decodes and returns the callback string
func (ui *UploadInitInfo) GetCallback() string {
	// Get the raw callback data first
	var rawCallback string
	data := ui.Data
	if data == nil {
		// Fallback to traditional structure
		// Try to unmarshal as object first
		var callbackStruct struct {
			Callback    string `json:"callback"`
			CallbackVar string `json:"callback_var"`
		}

		if err := json.Unmarshal(ui.Callback, &callbackStruct); err == nil && callbackStruct.Callback != "" {
			rawCallback = callbackStruct.Callback
		} else {
			// Try to unmarshal as array if object failed
			var callbackArray []string
			if err := json.Unmarshal(ui.Callback, &callbackArray); err == nil && len(callbackArray) > 0 {
				rawCallback = callbackArray[0]
			} else {
				// Fall back to string representation if both fail
				rawCallback = string(ui.Callback)
			}
		}
	} else {
		// Try to unmarshal as object first
		var callbackStruct struct {
			Callback    string `json:"callback"`
			CallbackVar string `json:"callback_var"`
		}

		if err := json.Unmarshal(data.Callback, &callbackStruct); err == nil && callbackStruct.Callback != "" {
			rawCallback = callbackStruct.Callback
		} else {
			// Try to unmarshal as array if object failed
			var callbackArray []string
			if err := json.Unmarshal(data.Callback, &callbackArray); err == nil && len(callbackArray) > 0 {
				rawCallback = callbackArray[0]
			} else {
				// Fall back to string representation if both fail
				rawCallback = string(data.Callback)
			}
		}
	}

	// Check if the callback data is already base64 encoded
	if _, err := base64.StdEncoding.DecodeString(rawCallback); err != nil {
		// Not valid base64, so encode it
		return base64.StdEncoding.EncodeToString([]byte(rawCallback))
	}

	// Already base64 encoded, return as is
	return rawCallback
}

// GetCallbackVar decodes and returns the callback variables string
func (ui *UploadInitInfo) GetCallbackVar() string {
	// Get the raw callback var data first
	var rawCallbackVar string
	data := ui.Data
	if data == nil {
		// Fallback to traditional structure
		// Try to unmarshal as object first
		var callbackStruct struct {
			Callback    string `json:"callback"`
			CallbackVar string `json:"callback_var"`
		}

		if err := json.Unmarshal(ui.Callback, &callbackStruct); err == nil && callbackStruct.CallbackVar != "" {
			rawCallbackVar = callbackStruct.CallbackVar
		} else {
			// Try to unmarshal as array if object failed
			var callbackArray []string
			if err := json.Unmarshal(ui.Callback, &callbackArray); err == nil && len(callbackArray) > 1 {
				rawCallbackVar = callbackArray[1]
			} else {
				// No callback var found
				return ""
			}
		}
	} else {
		// Try to unmarshal as object first
		var callbackStruct struct {
			Callback    string `json:"callback"`
			CallbackVar string `json:"callback_var"`
		}

		if err := json.Unmarshal(data.Callback, &callbackStruct); err == nil && callbackStruct.CallbackVar != "" {
			rawCallbackVar = callbackStruct.CallbackVar
		} else {
			// Try to unmarshal as array if object failed
			var callbackArray []string
			if err := json.Unmarshal(data.Callback, &callbackArray); err == nil && len(callbackArray) > 1 {
				rawCallbackVar = callbackArray[1]
			} else {
				// No callback var found
				return ""
			}
		}
	}

	// If we have callback var data, check if it's already base64 encoded
	if rawCallbackVar != "" {
		if _, err := base64.StdEncoding.DecodeString(rawCallbackVar); err != nil {
			// Not valid base64, so encode it
			return base64.StdEncoding.EncodeToString([]byte(rawCallbackVar))
		}
	}

	// Already base64 encoded or empty, return as is
	return rawCallbackVar
}

// GetPickCode returns the pick code from the appropriate field
func (ui *UploadInitInfo) GetPickCode() string {
	if ui.Data != nil {
		return ui.Data.PickCode
	}
	return ui.PickCode
}

// GetStatus returns the status code from the appropriate field
func (ui *UploadInitInfo) GetStatus() Int {
	if ui.Data != nil {
		return ui.Data.Status
	}
	return ui.Status
}

// GetFileID returns the file ID (on ç§’ä¼ ) from the appropriate field
func (ui *UploadInitInfo) GetFileID() string {
	if ui.Data != nil {
		return ui.Data.FileID
	}
	return ui.FileIDStr
}

// GetSignKey returns the sign key from the appropriate field
func (ui *UploadInitInfo) GetSignKey() string {
	if ui.Data != nil {
		return ui.Data.SignKey
	}
	return ui.SignKey
}

// GetSignCheck returns the sign check range from the appropriate field
func (ui *UploadInitInfo) GetSignCheck() string {
	if ui.Data != nil {
		return ui.Data.SignCheck
	}
	return ui.SignCheck
}

// GetBucket returns the OSS bucket from the appropriate field
func (ui *UploadInitInfo) GetBucket() string {
	if ui.Data != nil {
		return ui.Data.Bucket
	}
	return ui.Bucket
}

// GetObject returns the OSS object key from the appropriate field
func (ui *UploadInitInfo) GetObject() string {
	if ui.Data != nil {
		return ui.Data.Object
	}
	return ui.Object
}

// CallbackInfo represents the structure of the callback response after OSS upload (Traditional)
type CallbackInfo struct {
	TraditionalBase
	Data *CallbackData `json:"data,omitempty"`
}

// CallbackData holds the details from the upload callback
type CallbackData struct {
	AID      json.Number `json:"aid"`
	CID      string      `json:"cid"`
	FileID   string      `json:"file_id"`
	FileName string      `json:"file_name"`
	FileSize Int64       `json:"file_size"`
	IsVideo  int         `json:"is_video"`
	PickCode string      `json:"pick_code"`
	Sha      string      `json:"sha1"`
	ThumbURL string      `json:"thumb_url,omitempty"`
}

// OSSToken represents the structure for OSS credentials (adaptable)
type OSSToken struct {
	AccessKeyID     string `json:"AccessKeyId"`     // OpenAPI uses AccessKeyId
	AccessKeySecret string `json:"AccessKeySecret"` // OpenAPI uses AccessKeySecrett (typo in docs?) -> Corrected to AccessKeySecret based on common usage
	Expiration      Time   `json:"Expiration"`      // OpenAPI uses Expiration (RFC3339 format)
	SecurityToken   string `json:"SecurityToken"`   // OpenAPI uses SecurityToken
	Endpoint        string `json:"endpoint"`        // OpenAPI provides endpoint

	// Traditional fields (might be redundant)
	StatusCode   string `json:"StatusCode,omitempty"`
	ErrorCode    string `json:"ErrorCode,omitempty"`
	ErrorMessage string `json:"ErrorMessage,omitempty"`
}

// OSSTokenResp represents the response from GET /open/upload/get_token
type OSSTokenResp struct {
	OpenAPIBase
	Data *OSSToken `json:"data"` // Assuming data holds a single OSSToken object
}

// TimeToExpiry calculates duration until token expiry
func (t *OSSToken) TimeToExpiry() time.Duration {
	if t == nil {
		return 0
	}
	exp := time.Time(t.Expiration)
	if exp.IsZero() {
		// Should not happen with OpenAPI tokens, but handle defensively
		return 3e9 * time.Second // ~95 years
	}
	// Use a safety margin (e.g., 5 minutes)
	return time.Until(exp) - 5*time.Minute
}

// ------------------------------------------------------------
// Sharing related structs (Assume Traditional API for now)
// ------------------------------------------------------------

// NewURL represents the response for adding offline tasks (Traditional)
type NewURL struct {
	State    bool   `json:"state,omitempty"`
	ErrorMsg string `json:"error_msg,omitempty"`
	Errno    int    `json:"errno,omitempty"`
	Result   []struct {
		State    bool   `json:"state,omitempty"`
		ErrorMsg string `json:"error_msg,omitempty"`
		Errno    int    `json:"errno,omitempty"`
		Errtype  string `json:"errtype,omitempty"`
		Errcode  int    `json:"errcode,omitempty"`
		InfoHash string `json:"info_hash,omitempty"`
		URL      string `json:"url,omitempty"`
		Files    []struct {
			ID   string `json:"id,omitempty"`
			Name string `json:"name,omitempty"`
			Size int64  `json:"size,omitempty"`
		} `json:"files,omitempty"`
	} `json:"result,omitempty"`
	Errcode int `json:"errcode,omitempty"`
}

// ShareSnap represents the response for listing shared files (Traditional)
type ShareSnap struct {
	TraditionalBase
	Data *ShareSnapData `json:"data,omitempty"`
}

type ShareSnapData struct {
	Userinfo struct {
		UserID   string `json:"user_id,omitempty"`
		UserName string `json:"user_name,omitempty"`
		Face     string `json:"face,omitempty"`
	} `json:"userinfo,omitempty"`
	Shareinfo struct {
		SnapID           string      `json:"snap_id,omitempty"`
		FileSize         string      `json:"file_size,omitempty"`
		ShareTitle       string      `json:"share_title,omitempty"`
		ShareState       json.Number `json:"share_state,omitempty"`
		ForbidReason     string      `json:"forbid_reason,omitempty"`
		CreateTime       string      `json:"create_time,omitempty"`
		ReceiveCode      string      `json:"receive_code,omitempty"`
		ReceiveCount     string      `json:"receive_count,omitempty"`
		ExpireTime       int         `json:"expire_time,omitempty"`
		FileCategory     int         `json:"file_category,omitempty"`
		AutoRenewal      string      `json:"auto_renewal,omitempty"`
		AutoFillRecvcode string      `json:"auto_fill_recvcode,omitempty"`
		CanReport        int         `json:"can_report,omitempty"`
		CanNotice        int         `json:"can_notice,omitempty"`
		HaveVioFile      int         `json:"have_vio_file,omitempty"`
	} `json:"shareinfo,omitempty"`
	Count      int         `json:"count,omitempty"`
	List       []*File     `json:"list,omitempty"` // Uses the common File struct
	ShareState json.Number `json:"share_state,omitempty"`
	UserAppeal struct {
		CanAppeal       int `json:"can_appeal,omitempty"`
		CanShareAppeal  int `json:"can_share_appeal,omitempty"`
		PopupAppealPage int `json:"popup_appeal_page,omitempty"`
		CanGlobalAppeal int `json:"can_global_appeal,omitempty"`
	} `json:"user_appeal,omitempty"`
}

// ShareDownloadInfo represents the response for getting download URL from share (Traditional)
type ShareDownloadInfo struct {
	FileID   string      `json:"fid"`
	FileName string      `json:"fn"`
	FileSize Int64       `json:"fs"`
	URL      DownloadURL `json:"url"`
}

// SampleInitResp represents the response from sampleinitupload.php (Traditional)
type SampleInitResp struct {
	Object    string `json:"object"`
	AccessID  string `json:"accessid"`
	Host      string `json:"host"`
	Policy    string `json:"policy"`
	Signature string `json:"signature"`
	Expire    int64  `json:"expire"`
	Callback  string `json:"callback"`
	ErrorCode int    `json:"errno,omitempty"`
	Error     string `json:"error,omitempty"`
}

// APIç±»å‹æšä¸¾
type APIType int

const (
	OpenAPI APIType = iota
	TraditionalAPI
)

// Constants
const (
	domain             = "www.115.com"
	traditionalRootURL = "https://webapi.115.com"
	openAPIRootURL     = "https://proapi.115.com"
	passportRootURL    = "https://passportapi.115.com"
	qrCodeAPIRootURL   = "https://qrcodeapi.115.com"
	hnQrCodeAPIRootURL = "https://hnqrcodeapi.115.com"     // For confirm step
	defaultAppID       = "100195123"                       // Provided App ID
	tradUserAgent      = "Mozilla/5.0 115Browser/27.0.7.5" // Keep for traditional login mimicry?
	defaultUserAgent   = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"

	// ğŸš¦ 115ç½‘ç›˜ç»Ÿä¸€QPSæ§åˆ¶ï¼šå…¨å±€è´¦æˆ·çº§åˆ«é™åˆ¶ï¼Œé¿å…770004é”™è¯¯
	unifiedMinSleep = fs.Duration(200 * time.Millisecond) // ~6.7 QPS - ä¼˜åŒ–ä¸‹è½½æ€§èƒ½

	maxSleep      = 2 * time.Second
	decayConstant = 2 // bigger for slower decay, exponential

	// é»˜è®¤é…ç½®å¸¸é‡ï¼ˆæ— éœ€ç”¨æˆ·é…ç½®ï¼‰
	defaultMaxRetryAttempts   = 5                             // é»˜è®¤æœ€å¤§é‡è¯•5æ¬¡
	defaultRetryBaseDelay     = fs.Duration(2 * time.Second)  // é»˜è®¤åŸºç¡€é‡è¯•å»¶è¿Ÿ2ç§’
	defaultSlowSpeedThreshold = fs.SizeSuffix(100 * 1024)     // é»˜è®¤æ…¢é€Ÿé˜ˆå€¼100KB/s
	defaultSpeedCheckInterval = fs.Duration(30 * time.Second) // é»˜è®¤é€Ÿåº¦æ£€æŸ¥é—´éš”30ç§’
	defaultSlowSpeedAction    = "log"                         // é»˜è®¤æ…¢é€Ÿå¤„ç†åŠ¨ä½œï¼šè®°å½•æ—¥å¿—
	defaultChunkTimeout       = fs.Duration(5 * time.Minute)  // é»˜è®¤åˆ†ç‰‡è¶…æ—¶5åˆ†é’Ÿ

	maxUploadSize       = 115 * fs.Gibi    // 115 GiB from https://proapi.115.com/app/uploadinfo (or OpenAPI equivalent)
	maxUploadParts      = 10000            // Part number must be an integer between 1 and 10000, inclusive.
	defaultChunkSize    = 20 * fs.Mebi     // Reference OpenList: set to 20MB, consistent with OpenList
	minChunkSize        = 100 * fs.Kibi    // æœ€å°åˆ†ç‰‡å¤§å°ï¼š100KB
	maxChunkSize        = 5 * fs.Gibi      // æœ€å¤§åˆ†ç‰‡å¤§å°ï¼š5GBï¼ˆOSSé™åˆ¶ï¼‰
	defaultUploadCutoff = 50 * fs.Mebi     // é»˜è®¤ä¸Šä¼ åˆ‡æ¢é˜ˆå€¼ï¼š50MB
	defaultNohashSize   = 100 * fs.Mebi    // æ— å“ˆå¸Œä¸Šä¼ é˜ˆå€¼ï¼š100MBï¼Œå°æ–‡ä»¶ä¼˜å…ˆä¼ ç»Ÿä¸Šä¼ 
	StreamUploadLimit   = 5 * fs.Gibi      // Max size for sample/streamed upload (traditional)
	maxUploadCutoff     = 5 * fs.Gibi      // maximum allowed size for singlepart uploads (OSS PutObject limit)
	tokenRefreshWindow  = 10 * time.Minute // Refresh token 10 minutes before expiry
	pkceVerifierLength  = 64               // Length for PKCE code verifier

	// Unified file size judgment constants, consistent with 123 drive
	memoryBufferThreshold = int64(50 * 1024 * 1024) // 50MB memory buffer threshold
)

// Register with Fs
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "115",
		Description: "115ç½‘ç›˜é©±åŠ¨å™¨ (æ”¯æŒå¼€æ”¾API)",
		NewFs:       NewFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name: "cookie",
			Help: `æä¾›æ ¼å¼ä¸º "UID=...; CID=...; SEID=...;" çš„ç™»å½•Cookieã€‚
åˆæ¬¡ç™»å½•è·å–APIä»¤ç‰Œæ—¶å¿…éœ€ã€‚
ç¤ºä¾‹: "UID=123; CID=abc; SEID=def;"`,
			Required:  true,
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Default:  defaultUserAgent,
			Advanced: true,
			Help: fmt.Sprintf(`HTTPç”¨æˆ·ä»£ç†ã€‚ä¸»è¦ç”¨äºåˆæ¬¡ç™»å½•æ¨¡æ‹Ÿã€‚
é»˜è®¤å€¼ä¸º "%s"ã€‚`, defaultUserAgent),
		}, {
			Name: "root_folder_id",
			Help: `æ ¹æ–‡ä»¶å¤¹çš„IDã€‚
é€šå¸¸ç•™ç©ºï¼ˆä½¿ç”¨ç½‘ç›˜æ ¹ç›®å½• '0'ï¼‰ã€‚
å¦‚éœ€è®©rcloneä½¿ç”¨éæ ¹æ–‡ä»¶å¤¹ä½œä¸ºèµ·å§‹ç‚¹ï¼Œè¯·å¡«å…¥ç›¸åº”IDã€‚`,
			Advanced:  true,
			Sensitive: true,
		}, {
			Name:     "app_id",
			Default:  defaultAppID,
			Advanced: true,
			Help: fmt.Sprintf(`ç”¨äºèº«ä»½éªŒè¯çš„è‡ªå®šä¹‰åº”ç”¨IDã€‚
é»˜è®¤å€¼ä¸º "%s"ã€‚é™¤éæœ‰ç‰¹å®šç†ç”±ä½¿ç”¨ä¸åŒçš„åº”ç”¨IDï¼Œå¦åˆ™ä¸è¦æ›´æ”¹ã€‚`, defaultAppID),
		}, {
			Name:     "upload_hash_only",
			Default:  false,
			Advanced: true,
			Help: `ä»…å°è¯•åŸºäºå“ˆå¸Œçš„ä¸Šä¼ ï¼ˆç§’ä¼ ï¼‰ã€‚å¦‚æœæœåŠ¡å™¨æ²¡æœ‰è¯¥æ–‡ä»¶åˆ™è·³è¿‡ä¸Šä¼ ã€‚
éœ€è¦SHA1å“ˆå¸Œå€¼å¯ç”¨æˆ–å¯è®¡ç®—ã€‚`,
		}, {
			Name:     "only_stream",
			Default:  false,
			Advanced: true,
			Help:     `å¯¹æ‰€æœ‰ä¸è¶…è¿‡5 GiBçš„æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿæµå¼ä¸Šä¼ ï¼ˆæ ·æœ¬ä¸Šä¼ ï¼‰ã€‚å¤§äºæ­¤å¤§å°çš„æ–‡ä»¶ä¼šå¤±è´¥ã€‚`,
		}, {
			Name:     "fast_upload",
			Default:  false,
			Advanced: true,
			Help: `ä¸Šä¼ ç­–ç•¥ï¼š
- æ–‡ä»¶ <= nohash_sizeï¼šä½¿ç”¨ä¼ ç»Ÿæµå¼ä¸Šä¼ ã€‚
- æ–‡ä»¶ > nohash_sizeï¼šå°è¯•åŸºäºå“ˆå¸Œçš„ä¸Šä¼ ï¼ˆç§’ä¼ ï¼‰ã€‚
- å¦‚æœç§’ä¼ å¤±è´¥ä¸”å¤§å° <= 5 GiBï¼šä½¿ç”¨ä¼ ç»Ÿæµå¼ä¸Šä¼ ã€‚
- å¦‚æœç§’ä¼ å¤±è´¥ä¸”å¤§å° > 5 GiBï¼šä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ã€‚`,
		}, {
			Name:     "hash_memory_limit",
			Help:     "å¤§äºæ­¤å¤§å°çš„æ–‡ä»¶å°†ç¼“å­˜åˆ°ç£ç›˜ä»¥è®¡ç®—å“ˆå¸Œå€¼ï¼ˆå¦‚éœ€è¦ï¼‰ã€‚",
			Default:  fs.SizeSuffix(10 * 1024 * 1024),
			Advanced: true,
		}, {
			Name:     "nohash_size",
			Help:     `å°äºæ­¤å¤§å°çš„æ–‡ä»¶åœ¨å¯ç”¨fast_uploadæˆ–æœªå°è¯•/å¤±è´¥å“ˆå¸Œä¸Šä¼ æ—¶å°†ä½¿ç”¨ä¼ ç»Ÿæµå¼ä¸Šä¼ ã€‚æœ€å¤§å€¼ä¸º5 GiBã€‚`,
			Default:  defaultNohashSize,
			Advanced: true,
		}, {
			Name: "upload_cutoff",
			Help: `åˆ‡æ¢åˆ°åˆ†ç‰‡ä¸Šä¼ çš„æ–‡ä»¶å¤§å°é˜ˆå€¼ã€‚

å¤§äºæ­¤å¤§å°çš„æ–‡ä»¶å°†ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ã€‚å°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†ä½¿ç”¨OSS PutObjectå•æ–‡ä»¶ä¸Šä¼ ã€‚

æœ€å°å€¼ä¸º0ï¼Œæœ€å¤§å€¼ä¸º5 GiBã€‚`,
			Default:  defaultUploadCutoff,
			Advanced: true,
		}, {
			Name: "chunk_size",
			Help: `åˆ†ç‰‡ä¸Šä¼ æ—¶çš„åˆ†ç‰‡å¤§å°ã€‚

å½“ä¸Šä¼ å¤§äºupload_cutoffçš„æ–‡ä»¶æˆ–æœªçŸ¥å¤§å°çš„æ–‡ä»¶æ—¶ï¼Œå°†ä½¿ç”¨æ­¤åˆ†ç‰‡å¤§å°è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ã€‚

æ³¨æ„ï¼šæ¯ä¸ªä¼ è¾“ä¼šåœ¨å†…å­˜ä¸­ç¼“å†²æ­¤å¤§å°çš„æ•°æ®å—ã€‚

æœ€å°å€¼ä¸º100 KiBï¼Œæœ€å¤§å€¼ä¸º5 GiBã€‚`,
			Default:  defaultChunkSize,
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     `åˆ†ç‰‡ä¸Šä¼ ä¸­çš„æœ€å¤§åˆ†ç‰‡æ•°é‡ã€‚`,
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name:     "internal",
			Help:     `ä½¿ç”¨å†…éƒ¨OSSç«¯ç‚¹è¿›è¡Œä¸Šä¼ ï¼ˆéœ€è¦é€‚å½“çš„ç½‘ç»œè®¿é—®æƒé™ï¼‰ã€‚`,
			Default:  false,
			Advanced: true,
		}, {
			Name:     "dual_stack",
			Help:     `ä½¿ç”¨åŒæ ˆï¼ˆIPv4/IPv6ï¼‰OSSç«¯ç‚¹è¿›è¡Œä¸Šä¼ ã€‚`,
			Default:  false,
			Advanced: true,
		}, {
			Name:     "no_check",
			Default:  false,
			Advanced: true,
			Help:     "ç¦ç”¨ä¸Šä¼ åæ£€æŸ¥ï¼ˆé¿å…é¢å¤–APIè°ƒç”¨ä½†é™ä½ç¡®å®šæ€§ï¼‰ã€‚",
		}, {
			Name:     "no_buffer",
			Default:  false,
			Advanced: true,
			Help:     "è·³è¿‡ä¸Šä¼ æ—¶çš„ç£ç›˜ç¼“å†²ã€‚",
		}, {
			Name: "stream_hash_mode",
			Help: `å¯ç”¨æµå¼å“ˆå¸Œè®¡ç®—æ¨¡å¼ï¼Œç”¨äºè·¨äº‘ä¼ è¾“æ—¶èŠ‚çœæœ¬åœ°å­˜å‚¨ç©ºé—´ã€‚
- false: ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
- true: å…ˆæµå¼è®¡ç®—æ–‡ä»¶å“ˆå¸Œå°è¯•ç§’ä¼ ï¼Œå¤±è´¥åå†æµå¼ä¸Šä¼ 
é€‚ç”¨äºæœ¬åœ°å­˜å‚¨ç©ºé—´æœ‰é™çš„ç¯å¢ƒã€‚`,
			Default:  false,
			Advanced: true,
		}, {
			Name:     "download_concurrency",
			Help:     `å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°ã€‚115ç½‘ç›˜é™åˆ¶æ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªå¹¶å‘è¿æ¥ï¼Œå»ºè®®è®¾ç½®ä¸º2ã€‚è®¾ç½®ä¸º0ç¦ç”¨å¹¶å‘ä¸‹è½½ã€‚`,
			Default:  2, // 115ç½‘ç›˜é™åˆ¶ï¼šæ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªå¹¶å‘è¿æ¥
			Advanced: true,
		}, {
			Name:     "download_chunk_size",
			Help:     `ä¸‹è½½åˆ†ç‰‡å¤§å°ã€‚ä»…åœ¨å¯ç”¨å¹¶å‘ä¸‹è½½æ—¶æœ‰æ•ˆã€‚å»ºè®®32-64MBã€‚`,
			Default:  fs.SizeSuffix(64 * 1024 * 1024), // 64MBï¼Œé€‚åˆ2å¹¶å‘
			Advanced: true,
		}, {
			Name:     config.ConfigEncoding,
			Help:     config.ConfigEncodingHelp,
			Advanced: true,
			// é»˜è®¤åŒ…å«Slashå’ŒInvalidUtf8ç¼–ç ï¼Œé€‚é…ä¸­æ–‡æ–‡ä»¶åå’Œç‰¹æ®Šå­—ç¬¦
			Default: (encoder.EncodeLtGt |
				encoder.EncodeDoubleQuote |
				encoder.EncodeLeftSpace |
				encoder.EncodeCtl |
				encoder.EncodeRightSpace |
				encoder.EncodeSlash | // æ–°å¢ï¼šé»˜è®¤ç¼–ç æ–œæ 
				encoder.EncodeInvalidUtf8), // ä¿ç•™ï¼šç¼–ç æ— æ•ˆUTF-8å­—ç¬¦
		}},
	})
}

type Options struct {
	Cookie              string        `config:"cookie"` // Single cookie string now
	UserAgent           string        `config:"user_agent"`
	RootFolderID        string        `config:"root_folder_id"`
	HashMemoryThreshold fs.SizeSuffix `config:"hash_memory_limit"`
	UploadHashOnly      bool          `config:"upload_hash_only"`
	OnlyStream          bool          `config:"only_stream"`
	FastUpload          bool          `config:"fast_upload"`
	NohashSize          fs.SizeSuffix `config:"nohash_size"`
	UploadCutoff        fs.SizeSuffix `config:"upload_cutoff"`
	ChunkSize           fs.SizeSuffix `config:"chunk_size"`
	MaxUploadParts      int           `config:"max_upload_parts"`
	StreamHashMode      bool          `config:"stream_hash_mode"`

	Internal  bool                 `config:"internal"`
	DualStack bool                 `config:"dual_stack"`
	NoCheck   bool                 `config:"no_check"`
	NoBuffer  bool                 `config:"no_buffer"` // Skip disk buffering for uploads
	Enc       encoder.MultiEncoder `config:"encoding"`
	AppID     string               `config:"app_id"` // Custom App ID for authentication

	// APIé™æµæ§åˆ¶é€‰é¡¹
	QPSLimit fs.Duration `config:"qps_limit"` // APIè°ƒç”¨é—´éš”æ—¶é—´ (ä¾‹å¦‚: 250ms = 4 QPS)

	// ä¸‹è½½æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
	DownloadConcurrency int           `config:"download_concurrency"` // å¹¶å‘ä¸‹è½½çº¿ç¨‹æ•°
	DownloadChunkSize   fs.SizeSuffix `config:"download_chunk_size"`  // ä¸‹è½½åˆ†ç‰‡å¤§å°
}

// TransferSpeedMonitor ä¼ è¾“é€Ÿåº¦ç›‘æ§å™¨
type TransferSpeedMonitor struct {
	name           string              // ä¼ è¾“åç§°
	startTime      time.Time           // ä¼ è¾“å¼€å§‹æ—¶é—´
	lastCheckTime  time.Time           // ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
	lastBytes      int64               // ä¸Šæ¬¡æ£€æŸ¥æ—¶çš„å­—èŠ‚æ•°
	slowSpeedCount int                 // è¿ç»­æ…¢é€Ÿæ£€æµ‹æ¬¡æ•°
	account        *accounting.Account // å…³è”çš„Accountå¯¹è±¡
	fs             *Fs                 // æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	ctx            context.Context     // ä¼ è¾“ä¸Šä¸‹æ–‡
	cancel         context.CancelFunc  // å–æ¶ˆå‡½æ•°
	done           chan struct{}       // å®Œæˆä¿¡å·
	mu             sync.Mutex          // ä¿æŠ¤å¹¶å‘è®¿é—®
}

// Fs represents a remote 115 drive
type Fs struct {
	name          string
	originalName  string // Original config name without modifications
	root          string
	opt           Options
	features      *fs.Features
	tradClient    *rest.Client // Client for traditional (cookie, encrypted) API calls
	openAPIClient *rest.Client // Client for OpenAPI (token) calls
	dirCache      *dircache.DirCache
	pacer         *fs.Pacer // ç»Ÿä¸€çš„APIè°ƒé€Ÿå™¨ï¼Œç¬¦åˆrcloneæ ‡å‡†æ¨¡å¼
	rootFolder    string    // path of the absolute root
	rootFolderID  string
	appVer        string // parsed from user-agent; used in traditional calls
	userID        string // User ID from cookie/token
	userkey       string // User key from traditional uploadinfo (needed for traditional upload init signature)
	isShare       bool   // mark it is from shared or not
	fileObj       *fs.Object
	m             configmap.Mapper // config map for saving tokens
	tokenMu       sync.Mutex
	accessToken   string
	refreshToken  string
	tokenExpiry   time.Time
	codeVerifier  string // For PKCE
	tokenRenewer  *oauthutil.Renew
	isRefreshing  atomic.Bool // é˜²æ­¢é€’å½’è°ƒç”¨çš„æ ‡å¿—
	loginMu       sync.Mutex
	uploadingMu   sync.Mutex
	isUploading   bool

	// APIé™æµæ™ºèƒ½æ§åˆ¶
	apiLimitMu          sync.Mutex
	consecutiveAPILimit int       // è¿ç»­APIé™æµé”™è¯¯è®¡æ•°
	lastAPILimitTime    time.Time // æœ€åä¸€æ¬¡APIé™æµé”™è¯¯æ—¶é—´
	currentQPSLimit     float64   // å½“å‰åŠ¨æ€QPSé™åˆ¶

	// ä¼ è¾“é€Ÿåº¦ç›‘æ§
	speedMonitorMu  sync.Mutex
	activeTransfers map[string]*TransferSpeedMonitor // æ´»è·ƒä¼ è¾“çš„é€Ÿåº¦ç›‘æ§å™¨
}

// NewTransferSpeedMonitor åˆ›å»ºæ–°çš„ä¼ è¾“é€Ÿåº¦ç›‘æ§å™¨
func NewTransferSpeedMonitor(name string, account *accounting.Account, fs *Fs, ctx context.Context) *TransferSpeedMonitor {
	monitorCtx, cancel := context.WithCancel(ctx)

	monitor := &TransferSpeedMonitor{
		name:          name,
		startTime:     time.Now(),
		lastCheckTime: time.Now(),
		lastBytes:     0,
		account:       account,
		fs:            fs,
		ctx:           monitorCtx,
		cancel:        cancel,
		done:          make(chan struct{}),
	}

	// å¯åŠ¨ç›‘æ§goroutine
	go monitor.monitorLoop()

	return monitor
}

// monitorLoop ç›‘æ§å¾ªç¯ï¼Œå®šæœŸæ£€æŸ¥ä¼ è¾“é€Ÿåº¦
func (m *TransferSpeedMonitor) monitorLoop() {
	defer close(m.done)

	// ä¼ è¾“é€Ÿåº¦ç›‘æ§é»˜è®¤å¯ç”¨ï¼Œæ— éœ€æ£€æŸ¥é…ç½®

	ticker := time.NewTicker(time.Duration(defaultSpeedCheckInterval))
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			m.checkSpeed()
		case <-m.ctx.Done():
			return
		}
	}
}

// checkSpeed æ£€æŸ¥å½“å‰ä¼ è¾“é€Ÿåº¦
func (m *TransferSpeedMonitor) checkSpeed() {
	m.mu.Lock()
	defer m.mu.Unlock()

	if m.account == nil {
		return
	}

	// ç”±äºAccountçš„å¾ˆå¤šæ–¹æ³•éƒ½æ˜¯ç§æœ‰çš„ï¼Œæˆ‘ä»¬é‡‡ç”¨ç®€åŒ–çš„æ–¹æ³•
	// é€šè¿‡åå°„æˆ–è€…å…¶ä»–æ–¹å¼æ¥è·å–é€Ÿåº¦ä¿¡æ¯æ¯”è¾ƒå¤æ‚ï¼Œ
	// è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼šæ£€æŸ¥ä¼ è¾“æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´å†…æœ‰è¿›å±•

	now := time.Now()
	threshold := float64(defaultSlowSpeedThreshold)

	// æ£€æŸ¥ä¼ è¾“æ˜¯å¦é•¿æ—¶é—´æ²¡æœ‰è¿›å±•
	// è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°ï¼Œä¸»è¦æ£€æµ‹ä¼ è¾“æ˜¯å¦å¡ä½
	timeSinceLastCheck := now.Sub(m.lastCheckTime)

	// å¦‚æœæ£€æŸ¥é—´éš”è¿‡çŸ­ï¼Œè·³è¿‡
	if timeSinceLastCheck < time.Duration(defaultSpeedCheckInterval)/2 {
		return
	}

	// ç®€åŒ–çš„æ…¢é€Ÿæ£€æµ‹ï¼šå¦‚æœä¼ è¾“æ—¶é—´è¿‡é•¿ï¼Œè®¤ä¸ºå¯èƒ½å­˜åœ¨é—®é¢˜
	totalTime := now.Sub(m.startTime)
	if totalTime > 10*time.Minute { // å¦‚æœä¼ è¾“è¶…è¿‡10åˆ†é’Ÿ
		m.slowSpeedCount++
		fs.Debugf(m.fs, "ğŸ“‰ æ£€æµ‹åˆ°é•¿æ—¶é—´ä¼ è¾“: %s, å·²ç”¨æ—¶=%v, è¿ç»­æ£€æµ‹æ¬¡æ•°=%d",
			m.name, totalTime.Truncate(time.Second), m.slowSpeedCount)

		// æ ¹æ®é…ç½®çš„åŠ¨ä½œå¤„ç†æ…¢é€Ÿä¼ è¾“
		m.handleSlowSpeed(0, threshold) // ä¼ å…¥0ä½œä¸ºå½“å‰é€Ÿåº¦
	} else if m.slowSpeedCount > 0 {
		// ä¼ è¾“æ—¶é—´æ­£å¸¸ï¼Œé‡ç½®è®¡æ•°å™¨
		fs.Debugf(m.fs, "ğŸ“ˆ ä¼ è¾“æ—¶é—´æ­£å¸¸: %s, å·²ç”¨æ—¶=%v",
			m.name, totalTime.Truncate(time.Second))
		m.slowSpeedCount = 0
	}

	// æ›´æ–°æ£€æŸ¥æ—¶é—´
	m.lastCheckTime = now
}

// handleSlowSpeed å¤„ç†æ…¢é€Ÿä¼ è¾“
func (m *TransferSpeedMonitor) handleSlowSpeed(currentSpeed, threshold float64) {
	switch defaultSlowSpeedAction {
	case "log":
		// åªè®°å½•æ—¥å¿—ï¼Œä¸é‡‡å–å…¶ä»–åŠ¨ä½œ
		if m.slowSpeedCount == 1 {
			fs.Infof(m.fs, "âš ï¸ ä¼ è¾“é€Ÿåº¦è¿‡æ…¢: %s, å½“å‰é€Ÿåº¦=%.2f KB/s, é˜ˆå€¼=%.2f KB/s",
				m.name, currentSpeed/1024, threshold/1024)
		}

	case "abort":
		// è¿ç»­3æ¬¡æ…¢é€Ÿåä¸­æ­¢ä¼ è¾“
		if m.slowSpeedCount >= 3 {
			fs.Errorf(m.fs, "âŒ ä¼ è¾“é€Ÿåº¦æŒç»­è¿‡æ…¢ï¼Œä¸­æ­¢ä¼ è¾“: %s, é€Ÿåº¦=%.2f KB/s",
				m.name, currentSpeed/1024)
			m.cancel() // å–æ¶ˆä¼ è¾“ä¸Šä¸‹æ–‡
		}

	case "retry":
		// è¿ç»­5æ¬¡æ…¢é€Ÿåæ ‡è®°éœ€è¦é‡è¯•ï¼ˆç”±ä¸Šå±‚å¤„ç†ï¼‰
		if m.slowSpeedCount >= 5 {
			fs.Logf(m.fs, "ğŸ”„ ä¼ è¾“é€Ÿåº¦æŒç»­è¿‡æ…¢ï¼Œå»ºè®®é‡è¯•: %s, é€Ÿåº¦=%.2f KB/s",
				m.name, currentSpeed/1024)
			// è¿™é‡Œå¯ä»¥è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œç”±ä¸Šå±‚ä»£ç æ£€æŸ¥å¹¶å¤„ç†é‡è¯•
		}
	}
}

// Stop åœæ­¢ç›‘æ§
func (m *TransferSpeedMonitor) Stop() {
	m.cancel()
	<-m.done // ç­‰å¾…ç›‘æ§goroutineç»“æŸ
}

// startTransferMonitor ä¸ºä¼ è¾“å¯åŠ¨é€Ÿåº¦ç›‘æ§
func (f *Fs) startTransferMonitor(name string, account *accounting.Account, ctx context.Context) *TransferSpeedMonitor {
	// ä¼ è¾“é€Ÿåº¦ç›‘æ§é»˜è®¤å¯ç”¨

	f.speedMonitorMu.Lock()
	defer f.speedMonitorMu.Unlock()

	// åˆ›å»ºæ–°çš„ç›‘æ§å™¨
	monitor := NewTransferSpeedMonitor(name, account, f, ctx)

	// æ·»åŠ åˆ°æ´»è·ƒä¼ è¾“åˆ—è¡¨
	f.activeTransfers[name] = monitor

	fs.Debugf(f, "ğŸ“Š å¯åŠ¨ä¼ è¾“é€Ÿåº¦ç›‘æ§: %s", name)
	return monitor
}

// stopTransferMonitor åœæ­¢ä¼ è¾“é€Ÿåº¦ç›‘æ§
func (f *Fs) stopTransferMonitor(name string) {
	f.speedMonitorMu.Lock()
	defer f.speedMonitorMu.Unlock()

	if monitor, exists := f.activeTransfers[name]; exists {
		monitor.Stop()
		delete(f.activeTransfers, name)
		fs.Debugf(f, "ğŸ“Š åœæ­¢ä¼ è¾“é€Ÿåº¦ç›‘æ§: %s", name)
	}
}

// Object describes a 115 object
type Object struct {
	fs             *Fs
	remote         string
	hasMetaData    bool
	id             string
	parent         string
	size           int64
	sha1sum        string
	pickCode       string
	modTime        time.Time
	durl           *DownloadURL // link to download the object
	durlMu         *sync.Mutex
	durlRefreshing bool       // æ ‡è®°æ˜¯å¦æ­£åœ¨åˆ·æ–°URLï¼Œé˜²æ­¢å¹¶å‘åˆ·æ–°
	pickCodeMu     sync.Mutex // æ–°å¢ï¼šä¿æŠ¤pickCodeè·å–çš„å¹¶å‘è®¿é—®
}

type ApiResponse struct {
	State   bool                `json:"state"`   // Indicates success or failure
	Message string              `json:"message"` // Optional message
	Code    int                 `json:"code"`    // Status code
	Data    map[string]FileInfo `json:"data"`    // Map where keys are file IDs (strings) and values are FileInfo objects
}

// retryErrorCodes is a slice of HTTP status codes that we will retry
var retryErrorCodes = []int{
	429, // Too Many Requests.
	500, // Internal Server Error
	502, // Bad Gateway
	503, // Service Unavailable
	504, // Gateway Timeout
	509, // Bandwidth Limit Exceeded
}

// shouldRetry checks if a request should be retried based on the response, error, and API type.
// Use unified error classification and retry strategy
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	if fserrors.ContextError(ctx, &err) {
		return false, err
	}

	// Check HTTP response status codes first
	if resp != nil {
		switch resp.StatusCode {
		case 429: // Too Many Requests
			fs.Debugf(nil, "ğŸ”„ HTTP 429 è¯·æ±‚è¿‡å¤šï¼Œå»¶è¿Ÿé‡è¯•")
			return true, pacer.RetryAfterError(err, 15*time.Second)
		case 500, 502, 503, 504: // Server errors
			fs.Debugf(nil, "âŒ æœåŠ¡å™¨é”™è¯¯ %dï¼Œé‡è¯•ä¸­: %v", resp.StatusCode, err)
			return true, pacer.RetryAfterError(err, 5*time.Second)
		case 408: // Request Timeout
			fs.Debugf(nil, "â° è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­: %v", err)
			return true, pacer.RetryAfterError(err, 3*time.Second)
		}
	}

	// Use rclone standard error handling
	if err != nil {
		if fserrors.ShouldRetry(err) {
			return true, err
		}
	}

	// Use rclone standard HTTP status code retry logic
	return fserrors.ShouldRetryHTTP(resp, retryErrorCodes), err
}

// Credential holds the parsed cookie values needed for initial login
type Credential struct {
	UID  string
	CID  string
	SEID string
	KID  string // Keep KID as it might be used implicitly by the web API calls
}

// Valid reports whether the credential is valid.
func (cr *Credential) Valid() error {
	if cr == nil {
		return errors.New("nil credential")
	}
	// KID is optional/sometimes empty, SEID seems required for login mimicry
	if cr.UID == "" || cr.CID == "" || cr.SEID == "" {
		return errors.New("missing UID, CID, or SEID in cookie")
	}
	return nil
}

// FromCookie loads credential from cookie string
func (cr *Credential) FromCookie(cookieStr string) *Credential {
	for _, item := range strings.Split(cookieStr, ";") {
		kv := strings.SplitN(strings.TrimSpace(item), "=", 2)
		if len(kv) != 2 {
			continue
		}
		key := strings.TrimSpace(strings.ToUpper(kv[0]))
		val := strings.TrimSpace(kv[1])
		switch key {
		case "UID":
			cr.UID = val
		case "CID":
			cr.CID = val
		case "SEID":
			cr.SEID = val
		case "KID":
			cr.KID = val
		}
	}
	return cr
}

// Cookie turns the credential into a list of http cookie for the traditional client
func (cr *Credential) Cookie() []*http.Cookie {
	cookies := []*http.Cookie{
		{Name: "UID", Value: cr.UID, Domain: domain, Path: "/", HttpOnly: true},
		{Name: "CID", Value: cr.CID, Domain: domain, Path: "/", HttpOnly: true},
		{Name: "SEID", Value: cr.SEID, Domain: domain, Path: "/", HttpOnly: true},
	}
	// Add KID only if it's present
	if cr.KID != "" {
		cookies = append(cookies, &http.Cookie{Name: "KID", Value: cr.KID, Domain: domain, Path: "/", HttpOnly: true})
	}
	return cookies
}

// UserID parses userID from UID field
func (cr *Credential) UserID() string {
	userID, _, _ := strings.Cut(cr.UID, "_")
	return userID
}

// getOpenAPIHTTPClient creates an HTTP client with default UserAgent
func getOpenAPIHTTPClient(ctx context.Context) *http.Client {
	// Create a new context with the default UserAgent
	newCtx, ci := fs.AddConfig(ctx)
	ci.UserAgent = defaultUserAgent
	return fshttp.NewClient(newCtx)
}

// errorHandler parses a non 2xx error response into an error (Generic, might need adjustment per API)
func errorHandler(resp *http.Response) error {
	// Attempt to decode as OpenAPI error first
	openAPIErr := new(OpenAPIBase)
	bodyBytes, readErr := rest.ReadBody(resp) // Read body once
	if readErr != nil {
		fs.Debugf(nil, "âŒ æ— æ³•è¯»å–é”™è¯¯å“åº”ä½“: %v", readErr)
		// Fallback to status code if body read fails
		return NewTokenError(fmt.Sprintf("HTTP error %d (%s)", resp.StatusCode, resp.Status))
	}

	decodeErr := json.Unmarshal(bodyBytes, &openAPIErr)
	if decodeErr == nil && !openAPIErr.State {
		// Successfully decoded as OpenAPI error
		err := openAPIErr.Err()
		// Check for specific token-related errors
		if openAPIErr.ErrCode() == 401 || openAPIErr.ErrCode() == 100001 || strings.Contains(openAPIErr.ErrMsg(), "token") { // Example codes
			return NewTokenError(err.Error(), true) // Assume token error needs refresh/relogin
		}
		return err
	}

	// Attempt to decode as Traditional error
	tradErr := new(TraditionalBase)
	decodeErr = json.Unmarshal(bodyBytes, &tradErr)
	if decodeErr == nil && !tradErr.State {
		// Successfully decoded as Traditional error
		return tradErr.Err()
	}

	// Fallback if JSON decoding fails or state is true (but status code != 2xx)
	fs.Debugf(nil, "âŒ æ— æ³•è§£ç é”™è¯¯å“åº”: %v. å“åº”ä½“: %s", decodeErr, string(bodyBytes))
	return NewTokenError(fmt.Sprintf("HTTP error %d (%s): %s", resp.StatusCode, resp.Status, string(bodyBytes)))
}

// generatePKCE generates a code_verifier and code_challenge
func generatePKCE() (verifier, challenge string, err error) {
	verifierBytes := make([]byte, pkceVerifierLength)
	_, err = rand.Read(verifierBytes)
	if err != nil {
		return "", "", fmt.Errorf("failed to generate random verifier: %w", err)
	}
	// Use URL-safe base64 encoding without padding
	verifier = base64.RawURLEncoding.EncodeToString(verifierBytes)
	// Calculate SHA256 hash
	hash := sha256.Sum256([]byte(verifier))
	// Base64 encode the hash
	challenge = base64.RawURLEncoding.EncodeToString(hash[:])
	return verifier, challenge, nil
}

// login performs the initial authentication flow to get tokens
func (f *Fs) login(ctx context.Context) error {
	// Use a global mutex for login to prevent multiple concurrent login attempts
	// which could overwhelm the API and cause authentication failures
	f.loginMu.Lock()
	defer f.loginMu.Unlock()

	// Check if login is needed (avoid nested locking)
	needLogin := func() bool {
		f.tokenMu.Lock()
		defer f.tokenMu.Unlock()
		return f.accessToken == "" || time.Now().After(f.tokenExpiry)
	}()

	if !needLogin {
		fs.Debugf(f, "âœ… ç­‰å¾…ç™»å½•äº’æ–¥é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡ç™»å½•")
		return nil
	}

	// Parse cookie and setup clients
	if err := f.setupLoginEnvironment(ctx); err != nil {
		return err
	}

	fs.Debugf(f, "ğŸ” å¼€å§‹ç”¨æˆ·ç™»å½•æµç¨‹: %s", f.userID)

	// Generate PKCE
	var challenge string
	var err error
	if f.codeVerifier, challenge, err = generatePKCE(); err != nil {
		return fmt.Errorf("failed to generate PKCE codes: %w", err)
	}
	fs.Debugf(f, "ğŸ”‘ ç”ŸæˆPKCEæŒ‘æˆ˜ç æˆåŠŸ")

	// Request device code
	loginUID, err := f.getAuthDeviceCode(ctx, challenge)
	if err != nil {
		return err
	}

	// Mimic QR scan confirmation
	if err := f.simulateQRCodeScan(ctx, loginUID); err != nil {
		// Only log errors from these steps, don't fail
		fs.Logf(f, "âš ï¸ äºŒç»´ç æ‰«ææ¨¡æ‹Ÿæ­¥éª¤æœ‰é”™è¯¯ï¼ˆç»§ç»­æ‰§è¡Œï¼‰: %v", err)
	}

	// Exchange device code for access token
	if err := f.exchangeDeviceCodeForToken(ctx, loginUID); err != nil {
		return err
	}

	// Get userkey for traditional uploads if needed
	if f.userkey == "" {
		fs.Debugf(f, "ğŸ“¤ ä½¿ç”¨ä¼ ç»ŸAPIè·å–userkey...")
		if err := f.getUploadBasicInfo(ctx); err != nil {
			// Log error but don't fail login, userkey is only for traditional upload init
			fs.Logf(f, "âš ï¸ è·å–userkeyå¤±è´¥ï¼ˆæŸäº›ä¼ ç»Ÿä¸Šä¼ éœ€è¦ï¼‰: %v", err)
		} else {
			fs.Debugf(f, "âœ… æˆåŠŸè·å–userkey")
		}
	}

	return nil
}

// setupLoginEnvironment parses cookies and sets up HTTP clients
func (f *Fs) setupLoginEnvironment(ctx context.Context) error {
	// Parse cookie
	cred := (&Credential{}).FromCookie(f.opt.Cookie)
	if err := cred.Valid(); err != nil {
		return fmt.Errorf("invalid cookie provided: %w", err)
	}
	f.userID = cred.UserID() // Set userID early

	// Setup clients (needed for the login calls)
	// åˆ›å»ºOpenAPIå®¢æˆ·ç«¯
	httpClient := getOpenAPIHTTPClient(ctx)

	// OpenAPIå®¢æˆ·ç«¯ï¼Œç”¨äºtokenç›¸å…³æ“ä½œ
	f.openAPIClient = rest.NewClient(httpClient).SetErrorHandler(errorHandler)

	return nil
}

// getAuthDeviceCode calls the authDeviceCode API to start the login process
func (f *Fs) getAuthDeviceCode(ctx context.Context, challenge string) (string, error) {
	// Use configured AppID if provided, otherwise use default
	clientID := f.opt.AppID

	authData := url.Values{
		"client_id":             []string{clientID},
		"code_challenge":        []string{challenge},
		"code_challenge_method": []string{"sha256"}, // Use SHA256
	}
	authOpts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL, // Use passport API domain
		Path:         "/open/authDeviceCode",
		Parameters:   authData, // Send as query parameters for POST? Docs say body, let's try body.
		Body:         strings.NewReader(authData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}
	var authResp AuthDeviceCodeResp
	// This initial call uses the traditional client *but doesn't need encryption*
	// It still needs the cookie and pacing.
	err := f.CallTraditionalAPI(ctx, &authOpts, nil, &authResp, true) // Pass skipEncrypt=true
	if err != nil {
		return "", fmt.Errorf("authDeviceCode failed: %w", err)
	}
	if authResp.Data == nil || authResp.Data.UID == "" {
		return "", fmt.Errorf("authDeviceCode returned empty data: %v", authResp)
	}
	loginUID := authResp.Data.UID
	fs.Debugf(f, "âœ… è®¾å¤‡ç è®¤è¯æˆåŠŸï¼Œç™»å½•UID: %s", loginUID)
	return loginUID, nil
}

// simulateQRCodeScan mimics the QR code scan and confirm process
func (f *Fs) simulateQRCodeScan(ctx context.Context, loginUID string) error {
	// Call QR scan API
	scanErr := f.callQRScanAPI(ctx, loginUID)

	// Call QR confirm API - still proceed if scan had an error
	confirmErr := f.callQRConfirmAPI(ctx, loginUID)

	// Add a small delay after mimic steps, just in case
	time.Sleep(1 * time.Second)

	// Return an error if both steps failed, otherwise continue
	if scanErr != nil && confirmErr != nil {
		return fmt.Errorf("both scan and confirm steps failed: scan: %v, confirm: %v", scanErr, confirmErr)
	}
	return nil
}

// callQRScanAPI calls the QR scan API
func (f *Fs) callQRScanAPI(ctx context.Context, loginUID string) error {
	scanPayload := map[string]string{"uid": loginUID}
	scanOpts := rest.Opts{
		Method:     "GET",
		RootURL:    qrCodeAPIRootURL,
		Path:       "/api/2.0/prompt.php",
		Parameters: url.Values{"uid": []string{loginUID}}, // Send as query params
	}
	var scanResp TraditionalBase // Use base struct, don't care about response data much
	fs.Debugf(f, "ğŸ“± è°ƒç”¨äºŒç»´ç æ‰«ææ¥å£...")
	err := f.CallTraditionalAPI(ctx, &scanOpts, scanPayload, &scanResp, true)
	if err != nil {
		return fmt.Errorf("login_qrcode_scan failed: %w", err)
	}
	fs.Debugf(f, "âœ… äºŒç»´ç æ‰«æè°ƒç”¨æˆåŠŸ (çŠ¶æ€: %v)", scanResp.State)
	return nil
}

// callQRConfirmAPI calls the QR confirm API
func (f *Fs) callQRConfirmAPI(ctx context.Context, loginUID string) error {
	confirmPayload := map[string]string{"uid": loginUID, "key": loginUID, "client": "0"} // Key seems to be same as uid?
	confirmOpts := rest.Opts{
		Method:     "GET",
		RootURL:    hnQrCodeAPIRootURL,
		Path:       "/api/2.0/slogin.php",
		Parameters: url.Values{"uid": []string{loginUID}, "key": []string{loginUID}, "client": []string{"0"}}, // Send as query params
	}
	var confirmResp TraditionalBase
	fs.Debugf(f, "ğŸ“± è°ƒç”¨äºŒç»´ç æ‰«æç¡®è®¤æ¥å£...")
	err := f.CallTraditionalAPI(ctx, &confirmOpts, confirmPayload, &confirmResp, true) // Needs encryption? Assume yes.
	if err != nil {
		return fmt.Errorf("login_qrcode_scan_confirm failed: %w", err)
	}
	fs.Debugf(f, "âœ… äºŒç»´ç æ‰«æç¡®è®¤è°ƒç”¨æˆåŠŸ (çŠ¶æ€: %v)", confirmResp.State)
	return nil
}

// exchangeDeviceCodeForToken gets the access token using the device code
func (f *Fs) exchangeDeviceCodeForToken(ctx context.Context, loginUID string) error {
	tokenData := url.Values{
		"uid":           []string{loginUID},
		"code_verifier": []string{f.codeVerifier},
	}
	tokenOpts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL,
		Path:         "/open/deviceCodeToToken",
		Body:         strings.NewReader(tokenData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}
	var tokenResp DeviceCodeTokenResp
	// This call also uses traditional client but no encryption needed.
	err := f.CallTraditionalAPI(ctx, &tokenOpts, nil, &tokenResp, true) // skipEncrypt=true
	if err != nil {
		return fmt.Errorf("deviceCodeToToken failed: %w", err)
	}
	if tokenResp.Data == nil || tokenResp.Data.AccessToken == "" {
		return fmt.Errorf("deviceCodeToToken returned empty data: %v", tokenResp)
	}

	// Store tokens with proper locking
	f.tokenMu.Lock()
	f.accessToken = tokenResp.Data.AccessToken
	f.refreshToken = tokenResp.Data.RefreshToken
	f.tokenExpiry = time.Now().Add(time.Duration(tokenResp.Data.ExpiresIn) * time.Second)
	f.tokenMu.Unlock()

	fs.Debugf(f, "âœ… æˆåŠŸè·å–è®¿é—®ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)

	return nil
}

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	// Fix recursive calls: set refresh flag to prevent triggering refresh again during token refresh
	if !f.isRefreshing.CompareAndSwap(false, true) {
		fs.Debugf(f, "ğŸ”„ ä»¤ç‰Œåˆ·æ–°å·²åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡")
		return nil
	}
	defer f.isRefreshing.Store(false)

	f.tokenMu.Lock()

	// Check if token is already valid when we acquire the lock
	// This handles the case where another thread just refreshed the token
	if !refreshTokenExpired && !forceRefresh && isTokenStillValid(f) {
		fs.Debugf(f, "âœ… è·å–é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡åˆ·æ–°")
		f.tokenMu.Unlock()
		return nil
	}

	// Check if we need to perform a full login instead of a refresh
	if shouldPerformFullLogin(f, refreshTokenExpired) {
		f.tokenMu.Unlock()
		err := f.login(ctx) // login handles its own locking
		if err != nil {
			return err
		}
		// Save the token after successful login
		f.saveToken(f.m)
		return nil
	}

	// Check if token is still valid and refresh not forced
	if !forceRefresh && isTokenStillValid(f) {
		f.tokenMu.Unlock()
		return nil
	}

	// Prepare for token refresh
	refreshToken := f.refreshToken // Make a local copy of the refresh token
	f.tokenMu.Unlock()             // Unlock before making API call

	// Perform the actual token refresh
	result, err := f.performTokenRefresh(ctx, refreshToken)
	if err != nil {
		// Check if this is a refresh token expired error that requires re-login
		var tokenErr *TokenError
		if errors.As(err, &tokenErr) && tokenErr.IsRefreshTokenExpired {
			fs.Debugf(f, "ğŸ”„ åˆ·æ–°ä»¤ç‰Œå·²è¿‡æœŸï¼Œå°è¯•å®Œæ•´é‡æ–°ç™»å½•")

			// Clear token information before re-login
			f.tokenMu.Lock()
			f.clearTokenInfo()
			f.tokenMu.Unlock()

			// Attempt re-login
			loginErr := f.login(ctx)
			if loginErr != nil {
				return fmt.Errorf("re-login failed after refresh token expired: %w", loginErr)
			}

			// Save the new token after successful login
			f.saveToken(f.m)
			fs.Debugf(f, "âœ… åˆ·æ–°ä»¤ç‰Œè¿‡æœŸåé‡æ–°ç™»å½•æˆåŠŸ")
			return nil
		}

		return err // Error already formatted with context
	}

	// Update the tokens with new values
	f.updateTokens(result)

	// Save the refreshed token to config
	f.saveToken(f.m)

	return nil
}

// shouldPerformFullLogin determines if we should skip refresh and do a full login
func shouldPerformFullLogin(f *Fs, refreshTokenExpired bool) bool {
	// Skip directly to re-login if refresh token expired
	if refreshTokenExpired {
		fs.Debugf(f, "ğŸ”„ è·³è¿‡ä»¤ç‰Œåˆ·æ–°ï¼Œç”±äºåˆ·æ–°ä»¤ç‰Œè¿‡æœŸç›´æ¥é‡æ–°ç™»å½•")
		return true
	}

	// Re-login if no tokens available
	if f.accessToken == "" || f.refreshToken == "" {
		fs.Debugf(f, "ğŸ” æœªæ‰¾åˆ°ä»¤ç‰Œï¼Œå°è¯•ç™»å½•")
		return true
	}

	return false
}

// isTokenStillValid checks if the current token is still valid
func isTokenStillValid(f *Fs) bool {
	return time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow))
}

// clearTokenInfo clears all token information (must be called with tokenMu locked)
func (f *Fs) clearTokenInfo() {
	f.accessToken = ""
	f.refreshToken = ""
	f.tokenExpiry = time.Time{}
	fs.Debugf(f, "ğŸ§¹ å·²æ¸…ç©ºæ‰€æœ‰ä»¤ç‰Œä¿¡æ¯")
}

// performTokenRefresh handles the actual API call to refresh the token
func (f *Fs) performTokenRefresh(ctx context.Context, refreshToken string) (*RefreshTokenResp, error) {
	// ä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯ï¼Œæ— éœ€å•ç‹¬åˆå§‹åŒ–

	// Set up and make the refresh request
	refreshResp, err := f.callRefreshTokenAPI(ctx, refreshToken)
	if err != nil {
		return handleRefreshError(f, ctx, err)
	}

	// Validate the response
	if refreshResp.Data == nil || refreshResp.Data.AccessToken == "" {
		// Log detailed information about the empty response
		fs.Errorf(f, "âŒ åˆ·æ–°ä»¤ç‰Œå“åº”ä¸ºç©ºæˆ–æ— æ•ˆã€‚å®Œæ•´å“åº”: %#v", refreshResp)
		// Log OpenAPI base information (state, code, message)
		fs.Errorf(f, "âŒ å“åº”çŠ¶æ€: %v, ä»£ç : %d, æ¶ˆæ¯: %q",
			refreshResp.State, refreshResp.Code, refreshResp.Message)

		// Check if this is a refresh_token invalid error (40140116)
		if refreshResp.Code == 40140116 {
			fs.Errorf(f, "âŒ åˆ·æ–°ä»¤ç‰Œå·²å¤±æ•ˆ(40140116)ï¼Œæ¸…ç©ºä»¤ç‰Œä¿¡æ¯å¹¶è§¦å‘é‡æ–°ç™»å½•")

			// Clear all token information to force re-login
			f.tokenMu.Lock()
			f.clearTokenInfo()
			f.tokenMu.Unlock()

			// Return a specific error to trigger re-login
			return nil, NewTokenError("refresh token expired, need re-login", true)
		}

		fs.Errorf(f, "âŒ åˆ·æ–°ä»¤ç‰Œå“åº”ä¸ºç©ºï¼Œå°è¯•é‡æ–°ç™»å½•")

		// Re-lock before checking token again to avoid race condition
		f.tokenMu.Lock()
		// Only check if another thread refreshed if this wasn't a refresh_token invalid error
		if f.accessToken != "" && time.Now().Before(f.tokenExpiry) {
			fs.Debugf(f, "âœ… ç­‰å¾…æœŸé—´ä»¤ç‰Œå·²è¢«å…¶ä»–çº¿ç¨‹åˆ·æ–°")
			f.tokenMu.Unlock()
			return nil, nil
		}
		f.tokenMu.Unlock()

		loginErr := f.login(ctx)
		if loginErr != nil {
			return nil, fmt.Errorf("re-login failed after empty refresh response: %w", loginErr)
		}
		fs.Debugf(f, "âœ… ç©ºåˆ·æ–°å“åº”åé‡æ–°ç™»å½•æˆåŠŸ")
		return nil, nil // Re-login successful, no need to update tokens
	}

	return refreshResp, nil
}

// CallAPI ç»Ÿä¸€APIè°ƒç”¨æ–¹æ³•ï¼Œæ”¯æŒOpenAPIå’Œä¼ ç»ŸAPI
func (f *Fs) CallAPI(ctx context.Context, apiType APIType, opts *rest.Opts, request any, response any) error {
	// Fix null pointer issue: ensure client exists
	if err := f.ensureOpenAPIClient(ctx); err != nil {
		return fmt.Errorf("failed to ensure OpenAPI client: %w", err)
	}

	// æ ¹æ®APIç±»å‹åŠ¨æ€é…ç½®
	switch apiType {
	case OpenAPI:
		if opts.RootURL == "" {
			opts.RootURL = openAPIRootURL
		}
		// å‡†å¤‡Tokenè®¤è¯
		if err := f.prepareTokenForRequest(ctx, opts); err != nil {
			return err
		}
	case TraditionalAPI:
		if opts.RootURL == "" {
			opts.RootURL = traditionalRootURL
		}
		// å‡†å¤‡Cookieè®¤è¯
		if err := f.prepareCookieForRequest(opts); err != nil {
			return err
		}
	}

	// ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨å’Œå®¢æˆ·ç«¯
	return f.pacer.Call(func() (bool, error) {
		var resp *http.Response
		var err error

		if request != nil && response != nil {
			resp, err = f.openAPIClient.CallJSON(ctx, opts, request, response)
		} else if response != nil {
			resp, err = f.openAPIClient.CallJSON(ctx, opts, nil, response)
		} else {
			resp, err = f.openAPIClient.Call(ctx, opts)
		}

		return shouldRetry(ctx, resp, err)
	})
}

// prepareCookieForRequest ä¸ºä¼ ç»ŸAPIå‡†å¤‡Cookieè®¤è¯
func (f *Fs) prepareCookieForRequest(opts *rest.Opts) error {
	if f.opt.Cookie != "" {
		cred := (&Credential{}).FromCookie(f.opt.Cookie)
		if opts.ExtraHeaders == nil {
			opts.ExtraHeaders = make(map[string]string)
		}
		// è®¾ç½®Cookieå¤´
		for _, cookie := range cred.Cookie() {
			if opts.ExtraHeaders["Cookie"] == "" {
				opts.ExtraHeaders["Cookie"] = cookie.String()
			} else {
				opts.ExtraHeaders["Cookie"] += "; " + cookie.String()
			}
		}
	}
	return nil
}

// callRefreshTokenAPI makes the actual API call to refresh the token
func (f *Fs) callRefreshTokenAPI(ctx context.Context, refreshToken string) (*RefreshTokenResp, error) {
	refreshData := url.Values{
		"refresh_token": []string{refreshToken},
	}
	opts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL,
		Path:         "/open/refreshToken",
		Body:         strings.NewReader(refreshData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}

	var refreshResp RefreshTokenResp
	// Fix recursive calls: create a new rest client to completely avoid any token validation mechanism
	// because this method itself is refreshing the token and cannot trigger token validation again
	httpClient := getOpenAPIHTTPClient(ctx)
	if httpClient == nil {
		return nil, fmt.Errorf("failed to create HTTP client for token refresh")
	}
	tempClient := rest.NewClient(httpClient).SetErrorHandler(errorHandler)
	if tempClient == nil {
		return nil, fmt.Errorf("failed to create REST client for token refresh")
	}

	resp, err := tempClient.CallJSON(ctx, &opts, nil, &refreshResp)
	if err != nil {
		// Check if this is a 40140116 error (refresh_token invalid)
		if strings.Contains(err.Error(), "40140116") || strings.Contains(err.Error(), "no auth") {
			fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°åˆ·æ–°ä»¤ç‰Œæ— æ•ˆé”™è¯¯: %v", err)
			return nil, NewTokenError("refresh token invalid (40140116)", true)
		}
		return nil, err
	}

	// æ£€æŸ¥HTTPçŠ¶æ€ç 
	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("refresh token API returned status %d", resp.StatusCode)
	}

	// æ£€æŸ¥å“åº”ä¸­çš„é”™è¯¯ç 
	if !refreshResp.State && refreshResp.Code == 40140116 {
		fs.Debugf(f, "ğŸ” å“åº”ä¸­æ£€æµ‹åˆ°åˆ·æ–°ä»¤ç‰Œæ— æ•ˆé”™è¯¯(40140116)")
		return nil, NewTokenError("refresh token invalid (40140116)", true)
	}

	// å“åº”å·²ç»é€šè¿‡CallJSONè§£æåˆ°refreshRespä¸­
	fs.Debugf(f, "âœ… ä»¤ç‰Œåˆ·æ–°å“åº”æ¥æ”¶æˆåŠŸ")

	return &refreshResp, nil
}

// ensureOpenAPIClient ensures the OpenAPI client is initialized
func (f *Fs) ensureOpenAPIClient(ctx context.Context) error {
	if f.openAPIClient != nil {
		return nil
	}

	// Setup client if it doesn't exist
	httpClient := fshttp.NewClient(ctx)
	f.openAPIClient = rest.NewClient(httpClient).SetErrorHandler(errorHandler)
	if f.openAPIClient == nil {
		return fmt.Errorf("failed to create OpenAPI client")
	}

	return nil
}

// handleRefreshError handles errors from the refresh token API call
// Completely rewritten based on reference code, remove recursive checks, rely on correct API calls to avoid recursion
func handleRefreshError(f *Fs, ctx context.Context, err error) (*RefreshTokenResp, error) {
	fs.Errorf(f, "âŒ åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %v", err)

	// Check if the error indicates the refresh token itself is expired or invalid
	var tokenErr *TokenError
	if errors.As(err, &tokenErr) && tokenErr.IsRefreshTokenExpired ||
		strings.Contains(err.Error(), "refresh token expired") ||
		strings.Contains(err.Error(), "40140116") ||
		strings.Contains(err.Error(), "no auth") {

		fs.Debugf(f, "ğŸ”„ åˆ·æ–°ä»¤ç‰Œå·²å¤±æ•ˆï¼Œæ¸…ç©ºä»¤ç‰Œä¿¡æ¯å¹¶å°è¯•å®Œæ•´é‡æ–°ç™»å½•")

		// Clear all token information to ensure clean state
		f.tokenMu.Lock()
		f.clearTokenInfo()
		f.tokenMu.Unlock()

		loginErr := f.login(ctx) // login handles its own locking
		if loginErr != nil {
			return nil, fmt.Errorf("re-login failed after refresh token expired: %w", loginErr)
		}
		fs.Debugf(f, "âœ… åˆ·æ–°ä»¤ç‰Œè¿‡æœŸåé‡æ–°ç™»å½•æˆåŠŸ")
		return nil, nil // Re-login successful
	}

	// Return the original refresh error if it wasn't an expiry issue
	return nil, fmt.Errorf("token refresh failed: %w", err)
}

// updateTokens updates the token values with new values from a refresh response
func (f *Fs) updateTokens(refreshResp *RefreshTokenResp) {
	// If we got a nil response, it means we did a re-login instead of a refresh
	if refreshResp == nil {
		return
	}

	f.tokenMu.Lock()
	defer f.tokenMu.Unlock()

	f.accessToken = refreshResp.Data.AccessToken
	// OpenAPI spec says refresh_token might be updated, so store the new one
	if refreshResp.Data.RefreshToken != "" {
		f.refreshToken = refreshResp.Data.RefreshToken
	}
	f.tokenExpiry = time.Now().Add(time.Duration(refreshResp.Data.ExpiresIn) * time.Second)
	fs.Debugf(f, "âœ… ä»¤ç‰Œåˆ·æ–°æˆåŠŸï¼Œæ–°è¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "ğŸ’¾ ä¸ä¿å­˜ä»¤ç‰Œ - æ˜ å°„å™¨ä¸ºç©º")
		return
	}

	f.tokenMu.Lock()
	defer f.tokenMu.Unlock()

	if f.accessToken == "" || f.refreshToken == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ğŸ’¾ ä¸ä¿å­˜ä»¤ç‰Œ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	// Create the token structure
	token := &oauth2.Token{
		AccessToken:  f.accessToken,
		TokenType:    "Bearer",
		RefreshToken: f.refreshToken,
		Expiry:       f.tokenExpiry,
	}

	// Save the token directly using oauthutil's method
	// Note: This uses the originalName without brackets to ensure consistency
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Errorf(f, "âŒ ä¿å­˜ä»¤ç‰Œåˆ°é…ç½®å¤±è´¥: %v", err)
		return
	}

	fs.Debugf(f, "ğŸ’¾ ä½¿ç”¨åŸå§‹åç§°%qä¿å­˜ä»¤ç‰Œåˆ°é…ç½®æ–‡ä»¶", f.originalName)
}

// setupTokenRenewer initializes the token renewer to automatically refresh tokens
func (f *Fs) setupTokenRenewer(ctx context.Context, m configmap.Mapper) {
	// Only set up renewer if we have valid tokens
	if f.accessToken == "" || f.refreshToken == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ğŸ”„ ä¸è®¾ç½®ä»¤ç‰Œç»­æœŸå™¨ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	// Create a renewal transaction function
	transaction := func() error {
		fs.Debugf(f, "ğŸ”„ ä»¤ç‰Œç»­æœŸå™¨è§¦å‘ï¼Œåˆ·æ–°ä»¤ç‰Œ")
		// Use non-global function to avoid deadlocks
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Errorf(f, "âŒ ç»­æœŸå™¨ä¸­åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %v", err)
			return err
		}

		return nil // saveToken is already called in refreshTokenIfNecessary
	}

	// Create minimal OAuth config
	config := &oauthutil.Config{
		TokenURL: passportRootURL + "/open/refreshToken",
	}

	// Create a token source using the existing token
	token := &oauth2.Token{
		AccessToken:  f.accessToken,
		RefreshToken: f.refreshToken,
		Expiry:       f.tokenExpiry,
		TokenType:    "Bearer",
	}

	// Save token to config so it can be accessed by TokenSource
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Logf(f, "âŒ ä¸ºç»­æœŸå™¨ä¿å­˜ä»¤ç‰Œå¤±è´¥: %v", err)
		return
	}

	// Create a client with the token source
	_, ts, err := oauthutil.NewClientWithBaseClient(ctx, f.originalName, m, config, fshttp.NewClient(ctx))
	if err != nil {
		fs.Logf(f, "âŒ ä¸ºç»­æœŸå™¨åˆ›å»ºä»¤ç‰Œæºå¤±è´¥: %v", err)
		return
	}

	// Create token renewer that will trigger when the token is about to expire
	f.tokenRenewer = oauthutil.NewRenew(f.originalName, ts, transaction)
	f.tokenRenewer.Start() // Start the renewer immediately
	fs.Debugf(f, "ğŸ”„ ä»¤ç‰Œç»­æœŸå™¨å·²åˆå§‹åŒ–å¹¶å¯åŠ¨ï¼Œä½¿ç”¨åŸå§‹åç§°%q", f.originalName)
}

// CallOpenAPI performs a call to the OpenAPI endpoint.
// It handles token refresh and sets the Authorization header.
// If skipToken is true, it skips adding the Authorization header (used for refresh itself).
func (f *Fs) CallOpenAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// Smart QPS management: select appropriate pacer based on API endpoint
	smartPacer := f.getPacerForEndpoint()
	fs.Debugf(f, "ğŸ¯ æ™ºèƒ½QPSé€‰æ‹©: %s -> ä½¿ç”¨æ™ºèƒ½è°ƒé€Ÿå™¨", opts.Path)

	// Wrap the entire attempt sequence with the smart pacer, returning proper retry signals
	return smartPacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		// Ensure token is available and current
		if !skipToken {
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallOpenAPI: å‡†å¤‡è¯·æ±‚ä»¤ç‰Œå¤±è´¥: %v", err)
				return false, err
			}
		}

		// Make the API call
		resp, apiErr := f.executeOpenAPICall(ctx, opts, request, response)

		// Handle retries for network/server errors
		if retryNeeded, retryErr := shouldRetry(ctx, resp, apiErr); retryNeeded {
			fs.Debugf(f, "ğŸ”„ è°ƒé€Ÿå™¨: OpenAPIè°ƒç”¨éœ€è¦åº•å±‚é‡è¯• (é”™è¯¯: %v)", retryErr)
			return true, retryErr // Signal globalPacer to retry
		}

		// Handle token errors
		if apiErr != nil {
			if retryAfterTokenRefresh, err := f.handleTokenError(ctx, opts, apiErr, skipToken); retryAfterTokenRefresh {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, err
			}
			return false, apiErr // Non-token error, don't retry
		}

		// Check for API-level errors in the response
		if apiErr = f.checkResponseForAPIErrors(response); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, err
			}
			return false, apiErr // Other API error, don't retry
		}

		fs.Debugf(f, "âœ… è°ƒé€Ÿå™¨: OpenAPIè°ƒç”¨æˆåŠŸ")
		return false, nil // Success, don't retry
	})
}

// CallUploadAPI function specifically for upload-related API calls, using dedicated pacer
// Optimize upload API call frequency, balance performance and stability
func (f *Fs) CallUploadAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// Use unified pacer for all API calls
	return f.pacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		// Ensure token is available and current
		if !skipToken {
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallUploadAPI: å‡†å¤‡è¯·æ±‚ä»¤ç‰Œå¤±è´¥: %v", err)
				return false, err
			}
		}

		// Make the actual API call
		fs.Debugf(f, "ğŸ“¤ CallUploadAPI: æ‰§è¡ŒAPIè°ƒç”¨")
		err := f.CallAPI(ctx, OpenAPI, opts, request, response)

		// Check for API-level errors in the response
		if apiErr := f.checkResponseForAPIErrors(response); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, err
			}
			return false, apiErr // Other API error, don't retry
		}

		fs.Debugf(f, "âœ… CallUploadAPI: APIè°ƒç”¨æˆåŠŸ")
		return shouldRetry(ctx, nil, err)
	})
}

// CallDownloadURLAPI function specifically for download URL API calls, using dedicated pacer
// ä¿®å¤æ–‡ä»¶çº§å¹¶å‘ï¼šé€šè¿‡MaxConnectionsæ”¯æŒå¹¶å‘ä¸‹è½½URLè·å–
func (f *Fs) CallDownloadURLAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨
	return f.pacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		// Ensure token is available and current
		if !skipToken {
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallDownloadURLAPI: å‡†å¤‡è¯·æ±‚ä»¤ç‰Œå¤±è´¥: %v", err)
				return false, err
			}
		}

		// Make the actual API call
		fs.Debugf(f, "ğŸ“¥ CallDownloadURLAPI: æ‰§è¡ŒAPIè°ƒç”¨")
		err := f.CallAPI(ctx, OpenAPI, opts, request, response)

		// Check for API-level errors in the response
		if apiErr := f.checkResponseForAPIErrors(response); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, err
			}
			return false, apiErr // Other API error, don't retry
		}

		fs.Debugf(f, "âœ… CallDownloadURLAPI: APIè°ƒç”¨æˆåŠŸ")
		return shouldRetry(ctx, nil, err)
	})
}

// prepareTokenForRequest ensures a valid token is available and sets it in the request headers
func (f *Fs) prepareTokenForRequest(ctx context.Context, opts *rest.Opts) error {
	// Use double-checked locking pattern to minimize lock contention
	// First check without lock (fast path for common case)
	f.tokenMu.Lock()
	needsRefresh := !isTokenStillValid(f)
	currentToken := f.accessToken
	f.tokenMu.Unlock()

	// If refresh is needed, call refreshTokenIfNecessary which has its own locking
	// Fix recursive calls: avoid infinite recursion caused by triggering token refresh in API calls
	if needsRefresh {
		// æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨tokenåˆ·æ–°è¿‡ç¨‹ä¸­ï¼Œé¿å…é€’å½’è°ƒç”¨
		if f.isRefreshing.Load() {
			fs.Debugf(f, "ğŸ”„ ä»¤ç‰Œåˆ·æ–°å·²åœ¨è¿›è¡Œä¸­ï¼Œä½¿ç”¨å½“å‰ä»¤ç‰Œ")
			f.tokenMu.Lock()
			currentToken = f.accessToken
			f.tokenMu.Unlock()
		} else {
			refreshErr := f.refreshTokenIfNecessary(ctx, false, false)
			if refreshErr != nil {
				fs.Debugf(f, "âŒ ä»¤ç‰Œåˆ·æ–°å¤±è´¥: %v", refreshErr)
				return fmt.Errorf("token refresh failed: %w", refreshErr)
			}

			// Get the refreshed token
			f.tokenMu.Lock()
			currentToken = f.accessToken
			f.tokenMu.Unlock()
		}
	}

	// Validate we have a token before using it
	if currentToken == "" {
		return fmt.Errorf("no valid access token available")
	}

	if opts.ExtraHeaders == nil {
		opts.ExtraHeaders = make(map[string]string)
	}
	opts.ExtraHeaders["Authorization"] = "Bearer " + currentToken
	return nil
}

// executeOpenAPICall makes the actual API call with the provided parameters
func (f *Fs) executeOpenAPICall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	// Fix circular calls: use openAPIClient directly, not through CallAPI
	// ç¡®ä¿å®¢æˆ·ç«¯å­˜åœ¨
	if err := f.ensureOpenAPIClient(ctx); err != nil {
		return nil, fmt.Errorf("failed to ensure OpenAPI client: %w", err)
	}

	var resp *http.Response
	var err error

	if request != nil && response != nil {
		// Assume standard JSON request/response
		resp, err = f.openAPIClient.CallJSON(ctx, opts, request, response)
	} else if response != nil {
		// Assume GET request with JSON response
		resp, err = f.openAPIClient.CallJSON(ctx, opts, nil, response)
	} else {
		// Assume call without specific request/response body
		var baseResp OpenAPIBase
		resp, err = f.openAPIClient.CallJSON(ctx, opts, nil, &baseResp)
		if err == nil {
			err = baseResp.Err() // Check for API-level errors
		}
	}

	if err != nil {
		fs.Debugf(f, "âŒ executeOpenAPICallå¤±è´¥: %v", err)
	}

	return resp, err
}

// handleTokenError processes token-related errors and attempts to refresh or re-login
func (f *Fs) handleTokenError(ctx context.Context, opts *rest.Opts, apiErr error, skipToken bool) (bool, error) {
	var tokenErr *TokenError
	if errors.As(apiErr, &tokenErr) {
		fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°ä»¤ç‰Œé”™è¯¯: %v (éœ€è¦é‡æ–°ç™»å½•: %v)", tokenErr, tokenErr.IsRefreshTokenExpired)

		// Check if we're already in a refresh cycle to prevent infinite loops
		if f.isRefreshing.Load() {
			fs.Debugf(f, "âš ï¸ ä»¤ç‰Œåˆ·æ–°å·²åœ¨è¿›è¡Œä¸­ï¼Œé¿å…é€’å½’è°ƒç”¨")
			return false, fmt.Errorf("token refresh already in progress, avoiding recursion")
		}

		// Handle token refresh/re-login using refreshTokenIfNecessary
		refreshErr := f.refreshTokenIfNecessary(ctx, tokenErr.IsRefreshTokenExpired, !tokenErr.IsRefreshTokenExpired)
		if refreshErr != nil {
			fs.Debugf(f, "âŒ ä»¤ç‰Œåˆ·æ–°/é‡æ–°ç™»å½•å¤±è´¥: %v", refreshErr)
			return false, fmt.Errorf("token refresh/relogin failed: %w (original: %v)", refreshErr, apiErr)
		}

		// Token was successfully refreshed or re-login succeeded, retry the API call
		fs.Debugf(f, "âœ… ä»¤ç‰Œåˆ·æ–°/é‡æ–°ç™»å½•æˆåŠŸï¼Œé‡è¯•APIè°ƒç”¨")

		// Update the Authorization header with the new token
		if !skipToken {
			// Always get the freshest token right before using it
			f.tokenMu.Lock()
			token := f.accessToken
			f.tokenMu.Unlock()

			// Validate we have a valid token
			if token == "" {
				fs.Errorf(f, "âŒ åˆ·æ–°åä»ç„¶æ²¡æœ‰æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ")
				return false, fmt.Errorf("no valid access token after refresh")
			}

			if opts.ExtraHeaders == nil {
				opts.ExtraHeaders = make(map[string]string)
			}
			opts.ExtraHeaders["Authorization"] = "Bearer " + token
			fs.Debugf(f, "ğŸ”‘ å·²æ›´æ–°Authorizationå¤´éƒ¨ï¼Œä»¤ç‰Œé•¿åº¦: %d", len(token))
		}
		return true, nil // Signal retry with the refreshed token
	}
	return false, nil // Not a token error
}

// checkResponseForAPIErrors examines the response for API-level errors using reflection
func (f *Fs) checkResponseForAPIErrors(response any) error {
	if response == nil {
		return nil
	}

	// Use reflection to check for and call an Err() method
	val := reflect.ValueOf(response)
	if val.Kind() == reflect.Ptr && !val.IsNil() {
		method := val.MethodByName("Err")
		if method.IsValid() && method.Type().NumIn() == 0 && method.Type().NumOut() == 1 &&
			method.Type().Out(0).Implements(reflect.TypeOf((*error)(nil)).Elem()) {
			result := method.Call(nil)
			if !result[0].IsNil() {
				// Get the error and check if it's a rate limit error
				err := result[0].Interface().(error)
				if strings.Contains(err.Error(), "rate limit exceeded") {
					fs.Debugf(f, "ğŸ”„ å“åº”ä¸­æ£€æµ‹åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯: %v", err)
					// Return the error as is - the shouldRetry function will handle it
					// This ensures both CallOpenAPI and CallTraditionalAPI will retry rate limit errors
				}
				return err
			}
		}
	}
	return nil
}

// CallTraditionalAPI performs a call to the traditional (cookie, encrypted) API.
// It uses both the traditional and global pacers.
// If skipEncrypt is true, it skips the request/response encryption (used for some login steps).
func (f *Fs) CallTraditionalAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipEncrypt bool) error {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = traditionalRootURL
	}

	// Use unified pacer for all API calls
	return f.pacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {

		// Make the API call (with or without encryption)
		resp, apiErr := f.executeTraditionalAPICall(ctx, opts, request, response, skipEncrypt)

		// Process the result
		return f.processTraditionalAPIResult(ctx, resp, apiErr)
	})
}

// ç§»é™¤enforceTraditionalPacerDelayå‡½æ•°ï¼Œä½¿ç”¨ç»Ÿä¸€pacerä¸éœ€è¦é¢å¤–å»¶è¿Ÿ

// executeTraditionalAPICall makes the actual API call with proper handling of encryption
func (f *Fs) executeTraditionalAPICall(ctx context.Context, opts *rest.Opts, request any, response any, skipEncrypt bool) (*http.Response, error) {
	if skipEncrypt {
		return f.executeUnencryptedCall(ctx, opts, request, response)
	}
	return f.executeEncryptedCall(ctx, opts, request, response)
}

// executeUnencryptedCall makes a traditional API call without encryption
func (f *Fs) executeUnencryptedCall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	var resp *http.Response
	// ä½¿ç”¨ç»Ÿä¸€APIè°ƒç”¨æ–¹æ³•
	apiErr := f.CallAPI(ctx, TraditionalAPI, opts, request, response)

	return resp, apiErr
}

// executeEncryptedCall makes a traditional API call (encryption removed)
// é‡æ„ï¼šç§»é™¤åŠ å¯†åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†JSONè°ƒç”¨
func (f *Fs) executeEncryptedCall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	var resp *http.Response
	// é‡æ„ï¼šä½¿ç”¨ç»Ÿä¸€APIè°ƒç”¨æ–¹æ³•
	apiErr := f.CallAPI(ctx, TraditionalAPI, opts, request, response)

	return resp, apiErr
}

// processTraditionalAPIResult handles the result of a traditional API call
func (f *Fs) processTraditionalAPIResult(ctx context.Context, resp *http.Response, apiErr error) (bool, error) {
	// Check for retryable errors
	retryNeeded, retryErr := shouldRetry(ctx, resp, apiErr)
	if retryNeeded {
		fs.Debugf(f, "ğŸ”„ è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨éœ€è¦åº•å±‚é‡è¯• (é”™è¯¯: %v)", retryErr)
		return true, retryErr
	}

	// Handle non-retryable errors
	if apiErr != nil {
		fs.Debugf(f, "âŒ è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨é‡åˆ°æ°¸ä¹…é”™è¯¯: %v", apiErr)
		return false, apiErr
	}

	// Success
	fs.Debugf(f, "âœ… è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨æˆåŠŸ")
	return false, nil
}

// initializeOptions115 åˆå§‹åŒ–å’ŒéªŒè¯115ç½‘ç›˜é…ç½®é€‰é¡¹
func initializeOptions115(name, root string, m configmap.Mapper) (*Options, string, string, error) {
	// Parse config into Options struct
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, "", "", err
	}

	// Validate incompatible options
	if opt.FastUpload && opt.OnlyStream {
		return nil, "", "", errors.New("fast_upload and only_stream cannot be set simultaneously")
	}
	if opt.FastUpload && opt.UploadHashOnly {
		return nil, "", "", errors.New("fast_upload and upload_hash_only cannot be set simultaneously")
	}
	if opt.OnlyStream && opt.UploadHashOnly {
		return nil, "", "", errors.New("only_stream and upload_hash_only cannot be set simultaneously")
	}

	if opt.NohashSize > StreamUploadLimit {
		fs.Logf(name, "âš ï¸ nohash_size (%v) å‡å°‘åˆ°æµå¼ä¸Šä¼ é™åˆ¶ (%v)", opt.NohashSize, StreamUploadLimit)
		opt.NohashSize = StreamUploadLimit
	}

	// è®¾ç½®APIé™æµæ§åˆ¶çš„é»˜è®¤å€¼
	if opt.QPSLimit == 0 {
		opt.QPSLimit = unifiedMinSleep // é»˜è®¤ä½¿ç”¨300msé—´éš” (çº¦3.3 QPS)
	}

	// å…¶ä»–å¢å¼ºåŠŸèƒ½å·²è®¾ç½®ä¸ºåˆç†çš„é»˜è®¤å€¼ï¼Œæ— éœ€ç”¨æˆ·é…ç½®

	// Store the original name before any modifications for config operations
	// Extract the base name without the config override suffix {xxxx}
	originalName := name
	if idx := strings.IndexRune(name, '{'); idx > 0 {
		originalName = name[:idx]
	}
	root = strings.Trim(root, "/")

	return opt, originalName, root, nil
}

// createBasicFs115 åˆ›å»ºåŸºç¡€115ç½‘ç›˜æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
func createBasicFs115(name, originalName, root string, opt *Options, m configmap.Mapper) *Fs {
	return &Fs{
		name:            name,
		originalName:    originalName,
		root:            root,
		opt:             *opt,
		m:               m,
		activeTransfers: make(map[string]*TransferSpeedMonitor),
	}
}

// newFs constructs an Fs from the path, container:path
func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	// åˆå§‹åŒ–å’ŒéªŒè¯é…ç½®é€‰é¡¹
	opt, originalName, normalizedRoot, err := initializeOptions115(name, root, m)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŸºç¡€æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
	f := createBasicFs115(name, originalName, normalizedRoot, opt, m)

	// Fix null pointer issue: initialize client immediately, following standard practice of other backends
	httpClient := fshttp.NewClient(ctx)
	f.openAPIClient = rest.NewClient(httpClient).SetErrorHandler(errorHandler)
	if f.openAPIClient == nil {
		return nil, fmt.Errorf("failed to create OpenAPI REST client")
	}
	fs.Debugf(f, "âœ… OpenAPI RESTå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")

	// åˆå§‹åŒ–åŠŸèƒ½ç‰¹æ€§ - ä½¿ç”¨æ ‡å‡†Fillæ¨¡å¼
	f.features = (&fs.Features{
		DuplicateFiles:          false,
		CanHaveEmptyDirectories: true,
		NoMultiThreading:        true, // 115ç½‘ç›˜ä½¿ç”¨å•çº¿ç¨‹ä¸Šä¼ 
	}).Fill(ctx, f)

	// Setting appVer (needed for traditional calls)
	re := regexp.MustCompile(`\d+\.\d+\.\d+(\.\d+)?$`)
	if m := re.FindStringSubmatch(tradUserAgent); m == nil {
		fs.Logf(f, "âš ï¸ æ— æ³•ä»User-Agent %q è§£æåº”ç”¨ç‰ˆæœ¬ã€‚ä½¿ç”¨é»˜è®¤å€¼ã€‚", tradUserAgent)
		f.appVer = "27.0.7.5" // Default fallback
	} else {
		f.appVer = m[0]
		fs.Debugf(f, "ğŸŒ ä»User-Agent %q ä½¿ç”¨åº”ç”¨ç‰ˆæœ¬ %q", tradUserAgent, f.appVer)
	}

	// Initialize single pacerï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®çš„QPSé™åˆ¶
	// 115ç½‘ç›˜æ‰€æœ‰APIéƒ½å—åˆ°ç›¸åŒçš„å…¨å±€QPSé™åˆ¶ï¼Œä½¿ç”¨å•ä¸€pacerç¬¦åˆrcloneæ ‡å‡†æ¨¡å¼
	f.pacer = fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(time.Duration(opt.QPSLimit)),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))

	// åˆå§‹åŒ–å¢å¼ºåŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
	qpsValue := float64(time.Second) / float64(opt.QPSLimit)
	fs.Debugf(f, "ğŸš¦ APIé™æµæ§åˆ¶: é—´éš”=%v (çº¦%.1f QPS), æœ€å¤§é‡è¯•=%d, åŸºç¡€å»¶è¿Ÿ=%v",
		opt.QPSLimit, qpsValue, defaultMaxRetryAttempts, defaultRetryBaseDelay)
	fs.Debugf(f, "ğŸ“Š ä¼ è¾“é€Ÿåº¦ç›‘æ§å·²å¯ç”¨: æ…¢é€Ÿé˜ˆå€¼=%v/s, æ£€æŸ¥é—´éš”=%v, å¤„ç†åŠ¨ä½œ=%s",
		defaultSlowSpeedThreshold, defaultSpeedCheckInterval, defaultSlowSpeedAction)
	fs.Debugf(f, "â° åˆ†ç‰‡è¶…æ—¶æ§åˆ¶å·²å¯ç”¨: è¶…æ—¶æ—¶é—´=%v", defaultChunkTimeout)

	// Check if we have saved token in config file
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ğŸ’¾ ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %v (è¿‡æœŸæ—¶é—´ %v)", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if tokenLoaded {
		// Check if token is expired or will expire soon
		if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
			fs.Debugf(f, "â° ä»¤ç‰Œå·²è¿‡æœŸæˆ–å³å°†è¿‡æœŸï¼Œç«‹å³åˆ·æ–°")
			tokenRefreshNeeded = true
		}
	} else if f.opt.Cookie != "" {
		// No token but have cookie, so login
		fs.Debugf(f, "ğŸ” æœªæ‰¾åˆ°ä»¤ç‰Œä½†æä¾›äº†Cookieï¼Œå°è¯•ç™»å½•")
		err = f.login(ctx)
		if err != nil {
			return nil, fmt.Errorf("initial login failed: %w", err)
		}
		// Save token to config after successful login
		f.saveToken(m)
		fs.Debugf(f, "âœ… ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	} else {
		return nil, errors.New("no valid cookie or token found, please configure cookie")
	}

	// Try to refresh the token if needed
	if tokenRefreshNeeded {
		err = f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Debugf(f, "âŒ ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œå°è¯•å®Œæ•´ç™»å½•: %v", err)
			// If refresh fails, try full login
			err = f.login(ctx)
			if err != nil {
				return nil, fmt.Errorf("login failed after token refresh failure: %w", err)
			}
		}
		// Save the refreshed/new token
		f.saveToken(m)
		fs.Debugf(f, "âœ… ä»¤ç‰Œåˆ·æ–°/ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Setup token renewer for automatic refresh
	fs.Debugf(f, "ğŸ”„ è®¾ç½®ä»¤ç‰Œç»­æœŸå™¨")
	f.setupTokenRenewer(ctx, m)

	// Set the root folder ID based on config
	if f.opt.RootFolderID != "" {
		// Use configured root folder ID
		f.rootFolderID = f.opt.RootFolderID
	} else {
		// Use default root folder ID
		f.rootFolderID = "0"
	}

	// Initialize directory cache with persistent support
	configData := map[string]string{
		"root_folder_id": f.rootFolderID,
		"endpoint":       openAPIRootURL,
	}
	f.dirCache = dircache.NewWithPersistent(f.root, f.rootFolderID, f, "115", configData)

	// ğŸ”§ ä¼˜åŒ–çš„rcloneæ¨¡å¼ï¼šç»“åˆæ ‡å‡†æ¨¡å¼å’Œ115ç½‘ç›˜ç‰¹æ€§
	// Find the current root
	err = f.dirCache.FindRoot(ctx, false)

	if err != nil {
		// Assume it is a file (æ ‡å‡†rcloneæ¨¡å¼)
		newRoot, remote := dircache.SplitPath(f.root)
		// Fix: ä½¿ç”¨æ ‡å‡†çš„ç»“æ„ä½“å¤åˆ¶æ¨¡å¼ï¼Œé¿å…é—æ¼å­—æ®µï¼ˆå¦‚tokenï¼‰
		tempF := *f // ç›´æ¥å¤åˆ¶æ•´ä¸ªç»“æ„ä½“ï¼ŒåŒ…æ‹¬æ‰€æœ‰tokenå­—æ®µ
		tempF.root = newRoot
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, &tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// XXX: update the old f here instead of returning tempF, since
		// `features` were already filled with functions having *f as a receiver.
		// See https://github.com/rclone/rclone/issues/2182
		f.dirCache = tempF.dirCache
		f.root = tempF.root
		return f, fs.ErrorIsFile
	}
	// ğŸ”§ 115ç½‘ç›˜ç‰¹æ®Šå¤„ç†ï¼šå³ä½¿FindRootæˆåŠŸï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ä»¶è·¯å¾„
	// è¿™æ˜¯115ç½‘ç›˜çš„ç‰¹æ®Šéœ€æ±‚ï¼Œå› ä¸º115ç½‘ç›˜çš„ç›®å½•ç»“æ„ç‰¹æ®Š
	if strings.Contains(f.root, ".") && !strings.HasSuffix(f.root, ".") {
		newRoot, remote := dircache.SplitPath(f.root)

		// ä¿®å¤ï¼šä¿å­˜åŸå§‹çš„æ ¹ç›®å½•IDï¼Œé¿å…åœ¨åˆ›å»ºæ–°dirCacheæ—¶ä¸¢å¤±
		originalRootID, err := f.dirCache.RootID(ctx, false)
		if err != nil {
			fs.Debugf(f, "âš ï¸ æ— æ³•è·å–åŸå§‹æ ¹ç›®å½•IDï¼Œä½¿ç”¨é»˜è®¤: %v", err)
			originalRootID = f.rootFolderID
		}
		fs.Debugf(f, "ğŸ”§ æ–‡ä»¶è·¯å¾„æ£€æµ‹: newRoot=%s, remote=%s, originalRootID=%s", newRoot, remote, originalRootID)

		// Fix: ä½¿ç”¨æ ‡å‡†çš„ç»“æ„ä½“å¤åˆ¶æ¨¡å¼ï¼Œé¿å…é—æ¼å­—æ®µï¼ˆå¦‚tokenï¼‰
		tempF := *f // ç›´æ¥å¤åˆ¶æ•´ä¸ªç»“æ„ä½“ï¼ŒåŒ…æ‹¬æ‰€æœ‰tokenå­—æ®µ
		tempF.root = newRoot
		tempF.dirCache = dircache.New(newRoot, originalRootID, &tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err = tempF.NewObject(ctx, remote)
		if err != nil {
			// File doesn't exist, return original f as directory
			return f, nil
		}
		// File exists, return as file
		f.dirCache = tempF.dirCache
		f.root = tempF.root
		return f, fs.ErrorIsFile
	}

	return f, nil
}

// PathInfo 115ç½‘ç›˜è·¯å¾„ä¿¡æ¯ç»“æ„
type PathInfo struct {
	FileCategory string `json:"file_category"` // "1"=æ–‡ä»¶, "0"=æ–‡ä»¶å¤¹
	FileName     string `json:"file_name"`
	FileID       string `json:"file_id"`
	Size         string `json:"size"`
	SizeByte     int64  `json:"size_byte"`
}

// PathInfoResponse 115ç½‘ç›˜è·¯å¾„ä¿¡æ¯APIå“åº”
// ğŸ”§ ä¿®å¤ï¼šAPIå¯èƒ½è¿”å›æ•°ç»„æˆ–å¯¹è±¡ï¼Œéœ€è¦çµæ´»å¤„ç†
type PathInfoResponse struct {
	State   bool        `json:"state"`
	Code    int         `json:"code"`
	Message string      `json:"message"`
	Data    interface{} `json:"data"` // ä½¿ç”¨interface{}æ¥å¤„ç†æ•°ç»„æˆ–å¯¹è±¡
}

// isFromRemoteSource åˆ¤æ–­è¾“å…¥æµæ˜¯å¦æ¥è‡ªè¿œç¨‹æºï¼ˆç”¨äºè·¨äº‘ä¼ è¾“æ£€æµ‹ï¼‰
func (f *Fs) isFromRemoteSource(in io.Reader) bool {
	if ro, ok := in.(*RereadableObject); ok {
		fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°RereadableObjectï¼Œå¯èƒ½æ˜¯è·¨äº‘ä¼ è¾“: %v", ro)
		return true
	}
	readerType := fmt.Sprintf("%T", in)
	if strings.Contains(readerType, "Accounted") || strings.Contains(readerType, "Cross") {
		fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°ç‰¹æ®ŠReaderç±»å‹ï¼Œå¯èƒ½æ˜¯è·¨äº‘ä¼ è¾“: %s", readerType)
		return true
	}

	return false
}

// Name of the remote (as passed into NewFs)
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (as passed into NewFs)
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("115 %s", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	return f.features
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	// OpenAPI might allow setting modtime, but let's assume not for now
	return fs.ModTimeNotSupported
}

// DirCacheFlush resets the directory cache
func (f *Fs) DirCacheFlush() {
	f.dirCache.ResetRoot()
}

// Hashes returns the supported hash types of the filesystem
func (f *Fs) Hashes() hash.Set {
	return hash.Set(hash.SHA1)
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	if f.fileObj != nil { // Handle case where Fs points to a single file
		obj := *f.fileObj
		if obj.Remote() == remote || obj.Remote() == "isFile:"+remote {
			fs.Debugf(f, "âœ… NewObject: å•æ–‡ä»¶åŒ¹é…æˆåŠŸ")
			return obj, nil
		}
		fs.Debugf(f, "âŒ NewObject: å•æ–‡ä»¶ä¸åŒ¹é…ï¼Œè¿”å›NotFound")
		return nil, fs.ErrorObjectNotFound // If remote doesn't match the single file
	}

	result, err := f.newObjectWithInfo(ctx, remote, nil)
	if err != nil {
		fs.Debugf(f, "âŒ NewObjectå¤±è´¥: %v", err)
	}
	return result, err
}

// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	// ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šé¦–å…ˆæ£€æŸ¥dirCacheæ˜¯å¦å·²æœ‰ç»“æœ
	parentPath, ok := f.dirCache.GetInv(pathID)
	if ok {
		var itemPath string
		if parentPath == "" {
			itemPath = leaf
		} else {
			itemPath = path.Join(parentPath, leaf)
		}
		// æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
		if cachedID, found := f.dirCache.Get(itemPath); found {
			fs.Debugf(f, "ğŸ¯ FindLeafç¼“å­˜å‘½ä¸­: %s -> ID=%s", leaf, cachedID)
			return cachedID, true, nil
		}
	}

	// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å¸¸é‡å®šä¹‰çš„limitï¼Œæœ€å¤§åŒ–æ€§èƒ½
	listChunk := defaultListChunkSize // 115ç½‘ç›˜OpenAPIçš„æœ€å¤§é™åˆ¶

	// ğŸ”§ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡ç¼“å­˜æœºåˆ¶ï¼Œç±»ä¼¼123ç½‘ç›˜çš„å®ç°
	parentPath, parentPathOk := f.dirCache.GetInv(pathID)

	found, err = f.listAll(ctx, pathID, listChunk, false, func(item *File) bool {
		// Compare with decoded name to handle special characters correctly
		decodedName := f.opt.Enc.ToStandardName(item.FileNameBest())

		// ğŸ”§ æ‰¹é‡ç¼“å­˜ï¼šç¼“å­˜æ‰€æœ‰æ‰¾åˆ°çš„ç›®å½•é¡¹
		if item.IsDir() && parentPathOk {
			var itemPath string
			if parentPath == "" {
				itemPath = decodedName
			} else {
				itemPath = path.Join(parentPath, decodedName)
			}
			f.dirCache.Put(itemPath, item.ID())
			fs.Debugf(f, "ğŸ“¦ æ‰¹é‡ç¼“å­˜: %s -> ID=%s", decodedName, item.ID())
		}

		if decodedName == leaf {
			// æ£€æŸ¥æ‰¾åˆ°çš„é¡¹ç›®æ˜¯å¦ä¸ºç›®å½•
			if item.IsDir() {
				// è¿™æ˜¯ç›®å½•ï¼Œè¿”å›ç›®å½•ID
				foundID = item.ID()
				fs.Debugf(f, "âœ… FindLeafæ‰¾åˆ°ç›®æ ‡: %s -> ID=%s, Type=ç›®å½•", leaf, foundID)
			} else {
				foundID = "" // æ˜ç¡®è®¾ç½®ä¸ºç©ºï¼Œè¡¨ç¤ºæ‰¾åˆ°çš„æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
				fs.Debugf(f, "âœ… FindLeafæ‰¾åˆ°ç›®æ ‡: %s -> Type=æ–‡ä»¶", leaf)
			}
			return true // Stop searching
		}
		return false // Continue searching
	})

	// é‡æ„ï¼šå¦‚æœé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œæ¸…ç†dirCacheè€Œä¸æ˜¯pathCache
	if err != nil && (strings.Contains(err.Error(), "770004") || strings.Contains(err.Error(), "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™")) {
		f.dirCache.Flush()
	}

	return foundID, found, err
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "ğŸ“‹ Listè°ƒç”¨ï¼Œç›®å½•: %q", dir)

	// ğŸ”§ å¤„ç†ç©ºç›®å½•è·¯å¾„ï¼šå½“dirä¸ºç©ºæ—¶ï¼Œä½¿ç”¨dirCacheçš„æ ¹ç›®å½•ID
	var dirID string
	if dir == "" {
		// ä½¿ç”¨dirCacheçš„æ ¹ç›®å½•IDï¼Œè¿™æ˜¯å½“å‰Fså¯¹è±¡æ‰€ä»£è¡¨çš„ç›®å½•
		// ä¿®å¤ï¼šå¦‚æœæ ¹ç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºå®ƒ
		dirID, err = f.dirCache.RootID(ctx, true) // create = true
		if err != nil {
			fs.Debugf(f, "âŒ Listè·å–æ ¹ç›®å½•IDå¤±è´¥: %v", err)
			// å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•åˆ·æ–°ç¼“å­˜åé‡è¯•
			f.dirCache.Flush()
			dirID, err = f.dirCache.RootID(ctx, true)
			if err != nil {
				fs.Debugf(f, "âŒ Liståˆ·æ–°ç¼“å­˜åä»ç„¶å¤±è´¥: %v", err)
				return nil, err
			}
		}
		fs.Debugf(f, "âœ… Listä½¿ç”¨å½“å‰æ ¹ç›®å½•ID: %s", dirID)
	} else {
		// ä¿®å¤ï¼šå¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»ºå®ƒ
		dirID, err = f.dirCache.FindDir(ctx, dir, true) // create = true
		if err != nil {
			// If it's an API limit error, log details and return
			if fserrors.IsRetryError(err) {
				fs.Debugf(f, "ğŸ”„ Listé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œç›®å½•: %q, é”™è¯¯: %v", dir, err)
				return nil, err
			}
			fs.Debugf(f, "âŒ ListæŸ¥æ‰¾ç›®å½•å¤±è´¥ï¼Œç›®å½•: %q, é”™è¯¯: %v", dir, err)
			// å°è¯•åˆ·æ–°ç¼“å­˜åé‡è¯•
			f.dirCache.FlushDir(dir)
			dirID, err = f.dirCache.FindDir(ctx, dir, true)
			if err != nil {
				fs.Debugf(f, "âŒ Liståˆ·æ–°ç¼“å­˜åä»ç„¶å¤±è´¥ï¼Œç›®å½•: %q, é”™è¯¯: %v", dir, err)
				return nil, err
			}
		}
		fs.Debugf(f, "âœ… Listæ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %q", dirID, dir)
	}

	// Call API to get directory listing
	var fileList []File
	var iErr error

	// ğŸ”§ ä¼˜åŒ–ï¼šè®¾ç½®å›ºå®šçš„limitä¸º1150ï¼Œæœ€å¤§åŒ–æ€§èƒ½
	listChunk := defaultListChunkSize // 115ç½‘ç›˜OpenAPIçš„æœ€å¤§é™åˆ¶
	_, err = f.listAll(ctx, dirID, listChunk, false, func(item *File) bool {
		// ä¿å­˜åˆ°ä¸´æ—¶åˆ—è¡¨ç”¨äºç¼“å­˜
		fileList = append(fileList, *item)

		// æ„å»ºentries
		entry, err := f.itemToDirEntry(ctx, path.Join(dir, item.FileNameBest()), item)
		if err != nil {
			iErr = err // Capture error but continue listing
			return false
		}
		if entry != nil {
			entries = append(entries, entry)
		}
		return false
	})

	if err != nil {
		return nil, err // Return listing error
	}
	if iErr != nil {
		return nil, iErr // Return item processing error
	}
	return entries, nil
}

// CreateDir makes a directory with pathID as parent and name leaf. Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	if f.isShare {
		return "", errors.New("unsupported operation: Mkdir on shared filesystem")
	}
	// Use makeDir which now uses OpenAPI
	info, err := f.makeDir(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}
	if info.Data == nil || info.Data.FileID == "" {
		return "", errors.New("Mkdir response did not contain a file ID")
	}
	return info.Data.FileID, nil
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	// ğŸ”§ æ ‡å‡†rcloneæ¨¡å¼ï¼šéµå¾ªGoogle Driveç­‰ä¸»æµç½‘ç›˜çš„ç®€å•æ¨¡å¼
	existingObj, err := f.NewObject(ctx, src.Remote())
	switch err {
	case nil:
		return existingObj, existingObj.Update(ctx, in, src, options...)
	case fs.ErrorObjectNotFound:
		// Not found so create it
		return f.PutUnchecked(ctx, in, src, options...)
	default:
		return nil, err
	}
}

// PutUnchecked uploads the object without checking for existence first.
func (f *Fs) PutUnchecked(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	return f.putUnchecked(ctx, in, src, src.Remote(), options...)
}

// putUnchecked uploads the object
func (f *Fs) putUnchecked(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	if f.isShare {
		return nil, errors.New("unsupported operation: Put on shared filesystem")
	}

	// Handle cross-cloud transfers
	if f.isRemoteSource(src) {
		fs.Debugf(f, "â˜ï¸ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s -> 115 (å¤§å°: %s)", src.Fs().Name(), fs.SizeSuffix(src.Size()))

		// æ£€æŸ¥æ˜¯å¦å¯ç”¨æµå¼å“ˆå¸Œæ¨¡å¼
		if f.opt.StreamHashMode {
			fs.Infof(f, "ğŸŒŠ å¯ç”¨æµå¼å“ˆå¸Œæ¨¡å¼: %s (%s)", remote, fs.SizeSuffix(src.Size()))
			// ä½¿ç”¨å·²ç»æ‰“å¼€çš„Readerè¿›è¡Œæµå¼å“ˆå¸Œä¼ è¾“ï¼Œé¿å…é‡å¤ä¸‹è½½
			return f.smartStreamHashTransferWithReader115(ctx, in, src, remote, options...)
		} else {
			return f.crossCloudUploadWithLocalCache(ctx, in, src, remote, options...)
		}
	}

	// Call the main upload function
	newObj, err := f.upload(ctx, in, src, remote, options...)
	if err != nil {
		return nil, err
	}
	if newObj == nil {
		return nil, errors.New("internal error: upload returned nil object without error")
	}

	o := newObj.(*Object)

	// Post-upload metadata check
	if !f.opt.NoCheck && !o.hasMetaData {
		fs.Debugf(o, "ğŸ” è¿è¡Œä¸Šä¼ åå…ƒæ•°æ®æ£€æŸ¥")
		err = o.readMetaData(ctx)
		if err != nil {
			fs.Logf(o, "âŒ ä¸Šä¼ åæ£€æŸ¥å¤±è´¥: %v", err)
			o.hasMetaData = true // Mark as having metadata to avoid repeated checks
		}
	}
	f.dirCache.FlushDir(path.Dir(remote))

	return newObj, nil
}

// MergeDirs merges multiple source directories into the first one.
func (f *Fs) MergeDirs(ctx context.Context, dirs []fs.Directory) (err error) {
	if f.isShare {
		return errors.New("unsupported operation: MergeDirs on shared filesystem")
	}
	if len(dirs) < 2 {
		return nil
	}
	dstDir := dirs[0]
	dstDirID := dstDir.ID()

	for _, srcDir := range dirs[1:] {
		srcDirID := srcDir.ID()
		fs.Debugf(srcDir, "ğŸ”„ åˆå¹¶å†…å®¹åˆ° %v", dstDir)

		// List all items in the source directory
		var itemsToMove []*File
		// ğŸ”§ ä¼˜åŒ–ï¼šè®¾ç½®å›ºå®šçš„limitä¸º1150ï¼Œæœ€å¤§åŒ–æ€§èƒ½
		listChunk := defaultListChunkSize // 115ç½‘ç›˜OpenAPIçš„æœ€å¤§é™åˆ¶
		_, err = f.listAll(ctx, srcDirID, listChunk, false, func(item *File) bool {
			itemsToMove = append(itemsToMove, item)
			return false // Collect all items
		})
		if err != nil {
			return fmt.Errorf("MergeDirs list failed on %v: %w", srcDir, err)
		}

		// Move items in chunks
		chunkSize := listChunk // Use list chunk size for move chunks
		for i := 0; i < len(itemsToMove); i += chunkSize {
			end := min(i+chunkSize, len(itemsToMove))
			chunk := itemsToMove[i:end]
			if len(chunk) == 0 {
				continue
			}

			var idsToMove []string
			for _, item := range chunk {
				idsToMove = append(idsToMove, item.ID())
			}

			fs.Debugf(srcDir, "ğŸ“¦ ç§»åŠ¨ %d ä¸ªé¡¹ç›®åˆ° %v", len(idsToMove), dstDir)
			if err = f.moveFiles(ctx, idsToMove, dstDirID); err != nil {
				return fmt.Errorf("MergeDirs move failed for %v: %w", srcDir, err)
			}
		}
	}

	// Remove the source directories (now empty)
	var dirsToDelete []string
	for _, srcDir := range dirs[1:] {
		dirsToDelete = append(dirsToDelete, srcDir.ID())
	}

	// Delete directories in chunks
	chunkSize := fs.GetConfig(ctx).Checkers
	if chunkSize <= 0 {
		chunkSize = 1150 // 115ç½‘ç›˜OpenAPIçš„æœ€å¤§é™åˆ¶
	}
	for i := 0; i < len(dirsToDelete); i += chunkSize {
		end := min(i+chunkSize, len(dirsToDelete))
		chunkIDs := dirsToDelete[i:end]
		if len(chunkIDs) == 0 {
			continue
		}

		fs.Debugf(f, "ğŸ—‘ï¸ åˆ é™¤å·²åˆå¹¶çš„æºç›®å½•: %v", chunkIDs)
		if err = f.deleteFiles(ctx, chunkIDs); err != nil {
			// Log error but continue trying to delete others
			fs.Errorf(f, "âŒ MergeDirsåˆ é™¤åˆ†ç‰‡ç›®å½•å¤±è´¥ %v: %v", chunkIDs, err)
		}
	}

	// Flush the cache for the source directories
	for _, srcDir := range dirs[1:] {
		f.dirCache.FlushDir(srcDir.Remote())
	}

	return nil // Return nil even if some deletions failed, as merge likely succeeded partially
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	if f.isShare {
		return errors.New("unsupported operation: Mkdir on shared filesystem")
	}
	_, err := f.dirCache.FindDir(ctx, dir, true) // create = true
	if err == nil {
		// é‡æ„ï¼šåˆ›å»ºç›®å½•æˆåŠŸåï¼Œæ¸…ç†dirCacheç›¸å…³ç¼“å­˜
		// æ–°ç›®å½•åˆ›å»ºå¯èƒ½å½±å“çˆ¶ç›®å½•çš„ç¼“å­˜
		f.dirCache.FlushDir(path.Dir(dir))
	}
	return err
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	if f.isShare {
		return nil, fs.ErrorCantMove
	}
	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(src, "âŒ æ— æ³•ç§»åŠ¨ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantMove
	}
	// Ensure metadata is read for srcObj.id
	err := srcObj.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read source metadata for move: %w", err)
	}
	if srcObj.id == "" {
		return nil, errors.New("cannot move object with empty ID")
	}

	// Fix: æ­£ç¡®åˆ†å‰²è·¯å¾„ï¼Œé¿å…å°†æ–‡ä»¶åå½“ä½œç›®å½•åˆ›å»º
	dstDirectory, dstLeaf := dircache.SplitPath(remote)
	dstParentID, err := f.dirCache.FindDir(ctx, dstDirectory, true)
	if err != nil {
		return nil, fmt.Errorf("failed to find destination directory for move: %w", err)
	}

	// Find source parent directory ID
	srcLeaf, srcParentID, err := srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false)
	if err != nil {
		// Log error but proceed, maybe parent doesn't exist in cache but file does
		fs.Logf(src, "Could not find source path in cache for move: %v", err)
		// Attempt to get parent ID from object if available
		if srcObj.parent != "" {
			srcParentID = srcObj.parent
		} else {
			return nil, fmt.Errorf("failed to find source directory for move and object has no parent ID: %w", err)
		}
	}

	// Perform the move if parents differ
	if srcParentID != dstParentID {
		fs.Debugf(srcObj, "ğŸ“¦ ç§»åŠ¨ %q ä» %q åˆ° %q", srcObj.id, srcParentID, dstParentID)
		err = f.moveFiles(ctx, []string{srcObj.id}, dstParentID)
		if err != nil {
			return nil, fmt.Errorf("server-side move failed: %w", err)
		}
	}

	// Perform rename if names differ
	if srcLeaf != dstLeaf {
		fs.Debugf(srcObj, "ğŸ“ é‡å‘½å %q ä¸º %q", srcLeaf, dstLeaf)
		err = f.renameFile(ctx, srcObj.id, dstLeaf)
		if err != nil {
			// Attempt to move back if rename fails? Or just return error?
			return nil, fmt.Errorf("failed to rename after move: %w", err)
		}
	}

	// Create new object representing the destination
	dstObj := &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: false, // Mark metadata as stale
		id:          srcObj.id,
		parent:      dstParentID, // Update parent ID
		size:        srcObj.size,
		sha1sum:     srcObj.sha1sum,
		pickCode:    srcObj.pickCode,
		modTime:     srcObj.modTime, // Keep original modTime? Or update? Keep for now.
		durlMu:      new(sync.Mutex),
	}

	// Read metadata for the new object to confirm and update details
	err = dstObj.readMetaData(ctx)
	if err != nil {
		// Log error but return the object anyway, metadata might be eventually consistent
		fs.Logf(dstObj, "Failed to read metadata after move: %v", err)
	} else {
		dstObj.hasMetaData = true
	}

	// Flush source directory from cache
	dir, _ := dircache.SplitPath(srcObj.remote)
	srcObj.fs.dirCache.FlushDir(dir)

	// é‡æ„ï¼šç§»åŠ¨æˆåŠŸåï¼Œæ¸…ç†dirCacheç›¸å…³ç¼“å­˜
	// ç§»åŠ¨æ“ä½œå½±å“æºå’Œç›®æ ‡ç›®å½•ï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.dirCache.FlushDir(path.Dir(srcObj.remote))
	f.dirCache.FlushDir(path.Dir(remote))

	return dstObj, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	if f.isShare {
		return fs.ErrorCantDirMove
	}
	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(srcFs, "âŒ æ— æ³•ç§»åŠ¨ç›®å½• - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return fs.ErrorCantDirMove
	}

	// Use dircache helper to prepare for move
	srcID, srcParentID, srcLeaf, dstParentID, dstLeaf, err := f.dirCache.DirMove(ctx, srcFs.dirCache, srcFs.root, srcRemote, f.root, dstRemote)
	if err != nil {
		return err // Errors like DirExists, CantMoveRoot handled by DirMove helper
	}

	// Perform the move if parents differ
	if srcParentID != dstParentID {
		fs.Debugf(srcFs, "ğŸ“ ç§»åŠ¨ç›®å½• %q ä» %q åˆ° %q", srcID, srcParentID, dstParentID)
		err = f.moveFiles(ctx, []string{srcID}, dstParentID)
		if err != nil {
			return fmt.Errorf("server-side directory move failed: %w", err)
		}
	}

	// Perform rename if names differ
	if srcLeaf != dstLeaf {
		fs.Debugf(srcFs, "ğŸ“ é‡å‘½åç›®å½• %q ä¸º %q", srcLeaf, dstLeaf)
		err = f.renameFile(ctx, srcID, dstLeaf)
		if err != nil {
			return fmt.Errorf("failed to rename directory after move: %w", err)
		}
	}

	// Flush source directory from cache
	srcFs.dirCache.FlushDir(srcRemote)
	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(src, "âŒ æ— æ³•å¤åˆ¶ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantCopy
	}

	// Special handling for copying *from* a shared remote (using traditional API)
	if srcObj.fs.isShare {
		return nil, errors.New("copying from shared remotes is not supported")
	}

	// --- Standard Copy (Non-Share Source) ---
	if f.isShare {
		// Cannot copy *to* a shared remote
		return nil, errors.New("copying to a shared remote is not supported")
	}

	// Ensure metadata is read for srcObj.id
	err := srcObj.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read source metadata for copy: %w", err)
	}
	if srcObj.id == "" {
		return nil, errors.New("cannot copy object with empty ID")
	}

	// Fix: æ­£ç¡®åˆ†å‰²è·¯å¾„ï¼Œé¿å…å°†æ–‡ä»¶åå½“ä½œç›®å½•åˆ›å»º
	dstDirectory, dstLeaf := dircache.SplitPath(remote)
	dstParentID, err := f.dirCache.FindDir(ctx, dstDirectory, true)
	if err != nil {
		return nil, fmt.Errorf("failed to find destination directory for copy: %w", err)
	}

	// Check if source and destination parent are the same
	srcParentID := srcObj.parent
	if srcParentID == "" { // Try to get from cache if not on object
		_, srcParentID, _ = srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false)
	}
	if srcParentID == dstParentID {
		// API restriction: cannot copy within the same directory
		// We could potentially handle this by copying to a temp dir and moving,
		// but for now, return ErrorCantCopy.
		fs.Debugf(src, "âŒ æ— æ³•å¤åˆ¶ - æºç›®å½•å’Œç›®æ ‡ç›®å½•ç›¸åŒ (%q)", srcParentID)
		return nil, fs.ErrorCantCopy
	}

	// Perform the copy using OpenAPI
	fs.Debugf(srcObj, "ğŸ“‹ å¤åˆ¶ %q åˆ° %q", srcObj.id, dstParentID)
	err = f.copyFiles(ctx, []string{srcObj.id}, dstParentID)
	if err != nil {
		return nil, fmt.Errorf("server-side copy failed: %w", err)
	}

	// Find the newly created object in the destination directory
	// It will initially have the same name as the source object.
	srcLeaf, _, _ := srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false) // Get original leaf name
	dir, _ := dircache.SplitPath(remote)
	copiedObjPath := path.Join(dir, srcLeaf)       // Construct path where copied object should be
	newObj, err := f.NewObject(ctx, copiedObjPath) // Find the object at that path
	if err != nil {
		return nil, fmt.Errorf("failed to find copied object in destination: %w", err)
	}
	newObjConcrete := newObj.(*Object)

	// Rename the copied object if the target remote name is different
	if srcLeaf != dstLeaf {
		fs.Debugf(newObj, "ğŸ“ é‡å‘½åå¤åˆ¶çš„å¯¹è±¡ä¸º %q", dstLeaf)
		err = f.renameFile(ctx, newObjConcrete.id, dstLeaf)
		if err != nil {
			// Attempt to delete the wrongly named copy? Or just return error?
			_ = f.deleteFiles(ctx, []string{newObjConcrete.id}) // Best effort cleanup
			return nil, fmt.Errorf("failed to rename after copy: %w", err)
		}
		// Update the object's remote path and mark metadata stale
		newObjConcrete.remote = remote
		newObjConcrete.hasMetaData = false
		// Read metadata again to confirm rename and get latest info
		err = newObjConcrete.readMetaData(ctx)
		if err != nil {
			fs.Logf(newObj, "Failed to read metadata after rename: %v", err)
		} else {
			newObjConcrete.hasMetaData = true
		}
	} else {
		// If no rename needed, ensure the returned object has the correct remote path
		newObjConcrete.remote = remote
	}

	return newObjConcrete, nil
}

// purgeCheck removes the root directory. Refuses if check=true and not empty.
func (f *Fs) purgeCheck(ctx context.Context, dir string, check bool) error {
	if f.isShare {
		return errors.New("unsupported operation: Purge/Rmdir on shared filesystem")
	}
	root := path.Join(f.root, dir)
	if root == "" && dir != "" { // Check if trying to delete the effective root specified in config
		// This case needs careful handling. If root_folder_id is set, `dir` might be ""
		// but `f.rootFolderID` is not "0".
		if f.rootFolderID == "0" {
			return errors.New("internal error: attempting to purge root directory")
		}
		// Allow purging the configured root folder ID
	} else if root == "" && dir == "" && f.rootFolderID == "0" {
		// Explicitly prevent purging the absolute root "0"
		return errors.New("refusing to purge the absolute root directory '0'")
	}

	// Find the ID of the directory to purge
	dirID, err := f.dirCache.FindDir(ctx, dir, false) // Don't create
	if err != nil {
		return err // Return DirNotFound or other errors
	}

	if check {
		// Check if directory is empty using listAll with limit 1
		found, listErr := f.listAll(ctx, dirID, 1, false, func(item *File) bool {
			fs.Debugf(f, "ğŸ” Rmdiræ£€æŸ¥: ç›®å½• %q åŒ…å« %q", dir, item.FileNameBest())
			return true // Found an item, stop listing
		})
		if listErr != nil {
			return fmt.Errorf("failed to check if directory %q is empty: %w", dir, listErr)
		}
		if found {
			return fs.ErrorDirectoryNotEmpty
		}
	}

	// Perform the delete using OpenAPI
	fs.Debugf(f, "ğŸ—‘ï¸ æ¸…ç©ºç›®å½• %q (ID: %q)", dir, dirID)
	err = f.deleteFiles(ctx, []string{dirID})
	if err != nil {
		return fmt.Errorf("failed to delete directory %q (ID: %q): %w", dir, dirID, err)
	}

	// Flush the directory from cache
	f.dirCache.FlushDir(dir)
	return nil
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	return f.purgeCheck(ctx, dir, true) // check = true
}

// Purge removes a directory and all its contents.
func (f *Fs) Purge(ctx context.Context, dir string) error {
	return f.purgeCheck(ctx, dir, false) // check = false
}

// About gets quota information (currently uses traditional API).
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	// Using traditional indexInfo for now
	info, err := f.indexInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get usage info: %w", err)
	}

	usage := &fs.Usage{}
	if totalInfo, ok := info.SpaceInfo["all_total"]; ok {
		usage.Total = fs.NewUsageValue(int64(totalInfo.Size))
	}
	if useInfo, ok := info.SpaceInfo["all_use"]; ok {
		usage.Used = fs.NewUsageValue(int64(useInfo.Size))
	}
	if remainInfo, ok := info.SpaceInfo["all_remain"]; ok {
		usage.Free = fs.NewUsageValue(int64(remainInfo.Size))
	}

	return usage, nil
}

// Shutdown shuts down the fs, closing any background tasks
func (f *Fs) Shutdown(ctx context.Context) error {
	if f.tokenRenewer != nil {
		f.tokenRenewer.Shutdown()
		f.tokenRenewer = nil
	}

	// ç§»é™¤BadgerDBç¼“å­˜å…³é—­ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

	return nil
}

// itemToDirEntry converts an File to an fs.DirEntry
func (f *Fs) itemToDirEntry(ctx context.Context, remote string, item *File) (entry fs.DirEntry, err error) {
	if item.IsDir() {
		// Cache the directory ID
		f.dirCache.Put(remote, item.ID())
		d := fs.NewDir(remote, item.ModTime()).SetID(item.ID()).SetParentID(item.ParentID())
		return d, nil
	}
	// It's a file
	entry, err = f.newObjectWithInfo(ctx, remote, item)
	if err == fs.ErrorObjectNotFound {
		return nil, nil // Should not happen if item came from listing
	}
	return entry, err
}

// newObjectWithInfo creates an fs.Object from an File or by reading metadata.
func (f *Fs) newObjectWithInfo(ctx context.Context, remote string, info *File) (fs.Object, error) {

	o := &Object{
		fs:     f,
		remote: remote,
		durlMu: new(sync.Mutex),
	}
	var err error
	if info != nil {
		// Set metadata from provided info
		err = o.setMetaData(info)
	} else {
		// Read metadata from the backend
		err = o.readMetaData(ctx)
	}
	if err != nil {
		fs.Debugf(f, " newObjectWithInfoå¤±è´¥: %v", err)
		return nil, err
	}
	return o, nil
}

// readMetaDataForPath finds metadata for a specific file path.
func (f *Fs) readMetaDataForPath(ctx context.Context, path string) (info *File, err error) {
	fs.Debugf(f, " readMetaDataForPathå¼€å§‹: path=%q", path)

	leaf, dirID, err := f.dirCache.FindPath(ctx, path, false)
	if err != nil {
		fs.Debugf(f, " readMetaDataForPath: FindPathå¤±è´¥: %v", err)
		// æ£€æŸ¥æ˜¯å¦æ˜¯APIé™åˆ¶é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™ç«‹å³è¿”å›ï¼Œé¿å…è·¯å¾„æ··ä¹±
		if strings.Contains(err.Error(), "770004") || strings.Contains(err.Error(), "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™") {
			fs.Debugf(f, "readMetaDataForPathé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œè·¯å¾„: %q, é”™è¯¯: %v", path, err)
			// é‡æ„ï¼šæ¸…ç†dirCacheè€Œä¸æ˜¯pathCache
			f.dirCache.Flush()
			return nil, err
		}
		if err == fs.ErrorDirNotFound {
			// ä¿®å¤ï¼šåœ¨è·¨äº‘ä¼ è¾“æ—¶ï¼Œç›®å½•å¯èƒ½ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
			fs.Debugf(f, " readMetaDataForPath: ç›®å½•ä¸å­˜åœ¨ï¼Œè·¯å¾„: %q", path)
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, " readMetaDataForPath: FindPathæˆåŠŸ, leaf=%q, dirID=%q", leaf, dirID)

	// ğŸ”§ å¤„ç†ç©ºç›®å½•IDï¼šå½“dirIDä¸ºç©ºæ—¶ï¼Œä½¿ç”¨å½“å‰Fsçš„rootFolderID
	if dirID == "" {
		dirID = f.rootFolderID
		fs.Debugf(f, "ğŸ”§ readMetaDataForPath: ä½¿ç”¨æ ¹ç›®å½•ID: %s, è·¯å¾„: %q, å¶èŠ‚ç‚¹: %q", dirID, path, leaf)
	}

	// List the directory and find the leaf
	fs.Debugf(f, "readMetaDataForPath: starting listAll call, dirID=%q, leaf=%q", dirID, leaf)
	// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨å¸¸é‡å®šä¹‰çš„limitï¼Œæœ€å¤§åŒ–æ€§èƒ½
	listChunk := defaultListChunkSize // 115ç½‘ç›˜OpenAPIçš„æœ€å¤§é™åˆ¶
	found, err := f.listAll(ctx, dirID, listChunk, true, func(item *File) bool {
		// Compare with decoded name to handle special characters correctly
		decodedName := f.opt.Enc.ToStandardName(item.FileNameBest())
		if decodedName == leaf {
			info = item
			return true // Found it
		}
		return false // Keep looking
	})
	if err != nil {
		fs.Debugf(f, " readMetaDataForPath: listAllå¤±è´¥: %v", err)
		return nil, fmt.Errorf("failed to list directory %q to find %q: %w", dirID, leaf, err)
	}
	if !found {
		return nil, fs.ErrorObjectNotFound
	}
	return info, nil
}

// createObject creates a placeholder Object struct before upload.
func (f *Fs) createObject(ctx context.Context, remote string, modTime time.Time, size int64) (o *Object, leaf string, dirID string, err error) {
	// Fix: æ­£ç¡®åˆ†å‰²è·¯å¾„ï¼Œé¿å…å°†æ–‡ä»¶åå½“ä½œç›®å½•åˆ›å»º
	// å‚è€ƒå…¶ä»–ç½‘ç›˜backendçš„æ­£ç¡®åšæ³•
	directory, leaf := dircache.SplitPath(remote)

	// åªåˆ›å»ºç›®å½•éƒ¨åˆ†ï¼Œä¸è¦è®©FindPathå¤„ç†å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
	dirID, err = f.dirCache.FindDir(ctx, directory, true)
	if err != nil {
		return
	}
	// Temporary Object under construction
	o = &Object{
		fs:          f,
		remote:      remote,
		parent:      dirID,
		size:        size,
		modTime:     modTime,
		hasMetaData: false, // Metadata not set yet
		durlMu:      new(sync.Mutex),
	}
	return o, leaf, dirID, nil
}

// ------------------------------------------------------------
// Command Help & Execution
// ------------------------------------------------------------

var commandHelp = []fs.CommandHelp{{
	Name:  "media-sync",
	Short: "åŒæ­¥åª’ä½“åº“å¹¶åˆ›å»ºä¼˜åŒ–çš„.strmæ–‡ä»¶",
	Long: `å°†115ç½‘ç›˜ä¸­çš„è§†é¢‘æ–‡ä»¶åŒæ­¥åˆ°æœ¬åœ°ç›®å½•ï¼Œåˆ›å»ºå¯¹åº”çš„.strmæ–‡ä»¶ã€‚
.strmæ–‡ä»¶å°†åŒ…å«ä¼˜åŒ–çš„pick_codeæ ¼å¼ï¼Œæ”¯æŒç›´æ¥æ’­æ”¾å’Œåª’ä½“åº“ç®¡ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend media-sync 115:Movies /local/media/movies
rclone backend media-sync 115:Videos /local/media/videos -o min-size=200M -o strm-format=true

æ”¯æŒçš„è§†é¢‘æ ¼å¼: mp4, mkv, avi, mov, wmv, flv, webm, m4v, 3gp, ts, m2ts
.strmæ–‡ä»¶å†…å®¹æ ¼å¼: 115://pick_code (å¯é€šè¿‡strm-formaté€‰é¡¹è°ƒæ•´)`,
	Opts: map[string]string{
		"min-size":    "æœ€å°æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼Œå°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†è¢«å¿½ç•¥ (é»˜è®¤: 100M)",
		"strm-format": ".strmæ–‡ä»¶å†…å®¹æ ¼å¼: true(ä¼˜åŒ–æ ¼å¼)/false(è·¯å¾„æ ¼å¼) (é»˜è®¤: trueï¼Œå…¼å®¹: fileid/pickcode/path)",
		"include":     "åŒ…å«çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš” (é»˜è®¤: mp4,mkv,avi,mov,wmv,flv,webm,m4v,3gp,ts,m2ts)",
		"exclude":     "æ’é™¤çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš”",
		"update-mode": "æ›´æ–°æ¨¡å¼: full/incremental (é»˜è®¤: full)",
		"sync-delete": "åŒæ­¥åˆ é™¤: false(ä»…åˆ›å»º.strmæ–‡ä»¶)/true(åˆ é™¤å­¤ç«‹æ–‡ä»¶å’Œç©ºç›®å½•) (é»˜è®¤: trueï¼Œç±»ä¼¼rclone sync)",
		"dry-run":     "é¢„è§ˆæ¨¡å¼ï¼Œæ˜¾ç¤ºå°†è¦åˆ›å»ºçš„æ–‡ä»¶ä½†ä¸å®é™…åˆ›å»º (true/false)",
		"target-path": "ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœä¸åœ¨å‚æ•°ä¸­æŒ‡å®šåˆ™å¿…é¡»é€šè¿‡æ­¤é€‰é¡¹æä¾›",
	},
}, {
	Name:  "get-download-url",
	Short: "é€šè¿‡pick_codeæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL",
	Long: `é€šè¿‡115ç½‘ç›˜çš„pick_codeæˆ–.strmæ–‡ä»¶å†…å®¹è·å–å®é™…çš„ä¸‹è½½URLã€‚
æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œç‰¹åˆ«é€‚ç”¨äºåª’ä½“æœåŠ¡å™¨å’Œ.strmæ–‡ä»¶å¤„ç†ã€‚
è¿”å›æ ¼å¼ä¸getdownloadurluaä¿æŒä¸€è‡´ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend get-download-url 115: "115://eybr9y4jowdenzff0"
rclone backend get-download-url 115: "eybr9y4jowdenzff0"
rclone backend get-download-url 115: "/path/to/file.mp4"

è¾“å…¥æ ¼å¼æ”¯æŒ:
- 115://pick_code æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
- çº¯pick_code
- æ–‡ä»¶è·¯å¾„ (è‡ªåŠ¨è§£æä¸ºpick_code)`,
}, {
	Name:  "media-sync",
	Short: "åŒæ­¥åª’ä½“åº“å¹¶åˆ›å»ºä¼˜åŒ–çš„.strmæ–‡ä»¶",
	Long: `å°†115ç½‘ç›˜ä¸­çš„è§†é¢‘æ–‡ä»¶åŒæ­¥åˆ°æœ¬åœ°ç›®å½•ï¼Œåˆ›å»ºå¯¹åº”çš„.strmæ–‡ä»¶ã€‚
.strmæ–‡ä»¶å°†åŒ…å«ä¼˜åŒ–çš„pick_codeæ ¼å¼ï¼Œæ”¯æŒç›´æ¥æ’­æ”¾å’Œåª’ä½“åº“ç®¡ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend media-sync 115:Movies /local/media/movies
rclone backend media-sync 115:Videos /local/media/videos -o min-size=200M -o strm-format=pickcode

æ”¯æŒçš„è§†é¢‘æ ¼å¼: mp4, mkv, avi, mov, wmv, flv, webm, m4v, 3gp, ts, m2ts
.strmæ–‡ä»¶å†…å®¹æ ¼å¼: 115://pick_code (å¯é€šè¿‡strm-formaté€‰é¡¹è°ƒæ•´)`,
	Opts: map[string]string{
		"min-size":    "æœ€å°æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼Œå°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†è¢«å¿½ç•¥ (é»˜è®¤: 100M)",
		"strm-format": ".strmæ–‡ä»¶å†…å®¹æ ¼å¼: pickcode/path (é»˜è®¤: pickcode)",
		"include":     "åŒ…å«çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš” (é»˜è®¤: mp4,mkv,avi,mov,wmv,flv,webm,m4v,3gp,ts,m2ts)",
		"exclude":     "æ’é™¤çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš”",
		"update-mode": "æ›´æ–°æ¨¡å¼: full/incremental (é»˜è®¤: full)",
		"dry-run":     "é¢„è§ˆæ¨¡å¼ï¼Œæ˜¾ç¤ºå°†è¦åˆ›å»ºçš„æ–‡ä»¶ä½†ä¸å®é™…åˆ›å»º (true/false)",
		"target-path": "ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœä¸åœ¨å‚æ•°ä¸­æŒ‡å®šåˆ™å¿…é¡»é€šè¿‡æ­¤é€‰é¡¹æä¾›",
	},
},
}

// Command executes backend-specific commands.
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {

	case "media-sync":
		// New: Media library sync functionality
		return f.mediaSyncCommand(ctx, arg, opt)

	case "get-download-url":
		// New: Get download URL via pick_code
		return f.getDownloadURLCommand(ctx, arg, opt)

	case "refresh-cache":
		// ğŸ”„ æ–°å¢ï¼šåˆ·æ–°ç›®å½•ç¼“å­˜
		return f.refreshCacheCommand(ctx, arg)

	default:
		return nil, fs.ErrorCommandNotFound
	}
}

func (f *Fs) GetPickCodeByPath(ctx context.Context, path string) (string, error) {
	// Fix: Use correct parameter types and methods according to official API documentation
	fs.Debugf(f, "ğŸ” é€šè¿‡è·¯å¾„è·å–PickCode: %s", path)

	// æ ¹æ®APIæ–‡æ¡£ï¼Œå¯ä»¥ä½¿ç”¨GETæ–¹æ³•ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨POSTæ–¹æ³•
	// è¿™é‡Œä½¿ç”¨GETæ–¹æ³•æ›´ç®€æ´
	opts := rest.Opts{
		Method: "GET",
		Path:   "/open/folder/get_info",
		Parameters: url.Values{
			"path": []string{path},
		},
	}

	var response struct {
		State   bool   `json:"state"` // Fix: According to API docs, state is boolean type
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			PickCode     string `json:"pick_code"`
			FileName     string `json:"file_name"`
			FileID       string `json:"file_id"`
			FileCategory string `json:"file_category"` // 1:æ–‡ä»¶, 0:æ–‡ä»¶å¤¹
			Size         string `json:"size"`
			SizeByte     int64  `json:"size_byte"`
		} `json:"data"`
	}

	err := f.CallOpenAPI(ctx, &opts, nil, &response, false)
	if err != nil {
		return "", fmt.Errorf("failed to get PickCode: %w", err)
	}

	// Check API response status
	if response.Code != 0 {
		return "", fmt.Errorf("API returned error: %s (Code: %d)", response.Message, response.Code)
	}

	fs.Debugf(f, "âœ… è·å–æ–‡ä»¶ä¿¡æ¯: %s -> PickCode: %s, name: %s, size: %d bytes",
		path, response.Data.PickCode, response.Data.FileName, response.Data.SizeByte)

	// è¿”å› PickCode
	return response.Data.PickCode, nil
}

// getPickCodeByFileID é€šè¿‡æ–‡ä»¶IDè·å–pickCode
func (f *Fs) getPickCodeByFileID(ctx context.Context, fileID string) (string, error) {
	// ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„è·å–æ–‡ä»¶è¯¦æƒ…
	opts := rest.Opts{
		Method: "GET",
		Path:   "/open/folder/get_info",
		Parameters: url.Values{
			"file_id": []string{fileID},
		},
	}

	var response struct {
		State   bool   `json:"state"`
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			Count        any    `json:"count"` // å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
			Size         any    `json:"size"`  // å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
			SizeByte     int64  `json:"size_byte"`
			FolderCount  any    `json:"folder_count"` // å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
			PlayLong     int64  `json:"play_long"`
			ShowPlayLong int    `json:"show_play_long"`
			Ptime        string `json:"ptime"`
			Utime        string `json:"utime"`
			FileName     string `json:"file_name"`
			PickCode     string `json:"pick_code"`
			Sha1         string `json:"sha1"`
			FileID       string `json:"file_id"`
			IsMark       any    `json:"is_mark"` // å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
			OpenTime     int64  `json:"open_time"`
			FileCategory any    `json:"file_category"` // å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—
		} `json:"data"`
	}

	err := f.CallOpenAPI(ctx, &opts, nil, &response, false)
	if err != nil {
		return "", fmt.Errorf("failed to get pickCode via file ID: %w", err)
	}

	if !response.State || response.Code != 0 {
		return "", fmt.Errorf("API returned error: %s (Code: %d, State: %t)", response.Message, response.Code, response.State)
	}

	if response.Data.PickCode == "" {
		return "", fmt.Errorf("API returned empty pickCode for file ID: %s", fileID)
	}

	fs.Debugf(f, " æˆåŠŸé€šè¿‡æ–‡ä»¶IDè·å–pickCode: fileID=%s, pickCode=%s, fileName=%s",
		fileID, response.Data.PickCode, response.Data.FileName)

	return response.Data.PickCode, nil
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, UA string) (string, error) {
	// Fix: Add path processing logic consistent with 123 backend
	if filePath == "" {
		filePath = f.root
	}

	// Path cleaning logic, referencing 123 backend
	if filePath == "/" {
		return "", fmt.Errorf("root directory is not a file")
	}
	if len(filePath) > 1 && strings.HasSuffix(filePath, "/") {
		filePath = filePath[:len(filePath)-1]
	}

	fs.Debugf(f, "ğŸ”§ å¤„ç†æ–‡ä»¶è·¯å¾„: %s", filePath)

	// Performance optimization: Direct HTTP call to get PickCode, avoiding rclone framework overhead
	pickCode, err := f.getPickCodeByPathDirect(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to read metadata for file path %q: %w", filePath, err)
	}

	fs.Debugf(f, "âœ… æˆåŠŸé€šè¿‡è·¯å¾„è·å–pick_code: %s -> %s", filePath, pickCode)

	// ä½¿ç”¨é€šç”¨çš„åŸå§‹HTTPå®ç°
	return f.getDownloadURLByPickCodeHTTP(ctx, pickCode, UA)
}

// getPickCodeByPathDirect ä½¿ç”¨rcloneæ ‡å‡†æ–¹å¼è·å–PickCode
func (f *Fs) getPickCodeByPathDirect(ctx context.Context, path string) (string, error) {
	fs.Debugf(f, "ğŸ”§ ä½¿ç”¨rcloneæ ‡å‡†æ–¹å¼è·å–PickCode: %s", path)

	// æ„å»ºè¯·æ±‚é€‰é¡¹
	opts := rest.Opts{
		Method: "GET",
		Path:   "/open/folder/get_info",
		Parameters: url.Values{
			"path": []string{path},
		},
	}

	// å‡†å¤‡è®¤è¯ä¿¡æ¯
	_ = f.prepareTokenForRequest(ctx, &opts)

	// è§£æå“åº”
	var response struct {
		State   bool   `json:"state"`
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			PickCode     string `json:"pick_code"`
			FileName     string `json:"file_name"`
			FileID       string `json:"file_id"`
			FileCategory string `json:"file_category"`
		} `json:"data"`
	}

	// ä½¿ç”¨ç»Ÿä¸€APIè°ƒç”¨æ–¹æ³•
	err := f.CallAPI(ctx, OpenAPI, &opts, nil, &response)
	if err != nil {
		return "", fmt.Errorf("request failed: %w", err)
	}

	if response.Code != 0 {
		return "", fmt.Errorf("API returned error: %s (Code: %d)", response.Message, response.Code)
	}

	fs.Debugf(f, "âœ… ç›´æ¥HTTPè·å–PickCodeæˆåŠŸ: %s -> %s", path, response.Data.PickCode)
	return response.Data.PickCode, nil
}

// ------------------------------------------------------------
// Object Methods
// ------------------------------------------------------------

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	err := o.readMetaData(ctx)
	if err != nil {
		// åœ¨è·¨äº‘ä¼ è¾“æ—¶ï¼Œç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨æ˜¯æ­£å¸¸æƒ…å†µï¼Œé™çº§ä¸ºè°ƒè¯•ä¿¡æ¯
		if err == fs.ErrorObjectNotFound {
			fs.Debugf(o, "ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨ï¼ŒModTimeå°†ä½¿ç”¨é›¶å€¼: %v", err)
		} else {
			fs.Logf(o, "failed to read metadata for ModTime: %v", err)
		}
		// Return a zero time instead of Now() as Precision is NotSupported
		return time.Time{}
	}
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// Return size immediately if known, otherwise read metadata
	if o.hasMetaData || o.size > 0 { // Check if size is already populated
		return o.size
	}
	err := o.readMetaData(context.TODO()) // Use TODO context for simplicity here
	if err != nil {
		fs.Logf(o, "failed to read metadata for Size: %v", err)
		return -1 // Indicate error or unknown size
	}
	return o.size
}

// Hash returns the SHA1 checksum
func (o *Object) Hash(ctx context.Context, t hash.Type) (string, error) {
	if t != hash.SHA1 {
		return "", hash.ErrUnsupported
	}
	// Return hash immediately if known, otherwise read metadata
	if o.hasMetaData || o.sha1sum != "" {
		// å…³é”®ä¿®å¤ï¼šrcloneæœŸæœ›å°å†™å“ˆå¸Œå€¼ï¼Œä½†115ç½‘ç›˜å†…éƒ¨ä½¿ç”¨å¤§å†™
		// å¯¹å¤–è¿”å›å°å†™ï¼Œå¯¹å†…ä¿æŒå¤§å†™ç”¨äºAPIè°ƒç”¨
		return strings.ToLower(o.sha1sum), nil
	}
	err := o.readMetaData(ctx)
	if err != nil {
		return "", fmt.Errorf("failed to read metadata for Hash: %w", err)
	}
	// å…³é”®ä¿®å¤ï¼šrcloneæœŸæœ›å°å†™å“ˆå¸Œå€¼ï¼Œä½†115ç½‘ç›˜å†…éƒ¨ä½¿ç”¨å¤§å†™
	// å¯¹å¤–è¿”å›å°å†™ï¼Œå¯¹å†…ä¿æŒå¤§å†™ç”¨äºAPIè°ƒç”¨
	return strings.ToLower(o.sha1sum), nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// Return ID immediately if known, otherwise read metadata
	if o.hasMetaData || o.id != "" {
		return o.id
	}
	// Reading metadata just for ID might be inefficient, but necessary if not cached
	err := o.readMetaData(context.TODO()) // Use TODO context
	if err != nil {
		fs.Logf(o, "failed to read metadata for ID: %v", err)
		return "" // Return empty string on error
	}
	return o.id
}

// ParentID returns the parent ID of the Object
func (o *Object) ParentID() string {
	// Return parent immediately if known, otherwise read metadata
	if o.hasMetaData || o.parent != "" {
		return o.parent
	}
	err := o.readMetaData(context.TODO()) // Use TODO context
	if err != nil {
		fs.Logf(o, "failed to read metadata for ParentID: %v", err)
		return "" // Return empty string on error
	}
	return o.parent
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// Ensure metadata (specifically pickCode or ID) is available
	err := o.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read metadata before open: %w", err)
	}

	if o.size == 0 {
		// No need for download URL for 0-byte files
		return io.NopCloser(bytes.NewReader(nil)), nil
	}

	// 115ç½‘ç›˜ä¸‹è½½ç­–ç•¥ï¼šæ ¹æ®æ–‡ä»¶å¤§å°å’Œç”¨æˆ·é…ç½®å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘ä¸‹è½½
	if o.fs.opt.DownloadConcurrency > 0 && o.size >= 100*1024*1024 { // 100MBä»¥ä¸Šä¸”å¯ç”¨å¹¶å‘
		fs.Debugf(o, "ğŸ“¥ 115ä¸‹è½½ç­–ç•¥: å¯ç”¨å¹¶å‘ä¸‹è½½ (æ–‡ä»¶å¤§å°: %s, å¹¶å‘æ•°: %d)",
			fs.SizeSuffix(o.size), o.fs.opt.DownloadConcurrency)
		return o.openWithConcurrency(ctx, options...)
	}
	fs.Debugf(o, "ğŸ“¥ 115ä¸‹è½½ç­–ç•¥: ä½¿ç”¨æ™®é€šä¸‹è½½ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(o.size))

	// Get/refresh download URL
	err = o.setDownloadURL(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get download URL: %w", err)
	}
	if !o.durl.Valid() {
		// Attempt refresh again if invalid right after getting it
		fs.Debugf(o, "âš ï¸ ä¸‹è½½URLè·å–åç«‹å³æ— æ•ˆï¼Œé‡è¯•ä¸­...")
		// ä½¿ç”¨rcloneæ ‡å‡†pacerè€Œä¸æ˜¯ç›´æ¥sleep
		_ = o.fs.pacer.Call(func() (bool, error) {
			return false, nil // åªæ˜¯ä¸ºäº†åˆ©ç”¨pacerçš„å»¶è¿Ÿæœºåˆ¶
		})
		err = o.setDownloadURLWithForce(ctx, true) // Force refresh
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL on retry: %w", err)
		}
		if !o.durl.Valid() {
			return nil, errors.New("failed to obtain a valid download URL")
		}
	}

	// å¹¶å‘å®‰å…¨ä¿®å¤ï¼šä½¿ç”¨äº’æ–¥é”ä¿æŠ¤å¯¹durlçš„è®¿é—®
	o.durlMu.Lock()

	// æ£€æŸ¥URLæœ‰æ•ˆæ€§ï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if o.durl == nil || !o.durl.Valid() {
		o.durlMu.Unlock()

		// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•é‡æ–°è·å–ä¸‹è½½URL
		err := o.setDownloadURLWithForce(ctx, true)
		if err != nil {
			return nil, fmt.Errorf("failed to re-obtain download URL: %w", err)
		}

		// é‡æ–°è·å–é”ä»¥ç»§ç»­åç»­æ“ä½œ
		o.durlMu.Lock()
	}

	// åˆ›å»ºURLçš„æœ¬åœ°å‰¯æœ¬ï¼Œé¿å…åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­è¢«å…¶ä»–çº¿ç¨‹ä¿®æ”¹
	downloadURL := o.durl.URL
	o.durlMu.Unlock()

	// ä½¿ç”¨æœ¬åœ°å‰¯æœ¬åˆ›å»ºè¯·æ±‚ï¼Œé¿å…å¹¶å‘è®¿é—®é—®é¢˜
	req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
	if err != nil {
		return nil, err
	}

	// Rangeè¯·æ±‚è°ƒè¯•å’Œå¤„ç†
	fs.FixRangeOption(options, o.size)
	fs.OpenOptionAddHTTPHeaders(req.Header, options)

	// è°ƒè¯•Rangeè¯·æ±‚ä¿¡æ¯
	if rangeHeader := req.Header.Get("Range"); rangeHeader != "" {
		fs.Debugf(o, " 115ç½‘ç›˜Rangeè¯·æ±‚: %s (æ–‡ä»¶å¤§å°: %s)", rangeHeader, fs.SizeSuffix(o.size))
	}

	if o.size == 0 {
		// Don't supply range requests for 0 length objects as they always fail
		delete(req.Header, "Range")
	}

	// ä½¿ç”¨æ ‡å‡†HTTPå®¢æˆ·ç«¯è¿›è¡Œè¯·æ±‚
	httpClient := fshttp.NewClient(ctx)
	resp, err := httpClient.Do(req)
	if err != nil {
		fs.Debugf(o, "âŒ 115 HTTPè¯·æ±‚å¤±è´¥: %v", err)
		return nil, err
	}

	// Rangeå“åº”éªŒè¯
	if rangeHeader := req.Header.Get("Range"); rangeHeader != "" {
		contentLength := resp.Header.Get("Content-Length")
		contentRange := resp.Header.Get("Content-Range")
		fs.Debugf(o, "ğŸ“¥ 115 Rangeå“åº”: Status=%d, Content-Length=%s, Content-Range=%s",
			resp.StatusCode, contentLength, contentRange)

		// Check if Range request was handled correctly
		if resp.StatusCode != 206 {
			fs.Debugf(o, "âš ï¸ 115 Rangeè¯·æ±‚æœªè¿”å›206çŠ¶æ€ç : %d", resp.StatusCode)
		}
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥æä¾›ä¸‹è½½è¿›åº¦è·Ÿè¸ª
	return &ProgressReadCloser115{
		ReadCloser: resp.Body,
		fs:         o.fs,
		object:     o,
		totalSize:  o.size,
		startTime:  time.Now(),
	}, nil
}

// openWithConcurrency ä½¿ç”¨å¹¶å‘ä¸‹è½½æ‰“å¼€æ–‡ä»¶
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Debugf(o, "ğŸš€ å¯åŠ¨115ç½‘ç›˜å¹¶å‘ä¸‹è½½: æ–‡ä»¶å¤§å°=%s, å¹¶å‘æ•°=%d, åˆ†ç‰‡å¤§å°=%s",
		fs.SizeSuffix(o.size), o.fs.opt.DownloadConcurrency, fs.SizeSuffix(int64(o.fs.opt.DownloadChunkSize)))

	// æ£€æŸ¥æ˜¯å¦æœ‰Rangeé€‰é¡¹ï¼Œå¦‚æœæœ‰åˆ™å›é€€åˆ°æ™®é€šä¸‹è½½
	for _, option := range options {
		if _, ok := option.(*fs.RangeOption); ok {
			fs.Debugf(o, "âš ï¸ æ£€æµ‹åˆ°Rangeè¯·æ±‚ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½")
			return o.openNormal(ctx, options...)
		}
	}

	// è·å–ä¸‹è½½URL
	err := o.setDownloadURL(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get download URL: %w", err)
	}

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "115_concurrent_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "âš ï¸ åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// æ‰§è¡Œå¹¶å‘ä¸‹è½½
	err = o.downloadWithConcurrency(ctx, tempFile)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())

		// æ£€æŸ¥æ˜¯å¦æ˜¯403é”™è¯¯ï¼ˆ115ç½‘ç›˜ä¸æ”¯æŒå¹¶å‘Rangeè¯·æ±‚ï¼‰
		if strings.Contains(err.Error(), "403") {
			fs.Infof(o, "âš ï¸ 115ç½‘ç›˜ä¸æ”¯æŒå¹¶å‘Rangeè¯·æ±‚ï¼Œè‡ªåŠ¨ç¦ç”¨å¹¶å‘ä¸‹è½½åŠŸèƒ½")
			// ä¸´æ—¶ç¦ç”¨å½“å‰å¯¹è±¡çš„å¹¶å‘ä¸‹è½½
			return o.openNormal(ctx, options...)
		}

		fs.Debugf(o, "âš ï¸ å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("failed to seek temp file: %w", err)
	}

	// è¿”å›ä¸€ä¸ªè‡ªåŠ¨æ¸…ç†çš„ReadCloser
	return &tempFileReadCloser{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// openNormal ä½¿ç”¨æ™®é€šæ–¹å¼æ‰“å¼€æ–‡ä»¶ï¼ˆåŸæœ‰é€»è¾‘çš„é‡å‘½åï¼‰
func (o *Object) openNormal(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// è¿™é‡Œæ˜¯åŸæœ‰çš„Openå‡½æ•°é€»è¾‘ï¼Œä½†è·³è¿‡å¹¶å‘æ£€æŸ¥éƒ¨åˆ†
	// Ensure metadata (specifically pickCode or ID) is available
	err := o.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read metadata before open: %w", err)
	}

	if o.size == 0 {
		// No need for download URL for 0-byte files
		return io.NopCloser(bytes.NewReader(nil)), nil
	}

	// Get/refresh download URL
	err = o.setDownloadURL(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get download URL: %w", err)
	}
	if !o.durl.Valid() {
		// Attempt refresh again if invalid right after getting it
		fs.Debugf(o, "âš ï¸ ä¸‹è½½URLè·å–åç«‹å³æ— æ•ˆï¼Œé‡è¯•ä¸­...")
		// ä½¿ç”¨rcloneæ ‡å‡†pacerè€Œä¸æ˜¯ç›´æ¥sleep
		_ = o.fs.pacer.Call(func() (bool, error) {
			return false, nil // åªæ˜¯ä¸ºäº†åˆ©ç”¨pacerçš„å»¶è¿Ÿæœºåˆ¶
		})
		err = o.setDownloadURLWithForce(ctx, true) // Force refresh
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL on retry: %w", err)
		}
		if !o.durl.Valid() {
			return nil, errors.New("failed to obtain a valid download URL")
		}
	}

	// å¹¶å‘å®‰å…¨ä¿®å¤ï¼šä½¿ç”¨äº’æ–¥é”ä¿æŠ¤å¯¹durlçš„è®¿é—®
	o.durlMu.Lock()

	// æ£€æŸ¥URLæœ‰æ•ˆæ€§ï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if o.durl == nil || !o.durl.Valid() {
		o.durlMu.Unlock()

		// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•é‡æ–°è·å–ä¸‹è½½URL
		err := o.setDownloadURLWithForce(ctx, true)
		if err != nil {
			return nil, fmt.Errorf("failed to re-obtain download URL: %w", err)
		}

		// é‡æ–°è·å–é”ä»¥ç»§ç»­åç»­æ“ä½œ
		o.durlMu.Lock()
	}

	// åˆ›å»ºURLçš„æœ¬åœ°å‰¯æœ¬ï¼Œé¿å…åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­è¢«å…¶ä»–çº¿ç¨‹ä¿®æ”¹
	downloadURL := o.durl.URL
	o.durlMu.Unlock()

	// ä½¿ç”¨æœ¬åœ°å‰¯æœ¬åˆ›å»ºè¯·æ±‚ï¼Œé¿å…å¹¶å‘è®¿é—®é—®é¢˜
	req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
	if err != nil {
		return nil, err
	}

	// Apply any range options
	fs.FixRangeOption(options, o.size)
	for _, option := range options {
		switch x := option.(type) {
		case *fs.RangeOption:
			if x.Start >= 0 {
				rangeHeader := fmt.Sprintf("bytes=%d-", x.Start)
				if x.End >= 0 {
					rangeHeader = fmt.Sprintf("bytes=%d-%d", x.Start, x.End)
				}
				req.Header.Set("Range", rangeHeader)
				fs.Debugf(o, "ğŸ” 115 Rangeè¯·æ±‚: %s", rangeHeader)
			}
		case *fs.HTTPOption:
			key, value := x.Header()
			if key != "" && value != "" {
				req.Header.Set(key, value)
			}
		default:
			if option.Mandatory() {
				fs.Logf(o, "Unsupported mandatory option: %v", option)
			}
		}
	}

	// Execute the request
	resp, err := o.fs.openAPIClient.Do(req)
	if err != nil {
		return nil, err
	}

	// Check for successful response
	if resp.StatusCode < 200 || resp.StatusCode >= 300 {
		_ = resp.Body.Close()
		return nil, fmt.Errorf("HTTP error %d: %s", resp.StatusCode, resp.Status)
	}

	// Handle range requests
	if req.Header.Get("Range") != "" {
		// Check if Range request was handled correctly
		if resp.StatusCode != 206 {
			fs.Debugf(o, "âš ï¸ 115 Rangeè¯·æ±‚æœªè¿”å›206çŠ¶æ€ç : %d", resp.StatusCode)
		}
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥æä¾›ä¸‹è½½è¿›åº¦è·Ÿè¸ª
	return &ProgressReadCloser115{
		ReadCloser: resp.Body,
		fs:         o.fs,
		object:     o,
		totalSize:  o.size,
		startTime:  time.Now(),
	}, nil
}

// Update the object with new content.
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	if o.fs.isShare {
		return errors.New("unsupported operation: Update on shared filesystem")
	}
	if src.Size() < 0 {
		return errors.New("refusing to update with unknown size")
	}

	// Start the token renewer if we have a valid one
	if o.fs.tokenRenewer != nil {
		o.fs.tokenRenewer.Start()
		defer o.fs.tokenRenewer.Stop()
	}

	// Ensure metadata is read for the existing object
	err := o.readMetaData(ctx)
	if err != nil {
		return fmt.Errorf("failed to read metadata of existing object for update: %w", err)
	}
	oldID := o.id // Keep track of the old ID

	// Upload the new content using putUnchecked (handles creation logic)
	// This will place the new file at the same remote path.
	newObj, err := o.fs.putUnchecked(ctx, in, src, o.remote, options...)
	if err != nil {
		return fmt.Errorf("upload during update failed: %w", err)
	}

	newO := newObj.(*Object)

	// If the upload resulted in a *new* file ID (not an overwrite), delete the old one.
	if oldID != "" && newO.id != oldID {
		fs.Debugf(o, "ğŸ”„ Updateåˆ›å»ºäº†æ–°å¯¹è±¡ %qï¼Œåˆ é™¤æ—§å¯¹è±¡ %q", newO.id, oldID)
		err = o.fs.deleteFiles(ctx, []string{oldID})
		if err != nil {
			// Log error but don't fail the update, the new file is there
			fs.Errorf(o, "âŒ æ›´æ–°ååˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥ %q: %v", oldID, err)
		}
	} else {
		fs.Debugf(o, "ğŸ”„ Updateå¯èƒ½è¦†ç›–äº†ç°æœ‰å¯¹è±¡ %q", oldID)
	}

	// Replace the metadata of the original object `o` with the new object's data
	// Note: We must copy fields individually to avoid copying mutex fields
	o.fs = newO.fs
	o.remote = newO.remote
	o.hasMetaData = newO.hasMetaData
	o.id = newO.id
	o.parent = newO.parent
	o.size = newO.size
	o.sha1sum = newO.sha1sum
	o.pickCode = newO.pickCode
	o.modTime = newO.modTime
	o.durl = newO.durl
	o.durlRefreshing = newO.durlRefreshing
	// Note: durlMu and pickCodeMu are preserved from the original object

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	if o.fs.isShare {
		return errors.New("unsupported operation: Remove on shared filesystem")
	}
	// Ensure metadata (ID) is read
	err := o.readMetaData(ctx)
	if err != nil {
		// If object not found, Remove should succeed
		if errors.Is(err, fs.ErrorObjectNotFound) {
			return nil
		}
		return fmt.Errorf("failed to read metadata before remove: %w", err)
	}
	if o.id == "" {
		return errors.New("cannot remove object with empty ID")
	}

	err = o.fs.deleteFiles(ctx, []string{o.id})
	if err != nil {
		return fmt.Errorf("failed to delete object %q: %w", o.id, err)
	}
	o.fs.dirCache.FlushDir(path.Dir(o.remote))

	return nil
}

// setMetaData updates the object's metadata from an File struct.
func (o *Object) setMetaData(info *File) error {
	if info == nil {
		return errors.New("cannot set metadata from nil info")
	}
	if info.IsDir() {
		// This indicates we tried to create an Object for a directory path
		return fs.ErrorIsDir
	}
	o.id = info.ID()
	o.parent = info.ParentID()
	o.size = info.FileSizeBest()
	o.sha1sum = strings.ToLower(info.Sha1Best())

	pickCode := info.PickCodeBest()
	fileID := info.ID()

	fs.Debugf(o, " setMetaDataè°ƒè¯•: fileName=%s, pickCode=%s, fileID=%s",
		info.FileNameBest(), pickCode, fileID)

	if pickCode != "" && pickCode == fileID {
		fs.Debugf(o, "âš ï¸ æ£€æµ‹åˆ°pickCodeä¸fileIDç›¸åŒï¼Œè¿™æ˜¯é”™è¯¯çš„: %s", pickCode)
		pickCode = "" // Clear incorrect pickCode
	}

	if pickCode == "" {
		fs.Debugf(o, "ğŸ“ æ–‡ä»¶å…ƒæ•°æ®pickCodeä¸ºç©ºï¼Œéœ€è¦æ—¶å°†åŠ¨æ€è·å–")
	} else {
		isAllDigits := true
		for _, r := range pickCode {
			if r < '0' || r > '9' {
				isAllDigits = false
				break
			}
		}

		if len(pickCode) > 15 && isAllDigits {
			fs.Debugf(o, "âš ï¸ æ£€æµ‹åˆ°ç–‘ä¼¼æ–‡ä»¶IDè€ŒépickCode: %sï¼Œéœ€è¦æ—¶å°†é‡æ–°è·å–", pickCode)
			pickCode = "" // Clear incorrect pickCode, force re-obtain
		}
	}

	o.pickCode = pickCode
	o.modTime = info.ModTime()
	o.hasMetaData = true
	return nil
}

// setMetaDataFromCallBack updates metadata after an upload callback.
func (o *Object) setMetaDataFromCallBack(data *CallbackData) error {
	if data == nil {
		return errors.New("cannot set metadata from nil callback data")
	}
	// Assume size and modTime are already set from the source info
	o.id = data.FileID
	o.parent = data.CID // Callback provides parent CID
	o.pickCode = data.PickCode
	o.sha1sum = strings.ToLower(data.Sha)
	// Update size from callback if available and different?
	if data.FileSize > 0 {
		o.size = int64(data.FileSize)
	}
	// ModTime is usually the upload time, keep the original source ModTime
	o.hasMetaData = true
	return nil
}

// readMetaData gets the metadata if it hasn't already been fetched.
func (o *Object) readMetaData(ctx context.Context) error {
	if o.hasMetaData {
		return nil
	}

	// Use the path-based lookup
	info, err := o.fs.readMetaDataForPath(ctx, o.remote)
	if err != nil {
		fs.Debugf(o.fs, " readMetaDataå¤±è´¥: %v", err)
		return err // fs.ErrorObjectNotFound or other errors
	}

	err = o.setMetaData(info)
	if err != nil {
		fs.Debugf(o.fs, " readMetaData: setMetaDataå¤±è´¥: %v", err)
	}
	return err
}

// setDownloadURL ensures a valid download URL is available with optimized concurrent access.
func (o *Object) setDownloadURL(ctx context.Context) error {
	// å¹¶å‘å®‰å…¨ä¿®å¤ï¼šå¿«é€Ÿè·¯å¾„ä¹Ÿéœ€è¦é”ä¿æŠ¤
	o.durlMu.Lock()

	// æ£€æŸ¥URLæ˜¯å¦å·²ç»æœ‰æ•ˆï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if o.durl != nil && o.durl.Valid() {
		o.durlMu.Unlock()
		return nil
	}

	// URLæ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦è·å–æ–°çš„URLï¼ˆç»§ç»­æŒæœ‰é”ï¼‰
	// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ‰€æœ‰æ£€æŸ¥
	if o.durl != nil && o.durl.Valid() {
		fs.Debugf(o, "âœ… ä¸‹è½½URLå·²è¢«å…¶ä»–åç¨‹è·å–")
		o.durlMu.Unlock()
		return nil
	}
	// æ³¨æ„ï¼šè¿™é‡Œä¸é‡Šæ”¾é”ï¼Œç»§ç»­åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡ŒURLè·å–

	var err error
	var newURL *DownloadURL

	// Add timeout context for URL fetching to prevent hanging
	urlCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	if o.fs.isShare {
		// Share download is no longer supported
		return errors.New("share download is not supported")
	} else {
		// Use OpenAPI download URL endpoint
		// å¹¶å‘å®‰å…¨ï¼šè·å–pickCodeæ—¶ä½¿ç”¨é”ä¿æŠ¤
		if o.pickCode == "" {
			o.pickCodeMu.Lock()
			// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ£€æŸ¥
			if o.pickCode == "" {
				// If pickCode is missing, try getting it from metadata first
				metaErr := o.readMetaData(urlCtx)
				if metaErr != nil || o.pickCode == "" {
					// ä¿®å¤ï¼šå°è¯•é€šè¿‡æ–‡ä»¶IDè·å–pickCode
					if o.id != "" {
						fs.Debugf(o, " å°è¯•é€šè¿‡æ–‡ä»¶IDè·å–pickCode: fileID=%s", o.id)
						pickCode, pickErr := o.fs.getPickCodeByFileID(urlCtx, o.id)
						if pickErr != nil {
							o.pickCodeMu.Unlock()
							return fmt.Errorf("cannot get pick code by file ID %s: %w", o.id, pickErr)
						}
						o.pickCode = pickCode
						fs.Debugf(o, " æˆåŠŸé€šè¿‡æ–‡ä»¶IDè·å–pickCode: %s", pickCode)
					} else {
						o.pickCodeMu.Unlock()
						return fmt.Errorf("cannot get download URL without pick code (metadata read error: %v)", metaErr)
					}
				}
			}
			o.pickCodeMu.Unlock()
		}

		// ä¿®å¤å¹¶å‘é—®é¢˜ï¼šç¡®ä¿pickCodeæœ‰æ•ˆåå†è°ƒç”¨
		if o.pickCode == "" {
			return fmt.Errorf("cannot get download URL: pickCode is still empty after all attempts")
		}

		// æ ¹æœ¬æ€§ä¿®å¤ï¼šä½¿ç”¨éªŒè¯è¿‡çš„pickCode
		validPickCode, validationErr := o.fs.validateAndCorrectPickCode(urlCtx, o.pickCode)
		if validationErr != nil {
			return fmt.Errorf("pickCode validation failed: %w", validationErr)
		}

		// å¦‚æœpickCodeè¢«ä¿®æ­£ï¼Œæ›´æ–°Objectä¸­çš„pickCode
		if validPickCode != o.pickCode {
			fs.Debugf(o, " Object pickCodeå·²ä¿®æ­£: %s -> %s", o.pickCode, validPickCode)
			o.pickCode = validPickCode
		}

		newURL, err = o.fs.getDownloadURL(urlCtx, validPickCode)
	}

	if err != nil {
		o.durl = nil      // Clear invalid URL
		o.durlMu.Unlock() // é”™è¯¯è·¯å¾„é‡Šæ”¾é”

		return fmt.Errorf("failed to get download URL: %w", err)
	}

	o.durl = newURL
	if !o.durl.Valid() {
		// This might happen if the link expires immediately or is invalid
		fs.Logf(o, "Fetched download URL is invalid or expired immediately: %s", o.durl.URL)
		// æ¸…é™¤æ— æ•ˆçš„URLï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°è·å–
		o.durl = nil
		o.durlMu.Unlock() // æ— æ•ˆURLè·¯å¾„é‡Šæ”¾é”
		return fmt.Errorf("fetched download URL is invalid or expired immediately")
	} else {
		fs.Debugf(o, "âœ… æˆåŠŸè·å–ä¸‹è½½URL")
	}
	o.durlMu.Unlock() // æˆåŠŸè·¯å¾„é‡Šæ”¾é”
	return nil
}

// setDownloadURLWithForce sets the download URL for the object with optional cache bypass
func (o *Object) setDownloadURLWithForce(ctx context.Context, forceRefresh bool) error {
	o.durlMu.Lock()

	// å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ï¼Œæ£€æŸ¥URLæ˜¯å¦å·²ç»æœ‰æ•ˆï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if !forceRefresh && o.durl != nil && o.durl.Valid() {
		o.durlMu.Unlock()
		return nil
	}

	// URLæ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œæˆ–è€…å¼ºåˆ¶åˆ·æ–°ï¼Œéœ€è¦è·å–æ–°çš„URLï¼ˆç»§ç»­æŒæœ‰é”ï¼‰
	// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ‰€æœ‰æ£€æŸ¥
	if !forceRefresh && o.durl != nil && o.durl.Valid() {
		fs.Debugf(o, "âœ… ä¸‹è½½URLå·²è¢«å…¶ä»–åç¨‹è·å–")
		o.durlMu.Unlock()
		return nil
	}
	// æ³¨æ„ï¼šè¿™é‡Œä¸é‡Šæ”¾é”ï¼Œç»§ç»­åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡ŒURLè·å–

	var err error
	var newURL *DownloadURL

	// Add timeout context for URL fetching to prevent hanging
	urlCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	if o.fs.isShare {
		// Share download is no longer supported
		o.durlMu.Unlock()
		return errors.New("share download is not supported")
	} else {
		// Use OpenAPI download URL endpoint
		// å¹¶å‘å®‰å…¨ï¼šè·å–pickCodeæ—¶ä½¿ç”¨é”ä¿æŠ¤
		if o.pickCode == "" {
			o.pickCodeMu.Lock()
			// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ£€æŸ¥
			if o.pickCode == "" {
				// If pickCode is missing, try getting it from metadata first
				metaErr := o.readMetaData(urlCtx)
				if metaErr != nil || o.pickCode == "" {
					// ä¿®å¤ï¼šå°è¯•é€šè¿‡æ–‡ä»¶IDè·å–pickCode
					if o.id != "" {
						fs.Debugf(o, " å°è¯•é€šè¿‡æ–‡ä»¶IDè·å–pickCode: fileID=%s", o.id)
						pickCode, pickErr := o.fs.getPickCodeByFileID(urlCtx, o.id)
						if pickErr != nil {
							o.pickCodeMu.Unlock()
							o.durlMu.Unlock()
							return fmt.Errorf("cannot get pick code by file ID %s: %w", o.id, pickErr)
						}
						o.pickCode = pickCode
						fs.Debugf(o, " æˆåŠŸé€šè¿‡æ–‡ä»¶IDè·å–pickCode: %s", pickCode)
					} else {
						o.pickCodeMu.Unlock()
						o.durlMu.Unlock()
						return fmt.Errorf("cannot get download URL without pick code (metadata read error: %v)", metaErr)
					}
				}
			}
			o.pickCodeMu.Unlock()
		}

		// ä¿®å¤å¹¶å‘é—®é¢˜ï¼šç¡®ä¿pickCodeæœ‰æ•ˆåå†è°ƒç”¨
		if o.pickCode == "" {
			o.durlMu.Unlock()
			return fmt.Errorf("cannot get download URL: pickCode is still empty after all attempts")
		}

		// æ ¹æœ¬æ€§ä¿®å¤ï¼šä½¿ç”¨éªŒè¯è¿‡çš„pickCode
		validPickCode, validationErr := o.fs.validateAndCorrectPickCode(urlCtx, o.pickCode)
		if validationErr != nil {
			o.durlMu.Unlock()
			return fmt.Errorf("pickCode validation failed: %w", validationErr)
		}

		// å¦‚æœpickCodeè¢«ä¿®æ­£ï¼Œæ›´æ–°Objectä¸­çš„pickCode
		if validPickCode != o.pickCode {
			fs.Debugf(o, " Object pickCodeå·²ä¿®æ­£: %s -> %s", o.pickCode, validPickCode)
			o.pickCode = validPickCode
		}

		newURL, err = o.fs.getDownloadURLWithForce(urlCtx, validPickCode, forceRefresh)
	}

	if err != nil {
		o.durl = nil      // Clear invalid URL
		o.durlMu.Unlock() // é”™è¯¯è·¯å¾„é‡Šæ”¾é”

		// Check for server errors
		if fserrors.ShouldRetry(err) {
			return fmt.Errorf("server error when fetching download URL: %w", err)
		}

		// Check if it's a context timeout
		if errors.Is(err, context.DeadlineExceeded) {
			return fmt.Errorf("timeout fetching download URL: %w", err)
		}

		return fmt.Errorf("failed to get download URL: %w", err)
	}

	o.durl = newURL
	if !o.durl.Valid() {
		// This might happen if the link expires immediately or is invalid
		fs.Logf(o, "Fetched download URL is invalid or expired immediately: %s", o.durl.URL)
		// æ¸…é™¤æ— æ•ˆçš„URLï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°è·å–
		o.durl = nil
		o.durlMu.Unlock() // æ— æ•ˆURLè·¯å¾„é‡Šæ”¾é”
		return fmt.Errorf("fetched download URL is invalid or expired immediately")
	} else {
		fs.Debugf(o, "âœ… æˆåŠŸè·å–ä¸‹è½½URL")
	}
	o.durlMu.Unlock() // æˆåŠŸè·¯å¾„é‡Šæ”¾é”
	return nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs              = (*Fs)(nil)
	_ fs.Purger          = (*Fs)(nil)
	_ fs.Copier          = (*Fs)(nil)
	_ fs.Mover           = (*Fs)(nil)
	_ fs.DirMover        = (*Fs)(nil)
	_ fs.DirCacheFlusher = (*Fs)(nil)
	_ fs.MergeDirser     = (*Fs)(nil)
	_ fs.PutUncheckeder  = (*Fs)(nil)
	_ fs.Abouter         = (*Fs)(nil)
	_ fs.Commander       = (*Fs)(nil)
	_ fs.Object          = (*Object)(nil)
	_ fs.ObjectInfo      = (*Object)(nil)
	_ fs.IDer            = (*Object)(nil)
	_ fs.ParentIDer      = (*Object)(nil)
	_ fs.Shutdowner      = (*Fs)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, m configmap.Mapper) bool {
	// Try to load the token using oauthutil's method instead of
	// directly accessing the config file
	token, err := oauthutil.GetToken(f.originalName, m)
	if err != nil {
		fs.Debugf(f, "âŒ ä»é…ç½®è·å–ä»¤ç‰Œå¤±è´¥: %v", err)
		return false
	}

	if token == nil || token.AccessToken == "" || token.RefreshToken == "" {
		fs.Debugf(f, "âš ï¸ é…ç½®ä¸­çš„ä»¤ç‰Œä¸å®Œæ•´")
		return false
	}

	// Extract token components
	f.accessToken = token.AccessToken
	f.refreshToken = token.RefreshToken
	f.tokenExpiry = token.Expiry

	// Check if we got valid token data
	if f.accessToken == "" || f.refreshToken == "" {
		return false
	}

	fs.Debugf(f, "âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´ %v", f.tokenExpiry)
	return true
}

// ProgressReadCloser115 åŒ…è£…ReadCloserä»¥æä¾›115ç½‘ç›˜ä¸‹è½½è¿›åº¦è·Ÿè¸ª
type ProgressReadCloser115 struct {
	io.ReadCloser
	fs              *Fs
	object          *Object
	totalSize       int64
	transferredSize int64
	startTime       time.Time
	lastLogTime     time.Time
	lastLogPercent  int
}

// tempFileReadCloser åŒ…è£…ä¸´æ—¶æ–‡ä»¶çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
type tempFileReadCloser struct {
	file     *os.File
	tempPath string
}

func (t *tempFileReadCloser) Read(p []byte) (n int, err error) {
	return t.file.Read(p)
}

func (t *tempFileReadCloser) Close() error {
	err := t.file.Close()
	// æ¸…ç†ä¸´æ—¶æ–‡ä»¶
	if removeErr := os.Remove(t.tempPath); removeErr != nil {
		fs.Debugf(nil, "âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
	}
	return err
}

// downloadWithConcurrency æ‰§è¡Œå¹¶å‘ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
func (o *Object) downloadWithConcurrency(ctx context.Context, tempFile *os.File) error {
	downloadStartTime := time.Now()

	// è·å–é…ç½®å‚æ•°
	chunkSize := int64(o.fs.opt.DownloadChunkSize)
	maxConcurrency := o.fs.opt.DownloadConcurrency

	// 115ç½‘ç›˜é™åˆ¶ï¼šæ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªå¹¶å‘è¿æ¥
	if maxConcurrency > 2 {
		fs.Debugf(o, "âš ï¸ 115ç½‘ç›˜é™åˆ¶æ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªå¹¶å‘è¿æ¥ï¼Œå°†å¹¶å‘æ•°ä»%dè°ƒæ•´ä¸º2", maxConcurrency)
		maxConcurrency = 2
	}

	fileSize := o.size
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	if numChunks < int64(maxConcurrency) {
		maxConcurrency = int(numChunks)
	}

	// é’ˆå¯¹2å¹¶å‘ä¼˜åŒ–ï¼šå¦‚æœåˆ†ç‰‡æ•°å¤ªå°‘ï¼Œå¢åŠ åˆ†ç‰‡å¤§å°åˆ©ç”¨ç‡
	if maxConcurrency == 2 && numChunks < 4 && fileSize > 200*1024*1024 { // 200MBä»¥ä¸Šä¸”åˆ†ç‰‡å°‘äº4ä¸ª
		// é‡æ–°è®¡ç®—åˆ†ç‰‡å¤§å°ï¼Œç¡®ä¿è‡³å°‘æœ‰4ä¸ªåˆ†ç‰‡ä¾›2ä¸ªå¹¶å‘å¤„ç†
		optimalChunkSize := fileSize / 4
		if optimalChunkSize > chunkSize/2 && optimalChunkSize < chunkSize*2 {
			chunkSize = optimalChunkSize
			numChunks = (fileSize + chunkSize - 1) / chunkSize
			fs.Debugf(o, "ğŸ”§ 115ç½‘ç›˜2å¹¶å‘ä¼˜åŒ–: è°ƒæ•´åˆ†ç‰‡å¤§å°ä¸º%s, åˆ†ç‰‡æ•°=%d",
				fs.SizeSuffix(chunkSize), numChunks)
		}
	}

	fs.Infof(o, "ğŸ“Š 115ç½‘ç›˜å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d (115é™åˆ¶:æœ€å¤š2å¹¶å‘)",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// è·å–ä¸‹è½½URL
	o.durlMu.Lock()
	if o.durl == nil || !o.durl.Valid() {
		o.durlMu.Unlock()
		return fmt.Errorf("download URL not available")
	}
	downloadURL := o.durl.URL
	o.durlMu.Unlock()

	// åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡
	var wg sync.WaitGroup
	errChan := make(chan error, maxConcurrency)
	semaphore := make(chan struct{}, maxConcurrency)

	for i := int64(0); i < numChunks; i++ {
		start := i * chunkSize
		end := start + chunkSize - 1
		if end >= fileSize {
			end = fileSize - 1
		}

		wg.Add(1)
		go func(chunkIndex, start, end int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// ä¸‹è½½åˆ†ç‰‡ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
			var err error
			for retry := 0; retry < 3; retry++ {
				err = o.downloadChunk(ctx, downloadURL, tempFile, start, end, chunkIndex)
				if err == nil {
					break
				}
				if retry < 2 {
					fs.Debugf(o, "âš ï¸ åˆ†ç‰‡%dä¸‹è½½å¤±è´¥ï¼Œé‡è¯•%d/3: %v", chunkIndex+1, retry+1, err)
					time.Sleep(time.Duration(retry+1) * time.Second)
				}
			}
			if err != nil {
				errChan <- fmt.Errorf("åˆ†ç‰‡%dä¸‹è½½å¤±è´¥(é‡è¯•3æ¬¡å): %w", chunkIndex+1, err)
			}
		}(i, start, end)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆï¼Œä½¿ç”¨è¶…æ—¶æœºåˆ¶é˜²æ­¢æ­»é”
	done := make(chan struct{})
	go func() {
		wg.Wait()
		close(done)
	}()

	// ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case <-done:
		// æ­£å¸¸å®Œæˆ
		close(errChan)

		// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
		for err := range errChan {
			return err
		}
	case <-time.After(10 * time.Minute):
		// è¶…æ—¶å¤„ç†
		close(errChan)
		return fmt.Errorf("å¹¶å‘ä¸‹è½½è¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé—®é¢˜")
	}

	downloadDuration := time.Since(downloadStartTime)
	avgSpeed := float64(fileSize) / downloadDuration.Seconds()
	fs.Infof(o, "âœ… 115ç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: è€—æ—¶=%v, å¹³å‡é€Ÿåº¦=%s/s",
		downloadDuration, fs.SizeSuffix(int64(avgSpeed)))

	return nil
}

// downloadChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡
func (o *Object) downloadChunk(ctx context.Context, downloadURL string, tempFile *os.File, start, end, chunkIndex int64) error {
	chunkStartTime := time.Now()

	// åˆ›å»ºRangeè¯·æ±‚
	req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¯·æ±‚å¤±è´¥: %w", err)
	}

	rangeHeader := fmt.Sprintf("bytes=%d-%d", start, end)
	req.Header.Set("Range", rangeHeader)

	fs.Debugf(o, "ğŸ”„ å¼€å§‹ä¸‹è½½åˆ†ç‰‡%d: èŒƒå›´=%d-%d", chunkIndex+1, start, end)

	// æ‰§è¡Œè¯·æ±‚
	resp, err := o.fs.openAPIClient.Do(req)
	if err != nil {
		return fmt.Errorf("HTTPè¯·æ±‚å¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode == 403 {
		return fmt.Errorf("115ç½‘ç›˜å¹¶å‘é™åˆ¶: æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ”¯æŒ2ä¸ªå¹¶å‘è¿æ¥ï¼Œå½“å‰å¯èƒ½è¶…å‡ºé™åˆ¶")
	}
	if resp.StatusCode != 206 {
		return fmt.Errorf("rangeè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	// éªŒè¯Content-Length
	expectedSize := end - start + 1
	if contentLength := resp.Header.Get("Content-Length"); contentLength != "" {
		if actualSize, err := strconv.ParseInt(contentLength, 10, 64); err == nil {
			if actualSize != expectedSize {
				return fmt.Errorf("åˆ†ç‰‡å¤§å°ä¸åŒ¹é…: æœŸæœ›%d, å®é™…%d", expectedSize, actualSize)
			}
		}
	}

	// è¯»å–æ•°æ®å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶
	data, err := io.ReadAll(resp.Body)
	if err != nil {
		return fmt.Errorf("è¯»å–å“åº”æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯æ•°æ®å¤§å°
	if int64(len(data)) != expectedSize {
		return fmt.Errorf("æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%d, å®é™…%d", expectedSize, len(data))
	}

	// å†™å…¥åˆ°ä¸´æ—¶æ–‡ä»¶çš„æ­£ç¡®ä½ç½®
	n, err := tempFile.WriteAt(data, start)
	if err != nil {
		return fmt.Errorf("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	if int64(n) != expectedSize {
		return fmt.Errorf("å†™å…¥å¤§å°ä¸åŒ¹é…: æœŸæœ›%d, å®é™…%d", expectedSize, n)
	}

	chunkDuration := time.Since(chunkStartTime)
	chunkSpeed := float64(len(data)) / chunkDuration.Seconds()

	fs.Debugf(o, "âœ… 115ä¸‹è½½åˆ†ç‰‡å®Œæˆ: %d/%d, èŒƒå›´=%d-%d, å¤§å°=%d, è€—æ—¶=%v, é€Ÿåº¦=%s/s",
		chunkIndex+1, (o.size+int64(o.fs.opt.DownloadChunkSize)-1)/int64(o.fs.opt.DownloadChunkSize),
		start, end, len(data), chunkDuration, fs.SizeSuffix(int64(chunkSpeed)))

	return nil
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (prc *ProgressReadCloser115) Read(p []byte) (n int, err error) {
	n, err = prc.ReadCloser.Read(p)
	if n > 0 {
		prc.transferredSize += int64(n)

		// è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
		var percentage int
		if prc.totalSize > 0 {
			percentage = int(float64(prc.transferredSize) / float64(prc.totalSize) * 100)
		}

		now := time.Now()
		// å‡å°‘æ—¥å¿—é¢‘ç‡ï¼šåªåœ¨è¿›åº¦å˜åŒ–è¶…è¿‡10%æˆ–æ—¶é—´é—´éš”è¶…è¿‡5ç§’æ—¶è¾“å‡ºæ—¥å¿—
		shouldLog := (percentage >= prc.lastLogPercent+10) ||
			(now.Sub(prc.lastLogTime) > 5*time.Second) ||
			(prc.transferredSize == prc.totalSize) ||
			(percentage == 100)

		if shouldLog {
			elapsed := now.Sub(prc.startTime)
			var speed float64
			if elapsed.Seconds() > 0 {
				speed = float64(prc.transferredSize) / elapsed.Seconds() / 1024 / 1024 // MB/s
			}

			fs.Debugf(prc.object, "ğŸ“¥ 115ä¸‹è½½è¿›åº¦: %s/%s (%d%%) é€Ÿåº¦: %.2f MB/s",
				fs.SizeSuffix(prc.transferredSize),
				fs.SizeSuffix(prc.totalSize),
				percentage,
				speed)
			prc.lastLogTime = now
			prc.lastLogPercent = percentage
		}
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£
func (prc *ProgressReadCloser115) Close() error {
	// è¾“å‡ºæœ€ç»ˆä¸‹è½½ç»Ÿè®¡
	elapsed := time.Since(prc.startTime)
	var avgSpeed float64
	if elapsed.Seconds() > 0 {
		avgSpeed = float64(prc.transferredSize) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	fs.Debugf(prc.object, "âœ… 115ä¸‹è½½å®Œæˆ: %s, è€—æ—¶: %v, å¹³å‡é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(prc.transferredSize),
		elapsed.Truncate(time.Second),
		avgSpeed)

	return prc.ReadCloser.Close()
}

// TwoStepProgressReader115 ç”¨äº115ç½‘ç›˜å†…éƒ¨ä¸¤æ­¥ä¼ è¾“çš„è¿›åº¦è·Ÿè¸ª
type TwoStepProgressReader115 struct {
	io.Reader
	totalSize       int64
	transferredSize int64
	startTime       time.Time
	lastLogTime     time.Time
	lastLogPercent  int
	stepName        string
	fileName        string
	object          *Object
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (tpr *TwoStepProgressReader115) Read(p []byte) (n int, err error) {
	n, err = tpr.Reader.Read(p)
	if n > 0 {
		tpr.transferredSize += int64(n)

		// è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
		var percentage int
		if tpr.totalSize > 0 {
			percentage = int(float64(tpr.transferredSize) / float64(tpr.totalSize) * 100)
		}

		now := time.Now()
		// å‡å°‘æ—¥å¿—é¢‘ç‡ï¼šåªåœ¨è¿›åº¦å˜åŒ–è¶…è¿‡10%æˆ–æ—¶é—´é—´éš”è¶…è¿‡5ç§’æ—¶è¾“å‡ºæ—¥å¿—
		shouldLog := (percentage >= tpr.lastLogPercent+10) ||
			(now.Sub(tpr.lastLogTime) > 5*time.Second) ||
			(tpr.transferredSize == tpr.totalSize) ||
			(percentage == 100)

		if shouldLog {
			elapsed := now.Sub(tpr.startTime)
			var speed float64
			if elapsed.Seconds() > 0 {
				speed = float64(tpr.transferredSize) / elapsed.Seconds() / 1024 / 1024 // MB/s
			}

			fs.Debugf(tpr.object, "ğŸ“¥ %sè¿›åº¦: %s/%s (%d%%) é€Ÿåº¦: %.2f MB/s [%s]",
				tpr.stepName,
				fs.SizeSuffix(tpr.transferredSize),
				fs.SizeSuffix(tpr.totalSize),
				percentage,
				speed,
				tpr.fileName)
			tpr.lastLogTime = now
			tpr.lastLogPercent = percentage
		}
	}
	return n, err
}

// DownloadProgress 115ç½‘ç›˜ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
type DownloadProgress struct {
	totalChunks     int64
	completedChunks int64
	totalBytes      int64
	downloadedBytes int64
	startTime       time.Time
	lastUpdateTime  time.Time
	chunkSizes      map[int64]int64         // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„å¤§å°
	chunkTimes      map[int64]time.Duration // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„ä¸‹è½½æ—¶é—´
	peakSpeed       float64                 // å³°å€¼é€Ÿåº¦ MB/s
	mu              sync.RWMutex
}

// NewDownloadProgress åˆ›å»ºæ–°çš„ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
func NewDownloadProgress(totalChunks, totalBytes int64) *DownloadProgress {
	return &DownloadProgress{
		totalChunks:    totalChunks,
		totalBytes:     totalBytes,
		startTime:      time.Now(),
		lastUpdateTime: time.Now(),
		chunkSizes:     make(map[int64]int64),
		chunkTimes:     make(map[int64]time.Duration),
		peakSpeed:      0,
	}
}

// UpdateChunkProgress æ›´æ–°åˆ†ç‰‡ä¸‹è½½è¿›åº¦
func (dp *DownloadProgress) UpdateChunkProgress(chunkIndex, chunkSize int64, chunkDuration time.Duration) {
	dp.mu.Lock()
	defer dp.mu.Unlock()

	// å¦‚æœè¿™ä¸ªåˆ†ç‰‡è¿˜æ²¡æœ‰è®°å½•ï¼Œå¢åŠ å®Œæˆè®¡æ•°
	if _, exists := dp.chunkSizes[chunkIndex]; !exists {
		dp.completedChunks++
		dp.downloadedBytes += chunkSize
		dp.chunkSizes[chunkIndex] = chunkSize
		dp.chunkTimes[chunkIndex] = chunkDuration
		dp.lastUpdateTime = time.Now()

		// è®¡ç®—å¹¶æ›´æ–°å³°å€¼é€Ÿåº¦
		if chunkDuration.Seconds() > 0 {
			chunkSpeed := float64(chunkSize) / chunkDuration.Seconds() / 1024 / 1024 // MB/s
			if chunkSpeed > dp.peakSpeed {
				dp.peakSpeed = chunkSpeed
			}
		}
	}
}

// GetProgressInfo è·å–å½“å‰è¿›åº¦ä¿¡æ¯
func (dp *DownloadProgress) GetProgressInfo() (percentage float64, avgSpeed float64, peakSpeed float64, eta time.Duration, completed, total int64, downloadedBytes, totalBytes int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	elapsed := time.Since(dp.startTime)
	percentage = float64(dp.completedChunks) / float64(dp.totalChunks) * 100

	if elapsed.Seconds() > 0 {
		avgSpeed = float64(dp.downloadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	peakSpeed = dp.peakSpeed

	if avgSpeed > 0 && dp.downloadedBytes > 0 {
		remainingBytes := dp.totalBytes - dp.downloadedBytes
		etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
		eta = time.Duration(etaSeconds) * time.Second
	}

	return percentage, avgSpeed, peakSpeed, eta, dp.completedChunks, dp.totalChunks, dp.downloadedBytes, dp.totalBytes
}

// calculateOptimalChunkSize æ™ºèƒ½è®¡ç®—115ç½‘ç›˜ä¸Šä¼ åˆ†ç‰‡å¤§å°
// åŸºäºæ–‡ä»¶å¤§å°é€‰æ‹©æœ€ä¼˜åˆ†ç‰‡ç­–ç•¥ï¼Œæé«˜ä¼ è¾“æ•ˆç‡
func (f *Fs) calculateOptimalChunkSize(fileSize int64) fs.SizeSuffix {
	// å¦‚æœç”¨æˆ·é…ç½®äº†chunk_sizeï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·é…ç½®
	userChunkSize := int64(f.opt.ChunkSize)
	if userChunkSize > 0 {
		// ç”¨æˆ·æ˜ç¡®é…ç½®äº†åˆ†ç‰‡å¤§å°ï¼Œç›´æ¥ä½¿ç”¨
		fs.Debugf(f, "ğŸ”§ ä½¿ç”¨ç”¨æˆ·é…ç½®çš„åˆ†ç‰‡å¤§å°: %s", fs.SizeSuffix(userChunkSize))
		return fs.SizeSuffix(f.normalizeChunkSize(userChunkSize))
	}

	// ç”¨æˆ·æœªé…ç½®ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥
	// Based on 2MB/s target speed optimization: use larger chunks to reduce API call overhead
	// Larger chunks can better utilize network bandwidth and reduce inter-chunk overhead
	var partSize int64

	// ä¿®å¤ï¼šä¼˜åŒ–åˆ†ç‰‡ç­–ç•¥ï¼Œå‡å°‘å°æ–‡ä»¶çš„åˆ†ç‰‡æ•°é‡
	// å°æ–‡ä»¶ä½¿ç”¨æ›´å¤§çš„åˆ†ç‰‡ï¼Œå‡å°‘ç½‘ç»œå¼€é”€å’ŒAPIè°ƒç”¨æ¬¡æ•°
	switch {
	case fileSize <= 100*1024*1024: // â‰¤100MB
		partSize = fileSize // å°æ–‡ä»¶ç›´æ¥å•åˆ†ç‰‡ä¸Šä¼ ï¼Œé¿å…åˆ†ç‰‡å¼€é”€
	case fileSize <= 500*1024*1024: // â‰¤500MB
		partSize = 100 * 1024 * 1024 // 100MBåˆ†ç‰‡ï¼Œå‡å°‘åˆ†ç‰‡æ•°é‡
	case fileSize <= 2*1024*1024*1024: // â‰¤2GB
		partSize = 200 * 1024 * 1024 // 200MBåˆ†ç‰‡
	case fileSize <= 10*1024*1024*1024: // â‰¤10GB
		partSize = 500 * 1024 * 1024 // 500MBåˆ†ç‰‡
	case fileSize <= 100*1024*1024*1024: // â‰¤100GB
		partSize = 1 * 1024 * 1024 * 1024 // 1GBåˆ†ç‰‡
	default: // >100GB
		partSize = 2 * 1024 * 1024 * 1024 // 2GBåˆ†ç‰‡
	}

	fs.Debugf(f, "ğŸ¯ æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥: æ–‡ä»¶å¤§å° %s -> åˆ†ç‰‡å¤§å° %s", fs.SizeSuffix(fileSize), fs.SizeSuffix(partSize))
	return fs.SizeSuffix(f.normalizeChunkSize(partSize))
}

// normalizeChunkSize ç¡®ä¿åˆ†ç‰‡å¤§å°åœ¨åˆç†èŒƒå›´å†…
func (f *Fs) normalizeChunkSize(partSize int64) int64 {
	if partSize < int64(minChunkSize) {
		partSize = int64(minChunkSize)
	}
	if partSize > int64(maxChunkSize) {
		partSize = int64(maxChunkSize)
	}
	return partSize
}

// localFileInfo æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ç»“æ„ï¼Œç”¨äºè·¨äº‘ä¼ è¾“
type localFileInfo struct {
	name    string
	size    int64
	modTime time.Time
	remote  string
}

func (l *localFileInfo) String() string                        { return l.remote }
func (l *localFileInfo) Remote() string                        { return l.remote }
func (l *localFileInfo) ModTime(ctx context.Context) time.Time { return l.modTime }
func (l *localFileInfo) Size() int64                           { return l.size }
func (l *localFileInfo) Fs() fs.Info                           { return nil }
func (l *localFileInfo) Hash(ctx context.Context, t hash.Type) (string, error) {
	return "", hash.ErrUnsupported
}
func (l *localFileInfo) Storable() bool { return true }

// crossCloudUploadWithLocalCache è·¨äº‘ä¼ è¾“ä¸“ç”¨ä¸Šä¼ æ–¹æ³•ï¼Œæ™ºèƒ½æ£€æŸ¥å·²æœ‰ä¸‹è½½é¿å…é‡å¤
func (f *Fs) crossCloudUploadWithLocalCache(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "â˜ï¸ å¼€å§‹è·¨äº‘ä¼ è¾“æœ¬åœ°åŒ–: %s (å¤§å°: %s)", remote, fs.SizeSuffix(fileSize))

	// ä¿®å¤é‡å¤ä¸‹è½½ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´çš„ä¸´æ—¶æ–‡ä»¶
	if tempReader, tempSize, tempCleanup := f.checkExistingTempFile(src); tempReader != nil {
		fs.Infof(f, "âœ… å‘ç°ç°æœ‰å®Œæ•´ä¸‹è½½æ–‡ä»¶ï¼Œè·³è¿‡é‡å¤ä¸‹è½½: %s", fs.SizeSuffix(tempSize))

		// å…³é”®ä¿®å¤ï¼šå°†ä¸´æ—¶æ–‡ä»¶åŒ…è£…æˆå¸¦Accountå¯¹è±¡çš„Reader
		if tempFile, ok := tempReader.(*os.File); ok {
			accountedReader := NewAccountedFileReader(ctx, tempFile, tempSize, remote, tempCleanup)
			return f.upload(ctx, accountedReader, src, remote, options...)
		} else {
			defer tempCleanup()
			return f.upload(ctx, tempReader, src, remote, options...)
		}
	}

	// æ™ºèƒ½ä¸‹è½½ç­–ç•¥ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ä»å¤±è´¥çš„ä¸‹è½½ä¸­æ¢å¤
	var localDataSource io.Reader
	var cleanup func()
	var localFileSize int64

	// å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å†…å­˜ç¼“å†²
	maxMemoryBufferSize := memoryBufferThreshold // ä½¿ç”¨ç»Ÿä¸€çš„å†…å­˜ç¼“å†²é˜ˆå€¼
	useMemoryBuffer := false

	// ç®€åŒ–ï¼šå¯¹äºå°æ–‡ä»¶ç›´æ¥ä½¿ç”¨å†…å­˜ç¼“å†²ï¼Œä¸éœ€è¦å¤æ‚çš„å†…å­˜ä¼˜åŒ–å™¨
	if fileSize <= maxMemoryBufferSize {
		useMemoryBuffer = true
	}

	if useMemoryBuffer {
		// å°æ–‡ä»¶ï¼šä¸‹è½½åˆ°å†…å­˜
		fs.Infof(f, "ğŸ“ å°æ–‡ä»¶è·¨äº‘ä¼ è¾“ï¼Œä¸‹è½½åˆ°å†…å­˜: %s", fs.SizeSuffix(fileSize))
		data, err := io.ReadAll(in)
		if err != nil {
			return nil, fmt.Errorf("cross-cloud transfer download to memory failed: %w", err)
		}
		localDataSource = bytes.NewReader(data)
		localFileSize = int64(len(data))
		cleanup = func() {
			// ç®€åŒ–ï¼šä¸éœ€è¦å¤æ‚çš„å†…å­˜ç®¡ç†
		}
		fs.Infof(f, "âœ… è·¨äº‘ä¼ è¾“å†…å­˜ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(localFileSize))
	} else {
		// å¤§æ–‡ä»¶æˆ–å†…å­˜ä¸è¶³ï¼šä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
		if fileSize <= maxMemoryBufferSize {
			fs.Infof(f, "âš ï¸ å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶: %s", fs.SizeSuffix(fileSize))
		} else {
			fs.Infof(f, "ğŸ“ å¤§æ–‡ä»¶è·¨äº‘ä¼ è¾“ï¼Œä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶: %s", fs.SizeSuffix(fileSize))
		}
		tempFile, err := os.CreateTemp("", "115_cross_cloud_*.tmp")
		if err != nil {
			return nil, fmt.Errorf("failed to create temp file: %w", err)
		}

		written, err := io.Copy(tempFile, in)
		if err != nil {
			_ = tempFile.Close()
			_ = os.Remove(tempFile.Name())
			return nil, fmt.Errorf("cross-cloud transfer download to temp file failed: %w", err)
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
		if _, err := tempFile.Seek(0, io.SeekStart); err != nil {
			_ = tempFile.Close()
			_ = os.Remove(tempFile.Name())
			return nil, fmt.Errorf("failed to reset temp file pointer: %w", err)
		}

		// å…³é”®ä¿®å¤ï¼šåˆ›å»ºå¸¦Accountå¯¹è±¡çš„æ–‡ä»¶è¯»å–å™¨
		cleanupFunc := func() {
			_ = tempFile.Close()
			_ = os.Remove(tempFile.Name())
		}
		localDataSource = NewAccountedFileReader(ctx, tempFile, written, remote, cleanupFunc)
		localFileSize = written
		cleanup = func() {
			// AccountedFileReaderä¼šåœ¨Closeæ—¶è°ƒç”¨cleanupFunc
		}
		fs.Infof(f, "âœ… è·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(localFileSize))
	}
	defer cleanup()

	// Verify download integrity
	if localFileSize != fileSize {
		return nil, fmt.Errorf("cross-cloud transfer download incomplete: expected %d bytes, got %d bytes", fileSize, localFileSize)
	}

	// ä½¿ç”¨æœ¬åœ°æ•°æ®è¿›è¡Œä¸Šä¼ ï¼Œåˆ›å»ºæœ¬åœ°æ–‡ä»¶ä¿¡æ¯å¯¹è±¡
	localSrc := &localFileInfo{
		name:    filepath.Base(remote),
		size:    localFileSize,
		modTime: src.ModTime(ctx),
		remote:  remote,
	}

	fs.Infof(f, "ğŸš€ å¼€å§‹ä¸Šä¼ åˆ°115ç½‘ç›˜: %s", fs.SizeSuffix(localFileSize))

	// ä½¿ç”¨ä¸»ä¸Šä¼ å‡½æ•°ï¼Œä½†æ­¤æ—¶æºå·²ç»æ˜¯æœ¬åœ°æ•°æ®
	return f.upload(ctx, localDataSource, localSrc, remote, options...)
}

// 115ç½‘ç›˜æ‰€æœ‰APIéƒ½å—åˆ°ç›¸åŒçš„å…¨å±€QPSé™åˆ¶ï¼Œä¸éœ€è¦å¤æ‚çš„é€‰æ‹©é€»è¾‘
func (f *Fs) getPacerForEndpoint() *fs.Pacer {
	// æ‰€æœ‰APIä½¿ç”¨ç»Ÿä¸€çš„pacerï¼Œç¬¦åˆ115ç½‘ç›˜çš„å…¨å±€QPSé™åˆ¶
	return f.pacer
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file     *os.File
	tempPath string
}

// Read å®ç°io.Readeræ¥å£
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	if r.file != nil {
		r.file.Close()
	}
	if r.tempPath != "" {
		os.Remove(r.tempPath)
	}
	return nil
}

// listAll retrieves directory listings, using OpenAPI if possible.
// User function fn should return true to stop processing.
type listAllFn func(*File) bool

func (f *Fs) listAll(ctx context.Context, dirID string, limit int, filesOnly bool, fn listAllFn) (found bool, err error) {
	fs.Debugf(f, "ğŸ“‹ listAllå¼€å§‹: dirID=%q, limit=%d, filesOnly=%v", dirID, limit, filesOnly)

	// éªŒè¯ç›®å½•ID
	if dirID == "" {
		fs.Errorf(f, "âŒ listAll: ç›®å½•IDä¸ºç©ºï¼Œè¿™å¯èƒ½å¯¼è‡´æŸ¥è¯¢æ ¹ç›®å½•")
		// ä¸è¦ç›´æ¥è¿”å›é”™è¯¯ï¼Œè€Œæ˜¯è®°å½•è­¦å‘Šå¹¶ç»§ç»­ï¼Œå› ä¸ºæ ¹ç›®å½•æŸ¥è¯¢å¯èƒ½æ˜¯åˆæ³•çš„
	}

	if f.isShare {
		// Share listing is no longer supported
		return false, errors.New("share listing is not supported")
	}

	// Use OpenAPI listing
	fs.Debugf(f, "ğŸŒ listAll: ä½¿ç”¨OpenAPIæ¨¡å¼")
	params := url.Values{}
	params.Set("cid", dirID)
	params.Set("limit", strconv.Itoa(limit))
	params.Set("offset", "0")
	params.Set("show_dir", "1") // Include directories in the listing

	// Sorting (OpenAPI uses query params)
	params.Set("o", "user_utime") // Default sort: update time
	params.Set("asc", "0")        // Default sort: descending

	offset := 0

	fs.Debugf(f, "ğŸ”„ listAll: å¼€å§‹åˆ†é¡µå¾ªç¯")
	for {
		fs.Debugf(f, " listAll: å¤„ç†offset=%d", offset)
		params.Set("offset", strconv.Itoa(offset))
		opts := rest.Opts{
			Method:     "GET",
			Path:       "/open/ufile/files",
			Parameters: params,
		}

		fs.Debugf(f, "ğŸ”§ listAll: å‡†å¤‡è°ƒç”¨CallOpenAPI")
		var info FileList
		err = f.CallOpenAPI(ctx, &opts, nil, &info, false) // Use OpenAPI call
		if err != nil {
			fs.Debugf(f, "âŒ listAll: CallOpenAPIå¤±è´¥: %v", err)
			// å¤„ç†APIé™åˆ¶é”™è¯¯
			if err = f.handleAPILimitError(ctx, err, &opts, &info, dirID); err != nil {
				return found, err
			}
		}

		fs.Debugf(f, "âœ… listAll: CallOpenAPIæˆåŠŸï¼Œè¿”å›%dä¸ªæ–‡ä»¶", len(info.Files))

		if len(info.Files) == 0 {
			break // No more items
		}

		for _, item := range info.Files {
			isDir := item.IsDir()
			// Apply client-side filtering if needed
			if filesOnly && isDir {
				continue
			}

			// Decode name
			item.FileName = f.opt.Enc.ToStandardName(item.FileNameBest()) // Use best name getter

			if fn(item) {
				found = true
				return found, nil // Early exit
			}
		}

		// Check if we have fetched all items based on total count from response
		currentOffset, _ := strconv.Atoi(params.Get("offset"))
		offset = currentOffset + len(info.Files)

		// Stop listing when we've reached the total count
		if info.Count > 0 && offset >= info.Count {
			break // We've reached or exceeded the total count
		}
	}

	return found, nil
}

// updateAPILimitStats æ›´æ–°APIé™æµç»Ÿè®¡ä¿¡æ¯
func (f *Fs) updateAPILimitStats() {
	f.apiLimitMu.Lock()
	defer f.apiLimitMu.Unlock()

	now := time.Now()

	// å¦‚æœè·ç¦»ä¸Šæ¬¡APIé™æµé”™è¯¯è¶…è¿‡5åˆ†é’Ÿï¼Œé‡ç½®è®¡æ•°å™¨
	if now.Sub(f.lastAPILimitTime) > 5*time.Minute {
		f.consecutiveAPILimit = 0
	}

	f.consecutiveAPILimit++
	f.lastAPILimitTime = now

	fs.Debugf(f, "ğŸ“Š APIé™æµç»Ÿè®¡: è¿ç»­é”™è¯¯=%d, å½“å‰QPS=%.1f",
		f.consecutiveAPILimit, f.currentQPSLimit)
}

// adjustQPSLimit æ ¹æ®APIé™æµæƒ…å†µåŠ¨æ€è°ƒæ•´QPSé™åˆ¶
func (f *Fs) adjustQPSLimit() time.Duration {
	f.apiLimitMu.Lock()
	defer f.apiLimitMu.Unlock()

	// æ ¹æ®è¿ç»­é”™è¯¯æ¬¡æ•°è°ƒæ•´QPSé™åˆ¶
	if f.consecutiveAPILimit >= 3 {
		// è¿ç»­3æ¬¡ä»¥ä¸Šé”™è¯¯ï¼Œå¤§å¹…é™ä½QPS
		f.currentQPSLimit = math.Max(0.5, f.currentQPSLimit*0.5)
	} else if f.consecutiveAPILimit >= 2 {
		// è¿ç»­2æ¬¡é”™è¯¯ï¼Œé€‚åº¦é™ä½QPS
		f.currentQPSLimit = math.Max(1.0, f.currentQPSLimit*0.7)
	}

	// è®¡ç®—æ–°çš„æœ€å°ç¡çœ æ—¶é—´
	newMinSleep := time.Duration(float64(time.Second) / f.currentQPSLimit)

	fs.Debugf(f, "ğŸš¦ åŠ¨æ€è°ƒæ•´QPS: %.1f -> æœ€å°é—´éš”=%v", f.currentQPSLimit, newMinSleep)

	return newMinSleep
}

// calculateRetryDelay è®¡ç®—æ™ºèƒ½é‡è¯•å»¶è¿Ÿ
func (f *Fs) calculateRetryDelay(attempt int) time.Duration {
	baseDelay := time.Duration(defaultRetryBaseDelay)

	// æŒ‡æ•°é€€é¿ï¼šbaseDelay * 2^attemptï¼Œæœ€å¤§ä¸è¶…è¿‡60ç§’
	delay := baseDelay * time.Duration(1<<uint(attempt))
	if delay > 60*time.Second {
		delay = 60 * time.Second
	}

	// æ·»åŠ éšæœºæŠ–åŠ¨ï¼Œé¿å…é›·ç¾¤æ•ˆåº”
	jitter := time.Duration(time.Now().UnixNano()%1000) * time.Millisecond
	return delay + jitter
}

// handleAPILimitError å¤„ç†APIé™åˆ¶é”™è¯¯ï¼Œå®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶
func (f *Fs) handleAPILimitError(ctx context.Context, err error, opts *rest.Opts, info *FileList, dirID string) error {
	// Check if it's an API limit error
	if !fserrors.IsRetryError(err) {
		return fmt.Errorf("OpenAPI list failed for dir %s: %w", dirID, err)
	}

	// æ›´æ–°APIé™æµç»Ÿè®¡
	f.updateAPILimitStats()

	// æ™ºèƒ½é‡è¯•é»˜è®¤å¯ç”¨
	return f.handleAPILimitErrorWithSmartRetry(ctx, err, opts, info, dirID)
}

// handleAPILimitErrorWithSmartRetry ä½¿ç”¨æ™ºèƒ½é‡è¯•æœºåˆ¶å¤„ç†APIé™æµé”™è¯¯
func (f *Fs) handleAPILimitErrorWithSmartRetry(ctx context.Context, err error, opts *rest.Opts, info *FileList, dirID string) error {
	fs.Infof(f, "ğŸ§  å¯ç”¨æ™ºèƒ½é‡è¯•å¤„ç†APIé™æµé”™è¯¯...")

	for attempt := 0; attempt < defaultMaxRetryAttempts; attempt++ {
		// åŠ¨æ€è°ƒæ•´QPSé™åˆ¶
		newMinSleep := f.adjustQPSLimit()

		// è®¡ç®—é‡è¯•å»¶è¿Ÿ
		retryDelay := f.calculateRetryDelay(attempt)

		fs.Infof(f, "ğŸ”„ æ™ºèƒ½é‡è¯• %d/%d: ç­‰å¾…=%v, æ–°QPSé—´éš”=%v",
			attempt+1, defaultMaxRetryAttempts, retryDelay, newMinSleep)

		// ç­‰å¾…é‡è¯•å»¶è¿Ÿ
		select {
		case <-time.After(retryDelay):
			// ç»§ç»­é‡è¯•
		case <-ctx.Done():
			return fmt.Errorf("context cancelled during smart retry: %w", ctx.Err())
		}

		// æ‰§è¡Œé‡è¯•
		retryErr := f.CallOpenAPI(ctx, opts, nil, info, false)
		if retryErr == nil {
			fs.Infof(f, "âœ… æ™ºèƒ½é‡è¯•æˆåŠŸ (å°è¯• %d/%d)", attempt+1, defaultMaxRetryAttempts)

			// é‡è¯•æˆåŠŸï¼Œé‡ç½®è¿ç»­é”™è¯¯è®¡æ•°
			f.apiLimitMu.Lock()
			f.consecutiveAPILimit = 0
			f.apiLimitMu.Unlock()

			return nil
		}

		// æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯APIé™æµé”™è¯¯
		if !fserrors.IsRetryError(retryErr) {
			return fmt.Errorf("OpenAPI list failed for dir %s with non-retryable error: %w", dirID, retryErr)
		}

		fs.Debugf(f, "âš ï¸ æ™ºèƒ½é‡è¯• %d/%d å¤±è´¥ï¼Œç»§ç»­å°è¯•...", attempt+1, defaultMaxRetryAttempts)
	}

	return fmt.Errorf("OpenAPI list failed for dir %s after %d smart retries: %w", dirID, defaultMaxRetryAttempts, err)
}

// makeDir creates a directory using OpenAPI.
func (f *Fs) makeDir(ctx context.Context, pid, name string) (info *NewDir, err error) {
	if f.isShare {
		return nil, errors.New("makeDir unsupported for shared filesystem")
	}
	form := url.Values{}
	form.Set("pid", pid)
	form.Set("file_name", f.opt.Enc.FromStandardName(name))

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/folder/add",
		Body:   strings.NewReader(form.Encode()), // Send as form data in body
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	info = new(NewDir)
	err = f.CallOpenAPI(ctx, &opts, nil, info, false)
	if err != nil {
		// Check for specific "already exists" error code from OpenAPI
		// Assuming code 20004 or similar based on traditional API
		if info.ErrCode() == 20004 || strings.Contains(info.ErrMsg(), "exists") || strings.Contains(info.ErrMsg(), "å·²å­˜åœ¨") {
			// Try to find the existing directory's ID
			existingID, found, findErr := f.FindLeaf(ctx, pid, f.opt.Enc.FromStandardName(name))
			if findErr == nil && found {
				// Return info for the existing directory
				return &NewDir{
					OpenAPIBase: OpenAPIBase{State: true}, // Mark as success
					Data: &NewDirData{
						FileID:   existingID,
						FileName: name,
					},
				}, fs.ErrorDirExists // Return specific error
			}
			// If finding fails, return the original Mkdir error
			return nil, fmt.Errorf("makeDir failed and could not find existing dir: %w", err)
		}
		return nil, fmt.Errorf("OpenAPI makeDir failed: %w", err)
	}
	// Ensure FileID is populated
	if info.Data == nil || info.Data.FileID == "" {
		return nil, errors.New("OpenAPI makeDir response missing file_id")
	}
	return info, nil
}

// renameFile renames a file or folder using OpenAPI.
func (f *Fs) renameFile(ctx context.Context, fid, newName string) (err error) {
	if f.isShare {
		return errors.New("renameFile unsupported for shared filesystem")
	}
	form := url.Values{}
	form.Set("file_id", fid)
	form.Set("file_name", f.opt.Enc.FromStandardName(newName))

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/ufile/update", // Endpoint for renaming/starring
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var baseResp OpenAPIBase
	err = f.CallOpenAPI(ctx, &opts, nil, &baseResp, false)
	if err != nil {
		return fmt.Errorf("OpenAPI rename failed for ID %s: %w", fid, err)
	}
	return nil
}

// deleteFiles deletes files or folders by ID using OpenAPI.
func (f *Fs) deleteFiles(ctx context.Context, fids []string) (err error) {
	if f.isShare {
		return errors.New("deleteFiles unsupported for shared filesystem")
	}
	if len(fids) == 0 {
		return nil
	}
	form := url.Values{}
	form.Set("file_ids", strings.Join(fids, ","))
	// parent_id is optional according to docs

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/ufile/delete",
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var baseResp OpenAPIBase
	err = f.CallOpenAPI(ctx, &opts, nil, &baseResp, false)
	if err != nil {
		// Check for "not found" errors if possible, otherwise return generic error
		return fmt.Errorf("OpenAPI delete failed for IDs %v: %w", fids, err)
	}
	return nil
}

// moveFiles moves files or folders by ID using OpenAPI.
func (f *Fs) moveFiles(ctx context.Context, fids []string, pid string) (err error) {
	if f.isShare {
		return errors.New("moveFiles unsupported for shared filesystem")
	}
	if len(fids) == 0 {
		return nil
	}
	form := url.Values{}
	form.Set("file_ids", strings.Join(fids, ","))
	form.Set("to_cid", pid) // Target directory ID

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/ufile/move",
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var baseResp OpenAPIBase
	err = f.CallOpenAPI(ctx, &opts, nil, &baseResp, false)
	if err != nil {
		return fmt.Errorf("OpenAPI move failed for IDs %v to %s: %w", fids, pid, err)
	}
	return nil
}

// copyFiles copies files or folders by ID using OpenAPI.
func (f *Fs) copyFiles(ctx context.Context, fids []string, pid string) (err error) {
	if f.isShare {
		return errors.New("copyFiles unsupported for shared filesystem")
	}
	if len(fids) == 0 {
		return nil
	}
	form := url.Values{}
	form.Set("file_id", strings.Join(fids, ",")) // Note: param name is file_id (singular)
	form.Set("pid", pid)                         // Target directory ID
	form.Set("nodupli", "0")                     // Allow duplicates by default

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/ufile/copy",
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var baseResp OpenAPIBase
	err = f.CallOpenAPI(ctx, &opts, nil, &baseResp, false)
	if err != nil {
		return fmt.Errorf("OpenAPI copy failed for IDs %v to %s: %w", fids, pid, err)
	}
	return nil
}

// indexInfo gets user quota info (Traditional API).
func (f *Fs) indexInfo(ctx context.Context) (data *IndexData, err error) {
	if f.isShare {
		return nil, errors.New("indexInfo unsupported for shared filesystem")
	}
	opts := rest.Opts{
		Method: "GET",
		Path:   "/files/index_info", // Traditional endpoint
	}

	var info *IndexInfo
	// Use traditional API call
	err = f.CallTraditionalAPI(ctx, &opts, nil, &info, false) // Not skipping encryption
	if err != nil {
		return nil, fmt.Errorf("traditional indexInfo failed: %w", err)
	}
	if info.Data == nil {
		return nil, errors.New("traditional indexInfo returned no data")
	}
	return info.Data, nil
}

// getDownloadURL gets a download URL using OpenAPI.
func (f *Fs) getDownloadURL(ctx context.Context, pickCode string) (durl *DownloadURL, err error) {
	return f.getDownloadURLWithForce(ctx, pickCode, false)
}

// getDownloadURLWithForce gets a download URL using OpenAPI with optional cache bypass.
func (f *Fs) getDownloadURLWithForce(ctx context.Context, pickCode string, forceRefresh bool) (durl *DownloadURL, err error) {
	originalPickCode := pickCode
	pickCode, err = f.validateAndCorrectPickCode(ctx, pickCode)
	if err != nil {
		return nil, fmt.Errorf("pickCode validation failed: %w", err)
	}

	if originalPickCode != pickCode {
		fs.Infof(f, "âœ… pickCodeå·²çº æ­£: %s -> %s", originalPickCode, pickCode)
	}

	// Download URL cache removed, call API directly
	if forceRefresh {
		fs.Debugf(f, "ğŸ”„ 115å¼ºåˆ¶åˆ·æ–°ä¸‹è½½URL: pickCode=%s", pickCode)
	} else {
		fs.Debugf(f, "ğŸ“¥ 115ç½‘ç›˜è·å–ä¸‹è½½URL: pickCode=%s", pickCode)
	}

	form := url.Values{}
	form.Set("pick_code", pickCode)

	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/ufile/downurl",
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var respData OpenAPIDownloadResp
	// ä½¿ç”¨ä¸“ç”¨çš„ä¸‹è½½URLè°ƒé€Ÿå™¨ï¼Œé˜²æ­¢APIè°ƒç”¨é¢‘ç‡è¿‡é«˜
	err = f.CallDownloadURLAPI(ctx, &opts, nil, &respData, false)
	if err != nil {
		return nil, fmt.Errorf("OpenAPI downurl failed for pickcode %s: %w", pickCode, err)
	}

	// ä½¿ç”¨æ–°çš„GetDownloadInfoæ–¹æ³•å¤„ç†mapå’Œarrayä¸¤ç§æ ¼å¼
	downInfo, err := respData.GetDownloadInfo()
	if err != nil {
		fs.Debugf(f, "âŒ 115ç½‘ç›˜ä¸‹è½½URLå“åº”è§£æå¤±è´¥: %v, åŸå§‹æ•°æ®: %s", err, string(respData.Data))
		if string(respData.Data) == "[]" || string(respData.Data) == "{}" {
			return nil, fmt.Errorf("115 API returned empty data, file may be deleted or access denied. pickCode: %s", pickCode)
		}

		return nil, fmt.Errorf("failed to parse download URL response for pickcode %s: %w", pickCode, err)
	}

	if downInfo == nil {
		return nil, fmt.Errorf("no download info found for pickcode %s", pickCode)
	}

	fs.Debugf(f, "âœ… 115ç½‘ç›˜æˆåŠŸè·å–ä¸‹è½½URL: pickCode=%s, fileName=%s, fileSize=%d",
		pickCode, downInfo.FileName, int64(downInfo.FileSize))

	// è®¾ç½®åˆ›å»ºæ—¶é—´ï¼Œç”¨äºfallbackè¿‡æœŸæ£€æµ‹
	downloadURL := downInfo.URL
	downloadURL.CreatedAt = time.Now()

	return &downloadURL, nil
}

// validateAndCorrectPickCode éªŒè¯å¹¶ä¿®æ­£pickCodeæ ¼å¼
func (f *Fs) validateAndCorrectPickCode(ctx context.Context, pickCode string) (string, error) {
	// ç©ºpickCodeç›´æ¥è¿”å›é”™è¯¯
	if pickCode == "" {
		return "", fmt.Errorf("empty pickCode provided")
	}
	isAllDigits := true
	for _, r := range pickCode {
		if r < '0' || r > '9' {
			isAllDigits = false
			break
		}
	}

	// å¦‚æœæ˜¯ç–‘ä¼¼æ–‡ä»¶IDï¼Œå°è¯•è·å–æ­£ç¡®çš„pickCode
	if len(pickCode) > 15 && isAllDigits {
		fs.Debugf(f, "âš ï¸ æ£€æµ‹åˆ°ç–‘ä¼¼æ–‡ä»¶IDè€ŒépickCode: %s", pickCode)

		correctPickCode, err := f.getPickCodeByFileID(ctx, pickCode)
		if err != nil {
			return "", fmt.Errorf("invalid pickCode format (appears to be file ID): %s, failed to get correct pickCode: %w", pickCode, err)
		}

		fs.Debugf(f, "âœ… æˆåŠŸé€šè¿‡æ–‡ä»¶IDè·å–æ­£ç¡®çš„pickCode: %s -> %s", pickCode, correctPickCode)
		return correctPickCode, nil
	}

	// pickCodeæ ¼å¼çœ‹èµ·æ¥æ­£ç¡®ï¼Œç›´æ¥è¿”å›
	return pickCode, nil
}

// CallTraditionalAPIWithResp is a variant that returns the http.Response for cookie access.
func (f *Fs) CallTraditionalAPIWithResp(ctx context.Context, opts *rest.Opts, request any, response any, skipEncrypt bool) (*http.Response, error) {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = traditionalRootURL
	}

	var httpResp *http.Response
	err := f.pacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {

		// Make the API call (with or without encryption)
		var apiErr error
		httpResp, apiErr = f.executeTraditionalAPICall(ctx, opts, request, response, skipEncrypt)

		// Check for retryable errors
		retryNeeded, retryErr := shouldRetry(ctx, httpResp, apiErr)
		if retryNeeded {
			fs.Debugf(f, "ğŸ”„ è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨å“åº”éœ€è¦åº•å±‚é‡è¯• (é”™è¯¯: %v)", retryErr)
			return true, retryErr // Signal globalPacer to retry
		}

		// Handle non-retryable errors
		if apiErr != nil {
			fs.Debugf(f, "âŒ è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨å“åº”é‡åˆ°æ°¸ä¹…é”™è¯¯: %v", apiErr)
			return false, apiErr
		}

		// Check API-level errors in response struct
		if errResp := f.checkResponseForAPIErrors(response); errResp != nil {
			fs.Debugf(f, "âŒ è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨å“åº”é‡åˆ°æ°¸ä¹…APIé”™è¯¯: %v", errResp)
			return false, errResp
		}

		fs.Debugf(f, "âœ… è°ƒé€Ÿå™¨: ä¼ ç»Ÿè°ƒç”¨å“åº”æˆåŠŸ")
		return false, nil // Success, don't retry
	})

	return httpResp, err
}

// Globals
const (
	cachePrefix  = "rclone-115-sha1sum-"
	md5Salt      = "Qclm8MGWUv59TnrR0XPg"     // Salt for traditional token generation
	OSSRegion    = "cn-shenzhen"              // Default OSS region
	OSSUserAgent = "aliyun-sdk-android/2.9.1" // Keep or update as needed
)

// RereadableObject represents a source that can be re-opened for multiple reads
// å…³é”®ä¿®å¤ï¼šå®ç°Accounteræ¥å£ï¼Œç¡®ä¿Accountå¯¹è±¡èƒ½è¢«æ­£ç¡®è¯†åˆ«
type RereadableObject struct {
	src        fs.ObjectInfo
	ctx        context.Context
	currReader io.Reader
	options    []fs.OpenOption
	size       int64                // Track the source size for accounting
	acc        *accounting.Account  // Store the accounting object
	fsInfo     fs.Info              // Source filesystem info
	transfer   *accounting.Transfer // Keep track of the transfer object
	// æ–°å¢ï¼šæ”¯æŒå…±äº«ä¸»ä¼ è¾“çš„ä¼šè®¡ç³»ç»Ÿ
	parentTransfer      *accounting.Transfer // ä¸»ä¼ è¾“å¯¹è±¡ï¼Œç”¨äºç»Ÿä¸€è¿›åº¦æ˜¾ç¤º
	useParentAccounting bool                 // æ˜¯å¦ä½¿ç”¨çˆ¶ä¼ è¾“çš„ä¼šè®¡ç³»ç»Ÿ
}

// NewRereadableObject creates a wrapper that supports re-opening the source
func NewRereadableObject(ctx context.Context, src fs.ObjectInfo, options ...fs.OpenOption) (*RereadableObject, error) {
	// Try to extract the filesystem info from the source
	var fsInfo fs.Info

	// Try different ways of getting the filesystem info
	if o, ok := src.(fs.Object); ok {
		// If it's a direct Object
		fsInfo = o.Fs()
	} else if unwrapped := fs.UnWrapObjectInfo(src); unwrapped != nil {
		// Try to unwrap it first, only if it actually unwrapped something
		fsInfo = unwrapped.Fs()
	} else if i, ok := src.(interface{ Fs() fs.Info }); ok {
		// If it has an Fs() method that returns fs.Info
		fsInfo = i.Fs()
	}

	r := &RereadableObject{
		src:     src,
		ctx:     ctx,
		options: options,
		size:    src.Size(), // Remember the size for accounting
		fsInfo:  fsInfo,     // Store the filesystem info
	}

	// Open it once to make sure it works and assign to currReader
	reader, err := r.Open()
	if err != nil {
		return nil, err
	}
	r.currReader = reader
	return r, nil
}

// retryWithExponentialBackoff provides a standard implementation of sophisticated retry logic
// with exponential backoff for handling rate limiting issues.
func retryWithExponentialBackoff(
	ctx context.Context,
	description string, // Description of the operation being retried (for logging)
	loggingObj any, // Object to log against
	operation func() error, // Operation to execute and retry
	maxRetries int, // Maximum number of retries
	initialDelay time.Duration, // Initial delay between retries
	maxDelay time.Duration, // Maximum delay between retries
	maxElapsedTime time.Duration, // Maximum total retry time
) error {
	// Set up simple exponential backoff
	var retryCount int
	currentDelay := initialDelay

	// Create retry context with timeout based on maxElapsedTime
	// Give some buffer beyond MaxElapsedTime for the final attempt + operation time
	retryCtx, cancelRetry := context.WithTimeout(ctx, maxElapsedTime+maxDelay+10*time.Second)
	defer cancelRetry()

	// Define the retry wrapper
	retryOperation := func() error {
		// Check context cancellation before proceeding
		if err := retryCtx.Err(); err != nil {
			fs.Debugf(loggingObj, "âŒ é‡è¯•ä¸Šä¸‹æ–‡å·²å–æ¶ˆ '%s': %v", description, err)
			return fmt.Errorf("retry cancelled for '%s': %w", description, err)
		}

		if retryCount > 0 {
			// Check context cancellation again before sleeping
			if err := retryCtx.Err(); err != nil {
				fs.Debugf(loggingObj, "âŒ å»¶è¿Ÿå‰é‡è¯•ä¸Šä¸‹æ–‡å·²å–æ¶ˆ '%s': %v", description, err)
				return fmt.Errorf("retry cancelled before delay for '%s': %w", description, err)
			}

			// Calculate next delay with exponential backoff
			if currentDelay > maxDelay {
				currentDelay = maxDelay
			}

			// Use more visible logging for retries
			fs.Logf(loggingObj, "Retrying '%s': Waiting %v before retry %d/%d",
				description, currentDelay, retryCount+1, maxRetries)

			// Wait for the delay, but honor context cancellation
			select {
			case <-time.After(currentDelay):
				// Continue after delay
			case <-retryCtx.Done():
				fs.Logf(loggingObj, "Retry context cancelled while waiting for delay in '%s': %v", description, retryCtx.Err())
				return fmt.Errorf("retry cancelled while waiting for delay in '%s': %w", description, retryCtx.Err())
			}

			// Exponential backoff for next iteration
			currentDelay = time.Duration(float64(currentDelay) * 2.5)
			if currentDelay > maxDelay {
				currentDelay = maxDelay
			}
		}
		retryCount++ // Increment retry count *after* the first attempt (retryCount=0)

		// Execute the actual operation
		err := operation()

		// On success, return nil to break the retry loop
		if err == nil {
			return nil
		}

		// Check if we've hit max retries (retryCount is now 1 for the first attempt, so compare with >= maxRetries)
		if retryCount >= maxRetries {
			fs.Logf(loggingObj, "Giving up '%s' after %d attempts: %v",
				description, retryCount, err)
			return fmt.Errorf("giving up '%s' after %d attempts: %w", description, retryCount, err)
		}

		// Check if this is a rate limit error or other error that we should retry
		shouldRetryErr, classification := shouldRetry(retryCtx, nil, err) // Pass retryCtx
		if !shouldRetryErr {
			// Non-retryable error
			fs.Debugf(loggingObj, "âŒ ä¸å¯é‡è¯•é”™è¯¯ (%s) å½“ '%s': %v", classification, description, err)
			return err // Wrap in Permanent to stop retries
		}

		// Log retryable errors
		fs.Debugf(loggingObj, "ğŸ”„ å¯é‡è¯•é”™è¯¯ (%s) ç¬¬%dæ¬¡å°è¯• '%s'ï¼Œå°†é‡è¯•: %v", classification, retryCount, description, err)

		return err // Return the retryable error to trigger the next retry
	}

	// Execute the retry logic
	return retryOperation()
}

// Open (re)opens the source file
func (r *RereadableObject) Open() (io.Reader, error) {
	// Close existing reader if it's a ReadCloser
	if r.currReader != nil {
		if rc, ok := r.currReader.(io.ReadCloser); ok {
			_ = rc.Close() // Ignore errors on close
		}
	}

	// Try to get the original fs.Object if it's an fs.Object
	// This is for supporting direct methods like RangeSeek later
	obj := unWrapObjectInfo(r.src)
	if obj != nil {
		var rc io.ReadCloser
		var err error

		// ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨
		var pacer *fs.Pacer
		if fsObj, ok := obj.Fs().(*Fs); ok {
			pacer = fsObj.pacer // ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨
		}

		// Set retry parameters
		maxRetries := 15
		initialDelay := 1 * time.Second
		maxDelay := 300 * time.Second
		maxElapsedTime := 30 * time.Minute

		// Define the operation to retry
		openOperation := func() error {
			var openErr error
			if pacer != nil {
				// Apply pacer to handle rate limiting (429 errors)
				err = pacer.Call(func() (bool, error) {
					rc, openErr = obj.Open(r.ctx, r.options...)
					if openErr != nil {
						// Check for 429 rate limit error
						retry, _ := shouldRetry(r.ctx, nil, openErr)
						if retry {
							fs.Debugf(obj, "ğŸ”„ è°ƒé€Ÿå™¨: é€Ÿç‡é™åˆ¶é”™è¯¯åé‡è¯•Open: %v", openErr)
							return true, openErr // Let pacer handle retry timing
						}
					}
					return false, openErr
				})
			} else {
				// Fall back to direct open if we can't use pacer
				rc, err = obj.Open(r.ctx, r.options...)
			}
			return err
		}

		// Retry with exponential backoff
		err = retryWithExponentialBackoff(
			r.ctx,
			"reopening object",
			obj,
			openOperation,
			maxRetries,
			initialDelay,
			maxDelay,
			maxElapsedTime,
		)

		if err != nil {
			return nil, fmt.Errorf("failed to reopen source object after retries: %w", err)
		}

		// Check if we already have an accounting wrapper
		// If this is a fresh Open, extract the existing accounting if any
		if r.acc == nil {
			// Get the underlying accounting.Account if there is one
			// The accounting system might wrap our reader with its own accounting
			// We need to preserve this for speed tracking to work
			_, acc := accounting.UnWrapAccounting(rc)
			if acc != nil {
				r.acc = acc
				fs.Debugf(nil, "ğŸ’¾ ä¸ºRereadableObjectä¿ç•™ç°æœ‰ä¼šè®¡åŒ…è£…å™¨")
			}
		}

		// Always wrap with accounting, even if we found an existing one
		// The accounting system will handle this properly
		stats := accounting.Stats(r.ctx)
		if stats == nil {
			fs.Debugf(nil, "âš ï¸ æœªæ‰¾åˆ°Stats - ä¼šè®¡å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")
			r.currReader = rc
			return rc, nil
		}

		name := ""
		if obj != nil {
			name = obj.Remote()
		} else if r.src != nil {
			name = r.src.String()
		}

		// Try to get real Fs objects if available by downcasting fs.Info to fs.Fs
		var srcFs, dstFs fs.Fs
		if r.fsInfo != nil {
			if f, ok := r.fsInfo.(fs.Fs); ok {
				srcFs = f
			}
		}

		// ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨çˆ¶ä¼ è¾“çš„ä¼šè®¡ç³»ç»Ÿï¼Œå®ç°ç»Ÿä¸€è¿›åº¦æ˜¾ç¤º
		var accReader *accounting.Account
		if r.useParentAccounting && r.parentTransfer != nil {
			// ä½¿ç”¨çˆ¶ä¼ è¾“çš„ä¼šè®¡ç³»ç»Ÿï¼Œå®ç°ç»Ÿä¸€è¿›åº¦æ˜¾ç¤º
			fs.Debugf(nil, "ğŸ“Š RereadableObjectä½¿ç”¨çˆ¶ä¼ è¾“ä¼šè®¡: %s", name)
			accReader = r.parentTransfer.Account(r.ctx, rc).WithBuffer()
			r.transfer = r.parentTransfer // å¼•ç”¨çˆ¶ä¼ è¾“
		} else {
			// å…³é”®ä¿®å¤ï¼šæ£€æµ‹è·¨äº‘ä¼ è¾“åœºæ™¯
			if srcFs != nil && srcFs.Name() == "123" {
				fs.Debugf(nil, "â˜ï¸ æ£€æµ‹åˆ°123è·¨äº‘ä¼ è¾“åœºæ™¯")
			}

			// åˆ›å»ºç‹¬ç«‹ä¼ è¾“ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
			if r.transfer == nil {
				fs.Debugf(nil, "ğŸ“Š RereadableObjectåˆ›å»ºç‹¬ç«‹ä¼ è¾“: %s", name)
				r.transfer = stats.NewTransferRemoteSize(name, r.size, srcFs, dstFs)
			}
			accReader = r.transfer.Account(r.ctx, rc).WithBuffer()
		}

		r.currReader = accReader

		// Extract the accounting object for later use
		_, r.acc = accounting.UnWrapAccounting(accReader)
		fs.Debugf(nil, "ğŸ“Š RereadableObject AccountçŠ¶æ€: acc=%v, currReaderç±»å‹=%T",
			r.acc != nil, r.currReader)

		return r.currReader, nil
	}

	return nil, errors.New("source doesn't support reopening")
}

// Read reads from the current reader
func (r *RereadableObject) Read(p []byte) (n int, err error) {
	if r.currReader == nil {
		return 0, errors.New("no current reader available")
	}

	// å…³é”®ä¿®å¤ï¼šç¡®ä¿è¯»å–æ“ä½œèƒ½å¤Ÿè¢«Accountå¯¹è±¡æ­£ç¡®è·Ÿè¸ª
	n, err = r.currReader.Read(p)

	// è°ƒè¯•ä¿¡æ¯ï¼šè®°å½•è¯»å–æ“ä½œ
	if n > 0 {
		fs.Debugf(nil, "ğŸ“– RereadableObjectè¯»å–: %då­—èŠ‚, Account=%v", n, r.acc != nil)
	}

	return n, err
}

// å…³é”®ä¿®å¤ï¼šå®ç°Accounteræ¥å£ï¼Œç¡®ä¿Accountå¯¹è±¡èƒ½è¢«æ­£ç¡®è¯†åˆ«

// OldStream returns the underlying stream (å®ç°Accounteræ¥å£)
func (r *RereadableObject) OldStream() io.Reader {
	if r.currReader != nil {
		return r.currReader
	}
	return nil
}

// SetStream sets the underlying stream (å®ç°Accounteræ¥å£)
func (r *RereadableObject) SetStream(in io.Reader) {
	r.currReader = in
}

// WrapStream wraps the stream with accounting (å®ç°Accounteræ¥å£)
func (r *RereadableObject) WrapStream(in io.Reader) io.Reader {
	if r.acc != nil {
		return r.acc.WrapStream(in)
	}
	return in
}

func (r *RereadableObject) GetAccount() *accounting.Account {
	return r.acc
}

type AccountedFileReader struct {
	file    *os.File
	acc     *accounting.Account
	size    int64
	name    string
	cleanup func()
}

// NewAccountedFileReader åˆ›å»ºå¸¦Accountå¯¹è±¡çš„æ–‡ä»¶è¯»å–å™¨
func NewAccountedFileReader(ctx context.Context, file *os.File, size int64, name string, cleanup func()) *AccountedFileReader {

	return &AccountedFileReader{
		file:    file,
		acc:     nil, // ä¸åˆ›å»ºAccountå¯¹è±¡ï¼Œé¿å…é‡å¤è®¡æ•°
		size:    size,
		name:    name,
		cleanup: cleanup,
	}
}

func (a *AccountedFileReader) Read(p []byte) (n int, err error) {
	return a.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£
func (a *AccountedFileReader) Close() error {
	err := a.file.Close()
	if a.cleanup != nil {
		a.cleanup()
	}
	return err
}

func (a *AccountedFileReader) GetAccount() *accounting.Account {
	return nil
}

// Seek å®ç°io.Seekeræ¥å£
func (a *AccountedFileReader) Seek(offset int64, whence int) (int64, error) {
	return a.file.Seek(offset, whence)
}

// MarkComplete marks the transfer as complete with success
func (r *RereadableObject) MarkComplete(ctx context.Context) {
	if r.transfer != nil {
		// Mark the transfer as successful and done
		r.transfer.Done(ctx, nil)
		r.transfer = nil // Clear to avoid double completion
	}
}

// Close closes the current reader if it's a ReadCloser
func (r *RereadableObject) Close() error {
	var err error

	// Close the current reader
	if r.currReader != nil {
		if rc, ok := r.currReader.(io.ReadCloser); ok {
			err = rc.Close()
		}
		r.currReader = nil
	}

	// Don't finalize the transfer here - this is just closing a read
	// The transfer should be finalized by the caller when the operation is complete
	// via MarkComplete() or by creating a new transfer

	return err
}

// getUploadBasicInfo retrieves userkey using the traditional API (needed for traditional initUpload signature).
func (f *Fs) getUploadBasicInfo(ctx context.Context) error {
	if f.userkey != "" {
		return nil // Already have it
	}
	opts := rest.Opts{
		Method:  "GET",
		RootURL: "https://proapi.115.com/app/uploadinfo", // Traditional endpoint
	}
	var info *UploadBasicInfo

	// Use traditional API call (requires cookie)
	// Assume no encryption needed for this GET request? Let's try skipping.
	err := f.CallTraditionalAPI(ctx, &opts, nil, &info, true) // skipEncrypt = true
	if err != nil {
		return fmt.Errorf("traditional uploadinfo call failed: %w", err)
	}
	if err = info.Err(); err != nil {
		return fmt.Errorf("traditional uploadinfo API error: %s (%d)", info.Error, info.Errno)
	}
	userID := info.UserID.String()
	// Verify userID matches the one from login if possible
	if f.userID != "" && userID != f.userID {
		fs.Logf(f, "Warning: UserID from uploadinfo (%s) differs from login UserID (%s)", userID, f.userID)
		// Don't fail, but log discrepancy. Use the login UserID.
	} else if f.userID == "" {
		f.userID = userID // Set userID if not already set
	}

	if info.Userkey == "" {
		return errors.New("traditional uploadinfo returned empty userkey")
	}
	f.userkey = info.Userkey
	return nil
}

// bufferIOWithAccount handles buffering with explicit Account object preservation
func bufferIOWithAccount(f *Fs, in io.Reader, size, threshold int64, account *accounting.Account) (out io.Reader, cleanup func(), err error) {
	cleanup = func() {} // Default no-op cleanup

	// If NoBuffer option is enabled, don't buffer to disk or memory
	if f.opt.NoBuffer {
		// Just return the original reader
		fs.Debugf(f, "âš ï¸ ç”±äºno_bufferé€‰é¡¹è·³è¿‡ç¼“å†²")
		return in, cleanup, nil
	}

	// If size is unknown or below threshold, read into memory
	if size < 0 || size <= threshold {
		inData, err := io.ReadAll(in)
		if err != nil {
			return nil, cleanup, fmt.Errorf("failed to read input to memory buffer: %w", err)
		}
		return bytes.NewReader(inData), cleanup, nil
	}

	// Above threshold: buffer to temporary file
	tempFile, err := os.CreateTemp("", "rclone_buffer_*.tmp")
	if err != nil {
		return nil, cleanup, fmt.Errorf("failed to create temporary file: %w", err)
	}

	// Copy input to temporary file
	written, err := io.Copy(tempFile, in)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, cleanup, fmt.Errorf("failed to copy input to temporary file: %w", err)
	}

	// Seek back to the beginning of the temp file
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, cleanup, fmt.Errorf("failed to seek temp file: %w", err)
	}

	// å…³é”®ä¿®å¤ï¼šå¦‚æœæä¾›äº†Accountå¯¹è±¡ï¼Œç”¨AccountedFileReaderåŒ…è£…ä¸´æ—¶æ–‡ä»¶
	if account != nil {
		fs.Debugf(f, "ğŸ“Š ç”¨AccountedFileReaderåŒ…è£…ä¸´æ—¶æ–‡ä»¶ï¼Œä¿æŒAccountå¯¹è±¡ä¼ é€’")
		cleanupFunc := func() {
			closeErr := tempFile.Close()
			removeErr := os.Remove(tempFile.Name())
			if closeErr != nil {
				fs.Errorf(nil, "âŒ å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥ %s: %v", tempFile.Name(), closeErr)
			}
			if removeErr != nil {
				fs.Errorf(nil, "âŒ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ %s: %v", tempFile.Name(), removeErr)
			} else {
				fs.Debugf(nil, "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: %s", tempFile.Name())
			}
		}

		accountedReader := &AccountedFileReader{
			file:    tempFile,
			acc:     account,
			size:    written,
			name:    "buffered_temp_file_with_account",
			cleanup: cleanupFunc,
		}

		newCleanup := func() {
		}

		return accountedReader, newCleanup, nil
	}

	// Set up cleanup function for regular case
	cleanup = func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}

	fs.Debugf(f, "ğŸ’¾ ç¼“å†² %d å­—èŠ‚åˆ°ä¸´æ—¶æ–‡ä»¶", written)
	return tempFile, cleanup, nil
}

// bufferIOwithSHA1 buffers the input and calculates its SHA-1 hash.
// Returns the SHA-1 hash, the potentially buffered reader, and a cleanup function.
func bufferIOwithSHA1(f *Fs, in io.Reader, src fs.ObjectInfo, size, threshold int64, ctx context.Context, options ...fs.OpenOption) (sha1sum string, out io.Reader, cleanup func(), err error) {
	cleanup = func() {} // Default no-op cleanup

	// First check if the source object already has the SHA1 hash
	if srcObj, ok := src.(fs.Object); ok {
		hashVal, hashErr := srcObj.Hash(ctx, hash.SHA1)
		if hashErr == nil && hashVal != "" {
			fs.Debugf(srcObj, "ğŸ” ä½¿ç”¨é¢„è®¡ç®—çš„SHA1: %s", hashVal)
			return hashVal, in, cleanup, nil
		}
	}

	// If NoBuffer option is enabled, calculate the SHA1 using a non-buffering approach
	if f.opt.NoBuffer {
		fs.Debugf(f, "ğŸ” ç”±äºno_bufferé€‰é¡¹æ— ç¼“å†²è®¡ç®—SHA1")

		// Create a rereadable source if it's not already one
		var rereadable *RereadableObject
		if ro, ok := in.(*RereadableObject); ok {
			rereadable = ro
		} else {
			// Try to create a new rereadable source
			ro, roErr := NewRereadableObject(ctx, src, options...)
			if roErr != nil {
				return "", in, cleanup, fmt.Errorf("failed to create rereadable object: %w", roErr)
			}
			rereadable = ro
			cleanup = func() { _ = rereadable.Close() }
		}

		// Calculate SHA1 hash
		hasher := sha1.New()
		if _, hashErr := io.Copy(hasher, rereadable); hashErr != nil {
			return "", in, cleanup, fmt.Errorf("failed to calculate SHA1: %w", hashErr)
		}
		sha1Hash := fmt.Sprintf("%x", hasher.Sum(nil))

		// Reopen the source for the actual upload
		var newReader io.Reader
		var reopenErr error

		// Set retry parameters for reopening
		maxRetries := 12
		initialDelay := 1 * time.Second
		maxDelay := 180 * time.Second
		maxElapsedTime := 20 * time.Minute

		// Define the reopening operation
		reopenOperation := func() error {
			var openErr error
			newReader, openErr = rereadable.Open()
			return openErr
		}

		// Retry with exponential backoff
		reopenErr = retryWithExponentialBackoff(
			ctx,
			"reopening after SHA1",
			f,
			reopenOperation,
			maxRetries,
			initialDelay,
			maxDelay,
			maxElapsedTime,
		)

		if reopenErr != nil {
			return "", in, cleanup, fmt.Errorf("failed to reopen source after SHA1 calculation: %w", reopenErr)
		}

		sha1sum = sha1Hash
		fs.Debugf(f, "âœ… æ— ç¼“å†²è®¡ç®—SHA1å®Œæˆ: %s", sha1sum)
		return sha1sum, newReader, cleanup, nil
	}

	// ä½¿ç”¨æ ‡å‡†SHA1è®¡ç®—ï¼Œç®€åŒ–é€»è¾‘

	// Check and save Account object before creating TeeReader
	var originalAccount *accounting.Account
	if accountedReader, ok := in.(*AccountedFileReader); ok {
		originalAccount = accountedReader.GetAccount()
	}

	// ä½¿ç”¨æ›´ç®€å•å¯é çš„æ–¹æ³•ï¼šå…ˆç¼“å†²æ•°æ®ï¼Œç„¶åä»ç¼“å†²åŒºè®¡ç®—SHA1
	// è¿™é¿å…äº†goroutineåŒæ­¥é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨è·¨ç½‘ç›˜å¤åˆ¶çš„å¤æ‚åœºæ™¯ä¸‹

	// Buffer the input first
	out, cleanup, err = bufferIOWithAccount(f, in, size, threshold, originalAccount)
	if err != nil {
		return "", nil, cleanup, fmt.Errorf("failed to buffer input for SHA1 calculation: %w", err)
	}

	// Calculate SHA1 from the buffered data
	var sha1Hash string
	if seeker, ok := out.(io.Seeker); ok {
		// If the buffered output is seekable, calculate SHA1 from it
		hasher := sha1.New()
		_, err := io.Copy(hasher, out)
		if err != nil {
			return "", nil, cleanup, fmt.Errorf("failed to calculate SHA1 from buffered data: %w", err)
		}
		sha1Hash = fmt.Sprintf("%x", hasher.Sum(nil))

		// Seek back to the beginning for the actual upload
		_, err = seeker.Seek(0, io.SeekStart)
		if err != nil {
			return "", nil, cleanup, fmt.Errorf("failed to seek back after SHA1 calculation: %w", err)
		}
	} else {
		// If not seekable, we need to read the data twice
		// This should not happen with our current bufferIO implementation, but handle it gracefully
		return "", nil, cleanup, fmt.Errorf("buffered output is not seekable, cannot calculate SHA1")
	}

	sha1sum = sha1Hash
	fs.Debugf(f, "âœ… ä»ç¼“å†²æ•°æ®è®¡ç®—SHA1å®Œæˆ: %s", sha1sum)
	return sha1sum, out, cleanup, nil
}

// initUploadOpenAPI calls the OpenAPI /open/upload/init endpoint.
func (f *Fs) initUploadOpenAPI(ctx context.Context, size int64, name, dirID, sha1sum, preSha1, signKey, signVal string) (*UploadInitInfo, error) {
	form := url.Values{}

	cleanName := f.opt.Enc.FromStandardName(name)
	form.Set("file_name", cleanName)
	form.Set("file_size", strconv.FormatInt(size, 10))
	// æ ¹æ®115ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£ï¼Œtargetæ ¼å¼ä¸º"U_1_"+dirID
	form.Set("target", "U_1_"+dirID)
	if sha1sum != "" {
		form.Set("fileid", strings.ToUpper(sha1sum)) // fileid is the full SHA1
	}
	if preSha1 != "" {
		form.Set("preid", preSha1) // preid is the 128k SHA1
	}

	if signKey != "" && signVal != "" {
		form.Set("sign_key", signKey)
		form.Set("sign_val", signVal) // Value should be uppercase SHA1 of range
	}
	// æ ¹æ®115ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£ï¼Œæ·»åŠ topuploadå‚æ•°
	// 0ï¼šå•æ–‡ä»¶ä¸Šä¼ ä»»åŠ¡æ ‡è¯†ä¸€æ¡å•ç‹¬çš„æ–‡ä»¶ä¸Šä¼ è®°å½•
	form.Set("topupload", "0")

	// Log parameters for debugging, but mask sensitive values
	fs.Debugf(f, "Initializing upload for file_name=%q (cleaned: %q), size=%d, target=U_1_%s, has_fileid=%v, has_preid=%v, has_sign=%v",
		name, cleanName, size, dirID, sha1sum != "", preSha1 != "", signKey != "")

	// Log API parameters
	fs.Debugf(f, "API params: file_name=%q, file_size=%d, target=%q, has_fileid=%v, has_preid=%v",
		cleanName, size, "U_1_"+dirID, sha1sum != "", preSha1 != "")

	// Create request options
	opts := rest.Opts{
		Method: "POST",
		Path:   "/open/upload/init",
		Body:   strings.NewReader(form.Encode()),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
		},
	}

	var info UploadInitInfo // Response structure includes nested Data for OpenAPI
	// ä½¿ç”¨ä¸“ç”¨çš„ä¸Šä¼ è°ƒé€Ÿå™¨ï¼Œä¼˜åŒ–ä¸Šä¼ åˆå§‹åŒ–APIè°ƒç”¨é¢‘ç‡
	err := f.CallUploadAPI(ctx, &opts, nil, &info, false)
	if err != nil {
		// Use standard error handling instead of hardcoded error checks

		// General error handling
		return nil, fmt.Errorf("OpenAPI initUpload failed: %w", err)
	}

	// Error checking is handled by CallOpenAPI using info.Err()
	// Log API response
	fs.Debugf(f, "API response: State=%v, Code=%d, Message=%q, Status=%d",
		info.State, info.ErrCode(), info.ErrMsg(), info.GetStatus())

	if info.Data == nil {
		// If Data is nil but call succeeded, maybe it's a ç§’ä¼  response where fields are top-level?
		if info.State {
			if info.GetFileID() != "" && info.GetPickCode() != "" {
				// This is likely a successful ç§’ä¼  response with top-level fields
				fs.Debugf(f, "Detected direct ç§’ä¼  success response with top-level fields")
				return &info, nil
			}
			// ä¿®å¤ï¼šå³ä½¿æ²¡æœ‰è¶³å¤Ÿçš„é¡¶çº§å­—æ®µï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æœ‰ç”¨ä¿¡æ¯
			fs.Debugf(f, "API returned success but data incomplete, attempting to continue processing...")
			return &info, nil
		}

		// If state is false, CallOpenAPI should have returned an error, but let's add an extra check
		errMsg := info.ErrMsg()
		if info.ErrCode() != 0 || errMsg != "" {
			return nil, fmt.Errorf("OpenAPI initUpload failed with error code %d: %s",
				info.ErrCode(), errMsg)
		}

		return nil, errors.New("internal error: OpenAPI initUpload failed but CallOpenAPI returned no error")
	}

	// Log successful initialization
	statusMsg := "normal upload"
	if info.GetStatus() == 2 {
		statusMsg = "ç§’ä¼  success"
	}
	fs.Debugf(f, "Upload initialized: status=%d (%s), bucket=%q, object=%q",
		info.GetStatus(), statusMsg, info.GetBucket(), info.GetObject())

	return &info, nil
}

// postUpload processes the JSON callback after an upload to OSS.
// The callback result from OSS SDK v2 is already a map[string]any.
func (f *Fs) postUpload(callbackResult map[string]any) (*CallbackData, error) {
	if callbackResult == nil {
		return nil, errors.New("received nil callback result from OSS")
	}

	// Check for standard OSS callback status if present
	if statusVal, ok := callbackResult["Status"]; ok {
		if statusStr, ok := statusVal.(string); ok && !strings.HasPrefix(statusStr, "OK") {
			// Try to get more info from the map
			errMsg := fmt.Sprintf("OSS callback failed with Status: %s", statusStr)
			if bodyVal, ok := callbackResult["body"]; ok {
				if bodyStr, ok := bodyVal.(string); ok {
					errMsg += fmt.Sprintf(", Body: %s", bodyStr)
				}
			}
			return nil, errors.New(errMsg)
		}
	}

	// Check for OpenAPI format (with state/code/message/data structure)
	var cbData *CallbackData

	// First, check if this is an OpenAPI format response
	if stateVal, ok := callbackResult["state"]; ok {
		// Check if the state indicates failure
		if state, ok := stateVal.(bool); ok && !state {
			// This is a failed OpenAPI response, extract error information
			var errCode int
			var errMsg string

			if codeVal, ok := callbackResult["code"]; ok {
				if code, ok := codeVal.(float64); ok {
					errCode = int(code)
				}
			}

			if msgVal, ok := callbackResult["message"]; ok {
				if msg, ok := msgVal.(string); ok {
					errMsg = msg
				}
			}

			// Handle specific error codes
			switch errCode {
			case 10002:
				return nil, fmt.Errorf("115 file validation failed (10002): %s - file may be corrupted during transfer, retry recommended", errMsg)
			case 10001:
				return nil, fmt.Errorf("115 upload parameter error (10001): %s", errMsg)
			default:
				return nil, fmt.Errorf("115 upload failed (error code: %d): %s", errCode, errMsg)
			}
		}

		// This could be a successful OpenAPI format response
		if dataVal, ok := callbackResult["data"]; ok {
			if dataMap, ok := dataVal.(map[string]any); ok {
				// Try to extract file_id and pick_code from the data field
				fileID, fileIDExists := dataMap["file_id"].(string)
				pickCode, pickCodeExists := dataMap["pick_code"].(string)

				if fileIDExists && pickCodeExists {
					// Create a CallbackData from the nested data map
					cbData = &CallbackData{
						FileID:   fileID,
						PickCode: pickCode,
					}

					// Copy other fields if they exist
					if fileName, ok := dataMap["file_name"].(string); ok {
						cbData.FileName = fileName
					}
					if fileSize, ok := dataMap["file_size"].(string); ok {
						if size, err := strconv.ParseInt(fileSize, 10, 64); err == nil {
							cbData.FileSize = Int64(size)
						}
					}
					if sha1, ok := dataMap["sha1"].(string); ok {
						cbData.Sha = sha1
					}

					// Log the values for debugging
					fs.Debugf(f, "ğŸ“‹ OpenAPIå›è°ƒæ•°æ®è§£æ: file_id=%q, pick_code=%q", cbData.FileID, cbData.PickCode)

					// Early return if we successfully extracted data
					if cbData.FileID != "" && cbData.PickCode != "" {
						return cbData, nil
					}
				}
			}
		}
	}

	// If we couldn't extract from OpenAPI format, try traditional format
	// Need to marshal it back to JSON and then unmarshal to CallbackData for validation/typing
	callbackJSON, err := json.Marshal(callbackResult)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal OSS callback result: %w", err)
	}

	cbData = &CallbackData{}
	if err := json.Unmarshal(callbackJSON, cbData); err != nil {
		return nil, fmt.Errorf("failed to unmarshal OSS callback result into CallbackData: %w. JSON: %s", err, string(callbackJSON))
	}

	// Basic validation
	if cbData.FileID == "" || cbData.PickCode == "" {
		// Debugging information
		fs.Debugf(f, "âš ï¸ å›è°ƒæ•°æ®ç¼ºå°‘å¿…éœ€å­—æ®µ: file_id=%q, pick_code=%q, raw JSON: %s",
			cbData.FileID, cbData.PickCode, string(callbackJSON))

		// Additional debugging for error analysis
		if strings.Contains(string(callbackJSON), "10002") {
			fs.Logf(f, "115 error analysis: Error code 10002 usually indicates file corruption during transfer")
			fs.Logf(f, "Possible solutions: 1) Check network stability 2) Retry upload 3) Verify source file integrity")
		}

		return nil, fmt.Errorf("OSS callback data missing required fields (file_id or pick_code). JSON: %s", string(callbackJSON))
	}

	fs.Debugf(f, "ğŸ“‹ ä¼ ç»Ÿå›è°ƒæ•°æ®è§£æ: file_id=%q, pick_code=%q", cbData.FileID, cbData.PickCode)
	return cbData, nil
}

// getOSSToken fetches OSS credentials using the OpenAPI.
func (f *Fs) getOSSToken(ctx context.Context) (*OSSToken, error) {
	opts := rest.Opts{
		Method: "GET",
		Path:   "/open/upload/get_token",
	}
	var info OSSTokenResp
	err := f.CallOpenAPI(ctx, &opts, nil, &info, false)
	if err != nil {
		return nil, fmt.Errorf("OpenAPI get_token failed: %w", err)
	}
	if info.Data == nil {
		return nil, errors.New("OpenAPI get_token returned no data")
	}

	if info.Data.AccessKeyID == "" || info.Data.AccessKeySecret == "" || info.Data.SecurityToken == "" {
		return nil, errors.New("OpenAPI get_token response missing essential credential fields")
	}
	return info.Data, nil
}

// newOSSClient builds an OSS client with dynamic credentials from OpenAPI.
func (f *Fs) newOSSClient() (*oss.Client, error) {
	// Use CredentialsFetcherProvider from SDK v2
	provider := credentials.NewCredentialsFetcherProvider(
		credentials.CredentialsFetcherFunc(func(ctx context.Context) (credentials.Credentials, error) {
			fs.Debugf(f, "Fetching new OSS credentials via OpenAPI...")
			t, err := f.getOSSToken(ctx)
			if err != nil {
				fs.Errorf(f, "âŒ è·å–OSSä»¤ç‰Œå¤±è´¥: %v", err)
				return credentials.Credentials{}, fmt.Errorf("failed to fetch OSS token: %w", err)
			}
			fs.Debugf(f, "Successfully fetched OSS credentials, expires at %v", time.Time(t.Expiration))
			return credentials.Credentials{
				AccessKeyID:     t.AccessKeyID,
				AccessKeySecret: t.AccessKeySecret,
				SecurityToken:   t.SecurityToken,
				Expires:         (*time.Time)(&t.Expiration), // Convert Time to *time.Time
			}, nil
		}),
	)

	// ä¿®å¤ï¼š115ç½‘ç›˜OSSä¸Šä¼ æ€§èƒ½ä¼˜åŒ–é…ç½®
	cfg := oss.LoadDefaultConfig().
		WithCredentialsProvider(provider).
		WithRegion(OSSRegion). // Use constant region
		WithUserAgent(OSSUserAgent).
		WithUseDualStackEndpoint(f.opt.DualStack).
		WithUseInternalEndpoint(f.opt.Internal).
		// ä¿®å¤ï¼šä¼˜åŒ–è¿æ¥è¶…æ—¶ï¼Œå¿«é€Ÿå»ºç«‹è¿æ¥
		WithConnectTimeout(10 * time.Second). // å¢åŠ åˆ°10ç§’ï¼Œé¿å…è¿æ¥è¶…æ—¶
		// ä¿®å¤ï¼šä¼˜åŒ–è¯»å†™è¶…æ—¶ï¼Œé€‚åˆå¤§æ–‡ä»¶ä¸Šä¼ 
		WithReadWriteTimeout(300 * time.Second) // å¢åŠ åˆ°300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œé€‚åˆå¤§æ–‡ä»¶ä¸Šä¼ 

	// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	httpClient := fshttp.NewClient(context.Background())

	// å°†HTTPå®¢æˆ·ç«¯åº”ç”¨åˆ°OSSé…ç½®
	cfg = cfg.WithHttpClient(httpClient)

	// å…³é”®ä¿®å¤ï¼šè®¾ç½®OSSä¸“ç”¨User-Agentï¼Œå¯èƒ½å½±å“ä¸Šä¼ é€Ÿåº¦
	cfg = cfg.WithUserAgent(OSSUserAgent)

	// Create the client
	client := oss.NewClient(cfg)
	if client == nil {
		return nil, errors.New("failed to create OSS client")
	}
	return client, nil
}

// unWrapObjectInfo attempts to unwrap the underlying fs.Object from fs.ObjectInfo.
func unWrapObjectInfo(oi fs.ObjectInfo) fs.Object {
	if o, ok := oi.(fs.Object); ok {
		return fs.UnWrapObject(o) // Use standard unwrapper
	}
	// Handle specific wrappers like OverrideRemote if necessary
	// if do, ok := oi.(*fs.OverrideRemote); ok {
	// return do.UnWrap()
	// }
	return nil
}

// calcBlockSHA1 calculates SHA-1 for a specified byte range from a source reader.
// The reader `in` should ideally be seekable (e.g., buffered file).
func calcBlockSHA1(ctx context.Context, in io.Reader, src fs.ObjectInfo, rangeSpec string) (string, error) {
	var start, end int64
	// OpenAPI range is "start-end" (inclusive)
	if n, err := fmt.Sscanf(rangeSpec, "%d-%d", &start, &end); err != nil || n != 2 {
		return "", fmt.Errorf("invalid range spec format %q: %w", rangeSpec, err)
	}
	if start < 0 || end < start {
		return "", fmt.Errorf("invalid range spec values %q", rangeSpec)
	}
	length := end - start + 1

	var sectionReader io.Reader

	// Check if input is a RereadableObject first
	if ro, ok := in.(*RereadableObject); ok {
		// Try to open a new reader with robust retries for rate limiting
		var reader io.Reader // Holds the successfully opened reader
		var err error

		// Set retry parameters for reopening within calcBlockSHA1
		maxRetries := 10                       // Max retries for opening range
		initialDelay := 500 * time.Millisecond // Start with 500ms
		maxDelay := 60 * time.Second           // Max delay 1 minute
		maxElapsedTime := 5 * time.Minute      // Max total time

		// Define the reopening operation
		reopenOperation := func() error {
			var openErr error
			reader, openErr = ro.Open() // Attempt to open
			if openErr != nil {
				fs.Debugf(src, "Open attempt failed in calcBlockSHA1: %v", openErr)
			}
			return openErr // Return error for retry logic
		}

		// Retry with exponential backoff
		err = retryWithExponentialBackoff(
			ctx,
			"opening object for range SHA1",
			src, // Log against the source object info
			reopenOperation,
			maxRetries,
			initialDelay,
			maxDelay,
			maxElapsedTime,
		)

		if err != nil {
			return "", fmt.Errorf("failed to open RereadableObject reader for range SHA1 after retries: %w", err)
		}

		// If the obtained reader is an io.ReadCloser, ensure it's closed when calcBlockSHA1 finishes.
		// This does NOT close the parent RereadableObject 'ro'.
		closeReader := func() {} // No-op default
		if rc, ok := reader.(io.ReadCloser); ok {
			closeReader = func() { _ = rc.Close() }
		}
		defer closeReader() // Close the specific reader obtained for this SHA1 calc

		// Skip to the start position using the successfully opened 'reader'
		if seeker, ok := reader.(io.Seeker); ok {
			if _, err := seeker.Seek(start, io.SeekStart); err != nil {
				// Attempt to close the reader before returning error
				closeReader()
				return "", fmt.Errorf("failed to seek RereadableObject reader to %d: %w", start, err)
			}
			sectionReader = io.LimitReader(reader, length)
		} else {
			// If not seekable, try skipping bytes
			if start > 0 {
				// Use io.CopyN for skipping
				skipped, err := io.CopyN(io.Discard, reader, start)
				if err != nil {
					// Attempt to close the reader before returning error
					closeReader()
					return "", fmt.Errorf("failed to skip %d bytes in RereadableObject reader (skipped %d): %w", start, skipped, err)
				}
				if skipped != start {
					// Attempt to close the reader before returning error
					closeReader()
					return "", fmt.Errorf("failed to skip requested %d bytes in RereadableObject reader, only skipped %d", start, skipped)
				}
			}
			sectionReader = io.LimitReader(reader, length)
		}
	} else if seeker, ok := in.(io.Seeker); ok {
		// Try to create a SectionReader if the input is seekable
		// IMPORTANT: Seek back to start after reading the section
		currentOffset, seekErr := seeker.Seek(0, io.SeekCurrent)
		if seekErr != nil {
			return "", fmt.Errorf("failed to get current offset for SHA1 range: %w", seekErr)
		}
		defer func() {
			_, _ = seeker.Seek(currentOffset, io.SeekStart) // Restore original position
		}()

		// Seek to the start of the required section
		_, seekErr = seeker.Seek(start, io.SeekStart)
		if seekErr != nil {
			// Maybe the buffer doesn't contain the required range?
			return "", fmt.Errorf("failed to seek to start %d for SHA1 range: %w", start, seekErr)
		}
		sectionReader = io.LimitReader(in, length)
	} else {
		// If not seekable, we cannot reliably calculate the hash for an arbitrary range.
		// This might happen if the input is a direct network stream and hasn't been buffered.
		// Try opening the source object directly if possible.
		srcObj := unWrapObjectInfo(src)
		if srcObj != nil {
			fs.Debugf(src, "Input reader not seekable for SHA1 range, opening source object directly.")

			// Try to open with retries for rate limiting
			var rc io.ReadCloser
			var err error

			// Define maximum retries and use exponential backoff
			maxRetries := 10                       // Max retries
			initialDelay := 500 * time.Millisecond // Start delay
			maxDelay := 60 * time.Second           // Max delay
			maxElapsedTime := 5 * time.Minute      // Max total time

			// Define the operation
			openRangeOperation := func() error {
				var openErr error
				// Use RangeOption for efficiency if supported
				rc, openErr = srcObj.Open(ctx, &fs.RangeOption{Start: start, End: end})
				if openErr != nil {
					fs.Debugf(srcObj, "Open range attempt failed: %v", openErr)
				}
				return openErr
			}

			// Retry with exponential backoff
			err = retryWithExponentialBackoff(
				ctx,
				"opening source object range directly",
				srcObj,
				openRangeOperation,
				maxRetries,
				initialDelay,
				maxDelay,
				maxElapsedTime,
			)

			if err != nil {
				return "", fmt.Errorf("failed to open source object for SHA1 range %q after retries: %w", rangeSpec, err)
			}
			// Defer closing the reader obtained specifically for this operation
			defer fs.CheckClose(rc, &err)
			sectionReader = rc
		} else {
			return "", fmt.Errorf("cannot calculate SHA1 for range %q: input reader not seekable and source object unavailable", rangeSpec)
		}
	}

	// ä½¿ç”¨æ ‡å‡†SHA1è®¡ç®—
	hasher := sha1.New()
	_, err := io.Copy(hasher, sectionReader)
	if err != nil {
		return "", fmt.Errorf("failed to calculate SHA1 for range %q: %w", rangeSpec, err)
	}
	// å…³é”®ä¿®å¤ï¼šsign_valéœ€è¦å¤§å†™SHA1ï¼Œæ ¹æ®115ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£è¦æ±‚
	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil))
	return sha1Hash, nil
}

// sampleInitUpload prepares a traditional "simple form" upload (for smaller files).
func (f *Fs) sampleInitUpload(ctx context.Context, size int64, name, dirID string) (*SampleInitResp, error) {
	// Try to get userID if not already set (e.g., during initial login)
	if f.userID == "" {
		// Get userID from uploadinfo API
		if err := f.getUploadBasicInfo(ctx); err != nil {
			return nil, fmt.Errorf("failed to get userID: %w", err)
		}
	}

	form := url.Values{}
	form.Set("userid", f.userID)
	form.Set("filename", f.opt.Enc.FromStandardName(name))
	form.Set("filesize", strconv.FormatInt(size, 10))
	form.Set("target", "U_1_"+dirID)

	opts := rest.Opts{
		Method:      "POST",
		RootURL:     "https://uplb.115.com/3.0/sampleinitupload.php", // Traditional endpoint
		ContentType: "application/x-www-form-urlencoded",
		Body:        strings.NewReader(form.Encode()),
	}
	var info *SampleInitResp
	// Use traditional API call (requires cookie, assume no encryption needed for this endpoint?)
	err := f.CallTraditionalAPI(ctx, &opts, nil, &info, true) // skipEncrypt = true
	if err != nil {
		return nil, fmt.Errorf("traditional sampleInitUpload call failed: %w", err)
	}
	if info.ErrorCode != 0 {
		return nil, fmt.Errorf("traditional sampleInitUpload API error: %s (%d)", info.Error, info.ErrorCode)
	}
	if info.Host == "" || info.Object == "" {
		return nil, errors.New("traditional sampleInitUpload response missing required fields (host or object)")
	}
	return info, nil
}

// sampleUploadForm uses multipart form to upload smaller files via traditional sample upload flow.
func (f *Fs) sampleUploadForm(ctx context.Context, in io.Reader, initResp *SampleInitResp, name string, options ...fs.OpenOption) (*CallbackData, error) {
	// Safety check for nil input
	if in == nil {
		return nil, errors.New("nil input reader provided to sampleUploadForm")
	}
	if initResp == nil {
		return nil, errors.New("nil initResp provided to sampleUploadForm")
	}

	pipeReader, pipeWriter := io.Pipe()
	multipartWriter := multipart.NewWriter(pipeWriter)
	errChan := make(chan error, 1)

	// Start goroutine to write multipart data to the pipe
	go func() {
		var err error
		defer func() {
			closeErr := multipartWriter.Close()
			if err == nil {
				err = closeErr // Assign close error if no previous error
			}
			writeCloseErr := pipeWriter.CloseWithError(err) // Close pipe with error status
			if err == nil {
				err = writeCloseErr
			}
			errChan <- err // Send final status
		}()

		// Write standard fields
		fields := map[string]string{
			"name":                  name, // Use original name for form field?
			"key":                   initResp.Object,
			"policy":                initResp.Policy,
			"OSSAccessKeyId":        initResp.AccessID,
			"success_action_status": "200", // OSS expects 200 for success
			"callback":              initResp.Callback,
			"signature":             initResp.Signature,
		}
		for k, v := range fields {
			if v == "" {
				fs.Debugf(f, "Warning: empty value for form field %q", k)
				// Continue anyway, some fields might be optional
			}
			if err = multipartWriter.WriteField(k, v); err != nil {
				err = fmt.Errorf("failed to write field %s: %w", k, err)
				return
			}
		}

		// Add optional headers from fs.OpenOption
		for _, opt := range options {
			k, v := opt.Header()
			lowerK := strings.ToLower(k)
			// Include headers supported by OSS PostObject policy/form
			if lowerK == "cache-control" || lowerK == "content-disposition" || lowerK == "content-encoding" || lowerK == "content-type" || strings.HasPrefix(lowerK, "x-oss-meta-") {
				if err = multipartWriter.WriteField(k, v); err != nil {
					err = fmt.Errorf("failed to write optional field %s: %w", k, err)
					return
				}
			}
		}

		// Write file data
		filePart, err := multipartWriter.CreateFormFile("file", f.opt.Enc.FromStandardName(name)) // Use encoded name for file part?
		if err != nil {
			err = fmt.Errorf("failed to create form file: %w", err)
			return
		}

		// Double-check in is not nil again (just being extra careful)
		if in == nil {
			err = errors.New("input reader became nil before copy in sampleUploadForm")
			return
		}

		// Copy file data
		if _, err = io.Copy(filePart, in); err != nil {
			err = fmt.Errorf("failed to copy file data to form: %w", err)
			return
		}
	}()

	// Create and send the HTTP request
	req, err := http.NewRequestWithContext(ctx, "POST", initResp.Host, pipeReader)
	if err != nil {
		_ = pipeWriter.CloseWithError(err) // Ensure goroutine exits
		return nil, fmt.Errorf("failed to create sample upload request: %w", err)
	}
	req.Header.Set("Content-Type", multipartWriter.FormDataContentType())
	// Set Content-Length? OSS might require it for POST uploads.
	// However, with pipeReader, length is unknown beforehand. Let http client handle chunked encoding.
	// req.ContentLength = -1 // Indicate unknown length

	// ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨ï¼Œä¼˜åŒ–æ ·æœ¬ä¸Šä¼ APIè°ƒç”¨é¢‘ç‡
	var resp *http.Response
	err = f.pacer.Call(func() (bool, error) {
		// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
		httpClient := fshttp.NewClient(ctx)
		resp, err = httpClient.Do(req)
		if err != nil {
			retry, retryErr := shouldRetry(ctx, resp, err)
			if retry {
				return true, retryErr
			}
			return false, fmt.Errorf("sample upload POST failed: %w", err)
		}
		return false, nil // Success
	})

	// Wait for the goroutine writing to the pipe to finish
	writeErr := <-errChan
	if err == nil { // If HTTP call succeeded, check for writer error
		err = writeErr
	}
	if err != nil {
		// If there was an error during HTTP or writing, close response body if non-nil
		if resp != nil && resp.Body != nil {
			_ = resp.Body.Close()
		}
		return nil, fmt.Errorf("sample upload failed: %w", err)
	}

	// Process the response
	defer fs.CheckClose(resp.Body, &err)
	respBody, readErr := io.ReadAll(resp.Body)
	if readErr != nil {
		return nil, fmt.Errorf("failed to read sample upload response body: %w", readErr)
	}

	// OSS POST upload returns 200 on success with callback body, or other status on error
	if resp.StatusCode != 200 {
		return nil, fmt.Errorf("sample upload failed: status %s, body: %s", resp.Status, string(respBody))
	}

	// Response body contains the result from the 115 callback URL
	var respMap map[string]any
	if err := json.Unmarshal(respBody, &respMap); err != nil {
		// Sometimes the body might not be JSON if callback failed internally on 115 side
		fs.Logf(f, "Failed to unmarshal sample upload callback response as JSON: %v. Body: %s", err, string(respBody))
		// Try to find essential info heuristically? Risky. Return error.
		return nil, fmt.Errorf("failed to parse sample upload callback response: %w", err)
	}

	// Process the callback map using the existing postUpload function
	return f.postUpload(respMap)
}

// ------------------------------------------------------------
// Main Upload Logic
// ------------------------------------------------------------

// tryHashUpload attempts ç§’ä¼  using OpenAPI.
// Returns (found bool, uploadInitInfo *UploadInitInfo, potentiallyBufferedInput io.Reader, cleanup func(), err error)
func (f *Fs) tryHashUpload(
	ctx context.Context,
	in io.Reader,
	src fs.ObjectInfo,
	o *Object,
	leaf, dirID string,
	size int64,
	options ...fs.OpenOption,
) (found bool, ui *UploadInitInfo, newIn io.Reader, cleanup func(), err error) {
	cleanup = func() {} // Default no-op cleanup
	newIn = in          // Assume input reader doesn't change unless buffered

	defer func() {
		if err != nil && cleanup != nil {
			cleanup() // Ensure cleanup happens on error exit
		}
	}()

	fs.Infof(o, "âš¡ å°è¯•ç§’ä¼  (æ–‡ä»¶å·²å­˜åœ¨åˆ™è·³è¿‡ä¸Šä¼ )...")

	// 1. Get SHA1 hash
	hashStr, err := src.Hash(ctx, hash.SHA1)
	if err != nil || hashStr == "" {
		fs.Debugf(o, "Source SHA1 not available, calculating locally...")
		var localCleanup func()
		// Buffer the input while calculating hash
		hashStr, newIn, localCleanup, err = bufferIOwithSHA1(f, in, src, size, int64(f.opt.HashMemoryThreshold), ctx, options...)
		cleanup = localCleanup // Assign the cleanup function from bufferIO
		if err != nil {
			return false, nil, newIn, cleanup, fmt.Errorf("failed to calculate SHA1: %w", err)
		}
		fs.Debugf(o, "Calculated SHA1: %s", hashStr)

		// å…³é”®ä¿®å¤ï¼šå°†è®¡ç®—å‡ºçš„SHA1è®¾ç½®åˆ°Objectä¸­ï¼Œç¡®ä¿åç»­ä½¿ç”¨
		if o != nil {
			o.sha1sum = strings.ToUpper(hashStr)
			fs.Debugf(o, "SHA1 set to Object: %s", o.sha1sum)
		}
	} else {
		fs.Debugf(o, "Using provided SHA1: %s", hashStr)

		// ç¡®ä¿Objectä¸­ä¹Ÿæœ‰SHA1ä¿¡æ¯
		if o != nil && o.sha1sum == "" {
			o.sha1sum = strings.ToUpper(hashStr)
			fs.Debugf(o, "SHA1 set to Object: %s", o.sha1sum)
		}

		// If NoBuffer is enabled, wrap the input in a RereadableObject
		if f.opt.NoBuffer {
			// ä¿®å¤ï¼šæ£€æµ‹è·¨äº‘ä¼ è¾“å¹¶å°è¯•é›†æˆçˆ¶ä¼ è¾“
			var ro *RereadableObject
			var roErr error

			// æ£€æµ‹æ˜¯å¦ä¸ºè·¨äº‘ä¼ è¾“ï¼ˆç‰¹åˆ«æ˜¯123ç½‘ç›˜æºï¼‰
			if f.isRemoteSource(src) {
				fs.Debugf(o, "â˜ï¸ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“ï¼Œå°è¯•æŸ¥æ‰¾çˆ¶ä¼ è¾“å¯¹è±¡")

				// Try to get current transfer from accounting stats
				if stats := accounting.Stats(ctx); stats != nil {
					// Use standard method with cross-cloud transfer marker
					ro, roErr = NewRereadableObject(ctx, src, options...)
				}
			}

			// å¦‚æœä¸æ˜¯è·¨äº‘ä¼ è¾“æˆ–è€…ä¸Šé¢çš„é€»è¾‘å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•
			if ro == nil {
				ro, roErr = NewRereadableObject(ctx, src, options...)
			}

			if roErr != nil {
				// Continue with original reader if failed
				fs.Debugf(o, "Failed to create rereadable object: %v", roErr)
			} else {
				newIn = ro
				cleanup = func() { _ = ro.Close() }
			}
		}
	}
	// å…³é”®ä¿®å¤ï¼š115ç½‘ç›˜ä½¿ç”¨å¤§å†™SHA1ï¼Œä¿æŒä¸€è‡´æ€§ä»¥æ”¯æŒç§’ä¼ åŠŸèƒ½
	o.sha1sum = strings.ToUpper(hashStr) // Store hash in object (uppercase for 115 API consistency)

	// 2. Calculate PreID (128KB SHA1) as required by 115ç½‘ç›˜å®˜æ–¹API
	var preID string
	if size > 0 {
		// è®¡ç®—å‰128KBçš„SHA1ä½œä¸ºPreID
		const preHashSize int64 = 128 * 1024 // 128KB
		hashSize := min(size, preHashSize)

		// å°è¯•ä»newInè¯»å–å‰128KBè®¡ç®—PreID
		if seeker, ok := newIn.(io.ReadSeeker); ok {
			// å¦‚æœnewInæ”¯æŒSeekï¼ˆæ¯”å¦‚ä¸´æ—¶æ–‡ä»¶ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
			seeker.Seek(0, io.SeekStart) // é‡ç½®åˆ°æ–‡ä»¶å¼€å¤´
			preData := make([]byte, hashSize)
			n, err := io.ReadFull(seeker, preData)
			if err != nil && err != io.ErrUnexpectedEOF {
				fs.Debugf(o, "è¯»å–å‰128KBæ•°æ®å¤±è´¥: %v", err)
			} else if n > 0 {
				hasher := sha1.New()
				hasher.Write(preData[:n])
				// å…³é”®ä¿®å¤ï¼šPreIDä¹Ÿéœ€è¦å¤§å†™æ ¼å¼ï¼Œä¸115ç½‘ç›˜APIä¿æŒä¸€è‡´
				preID = fmt.Sprintf("%X", hasher.Sum(nil))
				fs.Debugf(o, "è®¡ç®—PreIDæˆåŠŸ: %s (å‰%då­—èŠ‚)", preID, n)
			}
			seeker.Seek(0, io.SeekStart) // é‡ç½®åˆ°æ–‡ä»¶å¼€å¤´ä¾›åç»­ä½¿ç”¨
		} else {
			// è¾“å…¥æµä¸æ”¯æŒSeekï¼Œå°è¯•ä»æºæ–‡ä»¶è®¡ç®—PreID
			fs.Debugf(o, "è¾“å…¥æµä¸æ”¯æŒSeekæ“ä½œï¼Œå°è¯•ä»æºæ–‡ä»¶è®¡ç®—PreID")
			if preIDFromSrc, err := f.calculatePreIDFromSource(ctx, src); err == nil && preIDFromSrc != "" {
				preID = preIDFromSrc
				fs.Debugf(o, "ä»æºæ–‡ä»¶è®¡ç®—PreIDæˆåŠŸ: %s", preID)
			} else {
				// åœ¨è·¨äº‘ä¼ è¾“æ—¶ï¼Œæºæ–‡ä»¶å…ƒæ•°æ®è¯»å–å¤±è´¥æ˜¯æ­£å¸¸æƒ…å†µ
				if strings.Contains(err.Error(), "object not found") || strings.Contains(err.Error(), "failed to read metadata") {
					fs.Debugf(o, "è·¨äº‘ä¼ è¾“æ—¶æºæ–‡ä»¶å…ƒæ•°æ®ä¸å¯ç”¨ï¼ŒPreIDå°†ä¸ºç©º: %v", err)
				} else {
					fs.Debugf(o, "ä»æºæ–‡ä»¶è®¡ç®—PreIDå¤±è´¥: %v", err)
				}
			}
		}
	}

	// 3. Call OpenAPI initUpload with SHA1 and PreID
	ui, err = f.initUploadOpenAPI(ctx, size, leaf, dirID, hashStr, preID, "", "")
	if err != nil {
		return false, nil, newIn, cleanup, fmt.Errorf("OpenAPI initUpload for hash check failed: %w", err)
	}

	// 3. Handle response status
	signKey, signVal := "", ""
	for {
		status := ui.GetStatus()
		switch status {
		case 2: // ç§’ä¼  success!
			fs.Infof(o, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å·²å­˜åœ¨æœåŠ¡å™¨ï¼Œæ— éœ€é‡å¤ä¸Šä¼ ")
			fs.Debugf(o, "âœ… ç§’ä¼ å®Œæˆ")
			// Mark accounting as server-side copy
			reader, _ := accounting.UnWrap(newIn)
			if acc, ok := reader.(*accounting.Account); ok && acc != nil {
				acc.ServerSideTransferStart() // Mark start
				acc.ServerSideCopyEnd(size)   // Mark end immediately
			}
			// Update object metadata from response (FileID is important)
			o.id = ui.GetFileID()
			o.pickCode = ui.GetPickCode() // Get pick code too
			o.hasMetaData = true          // Mark as having basic metadata

			// Mark complete on RereadableObject if applicable
			if ro, ok := newIn.(*RereadableObject); ok {
				ro.MarkComplete(ctx)
				fs.Debugf(o, "Marked RereadableObject transfer as complete after hash upload")
			}

			// Optionally, call getFile to get full metadata, but might be slow/costly
			// info, getErr := f.getFile(ctx, o.id, "")
			// if getErr == nil { o.setMetaData(info) }
			return true, ui, newIn, cleanup, nil // Found = true

		case 1: // Non-ç§’ä¼ , need actual upload
			fs.Infof(o, "âš¡ ç§’ä¼ ä¸å¯ç”¨ï¼Œå¼€å§‹æ­£å¸¸ä¸Šä¼ ")
			return false, ui, newIn, cleanup, nil // Found = false

		case 7: // Need secondary auth (sign_check)
			fs.Debugf(o, "Hash upload requires secondary auth (status 7). Calculating range SHA1...")
			signKey = ui.GetSignKey()
			signCheckRange := ui.GetSignCheck()
			if signKey == "" || signCheckRange == "" {
				return false, nil, newIn, cleanup, errors.New("hash upload status 7 but sign_key or sign_check missing")
			}
			// Calculate SHA1 for the specified range
			signVal, err = calcBlockSHA1(ctx, newIn, src, signCheckRange)
			if err != nil {
				return false, nil, newIn, cleanup, fmt.Errorf("failed to calculate SHA1 for range %q: %w", signCheckRange, err)
			}
			fs.Debugf(o, "Calculated range SHA1: %s for range %s", signVal, signCheckRange)

			// Retry initUpload with sign_key and sign_val with exponential backoff for network errors
			var retryErr error
			// Define retry parameters
			maxRetries := 12
			initialDelay := 1 * time.Second
			maxDelay := 60 * time.Second
			maxElapsedTime := 10 * time.Minute

			// Define the operation to be retried
			initUploadOperation := func() error {
				var initErr error
				ui, initErr = f.initUploadOpenAPI(ctx, size, leaf, dirID, hashStr, "", signKey, signVal)
				return initErr
			}

			// Execute with exponential backoff
			retryErr = retryWithExponentialBackoff(
				ctx,
				"OpenAPI initUpload with signature",
				o,
				initUploadOperation,
				maxRetries,
				initialDelay,
				maxDelay,
				maxElapsedTime,
			)

			if retryErr != nil {
				return false, nil, newIn, cleanup, fmt.Errorf("OpenAPI initUpload retry with signature failed after multiple attempts: %w", retryErr)
			}
			continue // Re-evaluate the new status

		case 6: // Auth-related status, treat as failure
			fs.Errorf(o, "âŒ å“ˆå¸Œä¸Šä¼ å¤±è´¥ï¼Œè®¤è¯çŠ¶æ€ %dã€‚æ¶ˆæ¯: %s", status, ui.ErrMsg())
			return false, nil, newIn, cleanup, fmt.Errorf("hash upload failed with status %d: %s", status, ui.ErrMsg())

		case 8: // çŠ¶æ€8ï¼šç‰¹æ®Šè®¤è¯çŠ¶æ€ï¼Œä½†å¯ä»¥ç»§ç»­ä¸Šä¼ 
			fs.Debugf(o, "Hash upload returned status 8 (special auth), continuing with upload. Message: %s", ui.ErrMsg())
			// çŠ¶æ€8ä¸æ˜¯é”™è¯¯ï¼Œè€Œæ˜¯è¡¨ç¤ºéœ€è¦ç»§ç»­ä¸Šä¼ æµç¨‹
			// è¿”å›falseè¡¨ç¤ºæ²¡æœ‰ç§’ä¼ æˆåŠŸï¼Œä½†uiåŒ…å«äº†ç»§ç»­ä¸Šä¼ æ‰€éœ€çš„ä¿¡æ¯
			return false, ui, newIn, cleanup, nil

		default: // Unexpected status
			fs.Errorf(o, "âŒ å“ˆå¸Œä¸Šä¼ å¤±è´¥ï¼Œæ„å¤–çŠ¶æ€ %dã€‚æ¶ˆæ¯: %s", status, ui.ErrMsg())
			return false, nil, newIn, cleanup, fmt.Errorf("unexpected hash upload status %d: %s", status, ui.ErrMsg())
		}
	}
}

// calculatePreIDFromSource ä»æºæ–‡ä»¶è®¡ç®—PreIDï¼ˆå‰128KBçš„SHA1ï¼‰
func (f *Fs) calculatePreIDFromSource(ctx context.Context, src fs.ObjectInfo) (string, error) {
	const preHashSize int64 = 128 * 1024 // 128KB

	// å°è¯•æ‰“å¼€æºæ–‡ä»¶
	srcObj, ok := src.(fs.Object)
	if !ok {
		return "", fmt.Errorf("æºå¯¹è±¡ä¸æ”¯æŒè¯»å–")
	}

	reader, err := srcObj.Open(ctx)
	if err != nil {
		return "", fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer reader.Close()

	// è¯»å–å‰128KBæ•°æ®
	hashSize := min(src.Size(), preHashSize)
	preData := make([]byte, hashSize)
	n, err := io.ReadFull(reader, preData)
	if err != nil && err != io.ErrUnexpectedEOF {
		return "", fmt.Errorf("è¯»å–å‰128KBæ•°æ®å¤±è´¥: %w", err)
	}

	if n == 0 {
		return "", fmt.Errorf("æ— æ³•è¯»å–ä»»ä½•æ•°æ®")
	}

	// è®¡ç®—SHA1
	hasher := sha1.New()
	hasher.Write(preData[:n])
	// å…³é”®ä¿®å¤ï¼šPreIDéœ€è¦å¤§å†™æ ¼å¼ï¼Œä¸115ç½‘ç›˜APIä¿æŒä¸€è‡´
	preID := fmt.Sprintf("%X", hasher.Sum(nil))

	fs.Debugf(f, "ä»æºæ–‡ä»¶è®¡ç®—PreID: %s (å‰%då­—èŠ‚)", preID, n)
	return preID, nil
}

// shouldFallbackToTraditional æ£€æŸ¥é”™è¯¯æ˜¯å¦åº”è¯¥é™çº§åˆ°ä¼ ç»Ÿä¸Šä¼ 
func (f *Fs) shouldFallbackToTraditional(err error) bool {
	if err == nil {
		return false
	}

	// é¦–å…ˆæ£€æŸ¥115ç½‘ç›˜ç‰¹å®šçš„é”™è¯¯ç 
	code, _ := f.parse115ErrorCode(err)
	if code > 0 {
		return f.should115ErrorFallback(code)
	}

	// æ£€æŸ¥OSSç›¸å…³é”™è¯¯
	errStr := err.Error()

	// OSSæœåŠ¡ä¸å¯ç”¨ç›¸å…³é”™è¯¯
	if strings.Contains(errStr, "OSS service unavailable") ||
		strings.Contains(errStr, "bucket not found") ||
		strings.Contains(errStr, "invalid credentials") ||
		strings.Contains(errStr, "invalid field, OperationInput.Bucket") ||
		strings.Contains(errStr, "multipart upload not available") {
		return true
	}

	// ç½‘ç»œç›¸å…³é”™è¯¯
	if strings.Contains(errStr, "connection refused") ||
		strings.Contains(errStr, "timeout") ||
		strings.Contains(errStr, "network") {
		return true
	}

	return false
}

// parse115ErrorCode è§£æ115ç½‘ç›˜APIé”™è¯¯ç 
func (f *Fs) parse115ErrorCode(err error) (int, string) {
	if err == nil {
		return 0, ""
	}

	errStr := err.Error()

	// å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–é”™è¯¯ç 
	re := regexp.MustCompile(`code[:\s]*(\d+)`)
	matches := re.FindStringSubmatch(errStr)
	if len(matches) >= 2 {
		if code, parseErr := strconv.Atoi(matches[1]); parseErr == nil {
			return code, errStr
		}
	}

	return 0, errStr
}

// should115ErrorFallback æ ¹æ®115ç½‘ç›˜é”™è¯¯ç å†³å®šæ˜¯å¦åº”è¯¥é™çº§
func (f *Fs) should115ErrorFallback(code int) bool {
	switch code {
	// è®¤è¯ç›¸å…³é”™è¯¯ - å…è®¸é™çº§
	case 700: // ç­¾åè®¤è¯å¤±è´¥
		return true
	case 10009: // ä¸Šä¼ é¢‘ç‡é™åˆ¶
		return true
	case 10002: // SHA1ä¸åŒ¹é…
		return true

	// æœåŠ¡å™¨é”™è¯¯ - å…è®¸é™çº§
	case 500, 502, 503, 504:
		return true

	default:
		return false
	}
}

// ensureOptimalMemoryConfig ç¡®ä¿rcloneæœ‰è¶³å¤Ÿçš„å†…å­˜é…ç½®æ¥å¤„ç†å¤§æ–‡ä»¶ä¸Šä¼ 
// VPSå‹å¥½ï¼šå°Šé‡ç”¨æˆ·çš„å†…å­˜é™åˆ¶è®¾ç½®
func (f *Fs) ensureOptimalMemoryConfig(fileSize int64) {
	ci := fs.GetConfig(context.Background())

	// VPSå‹å¥½ï¼šå¦‚æœç”¨æˆ·å·²ç»è®¾ç½®äº†å†…å­˜é™åˆ¶ï¼Œä¸è¦å¼ºåˆ¶è¦†ç›–
	currentMaxMemory := int64(ci.BufferSize)
	if currentMaxMemory > 0 {
		// ç”¨æˆ·å·²ç»è®¾ç½®äº†å†…å­˜é™åˆ¶ï¼Œå°Šé‡ç”¨æˆ·çš„VPSé…ç½®
		fs.Debugf(f, "VPSæ¨¡å¼ï¼šå°Šé‡ç”¨æˆ·å†…å­˜é™åˆ¶ %sï¼Œæ–‡ä»¶å¤§å° %s",
			fs.SizeSuffix(currentMaxMemory), fs.SizeSuffix(fileSize))
		return
	}

	// åªæœ‰åœ¨ç”¨æˆ·æ²¡æœ‰è®¾ç½®å†…å­˜é™åˆ¶æ—¶ï¼Œæ‰è¿›è¡Œè‡ªåŠ¨è°ƒæ•´
	var requiredMemory int64

	if fileSize > 0 {
		// å¯¹äºå¤§æ–‡ä»¶ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå†…å­˜å¤„ç†åˆ†ç‰‡ä¸Šä¼ 
		chunkSize := int64(f.calculateOptimalChunkSize(fileSize))
		// éœ€è¦è‡³å°‘2ä¸ªåˆ†ç‰‡çš„å†…å­˜ï¼šå½“å‰åˆ†ç‰‡ + ä¸‹ä¸€ä¸ªåˆ†ç‰‡
		requiredMemory = chunkSize * 2

		// ä½†ä¸èƒ½è¶…è¿‡åˆç†çš„é»˜è®¤å€¼
		maxDefaultMemory := int64(256 * 1024 * 1024) // 256MBé»˜è®¤ä¸Šé™
		if requiredMemory > maxDefaultMemory {
			requiredMemory = maxDefaultMemory
		}
	} else {
		// æœªçŸ¥å¤§å°æ–‡ä»¶ï¼Œä½¿ç”¨ä¿å®ˆçš„é»˜è®¤å€¼
		requiredMemory = int64(128 * 1024 * 1024) // 128MB
	}

	// è®¾ç½®åˆç†çš„é»˜è®¤å†…å­˜é™åˆ¶
	fs.Infof(f, "115ç½‘ç›˜è‡ªåŠ¨é…ç½®ï¼šæ–‡ä»¶å¤§å° %sï¼Œè®¾ç½®å†…å­˜é™åˆ¶ %s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(requiredMemory))
	ci.BufferSize = fs.SizeSuffix(requiredMemory)
}

// uploadToOSS performs the actual upload to OSS using multipart via OpenAPI info.
func (f *Fs) uploadToOSS(
	ctx context.Context,
	in io.Reader,
	src fs.ObjectInfo,
	o *Object,
	leaf, dirID string,
	size int64,
	ui *UploadInitInfo, // Pre-fetched info (e.g., from failed hash upload)
	options ...fs.OpenOption,
) (fs.Object, error) {
	fs.Debugf(o, "Starting OSS multipart upload...")

	// Initialize upload and get upload info if not provided
	uploadInfo, err := f.getUploadInfo(ctx, ui, leaf, dirID, size, o, in, src)
	if err != nil {
		return nil, err
	}

	// ä¿®å¤ç©ºæŒ‡é’ˆï¼šç¡®ä¿uploadInfoä¸ä¸ºç©º
	if uploadInfo == nil {
		return nil, fmt.Errorf("getUploadInfo returned nil UploadInitInfo")
	}

	// Handle case where initUpload resulted in instant upload
	if uploadInfo.GetStatus() == 2 {
		return o, nil
	}

	// Create OSS client
	ossClient, err := f.newOSSClient()
	if err != nil {
		return nil, fmt.Errorf("failed to create OSS client: %w", err)
	}

	// Configure and perform the upload
	callbackData, err := f.performOSSUpload(ctx, ossClient, in, src, o, leaf, dirID, size, uploadInfo, options...)
	if err != nil {
		return nil, err
	}

	// Update object metadata
	if err = o.setMetaDataFromCallBack(callbackData); err != nil {
		return nil, fmt.Errorf("failed to set metadata from callback: %w", err)
	}

	// 7. Mark complete on RereadableObject if applicable
	if ro, ok := in.(*RereadableObject); ok {
		ro.MarkComplete(ctx)
		fs.Debugf(o, "Marked RereadableObject transfer as complete")
	}

	fs.Infof(o, "âœ… åˆ†ç‰‡ä¸Šä¼ å®Œæˆ: %s", fs.SizeSuffix(size))
	fs.Debugf(o, "OSS multipart upload successful.")
	return o, nil
}

// getUploadInfo gets or validates upload information
func (f *Fs) getUploadInfo(
	ctx context.Context,
	ui *UploadInitInfo,
	leaf, dirID string,
	size int64,
	o *Object,
	in io.Reader,
	src fs.ObjectInfo,
) (*UploadInitInfo, error) {
	// If upload info is already provided, use it
	if ui != nil {
		return ui, nil
	}

	// ä¿®å¤OSS multipartä¸Šä¼ ï¼šéœ€è¦è®¡ç®—SHA1ç”¨äºAPIè°ƒç”¨
	// æ ¹æ®115ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£ï¼Œfileidï¼ˆSHA1ï¼‰æ˜¯å¿…éœ€å‚æ•°
	fs.Debugf(o, "OSS multipartä¸Šä¼ éœ€è¦è®¡ç®—SHA1...")

	// è·å–æ–‡ä»¶çš„SHA1å“ˆå¸Œ
	var sha1sum string
	if o != nil {
		if hash, err := o.Hash(ctx, hash.SHA1); err == nil && hash != "" {
			sha1sum = strings.ToUpper(hash)
			fs.Debugf(o, "ä½¿ç”¨å·²æœ‰SHA1: %s", sha1sum)
		}
	}

	// å…³é”®ä¿®å¤ï¼šå¦‚æœæ²¡æœ‰SHA1ï¼Œå¿…é¡»è¿”å›é”™è¯¯ï¼Œå› ä¸º115ç½‘ç›˜APIè¦æ±‚fileidå‚æ•°
	if sha1sum == "" {
		return nil, fmt.Errorf("OSS multipart upload requires SHA1 hash (fileid parameter) - this should be calculated by tryHashUpload first")
	}

	// Initialize upload with SHA1 (if available)
	ui, err := f.initUploadOpenAPI(ctx, size, leaf, dirID, sha1sum, "", "", "")
	if err != nil {
		return nil, fmt.Errorf("failed to initialize multipart upload: %w", err)
	}

	// ä¿®å¤ç©ºæŒ‡é’ˆï¼šç¡®ä¿uiä¸ä¸ºç©º
	if ui == nil {
		return nil, fmt.Errorf("initUploadOpenAPI returned nil UploadInitInfo")
	}

	// Handle response status with authentication loop
	signKey, signVal := "", ""
	for {
		status := ui.GetStatus()
		switch status {
		case 2: // ç§’ä¼  success!
			fs.Infof(o, "ğŸ‰ Multipart upload: ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å·²å­˜åœ¨æœåŠ¡å™¨")
			o.id = ui.GetFileID()
			o.pickCode = ui.GetPickCode()
			o.hasMetaData = true
			return ui, nil

		case 1: // Non-ç§’ä¼ , need actual upload
			fs.Debugf(o, "ğŸ“¤ Multipart upload: ç§’ä¼ ä¸å¯ç”¨ï¼Œç»§ç»­OSSä¸Šä¼ ")
			// Check if we have valid OSS credentials
			if ui.GetBucket() == "" || ui.GetObject() == "" {
				return nil, fmt.Errorf("status 1 with empty bucket/object, multipart upload not available")
			}
			return ui, nil

		case 7: // Need secondary auth (sign_check)
			fs.Debugf(o, "ğŸ” åˆ†ç‰‡ä¸Šä¼ éœ€è¦äºŒæ¬¡è®¤è¯ (çŠ¶æ€ 7)ã€‚è®¡ç®—èŒƒå›´SHA1...")
			signKey = ui.GetSignKey()
			signCheckRange := ui.GetSignCheck()
			if signKey == "" || signCheckRange == "" {
				return nil, errors.New("multipart upload status 7 but sign_key or sign_check missing")
			}
			// Calculate SHA1 for the specified range
			var err error
			signVal, err = calcBlockSHA1(ctx, in, src, signCheckRange)
			if err != nil {
				return nil, fmt.Errorf("failed to calculate SHA1 for range %q: %w", signCheckRange, err)
			}
			fs.Debugf(o, "âœ… è®¡ç®—èŒƒå›´SHA1: %s ç”¨äºèŒƒå›´ %s", signVal, signCheckRange)

			// Retry initUpload with sign_key and sign_val
			ui, err = f.initUploadOpenAPI(ctx, size, leaf, dirID, sha1sum, "", signKey, signVal)
			if err != nil {
				return nil, fmt.Errorf("multipart upload retry with signature failed: %w", err)
			}
			continue // Re-evaluate the new status

		case 8: // çŠ¶æ€8ï¼šç‰¹æ®Šè®¤è¯çŠ¶æ€ï¼Œä½†å¯ä»¥ç»§ç»­ä¸Šä¼ 
			fs.Debugf(o, "ğŸ” åˆ†ç‰‡ä¸Šä¼ è¿”å›çŠ¶æ€ 8 (ç‰¹æ®Šè®¤è¯)ï¼Œæ£€æŸ¥OSSå‡­æ®")
			// Check if we have valid OSS credentials
			if ui.GetBucket() == "" || ui.GetObject() == "" {
				return nil, fmt.Errorf("status 8 with empty bucket/object, multipart upload not available")
			}
			return ui, nil

		case 6: // Auth-related status, treat as failure
			fs.Errorf(o, "âŒ åˆ†ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œè®¤è¯çŠ¶æ€ %dã€‚æ¶ˆæ¯: %s", status, ui.ErrMsg())
			return nil, fmt.Errorf("multipart upload failed with status %d: %s", status, ui.ErrMsg())

		default: // Unexpected status
			fs.Errorf(o, "âŒ åˆ†ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œæ„å¤–çŠ¶æ€ %dã€‚æ¶ˆæ¯: %s", status, ui.ErrMsg())
			return nil, fmt.Errorf("unexpected multipart upload status %d: %s", status, ui.ErrMsg())
		}
	}
}

// performTraditionalUpload handles upload when OSS is not available (status 7/8 with empty bucket/object)
func (f *Fs) performTraditionalUpload(
	ctx context.Context,
	in io.Reader,
	o *Object,
	leaf string,
	dirID string,
	size int64,
	ui *UploadInitInfo,
	options ...fs.OpenOption,
) (fs.Object, error) {
	fs.Debugf(o, "ğŸ“¤ ç”±äºçŠ¶æ€ %d ä¸”bucket/objectä¸ºç©ºï¼Œæ‰§è¡Œä¼ ç»Ÿä¸Šä¼ ", ui.GetStatus())

	// Use the existing traditional sample upload method
	// This will handle the complete traditional upload flow
	return f.doSampleUpload(ctx, in, o, leaf, dirID, size, options...)
}

// performOSSUpload handles the actual upload process
func (f *Fs) performOSSUpload(
	ctx context.Context,
	ossClient *oss.Client,
	in io.Reader,
	src fs.ObjectInfo,
	o *Object,
	leaf, dirID string,
	size int64,
	ui *UploadInitInfo,
	options ...fs.OpenOption,
) (*CallbackData, error) {
	// ğŸ¯ æ™ºèƒ½é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šä½¿ç”¨ç”¨æˆ·é…ç½®çš„åˆ‡æ¢é˜ˆå€¼
	uploadCutoff := int64(f.opt.UploadCutoff) // ç”¨æˆ·é…ç½®çš„ä¸Šä¼ åˆ‡æ¢é˜ˆå€¼

	// ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ†ç‰‡å¤§å°è®¡ç®—å‡½æ•°ï¼Œç¡®ä¿ä¸åˆ†ç‰‡ä¸Šä¼ ä¸€è‡´
	optimalPartSize := int64(f.calculateOptimalChunkSize(size))
	fs.Debugf(o, "ğŸ¯ æ™ºèƒ½åˆ†ç‰‡å¤§å°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, æœ€ä¼˜åˆ†ç‰‡å¤§å°=%s",
		fs.SizeSuffix(size), fs.SizeSuffix(optimalPartSize))

	// å‚è€ƒOpenListï¼šæç®€è¿›åº¦å›è°ƒï¼Œæœ€å¤§åŒ–å‡å°‘å¼€é”€
	var lastLoggedPercent int
	var lastLogTime time.Time

	// Get Account object for progress integration
	var currentAccount *accounting.Account
	if unwrappedIn, acc := accounting.UnWrapAccounting(in); acc != nil {
		currentAccount = acc
		_ = unwrappedIn // Avoid unused variable warning
	}

	// å‚è€ƒé˜¿é‡Œäº‘OSSç¤ºä¾‹ï¼šåˆ›å»º115ç½‘ç›˜ä¸“ç”¨çš„ä¸Šä¼ ç®¡ç†å™¨é…ç½®
	uploaderConfig := &Upload115Config{
		PartSize:    optimalPartSize, // ä½¿ç”¨æ™ºèƒ½è®¡ç®—çš„åˆ†ç‰‡å¤§å°
		ParallelNum: 1,               // 115ç½‘ç›˜å¼ºåˆ¶å•çº¿ç¨‹ä¸Šä¼ 
		ProgressFn: func(increment, transferred, total int64) {
			// ä¿®å¤120%è¿›åº¦é—®é¢˜ï¼šç§»é™¤é‡å¤çš„Accountæ›´æ–°
			// AccountedFileReaderå·²ç»é€šè¿‡Read()æ–¹æ³•è‡ªåŠ¨æ›´æ–°Accountè¿›åº¦
			// è¿™é‡Œå†æ¬¡æ›´æ–°ä¼šå¯¼è‡´é‡å¤è®¡æ•°ï¼Œé€ æˆè¿›åº¦è¶…è¿‡100%

			if total > 0 {
				currentPercent := int(float64(transferred) / float64(total) * 100)
				now := time.Now()

				// å‡å°‘æ—¥å¿—é¢‘ç‡ï¼Œé¿å…åˆ·å±
				if (currentPercent >= lastLoggedPercent+10 || transferred == total) &&
					(now.Sub(lastLogTime) > 5*time.Second || transferred == total) {
					fs.Infof(o, "ğŸ“Š 115åˆ†ç‰‡ä¸Šä¼ : %d%% (%s/%s)",
						currentPercent, fs.SizeSuffix(transferred), fs.SizeSuffix(total))
					lastLoggedPercent = currentPercent
					lastLogTime = now
				}
			}

			// ä¿®å¤ï¼šåªä¿ç•™è°ƒè¯•æ—¥å¿—ï¼Œä¸é‡å¤æ›´æ–°Accountå¯¹è±¡
			// å‡å°‘è°ƒè¯•æ—¥å¿—é¢‘ç‡ï¼Œé¿å…åˆ·å±
			if currentAccount != nil && increment > 0 && transferred == total {
				fs.Debugf(o, " ProgressFnå®Œæˆ: increment=%s, transferred=%s, total=%s",
					fs.SizeSuffix(increment), fs.SizeSuffix(transferred), fs.SizeSuffix(total))
			}
		},
	}

	if size >= 0 && size < uploadCutoff {
		// ğŸ“¤ å°æ–‡ä»¶OSSå•æ–‡ä»¶ä¸Šä¼ ç­–ç•¥
		fs.Infof(o, "ğŸ“¤ 115ç½‘ç›˜OSSå•æ–‡ä»¶ä¸Šä¼ : %s (%s)", leaf, fs.SizeSuffix(size))
		return f.performOSSPutObject(ctx, ossClient, in, src, o, leaf, dirID, size, ui, uploaderConfig, options...)
	} else {
		// Large file: use OSS multipart upload
		fs.Debugf(o, "115 OSS multipart upload: %s (%s)", leaf, fs.SizeSuffix(size))
		return f.performOSSMultipart(ctx, ossClient, in, src, o, leaf, dirID, size, ui, options...)
	}
}

// Upload115Config 115ç½‘ç›˜ä¸Šä¼ ç®¡ç†å™¨é…ç½®
// å‚è€ƒé˜¿é‡Œäº‘OSS UploadFileç¤ºä¾‹çš„é…ç½®æ€æƒ³
type Upload115Config struct {
	PartSize    int64                                     // åˆ†ç‰‡å¤§å°
	ParallelNum int                                       // å¹¶è¡Œæ•°ï¼ˆ115ç½‘ç›˜å›ºå®šä¸º1ï¼‰
	ProgressFn  func(increment, transferred, total int64) // è¿›åº¦å›è°ƒå‡½æ•°
}

// calculateOptimalPartSize function removed, unified to use calculateOptimalChunkSize

// performOSSPutObject æ‰§è¡ŒOSSå•æ–‡ä»¶ä¸Šä¼ 
func (f *Fs) performOSSPutObject(
	ctx context.Context,
	ossClient *oss.Client,
	in io.Reader,
	_ fs.ObjectInfo,
	o *Object,
	_, _ string,
	size int64,
	ui *UploadInitInfo,
	config *Upload115Config,
	options ...fs.OpenOption,
) (*CallbackData, error) {
	// å‚è€ƒé˜¿é‡Œäº‘OSS UploadFileç¤ºä¾‹ï¼šå‡†å¤‡PutObjectè¯·æ±‚
	req := &oss.PutObjectRequest{
		Bucket:      oss.Ptr(ui.GetBucket()),
		Key:         oss.Ptr(ui.GetObject()),
		Body:        in,
		Callback:    oss.Ptr(ui.GetCallback()),
		CallbackVar: oss.Ptr(ui.GetCallbackVar()),
		// ä½¿ç”¨é…ç½®ä¸­çš„è¿›åº¦å›è°ƒå‡½æ•°ï¼Œå‚è€ƒé˜¿é‡Œäº‘OSS UploadFileç¤ºä¾‹
		ProgressFn: config.ProgressFn,
	}

	// Apply upload options
	for _, option := range options {
		key, value := option.Header()
		lowerKey := strings.ToLower(key)
		switch lowerKey {
		case "":
			// ignore
		case "cache-control":
			req.CacheControl = oss.Ptr(value)
		case "content-disposition":
			req.ContentDisposition = oss.Ptr(value)
		case "content-encoding":
			req.ContentEncoding = oss.Ptr(value)
		case "content-type":
			req.ContentType = oss.Ptr(value)
		}
	}

	// ğŸš€ OSSå•æ–‡ä»¶ä¸Šä¼ ï¼šä½¿ç”¨å®Œæ•´é‡è¯•æœºåˆ¶ç¡®ä¿ç¨³å®šæ€§
	fs.Debugf(f, "ğŸš€ OSSå•æ–‡ä»¶ä¸Šä¼ å¼€å§‹: bucket=%q, object=%q", ui.GetBucket(), ui.GetObject())
	var putRes *oss.PutObjectResult
	err := f.pacer.Call(func() (bool, error) {
		var putErr error
		putRes, putErr = ossClient.PutObject(ctx, req)

		if putErr != nil {
			retry, retryErr := shouldRetry(ctx, nil, putErr)
			if retry {
				// ğŸ”„ é‡ç½®æµä½ç½®å‡†å¤‡é‡è¯•
				if seeker, ok := in.(io.Seeker); ok {
					if _, seekErr := seeker.Seek(0, io.SeekStart); seekErr != nil {
						return false, fmt.Errorf("ğŸš« æ— æ³•é‡ç½®æµè¿›è¡Œé‡è¯•: %w", seekErr)
					}
				} else {
					return false, fmt.Errorf("ğŸš« ä¸å¯é‡ç½®æµæ— æ³•é‡è¯•: %w", putErr)
				}
				fs.Debugf(f, "ğŸ”„ OSSå•æ–‡ä»¶ä¸Šä¼ é‡è¯•: %v", putErr)
				return true, retryErr
			}
			return false, putErr
		}
		return false, nil
	})

	if err != nil {
		fs.Errorf(f, "âŒ OSSå•æ–‡ä»¶ä¸Šä¼ å¤±è´¥: %v", err)
		return nil, fmt.Errorf("âŒ OSSå•æ–‡ä»¶ä¸Šä¼ å¤±è´¥: %w", err)
	}

	// Process callback
	callbackData, err := f.postUpload(putRes.CallbackResult)
	if err != nil {
		return nil, fmt.Errorf("failed to process PutObject callback: %w", err)
	}

	// Mark complete on RereadableObject if applicable
	if ro, ok := in.(*RereadableObject); ok {
		ro.MarkComplete(ctx)
		fs.Debugf(o, "Marked RereadableObject transfer as complete after PutObject upload")
	}

	fs.Infof(o, "âœ… å•æ–‡ä»¶ä¸Šä¼ å®Œæˆ: %s", fs.SizeSuffix(size))
	return callbackData, nil
}

// performOSSMultipart æ‰§è¡ŒOSSåˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) performOSSMultipart(
	ctx context.Context,
	ossClient *oss.Client,
	in io.Reader,
	src fs.ObjectInfo,
	o *Object,
	_, _ string,
	_ int64,
	ui *UploadInitInfo,
	options ...fs.OpenOption,
) (*CallbackData, error) {
	// Use configuration for OSS multipart upload

	// å…³é”®ä¿®å¤ï¼šåˆ†ç‰‡ä¸Šä¼ ä¹Ÿä½¿ç”¨ä¼˜åŒ–åçš„OSSå®¢æˆ·ç«¯
	// Create the chunk writer with optimized OSS client
	chunkWriter, err := f.newChunkWriterWithClient(ctx, src, ui, in, o, ossClient, options...)
	if err != nil {
		return nil, fmt.Errorf("failed to create chunk writer: %w", err)
	}

	// TODO: å°†config.ProgressFné›†æˆåˆ°chunkWriterä¸­

	// Perform the upload
	if err := chunkWriter.Upload(ctx); err != nil {
		return nil, fmt.Errorf("OSS multipart upload failed: %w", err)
	}

	// Process upload callback
	callbackData, err := f.postUpload(chunkWriter.callbackRes)
	if err != nil {
		return nil, fmt.Errorf("failed to process OSS upload callback: %w", err)
	}

	// Mark complete on RereadableObject if applicable
	if ro, ok := in.(*RereadableObject); ok {
		ro.MarkComplete(ctx)
		fs.Debugf(o, "Marked RereadableObject transfer as complete")
	}

	return callbackData, nil
}

// doSampleUpload performs the traditional streamed upload.
func (f *Fs) doSampleUpload(
	ctx context.Context,
	in io.Reader,
	o *Object,
	leaf, dirID string,
	size int64,
	options ...fs.OpenOption,
) (fs.Object, error) {
	fs.Debugf(o, "Starting traditional sample upload for size=%d", size)
	// 1. Initialize sample upload (traditional API)
	initResp, err := f.sampleInitUpload(ctx, size, leaf, dirID)
	if err != nil {
		return nil, fmt.Errorf("traditional sampleInitUpload failed: %w", err)
	}

	// 2. Perform the form upload
	callbackData, err := f.sampleUploadForm(ctx, in, initResp, leaf, options...)
	if err != nil {
		return nil, fmt.Errorf("traditional sampleUploadForm failed: %w", err)
	}

	// 3. Update object metadata from callback
	err = o.setMetaDataFromCallBack(callbackData)
	if err != nil {
		return nil, fmt.Errorf("failed to set metadata from sample upload callback: %w", err)
	}

	// 4. Mark complete on RereadableObject if applicable
	if ro, ok := in.(*RereadableObject); ok {
		ro.MarkComplete(ctx)
		fs.Debugf(o, "Marked RereadableObject transfer as complete after sample upload")
	}

	fs.Infof(o, "âœ… ä¼ ç»Ÿä¸Šä¼ å®Œæˆ: %s", fs.SizeSuffix(size))
	fs.Debugf(o, "Traditional sample upload successful.")
	return o, nil
}

// isRemoteSource æ£€æŸ¥æºå¯¹è±¡æ˜¯å¦æ¥è‡ªè¿œç¨‹äº‘ç›˜ï¼ˆéæœ¬åœ°æ–‡ä»¶ï¼‰
func (f *Fs) isRemoteSource(src fs.ObjectInfo) bool {
	// æ£€æŸ¥æºå¯¹è±¡çš„ç±»å‹ï¼Œå¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œåˆ™è®¤ä¸ºæ˜¯è¿œç¨‹æº
	srcFs := src.Fs()
	if srcFs == nil {
		fs.Debugf(f, " isRemoteSource: srcFsä¸ºnilï¼Œè¿”å›false")
		return false
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	fsType := srcFs.Name()
	isRemote := fsType != "local" && fsType != ""

	fs.Debugf(f, " isRemoteSourceæ£€æµ‹: fsType='%s', isRemote=%v", fsType, isRemote)

	// ç‰¹åˆ«æ£€æµ‹123ç½‘ç›˜å’Œå…¶ä»–äº‘ç›˜
	if strings.Contains(fsType, "123") || strings.Contains(fsType, "pan") {
		fs.Debugf(f, " æ˜ç¡®è¯†åˆ«ä¸ºäº‘ç›˜æº: %s", fsType)
		return true
	}

	return isRemote
}

// ğŸ”§ ç§»é™¤éæ ‡å‡†çš„å¯å‘å¼æ–‡ä»¶åˆ¤æ–­å‡½æ•°
// éµå¾ªGoogle Driveã€OneDriveç­‰ä¸»æµç½‘ç›˜çš„åšæ³•ï¼šå®Œå…¨ä¿¡ä»»æœåŠ¡å™¨è¿”å›çš„ç±»å‹æ ‡è¯†

// checkExistingTempFile æ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´çš„ä¸´æ—¶ä¸‹è½½æ–‡ä»¶ï¼Œé¿å…é‡å¤ä¸‹è½½
func (f *Fs) checkExistingTempFile(src fs.ObjectInfo) (io.Reader, int64, func()) {
	// ä¿®å¤é‡å¤ä¸‹è½½ï¼šæ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„ä¸´æ—¶æ–‡ä»¶
	// è¿™é‡Œå¯ä»¥åŸºäºæ–‡ä»¶åã€å¤§å°ã€ä¿®æ”¹æ—¶é—´ç­‰ç”Ÿæˆä¸´æ—¶æ–‡ä»¶è·¯å¾„
	expectedSize := src.Size()

	// ç”Ÿæˆå¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆåŸºäºæ–‡ä»¶åå’Œå¤§å°ï¼‰
	tempPattern := fmt.Sprintf("*%s*%d*.tmp", src.Remote(), expectedSize)
	tempDir := os.TempDir()

	matches, err := filepath.Glob(filepath.Join(tempDir, tempPattern))
	if err != nil || len(matches) == 0 {
		return nil, 0, nil
	}

	// æ£€æŸ¥æœ€æ–°çš„åŒ¹é…æ–‡ä»¶
	for _, tempPath := range matches {
		stat, err := os.Stat(tempPath)
		if err != nil {
			continue
		}

		// æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åŒ¹é…
		if stat.Size() == expectedSize {
			// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯æœ€è¿‘åˆ›å»ºçš„ï¼ˆ1å°æ—¶å†…ï¼‰
			if time.Since(stat.ModTime()) < time.Hour {
				file, err := os.Open(tempPath)
				if err != nil {
					continue
				}

				fs.Debugf(f, "Found matching temp file: %s", tempPath)

				cleanup := func() {
					file.Close()
					// å¯é€‰ï¼šä½¿ç”¨ååˆ é™¤ä¸´æ—¶æ–‡ä»¶
					// os.Remove(tempPath)
				}

				return file, stat.Size(), cleanup
			}
		}
	}

	return nil, 0, nil
}

// upload is the main entry point that decides which upload strategy to use.
func (f *Fs) upload(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	if f.isShare {
		return nil, errors.New("upload unsupported for shared filesystem")
	}

	// å…³é”®ä¿®å¤ï¼šåœ¨ä¸Šä¼ å¼€å§‹æ—¶æ£€æŸ¥å’Œä¼˜åŒ–å†…å­˜é…ç½®
	f.ensureOptimalMemoryConfig(src.Size())

	// è®¾ç½®ä¸Šä¼ æ ‡å¿—ï¼Œé˜²æ­¢é¢„çƒ­å¹²æ‰°ä¸Šä¼ 
	f.uploadingMu.Lock()
	f.isUploading = true
	f.uploadingMu.Unlock()

	defer func() {
		f.uploadingMu.Lock()
		f.isUploading = false
		f.uploadingMu.Unlock()
	}()

	// Start the token renewer if we have a valid one
	if f.tokenRenewer != nil {
		f.tokenRenewer.Start()
		defer f.tokenRenewer.Stop()
	}

	size := src.Size()

	// If NoBuffer option is enabled, try to wrap the input in a RereadableObject early
	if f.opt.NoBuffer && size >= 0 {
		fs.Debugf(src, "Using no_buffer option: file will be read multiple times instead of buffered to disk")
		if _, ok := in.(*RereadableObject); !ok {
			// Create a rereadable wrapper if not already wrapped
			ro, roErr := NewRereadableObject(ctx, src, options...)
			if roErr != nil {
				// Log but continue with original reader
				fs.Logf(src, "Warning: Failed to create rereadable source: %v - will attempt to continue with original reader", roErr)
			} else {
				in = ro
			}
		}
	}

	// Check size limits
	if size > int64(maxUploadSize) {
		return nil, fmt.Errorf("file size %v exceeds upload limit %v", fs.SizeSuffix(size), maxUploadSize)
	}
	if size < 0 {
		// Streaming upload with unknown size - check if allowed
		if f.opt.OnlyStream {
			fs.Logf(src, "Streaming upload with unknown size using traditional sample upload (limit %v)", StreamUploadLimit)
			// Proceed with sample upload, it might fail if size > limit
		} else if f.opt.FastUpload {
			fs.Logf(src, "Streaming upload with unknown size using traditional sample upload (limit %v) due to fast_upload", StreamUploadLimit)
			// Proceed with sample upload
		} else {
			// Default behavior: Use OSS multipart for unknown size
			fs.Logf(src, "Streaming upload with unknown size using OSS multipart upload")
			// Proceed with OSS multipart
		}
		// Note: Hash upload is not possible for unknown size.
	}

	// Create placeholder object and ensure parent directory exists
	o, leaf, dirID, err := f.createObject(ctx, remote, src.ModTime(ctx), size)
	if err != nil {
		return nil, fmt.Errorf("failed to create object placeholder: %w", err)
	}

	// Defer cleanup for any temporary buffers created
	var cleanup func()
	defer func() {
		if cleanup != nil {
			cleanup()
		}
	}()

	skipHashUpload := false
	// ğŸŒ è·¨äº‘ç›˜ä¼ è¾“æ£€æµ‹ï¼šä¼˜å…ˆå°è¯•ç§’ä¼ ï¼Œå¿½ç•¥å¤§å°é™åˆ¶
	if f.isRemoteSource(src) && size >= 0 {
		fs.Infof(o, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ç›˜ä¼ è¾“ï¼Œå¼ºåˆ¶å°è¯•ç§’ä¼ ...")
		gotIt, _, newIn, localCleanup, err := f.tryHashUpload(ctx, in, src, o, leaf, dirID, size, options...)
		cleanup = localCleanup // è®¾ç½®æ¸…ç†å‡½æ•°
		if err != nil {
			fs.Logf(o, "ğŸŒ è·¨äº‘ç›˜ç§’ä¼ å¤±è´¥ï¼Œå›é€€åˆ°æ­£å¸¸ä¸Šä¼ : %v", err)
			// Set flag to skip subsequent instant upload attempts
			skipHashUpload = true
			// Reset state, continue normal upload process
			gotIt = false
			if !f.opt.NoBuffer {
				newIn = in // æ¢å¤åŸå§‹è¾“å…¥
				if cleanup != nil {
					cleanup()
					cleanup = nil
				}
			}
		} else if gotIt {
			fs.Infof(o, "ğŸš€ è·¨äº‘ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å·²å­˜åœ¨äº115æœåŠ¡å™¨")
			return o, nil
		} else {
			// Set flag to skip subsequent instant upload attempts
			skipHashUpload = true
			fs.Infof(o, "â˜ï¸ è·¨äº‘ä¼ è¾“ç›´æ¥è¿›è¡ŒOSSä¸Šä¼ ï¼Œé¿å…é‡å¤å¤„ç†")
			// Continue using newIn for subsequent upload
			in = newIn
		}
	}

	// 1. OnlyStream flag
	if f.opt.OnlyStream {
		if size < 0 || size <= int64(StreamUploadLimit) {
			return f.doSampleUpload(ctx, in, o, leaf, dirID, size, options...)
		}
		return nil, fserrors.NoRetryError(fmt.Errorf("only_stream is enabled but file size %v exceeds stream upload limit %v", fs.SizeSuffix(size), StreamUploadLimit))
	}

	// 2. FastUpload flag
	if f.opt.FastUpload {
		noHashSize := int64(f.opt.NohashSize)
		streamLimit := int64(StreamUploadLimit)

		if size >= 0 && size <= noHashSize {
			// Small file: Use sample upload directly
			return f.doSampleUpload(ctx, in, o, leaf, dirID, size, options...)
		}

		// Larger file or unknown size: Try hash upload first
		var gotIt bool
		var ui *UploadInitInfo
		var newIn io.Reader = in // Assume input doesn't change initially

		if size >= 0 { // Hash upload only possible for known size
			var localCleanup func()
			gotIt, ui, newIn, localCleanup, err = f.tryHashUpload(ctx, in, src, o, leaf, dirID, size, options...)
			cleanup = localCleanup // Assign cleanup function
			if err != nil {
				// Log hash upload error but fallback based on size
				fs.Logf(o, "FastUpload: Hash upload failed, falling back: %v", err)
				// Reset gotIt, ui, and newIn to ensure fallback happens correctly
				// with the original reader since bufferIO may have failed
				gotIt = false
				ui = nil

				// If no_buffer is not enabled, revert to original input
				if !f.opt.NoBuffer {
					newIn = in // Important: revert to original input if hash calculation failed
					if cleanup != nil {
						cleanup()     // Clean up any partial buffers
						cleanup = nil // Mark as cleaned up
					}
				} else {
					// With no_buffer, we need to explicitly reopen the RereadableObject
					// to ensure we're at the beginning of the stream for the next upload attempt
					if ro, ok := newIn.(*RereadableObject); ok {
						fs.Debugf(o, "Reopening RereadableObject after failed hash calculation")
						reopenedReader, reopenErr := ro.Open()
						if reopenErr != nil {
							return nil, fmt.Errorf("failed to reopen source after hash upload failure: %w", reopenErr)
						}
						// IMPORTANT: Actually use the returned reader instead of assuming internal state is reset
						newIn = reopenedReader
						fs.Debugf(o, "Successfully reopened RereadableObject for upload attempt")
					} else {
						// If not a RereadableObject, fall back to original input as a last resort
						fs.Logf(o, "Warning: Expected RereadableObject in no_buffer mode but got %T, reverting to original input", newIn)
						newIn = in
					}
				}
			}
			if gotIt {
				return o, nil // Hash upload successful
			}
		} else {
			fs.Debugf(o, "FastUpload: Skipping hash upload for unknown size.")
		}

		// Hash upload failed or skipped: Fallback based on size
		if size < 0 || size <= streamLimit {
			// Use sample upload if within limit or size unknown
			return f.doSampleUpload(ctx, newIn, o, leaf, dirID, size, options...)
		}
		// Size > streamLimit: Try OSS multipart, fallback to sample upload if OSS not available
		result, err := f.uploadToOSS(ctx, newIn, src, o, leaf, dirID, size, ui, options...)
		if err != nil && f.shouldFallbackToTraditional(err) {
			fs.Debugf(o, "OSS multipart failed (%v), falling back to sample upload", err)
			return f.doSampleUpload(ctx, newIn, o, leaf, dirID, size, options...)
		}
		return result, err
	}

	// 3. UploadHashOnly flag
	if f.opt.UploadHashOnly {
		if size < 0 {
			return nil, fserrors.NoRetryError(errors.New("upload_hash_only requires known file size"))
		}
		gotIt, _, _, localCleanup, err := f.tryHashUpload(ctx, in, src, o, leaf, dirID, size, options...)
		// Handle cleanup of any temporary resources
		if localCleanup != nil {
			defer localCleanup()
		}
		if err != nil {
			return nil, fmt.Errorf("upload_hash_only: hash upload attempt failed: %w", err)
		}
		if gotIt {
			return o, nil // Success
		}
		// Hash upload didn't find the file on server
		return nil, fserrors.NoRetryError(errors.New("upload_hash_only: file not found on server via hash check, skipping upload"))
	}

	// Cross-cloud transfer check is already handled above

	// ğŸ¯ é»˜è®¤ä¸Šä¼ ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨OSSä¸Šä¼ ï¼Œæ™ºèƒ½é€‰æ‹©å•æ–‡ä»¶æˆ–åˆ†ç‰‡
	uploadCutoff := int64(f.opt.UploadCutoff) // ç”¨æˆ·é…ç½®çš„OSSä¼˜åŒ–é˜ˆå€¼

	// For very small files (<1MB), use traditional upload for efficiency
	if size >= 0 && size < int64(1*fs.Mebi) {
		fs.Debugf(o, "Small file (%s) using traditional upload", fs.SizeSuffix(size))
		return f.doSampleUpload(ctx, in, o, leaf, dirID, size, options...)
	}

	// Known size >= noHashSize OR unknown size: Try hash upload first (if size known)
	var gotIt bool
	var ui *UploadInitInfo
	var newIn io.Reader = in

	if size >= 0 && !skipHashUpload {
		var localCleanup func()
		gotIt, ui, newIn, localCleanup, err = f.tryHashUpload(ctx, in, src, o, leaf, dirID, size, options...)
		cleanup = localCleanup
		if err != nil {
			fs.Logf(o, "Normal Upload: Hash upload failed, falling back to OSS/Sample: %v", err)
			// Reset gotIt, ui, and newIn for fallback
			gotIt = false
			ui = nil

			// If no_buffer is not enabled, revert to original input
			if !f.opt.NoBuffer {
				newIn = in // Important: revert to original input if hash calculation failed
				if cleanup != nil {
					cleanup()     // Clean up any partial buffers
					cleanup = nil // Mark as cleaned up
				}
			} else {
				// With no_buffer, we need to explicitly reopen the RereadableObject
				// to ensure we're at the beginning of the stream for the next upload attempt
				if ro, ok := newIn.(*RereadableObject); ok {
					fs.Debugf(o, "Reopening RereadableObject after failed hash calculation")
					reopenedReader, reopenErr := ro.Open()
					if reopenErr != nil {
						return nil, fmt.Errorf("failed to reopen source after hash upload failure: %w", reopenErr)
					}
					// IMPORTANT: Actually use the returned reader instead of assuming internal state is reset
					newIn = reopenedReader
					fs.Debugf(o, "Successfully reopened RereadableObject for upload attempt")
				} else {
					// If not a RereadableObject, fall back to original input as a last resort
					fs.Logf(o, "Warning: Expected RereadableObject in no_buffer mode but got %T, reverting to original input", newIn)
					newIn = in
				}
			}
		}
		if gotIt {
			return o, nil // Hash upload successful
		}
	} else {
		fs.Debugf(o, "Normal Upload: Skipping hash upload for unknown size.")
	}

	// Reset state if hash upload was skipped
	if skipHashUpload {
		fs.Debugf(o, "Skipping hash upload (already attempted), proceeding to OSS upload")
		gotIt = false
		ui = nil
		newIn = in
	}

	// Hash upload failed or skipped: Decide between Sample and OSS Multipart
	// Note: OpenAPI doesn't support traditional sample upload.
	// If hash upload failed, we *must* use OSS multipart upload via OpenAPI.
	// The only time sample upload is used now is with OnlyStream or FastUpload flags for small files.
	// Therefore, the default path always leads to OSS multipart here.

	// Check against UploadCutoff to decide multipart (though logic above mostly covers this)
	if size < 0 || size >= uploadCutoff {
		// Try OSS multipart upload, fallback to sample upload if OSS not available
		result, err := f.uploadToOSS(ctx, newIn, src, o, leaf, dirID, size, ui, options...)
		if err != nil && f.shouldFallbackToTraditional(err) {
			fs.Debugf(o, "OSS multipart failed (%v), falling back to sample upload", err)
			return f.doSampleUpload(ctx, newIn, o, leaf, dirID, size, options...)
		}
		return result, err
	}

	// Size is known, >= noHashSize, and < uploadCutoff
	// This case implies hash upload failed, and we should use OSS multipart (PutObject)
	// because sample upload is traditional only.
	fs.Debugf(o, "Normal Upload: Using OSS PutObject (single part) for size %v", fs.SizeSuffix(size))
	// Need a function similar to uploadToOSS but using PutObject instead of multipart.
	// Let's adapt uploadToOSS slightly or create a helper.
	// For simplicity, let uploadToOSS handle the PutObject case internally based on size < cutoff.
	// We need to modify uploadToOSS or multipart.go to handle this.
	// Let's assume uploadToOSS handles it for now. Revisit if multipart.go doesn't.
	// *** Correction: The current multipart.go doesn't handle single PutObject. ***
	// We need to implement the PutObject path here.

	// --- OSS PutObject Implementation ---
	fs.Debugf(o, "Executing OSS PutObject...")
	// 1. Get UploadInitInfo if not already available (from failed hash check)
	if ui == nil {
		// å…³é”®ä¿®å¤ï¼šOSS PutObjectä¹Ÿéœ€è¦SHA1ï¼Œå¿…é¡»å…ˆè®¡ç®—
		var sha1sum string
		if o.sha1sum != "" {
			sha1sum = o.sha1sum
		} else {
			// å°è¯•ä»æºå¯¹è±¡è·å–SHA1
			if hash, hashErr := src.Hash(ctx, hash.SHA1); hashErr == nil && hash != "" {
				sha1sum = strings.ToUpper(hash)
			} else {
				// è·¨ç½‘ç›˜å¤åˆ¶æ—¶æºå¯¹è±¡å¯èƒ½æ— æ³•æä¾›SHA1ï¼Œéœ€è¦ä»æ•°æ®æµè®¡ç®—
				fs.Debugf(o, "Source SHA1 not available, will calculate from data stream during upload")
				// æš‚æ—¶ä½¿ç”¨ç©ºSHA1ï¼Œåœ¨å®é™…ä¸Šä¼ æ—¶è®¡ç®—
				sha1sum = ""
			}
		}

		var initErr error
		ui, initErr = f.initUploadOpenAPI(ctx, size, leaf, dirID, sha1sum, "", "", "")
		if initErr != nil {
			return nil, fmt.Errorf("failed to initialize PutObject upload: %w", initErr)
		}
		// ğŸ” æ£€æŸ¥ä¸Šä¼ åˆå§‹åŒ–çŠ¶æ€å’ŒOSSä¿¡æ¯
		if ui.GetStatus() == 1 {
			// çŠ¶æ€1ï¼šéœ€è¦ä¸Šä¼ ï¼Œæ£€æŸ¥OSSä¿¡æ¯æ˜¯å¦å®Œæ•´
			if ui.GetBucket() == "" || ui.GetObject() == "" {
				fs.Debugf(o, "ğŸ”§ çŠ¶æ€1ä½†OSSä¿¡æ¯ç¼ºå¤± (bucket=%q, object=%q)ï¼Œé™çº§åˆ°ä¼ ç»Ÿä¸Šä¼ ",
					ui.GetBucket(), ui.GetObject())
				return f.performTraditionalUpload(ctx, in, o, leaf, dirID, size, ui, options...)
			}
			fs.Debugf(o, "âœ… çŠ¶æ€1ä¸”OSSä¿¡æ¯å®Œæ•´ï¼Œç»§ç»­OSSä¸Šä¼ : bucket=%q, object=%q",
				ui.GetBucket(), ui.GetObject())
		} else if ui.GetStatus() == 2 {
			// çŠ¶æ€2ï¼šæ„å¤–çš„ç§’ä¼ æˆåŠŸ
			fs.Infof(o, "ğŸ‰ åˆå§‹åŒ–æ—¶å‘ç°æ–‡ä»¶å·²å­˜åœ¨ï¼Œç§’ä¼ æˆåŠŸ")
			o.id = ui.GetFileID()
			o.pickCode = ui.GetPickCode()
			o.hasMetaData = true
			return o, nil
		} else if ui.GetStatus() == 7 {
			// çŠ¶æ€7ï¼šéœ€è¦äºŒæ¬¡è®¤è¯
			fs.Debugf(o, "OSS PutObject requires secondary auth (status 7). Calculating range SHA1...")
			signKey := ui.GetSignKey()
			signCheckRange := ui.GetSignCheck()
			if signKey == "" || signCheckRange == "" {
				return nil, errors.New("OSS PutObject status 7 but sign_key or sign_check missing")
			}

			// å…³é”®ä¿®å¤ï¼šåˆ›å»ºå¯é‡è¯»çš„å¯¹è±¡æ¥è®¡ç®—range SHA1ï¼Œé¿å…æ¶ˆè€—åŸå§‹æµ
			var rereadableObj *RereadableObject
			var signVal string

			if ro, ok := in.(*RereadableObject); ok {
				// å·²ç»æ˜¯RereadableObjectï¼Œç›´æ¥ä½¿ç”¨
				rereadableObj = ro
			} else {
				// åˆ›å»ºæ–°çš„RereadableObject
				var roErr error
				rereadableObj, roErr = NewRereadableObject(ctx, src, options...)
				if roErr != nil {
					return nil, fmt.Errorf("failed to create rereadable object for range SHA1: %w", roErr)
				}
				defer func() { _ = rereadableObj.Close() }()
			}

			// Calculate SHA1 for the specified range using rereadable object
			signVal, err = calcBlockSHA1(ctx, rereadableObj, src, signCheckRange)
			if err != nil {
				return nil, fmt.Errorf("failed to calculate SHA1 for range %q: %w", signCheckRange, err)
			}
			fs.Debugf(o, "Calculated range SHA1: %s for range %s", signVal, signCheckRange)

			// Retry initUpload with sign_key and sign_val
			ui, err = f.initUploadOpenAPI(ctx, size, leaf, dirID, sha1sum, "", signKey, signVal)
			if err != nil {
				return nil, fmt.Errorf("OSS PutObject retry with signature failed: %w", err)
			}
			// Re-check the new status
			if ui.GetStatus() == 2 {
				// æ„å¤–è§¦å‘ç§’ä¼ 
				fs.Infof(o, "ğŸ‰ äºŒæ¬¡è®¤è¯åè§¦å‘ç§’ä¼ æˆåŠŸ")
				o.id = ui.GetFileID()
				o.pickCode = ui.GetPickCode()
				o.hasMetaData = true
				return o, nil
			} else if ui.GetStatus() != 1 && ui.GetStatus() != 8 {
				return nil, fmt.Errorf("OSS PutObject auth retry failed with status %d: %s", ui.GetStatus(), ui.ErrMsg())
			}
			fs.Debugf(o, "âœ… äºŒæ¬¡è®¤è¯æˆåŠŸï¼ŒçŠ¶æ€%dï¼Œç»§ç»­OSSä¸Šä¼ ", ui.GetStatus())
		} else if ui.GetStatus() == 8 {
			// çŠ¶æ€8ï¼šç‰¹æ®Šè®¤è¯çŠ¶æ€ï¼Œæ£€æŸ¥OSSä¿¡æ¯
			fs.Debugf(o, "ğŸ”„ çŠ¶æ€8ï¼Œæ£€æŸ¥OSSä¿¡æ¯: bucket=%q, object=%q",
				ui.GetBucket(), ui.GetObject())
			if ui.GetBucket() == "" || ui.GetObject() == "" {
				fs.Debugf(o, "ğŸ”„ çŠ¶æ€8ä¸”OSSä¿¡æ¯ç¼ºå¤±ï¼Œé™çº§åˆ°ä¼ ç»Ÿä¸Šä¼ ")
				return f.performTraditionalUpload(ctx, in, o, leaf, dirID, size, ui, options...)
			}
			fs.Debugf(o, "âœ… çŠ¶æ€8ä¸”OSSä¿¡æ¯å®Œæ•´ï¼Œç»§ç»­OSSä¸Šä¼ ")
		} else {
			return nil, fmt.Errorf("âŒ OSSä¸Šä¼ åˆå§‹åŒ–çŠ¶æ€å¼‚å¸¸: çŠ¶æ€=%d, æ¶ˆæ¯=%s",
				ui.GetStatus(), ui.ErrMsg())
		}
	}

	// ğŸ¯ æ™ºèƒ½OSSç­–ç•¥ï¼šä¼˜å…ˆå°è¯•è·å–OSSä¿¡æ¯ï¼Œå¿…è¦æ—¶é™çº§
	if ui.GetBucket() == "" || ui.GetObject() == "" {
		fs.Infof(o, "ğŸ”§ OSSä¿¡æ¯ç¼ºå¤± (bucket=%q, object=%q, status=%d)ï¼Œå°è¯•è·å–å®Œæ•´OSSä¿¡æ¯",
			ui.GetBucket(), ui.GetObject(), ui.GetStatus())

		// å¯¹äºçŠ¶æ€7/8ï¼Œå…ˆå°è¯•è·å–OSSä¿¡æ¯ï¼Œå¤±è´¥åå†é™çº§
		if ui.GetStatus() == 7 || ui.GetStatus() == 8 {
			fs.Debugf(o, "ğŸ” çŠ¶æ€%dé€šå¸¸æ— OSSä¿¡æ¯ï¼Œä½†ä»å°è¯•è·å–", ui.GetStatus())
		}

		// ğŸ¯ ç­–ç•¥1ï¼šå°è¯•æ— SHA1åˆå§‹åŒ–ï¼Œå¼ºåˆ¶è·å–OSSä¿¡æ¯
		fs.Debugf(o, "ğŸ¯ ç­–ç•¥1ï¼šæ— SHA1åˆå§‹åŒ–ï¼Œå¼ºåˆ¶è·å–OSSä¿¡æ¯")
		newUI, initErr := f.initUploadOpenAPI(ctx, size, leaf, dirID, "", "", "", "")
		if initErr != nil {
			fs.Debugf(o, "ğŸ¯ ç­–ç•¥1å¤±è´¥: %v", initErr)

			// ğŸ¯ ç­–ç•¥2ï¼šå°è¯•ä½¿ç”¨æ­£ç¡®SHA1é‡æ–°åˆå§‹åŒ–
			var correctSHA1 string
			if o.sha1sum != "" {
				correctSHA1 = o.sha1sum
			} else if hash, hashErr := src.Hash(ctx, hash.SHA1); hashErr == nil && hash != "" {
				correctSHA1 = strings.ToUpper(hash)
			}

			if correctSHA1 != "" {
				fs.Debugf(o, "ğŸ¯ ç­–ç•¥2ï¼šä½¿ç”¨æ­£ç¡®SHA1é‡æ–°åˆå§‹åŒ–: %s", correctSHA1)
				newUI, initErr = f.initUploadOpenAPI(ctx, size, leaf, dirID, correctSHA1, "", "", "")
			}

			if initErr != nil {
				fs.Infof(o, "ğŸ“‹ æ— æ³•è·å–OSSä¿¡æ¯ï¼Œä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ : %v", initErr)
				return f.performTraditionalUpload(ctx, in, o, leaf, dirID, size, ui, options...)
			}
		}

		// ğŸ” è¯¦ç»†æ£€æŸ¥è·å–ç»“æœ
		fs.Infof(o, "ğŸ” è¯¦ç»†APIå“åº”åˆ†æ:")
		fs.Infof(o, "   çŠ¶æ€ç : %d", newUI.GetStatus())
		fs.Infof(o, "   Bucket: %q", newUI.GetBucket())
		fs.Infof(o, "   Object: %q", newUI.GetObject())
		fs.Infof(o, "   FileID: %q", newUI.GetFileID())
		fs.Infof(o, "   PickCode: %q", newUI.GetPickCode())
		fs.Infof(o, "   Callback: %q", newUI.GetCallback())
		fs.Infof(o, "   CallbackVar: %q", newUI.GetCallbackVar())
		fs.Infof(o, "   SignKey: %q", newUI.GetSignKey())
		fs.Infof(o, "   SignCheck: %q", newUI.GetSignCheck())
		fs.Infof(o, "   é”™è¯¯ä¿¡æ¯: %q", newUI.ErrMsg())

		if newUI.GetStatus() == 2 {
			// æ„å¤–è§¦å‘ç§’ä¼ 
			fs.Infof(o, "ğŸ‰ è·å–OSSä¿¡æ¯æ—¶è§¦å‘ç§’ä¼ æˆåŠŸ")
			o.id = newUI.GetFileID()
			o.pickCode = newUI.GetPickCode()
			o.hasMetaData = true
			return o, nil
		} else if newUI.GetBucket() != "" && newUI.GetObject() != "" {
			// æˆåŠŸè·å–OSSä¿¡æ¯
			fs.Infof(o, "âœ… OSSä¿¡æ¯è·å–æˆåŠŸ: bucket=%q, object=%q",
				newUI.GetBucket(), newUI.GetObject())
			ui = newUI // ä½¿ç”¨æ–°çš„UIä¿¡æ¯
		} else {
			// ä»ç„¶æ— æ³•è·å–OSSä¿¡æ¯ï¼Œé™çº§åˆ°ä¼ ç»Ÿä¸Šä¼ 
			fs.Infof(o, "ğŸ“‹ 115ç½‘ç›˜æ­¤æ–‡ä»¶ä¸æ”¯æŒOSSä¸Šä¼  (çŠ¶æ€%d)ï¼Œä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ ",
				newUI.GetStatus())
			return f.performTraditionalUpload(ctx, in, o, leaf, dirID, size, ui, options...)
		}
	}

	// 2. Create OSS client
	ossClient, err := f.newOSSClient()
	if err != nil {
		return nil, fmt.Errorf("failed to create OSS client for PutObject: %w", err)
	}

	// 3. Prepare PutObject request
	// å‚è€ƒé˜¿é‡Œäº‘OSSç¤ºä¾‹ï¼Œä¼˜åŒ–PutObjectè¯·æ±‚é…ç½®
	req := &oss.PutObjectRequest{
		Bucket:      oss.Ptr(ui.GetBucket()),
		Key:         oss.Ptr(ui.GetObject()),
		Body:        newIn, // Use potentially buffered reader
		Callback:    oss.Ptr(ui.GetCallback()),
		CallbackVar: oss.Ptr(ui.GetCallbackVar()),
		// ä¿®å¤è¿›åº¦å›è°ƒï¼šç¡®ä¿totalå‚æ•°æœ‰æ•ˆï¼Œé¿å…è´Ÿæ•°æˆ–é›¶å€¼å¯¼è‡´çš„å¼‚å¸¸
		ProgressFn: func() func(increment, transferred, total int64) {
			var (
				lastTransferred int64
				lastLogTime     time.Time
				lastLogPercent  int
			)
			return func(increment, transferred, total int64) {
				// ä¿®å¤120%è¿›åº¦é—®é¢˜ï¼šç§»é™¤é‡å¤çš„Accountæ›´æ–°
				// AccountedFileReaderå·²ç»é€šè¿‡Read()æ–¹æ³•è‡ªåŠ¨æ›´æ–°Accountè¿›åº¦
				// è¿™é‡Œå†æ¬¡æ›´æ–°ä¼šå¯¼è‡´é‡å¤è®¡æ•°ï¼Œé€ æˆè¿›åº¦è¶…è¿‡100%

				// åªä¿ç•™è°ƒè¯•æ—¥å¿—ï¼Œä¸æ›´æ–°Accountå¯¹è±¡
				if transferred > lastTransferred {
					// ä¿®å¤ï¼šæ£€æŸ¥totalå‚æ•°æœ‰æ•ˆæ€§ï¼Œé¿å…é™¤é›¶æˆ–è´Ÿæ•°
					var percentage int
					if total > 0 {
						percentage = int(float64(transferred) / float64(total) * 100)
					} else {
						// å¦‚æœtotalæ— æ•ˆï¼Œä½¿ç”¨æ–‡ä»¶å¤§å°ä½œä¸ºå‚è€ƒ
						if size > 0 {
							percentage = int(float64(transferred) / float64(size) * 100)
						} else {
							percentage = 0
						}
					}

					now := time.Now()
					// å‡å°‘æ—¥å¿—é¢‘ç‡ï¼šåªåœ¨è¿›åº¦å˜åŒ–è¶…è¿‡10%æˆ–æ—¶é—´é—´éš”è¶…è¿‡5ç§’æ—¶è¾“å‡ºæ—¥å¿—
					shouldLog := (percentage >= lastLogPercent+10) ||
						(now.Sub(lastLogTime) > 5*time.Second) ||
						(transferred == total) ||
						(percentage == 100)

					if shouldLog {
						fs.Debugf(o, "ğŸ“Š OSSä¸Šä¼ è¿›åº¦: %s/%s (%d%%) [æ–‡ä»¶å¤§å°: %s]",
							fs.SizeSuffix(transferred),
							fs.SizeSuffix(max(total, size)),
							percentage,
							fs.SizeSuffix(size))
						lastLogTime = now
						lastLogPercent = percentage
					}
					lastTransferred = transferred
				}
			}
		}(),
	}
	// Apply headers from options
	for _, option := range options {
		key, value := option.Header()
		lowerKey := strings.ToLower(key)
		switch lowerKey {
		case "cache-control":
			req.CacheControl = oss.Ptr(value)
		case "content-disposition":
			req.ContentDisposition = oss.Ptr(value)
		case "content-encoding":
			req.ContentEncoding = oss.Ptr(value)
		case "content-type":
			req.ContentType = oss.Ptr(value)
		}
	}

	// ğŸš€ å¼€å§‹OSSå•æ–‡ä»¶ä¸Šä¼ 
	fs.Infof(o, "ğŸš€ å¼€å§‹ä¸Šä¼ : %s (%s)",
		leaf, fs.SizeSuffix(size))

	// å¹³è¡¡æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨OSSä¸“ç”¨è°ƒé€Ÿå™¨ï¼Œé¿å…è¿‡åº¦è¯·æ±‚å¯¼è‡´é™åˆ¶
	// OSSä¸Šä¼ è™½ç„¶ç›´è¿é˜¿é‡Œäº‘ï¼Œä½†ä»éœ€è¦åˆç†çš„QPSæ§åˆ¶é¿å…è§¦å‘ååˆ¶æªæ–½

	var putRes *oss.PutObjectResult
	err = f.pacer.Call(func() (bool, error) {
		var putErr error
		putRes, putErr = ossClient.PutObject(ctx, req)
		retry, retryErr := shouldRetry(ctx, nil, putErr)
		if retry {
			// Rewind body if possible before retry
			if seeker, ok := newIn.(io.Seeker); ok {
				_, _ = seeker.Seek(0, io.SeekStart)
			} else {
				// Cannot retry non-seekable stream after partial read
				return false, fmt.Errorf("cannot retry PutObject with non-seekable stream: %w", putErr)
			}
			return true, retryErr
		}
		if putErr != nil {
			return false, putErr
		}
		return false, nil // Success
	})
	if err != nil {
		return nil, fmt.Errorf("OSS PutObject failed: %w", err)
	}

	// 5. Process callback
	callbackData, err := f.postUpload(putRes.CallbackResult)
	if err != nil {
		return nil, fmt.Errorf("failed to process PutObject callback: %w", err)
	}

	// 6. Update metadata
	err = o.setMetaDataFromCallBack(callbackData)
	if err != nil {
		return nil, fmt.Errorf("failed to set metadata from PutObject callback: %w", err)
	}

	// 7. Mark complete on RereadableObject if applicable
	if ro, ok := newIn.(*RereadableObject); ok {
		ro.MarkComplete(ctx)
		fs.Debugf(o, "âœ… PutObjectä¸Šä¼ åæ ‡è®°RereadableObjectä¼ è¾“å®Œæˆ")
	}

	fs.Infof(o, "âœ… æ–‡ä»¶ä¸Šä¼ å®Œæˆ: %s", fs.SizeSuffix(size))
	fs.Debugf(o, "âœ… OSS PutObjectæˆåŠŸ")
	return o, nil
}

// ============================================================================
// Functions from multipart.go
// ============================================================================

// 115ç½‘ç›˜åˆ†ç‰‡ä¸Šä¼ å®ç°
// é‡‡ç”¨å•çº¿ç¨‹é¡ºåºä¸Šä¼ æ¨¡å¼ï¼Œç¡®ä¿ä¸Šä¼ ç¨³å®šæ€§å’ŒSHA1éªŒè¯é€šè¿‡

const (
	// ğŸ’¾ ä¼˜åŒ–ç¼“å†²åŒºé…ç½®ï¼š512MBç¼“å†²åŒºï¼Œæå‡ä¸‹è½½æ€§èƒ½
	bufferSize           = 512 * 1024 * 1024 // 512MBç¼“å†²åŒºï¼Œæ€§èƒ½ä¼˜åŒ–
	bufferCacheFlushTime = 5 * time.Second   // ç¼“å†²åŒºæ¸…ç†æ—¶é—´
)

// bufferPool is a global pool of buffers
var (
	bufferPool     *pool.Pool
	bufferPoolOnce sync.Once
)

// get a buffer pool
func getPool() *pool.Pool {
	bufferPoolOnce.Do(func() {
		ci := fs.GetConfig(context.Background())

		// VPSå‹å¥½ï¼šæ ¹æ®ç”¨æˆ·çš„--max-buffer-memoryè®¾ç½®åŠ¨æ€è°ƒæ•´
		maxMemory := int64(ci.BufferSize)

		// è®¡ç®—åˆé€‚çš„ç¼“å†²åŒºé…ç½®
		actualBufferSize := int64(bufferSize) // é»˜è®¤256MB
		bufferCount := 4                      // é»˜è®¤4ä¸ª

		// å¦‚æœç”¨æˆ·è®¾ç½®äº†å†…å­˜é™åˆ¶ï¼Œæ™ºèƒ½è°ƒæ•´é…ç½®
		if maxMemory > 0 {
			// æ ¹æ®ç”¨æˆ·å†…å­˜é™åˆ¶è°ƒæ•´ç¼“å†²åŒºå¤§å°å’Œæ•°é‡
			if maxMemory <= 64*1024*1024 { // â‰¤64MBï¼šè¶…ä½é…VPS
				actualBufferSize = 16 * 1024 * 1024 // 16MBç¼“å†²åŒº
				bufferCount = 2                     // 2ä¸ª = 32MBæ€»å†…å­˜
			} else if maxMemory <= 128*1024*1024 { // â‰¤128MBï¼šä½é…VPS
				actualBufferSize = 32 * 1024 * 1024 // 32MBç¼“å†²åŒº
				bufferCount = 2                     // 2ä¸ª = 64MBæ€»å†…å­˜
			} else if maxMemory <= 256*1024*1024 { // â‰¤256MBï¼šä¸­é…VPS
				actualBufferSize = 64 * 1024 * 1024 // 64MBç¼“å†²åŒº
				bufferCount = 2                     // 2ä¸ª = 128MBæ€»å†…å­˜
			} else if maxMemory <= 512*1024*1024 { // â‰¤512MBï¼šé«˜é…VPS
				actualBufferSize = 128 * 1024 * 1024 // 128MBç¼“å†²åŒº
				bufferCount = 2                      // 2ä¸ª = 256MBæ€»å†…å­˜
			}
			// å¦åˆ™ä½¿ç”¨é»˜è®¤çš„256MB Ã— 4ä¸ªé…ç½®

			fs.Infof(nil, "115ç½‘ç›˜VPSä¼˜åŒ–ï¼šç”¨æˆ·å†…å­˜é™åˆ¶ %sï¼Œä½¿ç”¨ %s Ã— %d ç¼“å†²åŒº",
				fs.SizeSuffix(maxMemory), fs.SizeSuffix(actualBufferSize), bufferCount)
		}

		// åˆ›å»ºç¼“å†²åŒºæ± 
		bufferPool = pool.New(bufferCacheFlushTime, int(actualBufferSize), bufferCount, ci.UseMmap)
		totalMemory := actualBufferSize * int64(bufferCount)
		fs.Debugf(nil, "115ç½‘ç›˜ç¼“å†²åŒºæ± ï¼š%s Ã— %d = %s æ€»å†…å­˜",
			fs.SizeSuffix(actualBufferSize), bufferCount, fs.SizeSuffix(totalMemory))
	})
	return bufferPool
}

// NewRW gets a pool.RW using the multipart pool
func NewRW() *pool.RW {
	return pool.NewRW(getPool())
}

// Upload æ‰§è¡Œ115ç½‘ç›˜å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
// å‚è€ƒé˜¿é‡Œäº‘OSSç¤ºä¾‹ä¼˜åŒ–ï¼šé‡‡ç”¨é¡ºåºä¸Šä¼ æ¨¡å¼ï¼Œç¡®ä¿ä¸Šä¼ ç¨³å®šæ€§å’ŒSHA1éªŒè¯é€šè¿‡
func (w *ossChunkWriter) Upload(ctx context.Context) (err error) {
	uploadCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// ğŸ§¹ ç¡®ä¿å¤±è´¥æ—¶æ¸…ç†OSSèµ„æº
	defer func() {
		if err != nil && w.imur != nil {
			fs.Debugf(w.o, "ğŸ§¹ ä¸Šä¼ å¤±è´¥ï¼Œæ¸…ç†OSSåˆ†ç‰‡ä¸Šä¼ : %v", err)
			// ğŸ”’ ä½¿ç”¨ç‹¬ç«‹contextè¿›è¡Œæ¸…ç†ï¼Œé¿å…è¢«å–æ¶ˆçš„contextå½±å“æ¸…ç†æ“ä½œ
			cleanupCtx, cleanupCancel := context.WithTimeout(context.Background(), 30*time.Second)
			defer cleanupCancel()

			if abortErr := w.Abort(cleanupCtx); abortErr != nil {
				fs.Errorf(w.o, "âŒ æ¸…ç†OSSåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %v", abortErr)
			} else {
				fs.Debugf(w.o, "âœ… OSSåˆ†ç‰‡ä¸Šä¼ æ¸…ç†æˆåŠŸ")
			}
		}
	}()

	// åˆå§‹åŒ–ä¸Šä¼ å‚æ•°
	uploadParams := w.initializeUploadParams()

	// è·å–Accountå¯¹è±¡å’Œè¾“å…¥æµ
	acc, in := w.setupAccountingAndInput()

	// æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ å¾ªç¯
	actualParts, err := w.performChunkedUpload(uploadCtx, in, acc, uploadParams)
	if err != nil {
		return err
	}

	// å®Œæˆä¸Šä¼ å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
	return w.finalizeUpload(ctx, actualParts, uploadParams.startTime)
}

// uploadParams ä¸Šä¼ å‚æ•°ç»“æ„ä½“
type uploadParams struct {
	size       int64
	chunkSize  int64
	totalParts int64
	startTime  time.Time
}

// initializeUploadParams åˆå§‹åŒ–ä¸Šä¼ å‚æ•°
func (w *ossChunkWriter) initializeUploadParams() *uploadParams {
	params := &uploadParams{
		size:      w.size,
		chunkSize: w.chunkSize,
		startTime: time.Now(),
	}

	// è®¡ç®—æ€»åˆ†ç‰‡æ•°ç”¨äºè¿›åº¦æ˜¾ç¤º
	if params.size > 0 {
		params.totalParts = (params.size + params.chunkSize - 1) / params.chunkSize
	} else {
		fs.Debugf(w.o, "File size unknown, will calculate chunk count dynamically")
	}

	return params
}

// setupAccountingAndInput è®¾ç½®Accountå¯¹è±¡å’Œè¾“å…¥æµ
func (w *ossChunkWriter) setupAccountingAndInput() (*accounting.Account, io.Reader) {
	var acc *accounting.Account
	var in io.Reader

	// é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºRereadableObjectç±»å‹
	if rereadableObj, ok := w.in.(*RereadableObject); ok {
		acc = rereadableObj.GetAccount()
		in = rereadableObj.OldStream()
		if acc != nil {
			fs.Debugf(w.o, "Obtained Account object from RereadableObject, upload progress will display correctly")
		} else {
			fs.Debugf(w.o, "No Account object in RereadableObject")
		}
	} else if accountedReader, ok := w.in.(*AccountedFileReader); ok {
		// AccountedFileReader progress managed by rclone standard mechanism
		acc = nil
		in = accountedReader
	} else {
		// å›é€€åˆ°æ ‡å‡†æ–¹æ³•
		in, acc = accounting.UnWrapAccounting(w.in)
		if acc != nil {
			fs.Debugf(w.o, "Obtained Account object through UnWrapAccounting")
		} else {
			fs.Debugf(w.o, "Account object not found, input type: %T", w.in)
		}
	}

	return acc, in
}

// performChunkedUpload æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ å¾ªç¯
func (w *ossChunkWriter) performChunkedUpload(ctx context.Context, in io.Reader, acc *accounting.Account, params *uploadParams) (int64, error) {
	fs.Infof(w.o, "Starting 115 single-threaded chunk upload")
	fs.Infof(w.o, "Upload parameters: file size=%v, chunk size=%v, estimated chunks=%d",
		fs.SizeSuffix(params.size), fs.SizeSuffix(params.chunkSize), params.totalParts)

	// å¯åŠ¨ä¼ è¾“é€Ÿåº¦ç›‘æ§
	var monitor *TransferSpeedMonitor
	if acc != nil {
		monitor = w.f.startTransferMonitor(w.o.remote, acc, ctx)
	}

	// ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶åœæ­¢ç›‘æ§
	defer func() {
		if monitor != nil {
			w.f.stopTransferMonitor(w.o.remote)
		}
	}()

	var (
		finished = false
		off      int64
		partNum  int64
	)

	for partNum = 0; !finished; partNum++ {
		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
		if ctx.Err() != nil {
			fs.Debugf(w.o, " ä¸Šä¼ è¢«å–æ¶ˆï¼Œåœæ­¢åˆ†ç‰‡ä¸Šä¼ ")
			break
		}

		// åˆ†ç‰‡ä¸Šä¼ å¥åº·æ£€æŸ¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
		if partNum > 0 {
			timeoutCount := w.getTimeoutStats()
			// å¦‚æœè¶…æ—¶æ¬¡æ•°è¿‡å¤šï¼Œæä¾›è­¦å‘Š
			if timeoutCount >= 5 {
				fs.Logf(w.o, "âš ï¸ åˆ†ç‰‡ä¸Šä¼ å¥åº·æ£€æŸ¥: å·²å‘ç”Ÿ %d æ¬¡è¶…æ—¶ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥", timeoutCount)
			}
		}

		// ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
		n, chunkFinished, err := w.uploadSinglePart(ctx, in, acc, partNum, params, off)
		if err != nil {
			return partNum, err
		}

		finished = chunkFinished
		off += n
	}

	return partNum, nil
}

// uploadSinglePart ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
func (w *ossChunkWriter) uploadSinglePart(ctx context.Context, in io.Reader, acc *accounting.Account, partNum int64, params *uploadParams, off int64) (int64, bool, error) {
	// è·å–å†…å­˜ç¼“å†²åŒº
	rw := NewRW()
	defer rw.Close()

	if acc != nil {
		rw.SetAccounting(acc.AccountRead)
	}

	// è¯»å–åˆ†ç‰‡æ•°æ®
	chunkStartTime := time.Now()
	fs.Debugf(w.o, "å¼€å§‹è¯»å–åˆ†ç‰‡ %dï¼Œç›®æ ‡å¤§å°: %s", partNum+1, fs.SizeSuffix(params.chunkSize))
	n, err := io.CopyN(rw, in, params.chunkSize)
	fs.Debugf(w.o, "åˆ†ç‰‡ %d è¯»å–å®Œæˆï¼Œå®é™…å¤§å°: %sï¼Œè€—æ—¶: %v", partNum+1, fs.SizeSuffix(n), time.Since(chunkStartTime))
	finished := false

	if err == io.EOF {
		if n == 0 && partNum != 0 {
			// All chunks read complete
			return 0, true, nil
		}
		finished = true
	} else if err != nil {
		return 0, false, fmt.Errorf("failed to read chunk data: %w", err)
	}

	// æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
	w.logUploadProgress(partNum, n, off, params)

	// Upload chunk
	currentPart := partNum + 1

	chunkSize, err := w.uploadChunkWithTimeout(ctx, int32(partNum), rw)
	if err != nil {
		return 0, false, fmt.Errorf("upload chunk %d failed (size: %v, offset: %v, timeout/retried): %w",
			currentPart, fs.SizeSuffix(n), fs.SizeSuffix(off), err)
	}

	// è®°å½•åˆ†ç‰‡ä¸Šä¼ æˆåŠŸä¿¡æ¯
	w.logChunkSuccess(partNum, chunkSize, chunkStartTime)

	return n, finished, nil
}

// logUploadProgress è®°å½•ä¸Šä¼ è¿›åº¦
func (w *ossChunkWriter) logUploadProgress(partNum, n, off int64, params *uploadParams) {
	currentPart := partNum + 1
	elapsed := time.Since(params.startTime)

	if params.totalParts > 0 {
		percentage := float64(currentPart) / float64(params.totalParts) * 100

		// æ¯2ä¸ªåˆ†ç‰‡æˆ–å…³é”®èŠ‚ç‚¹æ˜¾ç¤ºè¿›åº¦
		shouldLog := (currentPart == 1) || (currentPart%2 == 0) || (currentPart == params.totalParts)

		if shouldLog {
			avgTimePerPart := elapsed / time.Duration(currentPart)
			remainingParts := params.totalParts - currentPart
			estimatedRemaining := avgTimePerPart * time.Duration(remainingParts)

			fs.Infof(w.o, "ğŸ“¤ ä¸Šä¼ è¿›åº¦: %d/%d (%.1f%%) | %v | â±ï¸ %v | ğŸ•’ å‰©ä½™ %v",
				currentPart, params.totalParts, percentage, fs.SizeSuffix(n),
				elapsed.Truncate(time.Second), estimatedRemaining.Truncate(time.Second))
		}
	} else if currentPart == 1 {
		// æœªçŸ¥å¤§å°æ—¶åªåœ¨ç¬¬ä¸€ä¸ªåˆ†ç‰‡è¾“å‡ºæ—¥å¿—
		fs.Infof(w.o, "ğŸ“¤ ä¸Šä¼ åˆ†ç‰‡: %d | %v | åç§»: %v | â±ï¸ %v",
			currentPart, fs.SizeSuffix(n), fs.SizeSuffix(off), elapsed.Truncate(time.Second))
	}
}

// logChunkSuccess records chunk upload success (simplified)
func (w *ossChunkWriter) logChunkSuccess(partNum, chunkSize int64, chunkStartTime time.Time) {
	// Chunk upload success logged at higher level
}

// finalizeUpload å®Œæˆä¸Šä¼ å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
func (w *ossChunkWriter) finalizeUpload(ctx context.Context, actualParts int64, startTime time.Time) error {
	totalDuration := time.Since(startTime)

	fs.Infof(w.o, "ğŸ å®Œæˆåˆ†ç‰‡ä¸Šä¼ : å…±%dä¸ªåˆ†ç‰‡ | â±ï¸ æ€»ç”¨æ—¶ %v", actualParts, totalDuration.Truncate(time.Second))

	err := w.Close(ctx)
	if err != nil {
		return fmt.Errorf("failed to complete chunk upload: %w", err)
	}

	// æ˜¾ç¤ºä¸Šä¼ å®Œæˆç»Ÿè®¡ä¿¡æ¯
	timeoutCount := w.getTimeoutStats()

	if w.size > 0 && totalDuration.Seconds() > 0 {
		avgSpeed := float64(w.size) / totalDuration.Seconds() / 1024 / 1024 // MB/s
		fs.Infof(w.o, "115 multipart upload completed")

		if timeoutCount > 0 {
			fs.Infof(w.o, "Upload stats: size=%v, parts=%d, duration=%v, speed=%.2fMB/s, timeouts=%d",
				fs.SizeSuffix(w.size), actualParts, totalDuration.Truncate(time.Second), avgSpeed, timeoutCount)
		} else {
			fs.Infof(w.o, "Upload stats: size=%v, parts=%d, duration=%v, speed=%.2fMB/s",
				fs.SizeSuffix(w.size), actualParts, totalDuration.Truncate(time.Second), avgSpeed)
		}
	} else {
		if timeoutCount > 0 {
			fs.Infof(w.o, "115 multipart upload completed: parts=%d, duration=%v, timeouts=%d",
				actualParts, totalDuration.Truncate(time.Second), timeoutCount)
		} else {
			fs.Infof(w.o, "115 multipart upload completed: parts=%d, duration=%v",
				actualParts, totalDuration.Truncate(time.Second))
		}
	}

	// å¦‚æœæœ‰è¶…æ—¶ï¼Œæä¾›å»ºè®®
	if timeoutCount > 0 {
		fs.Logf(w.o, "ğŸ’¡ å»ºè®®ï¼šå¦‚æœç»å¸¸å‡ºç°è¶…æ—¶ï¼Œå¯ä»¥å°è¯•å¢åŠ chunk_timeouté…ç½®æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
	}

	return nil
}

var warnStreamUpload sync.Once

// ossChunkWriter 115ç½‘ç›˜åˆ†ç‰‡ä¸Šä¼ å†™å…¥å™¨
// ç®€åŒ–ç‰ˆæœ¬ï¼šå•çº¿ç¨‹é¡ºåºä¸Šä¼ æ¨¡å¼ï¼Œæ— æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
type ossChunkWriter struct {
	chunkSize     int64                              // åˆ†ç‰‡å¤§å°
	size          int64                              // æ–‡ä»¶æ€»å¤§å°
	f             *Fs                                // æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	o             *Object                            // å¯¹è±¡å®ä¾‹
	in            io.Reader                          // è¾“å…¥æµ
	uploadedParts []oss.UploadPart                   // å·²ä¸Šä¼ çš„åˆ†ç‰‡åˆ—è¡¨
	client        *oss.Client                        // OSSå®¢æˆ·ç«¯
	callback      string                             // 115ç½‘ç›˜å›è°ƒURL
	callbackVar   string                             // 115ç½‘ç›˜å›è°ƒå˜é‡
	callbackRes   map[string]any                     // å›è°ƒç»“æœ
	imur          *oss.InitiateMultipartUploadResult // åˆ†ç‰‡ä¸Šä¼ åˆå§‹åŒ–ç»“æœ

	// è¶…æ—¶æ§åˆ¶ç»Ÿè®¡
	timeoutCount int        // è¶…æ—¶æ¬¡æ•°ç»Ÿè®¡
	mu           sync.Mutex // ä¿æŠ¤ç»Ÿè®¡æ•°æ®
}

// newChunkWriterWithClient åˆ›å»ºåˆ†ç‰‡å†™å…¥å™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„ä¼˜åŒ–OSSå®¢æˆ·ç«¯
func (f *Fs) newChunkWriterWithClient(ctx context.Context, src fs.ObjectInfo, ui *UploadInitInfo, in io.Reader, o *Object, ossClient *oss.Client, options ...fs.OpenOption) (w *ossChunkWriter, err error) {
	uploadParts := min(max(1, f.opt.MaxUploadParts), maxUploadParts)
	size := src.Size()

	// å‚è€ƒOpenListï¼šä½¿ç”¨ç®€åŒ–çš„åˆ†ç‰‡å¤§å°è®¡ç®—
	chunkSize := f.calculateOptimalChunkSize(size)

	// å¤„ç†æœªçŸ¥æ–‡ä»¶å¤§å°çš„æƒ…å†µï¼ˆæµå¼ä¸Šä¼ ï¼‰
	if size == -1 {
		warnStreamUpload.Do(func() {
			fs.Logf(f, "æµå¼ä¸Šä¼ ä½¿ç”¨åˆ†ç‰‡å¤§å° %vï¼Œæœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶ä¸º %v",
				chunkSize, fs.SizeSuffix(int64(chunkSize)*int64(uploadParts)))
		})
	} else {
		// ç®€åŒ–åˆ†ç‰‡è®¡ç®—ï¼šç›´æ¥ä½¿ç”¨OpenListé£æ ¼çš„åˆ†ç‰‡å¤§å°ï¼Œå‡å°‘å¤æ‚æ€§
		// ä¿æŒOpenListçš„ç®€å•æœ‰æ•ˆç­–ç•¥
		fs.Debugf(f, "115ç½‘ç›˜åˆ†ç‰‡ä¸Šä¼ : ä½¿ç”¨OpenListé£æ ¼åˆ†ç‰‡å¤§å° %v", chunkSize)
	}

	// 115ç½‘ç›˜é‡‡ç”¨å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ æ¨¡å¼ï¼Œç¡®ä¿ç¨³å®šæ€§
	fs.Debugf(f, "115ç½‘ç›˜åˆ†ç‰‡ä¸Šä¼ : æ–‡ä»¶å¤§å° %v, åˆ†ç‰‡å¤§å° %v", fs.SizeSuffix(size), fs.SizeSuffix(int64(chunkSize)))

	// Fix 10002 error: use internal two-step transfer for cross-cloud transfers
	if f.isFromRemoteSource(in) {
		fs.Debugf(o, "Cross-cloud transfer detected, using internal two-step transfer to avoid 10002 error")
		fs.Debugf(o, "Step 1: Download to local, Step 2: Upload to 115 from local")

		// æ‰§è¡Œå†…éƒ¨ä¸¤æ­¥ä¼ è¾“
		return f.internalTwoStepTransfer(ctx, src, in, o, size, options...)
	}

	// Simplified: no resume support
	fs.Debugf(f, "Simplified mode: skip resume management")

	w = &ossChunkWriter{
		chunkSize: int64(chunkSize),
		size:      size,
		f:         f,
		o:         o,
		in:        in,
	}

	// å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨ä¼ å…¥çš„ä¼˜åŒ–OSSå®¢æˆ·ç«¯ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„
	w.client = ossClient
	fs.Debugf(o, "Chunk upload using optimized OSS client")

	req := &oss.InitiateMultipartUploadRequest{
		Bucket: oss.Ptr(ui.GetBucket()),
		Key:    oss.Ptr(ui.GetObject()),
	}
	// 115ç½‘ç›˜OSSä½¿ç”¨Sequentialæ¨¡å¼ï¼Œç¡®ä¿åˆ†ç‰‡æŒ‰é¡ºåºå¤„ç†
	req.Parameters = map[string]string{
		"sequential": "",
	}
	// Apply upload options
	for _, option := range options {
		key, value := option.Header()
		lowerKey := strings.ToLower(key)
		switch lowerKey {
		case "":
			// ignore
		case "cache-control":
			req.CacheControl = oss.Ptr(value)
		case "content-disposition":
			req.ContentDisposition = oss.Ptr(value)
		case "content-encoding":
			req.ContentEncoding = oss.Ptr(value)
		case "content-type":
			req.ContentType = oss.Ptr(value)
		}
	}
	// è¶…æ¿€è¿›ä¼˜åŒ–ï¼šåˆ†ç‰‡ä¸Šä¼ åˆå§‹åŒ–ä¹Ÿç»•è¿‡QPSé™åˆ¶
	fs.Debugf(w.f, "Chunk upload initialization: bypassing QPS limits")
	// ç›´æ¥è°ƒç”¨ï¼Œä¸ä½¿ç”¨è°ƒé€Ÿå™¨
	w.imur, err = w.client.InitiateMultipartUpload(ctx, req)
	if err != nil {
		return nil, fmt.Errorf("create multipart upload failed: %w", err)
	}
	w.callback, w.callbackVar = ui.GetCallback(), ui.GetCallbackVar()
	fs.Debugf(w.o, "115 multipart upload initialized: %q", *w.imur.UploadId)
	return
}

// shouldRetry returns a boolean as to whether this err
// deserve to be retried. Uses rclone standard error handling with OSS-specific logic.
func (w *ossChunkWriter) shouldRetry(ctx context.Context, err error) (bool, error) {
	if fserrors.ContextError(ctx, &err) {
		return false, err
	}
	if fserrors.ShouldRetry(err) {
		return true, err
	}

	// ğŸ” OSSé”™è¯¯è¯¦ç»†åˆ†æå’Œå¤„ç†
	if opErr, ok := err.(*oss.OperationError); ok {
		fs.Debugf(w.o, "ğŸ” OSSæ“ä½œé”™è¯¯: %v", opErr)

		// âœ… æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯
		unwrappedErr := opErr.Unwrap()
		if fserrors.ShouldRetry(unwrappedErr) {
			return true, opErr
		}

		// ğŸ›¡ï¸ OSSç½‘ç»œé”™è¯¯ä¿å®ˆé‡è¯•ç­–ç•¥
		errStr := opErr.Error()
		if strings.Contains(errStr, "timeout") ||
			strings.Contains(errStr, "connection") ||
			strings.Contains(errStr, "network") ||
			strings.Contains(errStr, "503") ||
			strings.Contains(errStr, "502") ||
			strings.Contains(errStr, "500") {
			return true, opErr
		}

		// ğŸ”„ å…¶ä»–é”™è¯¯ä½¿ç”¨é»˜è®¤é€»è¾‘
		err = unwrappedErr
	}

	if ossErr, ok := err.(*oss.ServiceError); ok {
		// å¤„ç†PartAlreadyExisté”™è¯¯ï¼šåˆ†ç‰‡å·²å­˜åœ¨ï¼Œä¸éœ€è¦é‡è¯•
		if ossErr.Code == "PartAlreadyExist" {
			fs.Debugf(w.o, "åˆ†ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡é‡è¯•: %v", err)
			return false, nil // ä¸é‡è¯•ï¼Œè§†ä¸ºæˆåŠŸ
		}

		// Don't retry authentication errors
		if ossErr.StatusCode == 403 && (ossErr.Code == "InvalidAccessKeyId" || ossErr.Code == "SecurityTokenExpired") {
			return false, fserrors.FatalError(err)
		}
		// Retry server errors
		if ossErr.StatusCode >= 500 && ossErr.StatusCode < 600 {
			return true, err
		}
	}
	return false, err
}

// uploadChunkWithTimeout å¸¦è¶…æ—¶æ§åˆ¶çš„åˆ†ç‰‡ä¸Šä¼ 
func (w *ossChunkWriter) uploadChunkWithTimeout(ctx context.Context, chunkNumber int32, reader io.ReadSeeker) (currentChunkSize int64, err error) {
	// åˆ†ç‰‡è¶…æ—¶æ§åˆ¶é»˜è®¤å¯ç”¨

	startTime := time.Now()

	// åˆ›å»ºå¸¦è¶…æ—¶çš„context
	chunkCtx, cancel := context.WithTimeout(ctx, time.Duration(defaultChunkTimeout))
	defer cancel()

	// ä½¿ç”¨channelæ¥å¤„ç†è¶…æ—¶
	type result struct {
		size int64
		err  error
	}

	done := make(chan result, 1)

	// åœ¨goroutineä¸­æ‰§è¡Œä¸Šä¼ 
	go func() {
		size, err := w.uploadChunkWithRetry(chunkCtx, chunkNumber, reader)
		done <- result{size: size, err: err}
	}()

	// ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case res := <-done:
		elapsed := time.Since(startTime)
		if res.err == nil {
			fs.Debugf(w.o, "âœ… åˆ†ç‰‡ %d ä¸Šä¼ æˆåŠŸ (è€—æ—¶: %v)", chunkNumber+1, elapsed.Truncate(time.Millisecond))
		}
		return res.size, res.err

	case <-chunkCtx.Done():
		elapsed := time.Since(startTime)

		// æ›´æ–°è¶…æ—¶ç»Ÿè®¡
		w.mu.Lock()
		w.timeoutCount++
		timeoutCount := w.timeoutCount
		w.mu.Unlock()

		if chunkCtx.Err() == context.DeadlineExceeded {
			fs.Errorf(w.o, "â° åˆ†ç‰‡ %d ä¸Šä¼ è¶…æ—¶ (è€—æ—¶: %v, è¶…æ—¶é˜ˆå€¼: %v, ç´¯è®¡è¶…æ—¶: %dæ¬¡)",
				chunkNumber+1, elapsed.Truncate(time.Millisecond), defaultChunkTimeout, timeoutCount)

			// å¦‚æœè¶…æ—¶æ¬¡æ•°è¿‡å¤šï¼Œæä¾›é¢å¤–çš„è¯Šæ–­ä¿¡æ¯
			if timeoutCount >= 3 {
				fs.Logf(w.o, "âš ï¸ æ£€æµ‹åˆ°å¤šæ¬¡åˆ†ç‰‡ä¸Šä¼ è¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨ç½‘ç»œé—®é¢˜æˆ–æœåŠ¡å™¨å“åº”æ…¢")
			}

			return 0, fmt.Errorf("chunk %d upload timeout after %v (attempt %d)",
				chunkNumber+1, defaultChunkTimeout, timeoutCount)
		}
		return 0, chunkCtx.Err()
	}
}

// uploadChunkWithRetry uploads a chunk with retry logic
func (w *ossChunkWriter) uploadChunkWithRetry(ctx context.Context, chunkNumber int32, reader io.ReadSeeker) (currentChunkSize int64, err error) {
	// Use rclone's pacer for retry logic instead of custom implementation
	err = w.f.pacer.Call(func() (bool, error) {
		// Reset reader position
		if _, seekErr := reader.Seek(0, io.SeekStart); seekErr != nil {
			return false, fmt.Errorf("failed to reset reader: %w", seekErr)
		}

		// Upload chunk
		currentChunkSize, err = w.WriteChunk(ctx, chunkNumber, reader)
		if err != nil {
			return w.shouldRetry(ctx, err)
		}
		return false, nil
	})

	return currentChunkSize, err
}

// getTimeoutStats è·å–è¶…æ—¶ç»Ÿè®¡ä¿¡æ¯
func (w *ossChunkWriter) getTimeoutStats() int {
	w.mu.Lock()
	defer w.mu.Unlock()
	return w.timeoutCount
}

// addCompletedPart æ·»åŠ å·²å®Œæˆçš„åˆ†ç‰‡åˆ°åˆ—è¡¨
// å•çº¿ç¨‹æ¨¡å¼ä¸‹æŒ‰é¡ºåºæ·»åŠ ï¼Œæ— éœ€å¤æ‚çš„å¹¶å‘æ§åˆ¶
func (w *ossChunkWriter) addCompletedPart(part oss.UploadPart) {
	w.uploadedParts = append(w.uploadedParts, part)
}

// WriteChunk uploads a chunk to OSS
func (w *ossChunkWriter) WriteChunk(ctx context.Context, chunkNumber int32, reader io.ReadSeeker) (currentChunkSize int64, err error) {
	if chunkNumber < 0 {
		return -1, fmt.Errorf("invalid chunk number: %v", chunkNumber)
	}

	ossPartNumber := chunkNumber + 1

	// Get chunk size
	currentChunkSize, err = reader.Seek(0, io.SeekEnd)
	if err != nil {
		return 0, fmt.Errorf("failed to get chunk %d size: %w", ossPartNumber, err)
	}

	// Reset reader position
	_, err = reader.Seek(0, io.SeekStart)
	if err != nil {
		return 0, fmt.Errorf("failed to reset chunk %d reader position: %w", ossPartNumber, err)
	}

	fs.Debugf(w.o, "Uploading chunk %d to OSS: size=%v", ossPartNumber, fs.SizeSuffix(currentChunkSize))

	// ğŸ“¤ OSSåˆ†ç‰‡ä¸Šä¼ 
	fs.Debugf(w.o, "ğŸ“¤ ä¸Šä¼ åˆ†ç‰‡ %d (å¤§å°: %v)", ossPartNumber, fs.SizeSuffix(currentChunkSize))

	res, err := w.client.UploadPart(ctx, &oss.UploadPartRequest{
		Bucket:     oss.Ptr(*w.imur.Bucket),
		Key:        oss.Ptr(*w.imur.Key),
		UploadId:   w.imur.UploadId,
		PartNumber: ossPartNumber,
		Body:       reader,
	})

	if err != nil {
		fs.Errorf(w.o, "âŒ OSSåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: åˆ†ç‰‡=%d, å¤§å°=%v, é”™è¯¯=%v",
			ossPartNumber, fs.SizeSuffix(currentChunkSize), err)
		return 0, fmt.Errorf("âŒ åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥ (å¤§å°: %v): %w", ossPartNumber, fs.SizeSuffix(currentChunkSize), err)
	}

	// Record completed part
	part := oss.UploadPart{
		PartNumber: ossPartNumber,
		ETag:       res.ETag,
	}
	w.addCompletedPart(part)

	fs.Debugf(w.o, "Chunk %d upload completed: size=%v, ETag=%s", ossPartNumber, fs.SizeSuffix(currentChunkSize), *res.ETag)
	return currentChunkSize, nil
}

// Abort the multipart upload
func (w *ossChunkWriter) Abort(ctx context.Context) (err error) {
	// ä½¿ç”¨ç»Ÿä¸€è°ƒé€Ÿå™¨ï¼Œä¼˜åŒ–åˆ†ç‰‡ä¸Šä¼ ä¸­æ­¢APIè°ƒç”¨é¢‘ç‡
	err = w.f.pacer.Call(func() (bool, error) {
		_, err = w.client.AbortMultipartUpload(ctx, &oss.AbortMultipartUploadRequest{
			Bucket:   oss.Ptr(*w.imur.Bucket),
			Key:      oss.Ptr(*w.imur.Key),
			UploadId: w.imur.UploadId,
		})
		return w.shouldRetry(ctx, err)
	})
	if err != nil {
		return fmt.Errorf("failed to abort multipart upload %q: %w", *w.imur.UploadId, err)
	}
	// w.shutdownRenew()
	fs.Debugf(w.o, "multipart upload: %q aborted", *w.imur.UploadId)
	return
}

// Close å®Œæˆå¹¶ç¡®è®¤åˆ†ç‰‡ä¸Šä¼ 
func (w *ossChunkWriter) Close(ctx context.Context) (err error) {
	// å•çº¿ç¨‹æ¨¡å¼ä¸‹åˆ†ç‰‡å·²æŒ‰é¡ºåºæ·»åŠ ï¼Œä½†ä¸ºä¿é™©èµ·è§ä»è¿›è¡Œæ’åº
	sort.Slice(w.uploadedParts, func(i, j int) bool {
		return w.uploadedParts[i].PartNumber < w.uploadedParts[j].PartNumber
	})

	fs.Infof(w.o, "å‡†å¤‡å®Œæˆåˆ†ç‰‡ä¸Šä¼ : å…± %d ä¸ªåˆ†ç‰‡", len(w.uploadedParts))

	// å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
	var res *oss.CompleteMultipartUploadResult
	req := &oss.CompleteMultipartUploadRequest{
		Bucket:   oss.Ptr(*w.imur.Bucket),
		Key:      oss.Ptr(*w.imur.Key),
		UploadId: w.imur.UploadId,
		CompleteMultipartUpload: &oss.CompleteMultipartUpload{
			Parts: w.uploadedParts,
		},
		Callback:    oss.Ptr(w.callback),
		CallbackVar: oss.Ptr(w.callbackVar),
	}

	// ğŸ¯ OSSåˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼šä½¿ç”¨å®Œæ•´é‡è¯•æœºåˆ¶ç¡®ä¿ç¨³å®šæ€§
	fs.Debugf(w.o, "ğŸ¯ OSSåˆ†ç‰‡ä¸Šä¼ å®Œæˆä¸­")

	err = w.f.pacer.Call(func() (bool, error) {
		var completeErr error
		res, completeErr = w.client.CompleteMultipartUpload(ctx, req)

		if completeErr != nil {
			retry, retryErr := w.shouldRetry(ctx, completeErr)
			if retry {
				fs.Debugf(w.o, "ğŸ”„ OSSåˆ†ç‰‡ä¸Šä¼ å®Œæˆé‡è¯•: %v", completeErr)
				return true, retryErr
			}
			return false, completeErr
		}
		return false, nil
	})

	if err != nil {
		return fmt.Errorf("âŒ OSSåˆ†ç‰‡ä¸Šä¼ å®Œæˆå¤±è´¥: %w", err)
	}

	w.callbackRes = res.CallbackResult
	fs.Infof(w.o, "âœ… 115ç½‘ç›˜ä¸Šä¼ å®Œæˆ")
	return
}

// getDownloadURLCommand é€šè¿‡pick_codeæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL
func (f *Fs) getDownloadURLCommand(ctx context.Context, args []string, opt map[string]string) (any, error) {
	if len(args) < 1 {
		return nil, fmt.Errorf("pick_code, 115://pick_code format, or file path required")
	}

	input := args[0]
	fs.Debugf(f, "Processing download URL request: %s", input)

	// Parse input format
	if strings.HasPrefix(input, "/") {
		// File path format, use getDownloadURLByUA method (supports 302 redirect)
		fs.Debugf(f, "Using file path with UA method: %s", input)

		userAgent := opt["user-agent"]
		if userAgent == "" {
			userAgent = defaultUserAgent
		}

		fs.Debugf(f, "Getting download URL with UA: path=%s, UA=%s", input, userAgent)
		downloadURL, err := f.getDownloadURLByUA(ctx, input, userAgent)
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL with UA: %w", err)
		}

		fs.Debugf(f, "Successfully got 115 playable URL: path=%s", input)
		return downloadURL, nil
	}

	// pick_code æ ¼å¼ï¼Œä½¿ç”¨åŸå§‹HTTPå®ç°
	var pickCode string
	if code, found := strings.CutPrefix(input, "115://"); found {
		// 115://pick_code æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
		pickCode = code
		fs.Debugf(f, "Parsed .strm format: pick_code=%s", pickCode)
	} else {
		// Assume pure pick_code
		pickCode = input
		fs.Debugf(f, "Using pure pick_code: %s", pickCode)
	}

	// Validate pick_code format
	if pickCode == "" {
		return nil, fmt.Errorf("invalid pick_code: %s", input)
	}

	// ä½¿ç”¨åŸå§‹HTTPå®ç°è·å–ä¸‹è½½URL
	fs.Debugf(f, "Using raw HTTP method to get download URL: pick_code=%s", pickCode)
	downloadURL, err := f.getDownloadURLByPickCodeHTTP(ctx, pickCode, opt["user-agent"])
	if err != nil {
		return nil, fmt.Errorf("failed to get download URL: %w", err)
	}

	fs.Infof(f, "Successfully obtained 115 download URL: pick_code=%s", pickCode)

	// è¿”å›ä¸‹è½½URLå­—ç¬¦ä¸²ï¼ˆä¸getdownloadurluaä¿æŒä¸€è‡´ï¼‰
	return downloadURL, nil
}

// refreshCacheCommand åˆ·æ–°ç›®å½•ç¼“å­˜
func (f *Fs) refreshCacheCommand(ctx context.Context, args []string) (any, error) {
	fs.Infof(f, "ğŸ”„ å¼€å§‹åˆ·æ–°ç¼“å­˜...")

	// æ¸…é™¤æŒä¹…åŒ–dirCache
	if err := f.dirCache.ForceRefreshPersistent(); err != nil {
		fs.Logf(f, "âš ï¸ æ¸…é™¤æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: %v", err)
	} else {
		fs.Infof(f, "âœ… å·²æ¸…é™¤æŒä¹…åŒ–dirCache")
	}

	// é‡ç½®dirCache
	f.dirCache.Flush()
	fs.Infof(f, "âœ… å·²é‡ç½®å†…å­˜dirCache")

	// å¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼Œå°è¯•é‡æ–°æ„å»ºè¯¥è·¯å¾„çš„ç¼“å­˜
	if len(args) > 0 && args[0] != "" {
		targetPath := args[0]
		fs.Infof(f, "ğŸ”„ é‡æ–°æ„å»ºè·¯å¾„ç¼“å­˜: %s", targetPath)

		// å°è¯•æŸ¥æ‰¾ç›®å½•ä»¥é‡æ–°æ„å»ºç¼“å­˜
		if _, err := f.dirCache.FindDir(ctx, targetPath, false); err != nil {
			fs.Logf(f, "âš ï¸ é‡æ–°æ„å»ºè·¯å¾„ç¼“å­˜å¤±è´¥: %v", err)
		} else {
			fs.Infof(f, "âœ… è·¯å¾„ç¼“å­˜é‡æ–°æ„å»ºæˆåŠŸ: %s", targetPath)
		}
	}

	return map[string]any{
		"status":  "success",
		"message": "ç¼“å­˜åˆ·æ–°å®Œæˆ",
	}, nil
}

// getDownloadURLByPickCodeHTTP ä½¿ç”¨rcloneæ ‡å‡†æ–¹å¼é€šè¿‡pick_codeè·å–ä¸‹è½½URL
func (f *Fs) getDownloadURLByPickCodeHTTP(ctx context.Context, pickCode string, userAgent string) (string, error) {
	// å¦‚æœæ²¡æœ‰æä¾› UAï¼Œä½¿ç”¨é»˜è®¤å€¼
	if userAgent == "" {
		userAgent = defaultUserAgent
	}

	fs.Debugf(f, "Using rclone standard method to get download URL: pick_code=%s, UA=%s", pickCode, userAgent)

	// ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯
	opts := rest.Opts{
		Method:  "POST",
		RootURL: openAPIRootURL, // ğŸ”§ ä¿®å¤ï¼šè®¾ç½®RootURL
		Path:    "/open/ufile/downurl",
		Body:    strings.NewReader("pick_code=" + pickCode),
		ExtraHeaders: map[string]string{
			"Content-Type": "application/x-www-form-urlencoded",
			"User-Agent":   userAgent,
		},
	}

	// å‡†å¤‡è®¤è¯ä¿¡æ¯
	err := f.prepareTokenForRequest(ctx, &opts)
	if err != nil {
		return "", fmt.Errorf("failed to prepare token: %w", err)
	}

	// å‘é€è¯·æ±‚å¹¶å¤„ç†å“åº”
	res, err := f.openAPIClient.Call(ctx, &opts)
	if err != nil {
		fs.Errorf(f, "è¯·æ±‚å¤±è´¥: %v", err)
		return "", err
	}
	defer res.Body.Close()

	// è§£æå“åº” - ä½¿ç”¨åŸå§‹ä»£ç ä¸­çš„å“åº”ç»“æ„
	var response struct {
		State   bool   `json:"state"`
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    map[string]struct {
			URL struct {
				URL string `json:"url"`
			} `json:"url"`
		} `json:"data"`
	}

	if decodeErr := json.NewDecoder(res.Body).Decode(&response); decodeErr != nil {
		fs.Errorf(f, "è§£æå“åº”å¤±è´¥: %v", decodeErr)
		return "", decodeErr
	}

	for _, downInfo := range response.Data {
		if downInfo.URL.URL != "" {
			fs.Debugf(f, "Obtained download URL: %s", downInfo.URL.URL)
			return downInfo.URL.URL, nil
		}
	}
	return "", fmt.Errorf("download URL not found")
}

// internalTwoStepTransfer å†…éƒ¨ä¸¤æ­¥ä¼ è¾“ï¼šå…ˆå®Œæ•´ä¸‹è½½åˆ°æœ¬åœ°ï¼Œå†ä»æœ¬åœ°ä¸Šä¼ 
// ä¿®å¤10002é”™è¯¯ï¼šå½»åº•è§£å†³è·¨äº‘ä¼ è¾“SHA1ä¸ä¸€è‡´é—®é¢˜
func (f *Fs) internalTwoStepTransfer(ctx context.Context, src fs.ObjectInfo, in io.Reader, o *Object, size int64, options ...fs.OpenOption) (*ossChunkWriter, error) {
	fs.Infof(o, "Starting internal two-step transfer: %s (%s)", src.Remote(), fs.SizeSuffix(size))

	// Step 1: Complete download to local temp file
	fs.Infof(o, "Step 1: Downloading to local temp file...")

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶
	tempFile, err := os.CreateTemp("", "rclone_two_step_*.tmp")
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	tempPath := tempFile.Name()

	// ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
	defer func() {
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(o, "âš ï¸ å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
		}
		if removeErr := os.Remove(tempPath); removeErr != nil {
			fs.Debugf(o, "âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
		}
		fs.Debugf(o, "âœ… æ¸…ç†ä¸´æ—¶æ–‡ä»¶: %s", tempPath)
	}()

	// ä½¿ç”¨ä¼ å…¥çš„Readerè¿›è¡Œå®Œæ•´ä¸‹è½½ï¼Œå¹¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦
	// æ³¨æ„ï¼šinå·²ç»æ˜¯æ‰“å¼€çš„Readerï¼Œç›´æ¥ä½¿ç”¨
	fs.Infof(o, "Downloading: %s -> %s", src.Remote(), tempPath)

	// åˆ›å»ºè¿›åº¦åŒ…è£…å™¨æ¥æ˜¾ç¤ºStep 1ä¸‹è½½è¿›åº¦
	progressReader := &TwoStepProgressReader115{
		Reader:          in,
		totalSize:       size,
		transferredSize: 0,
		startTime:       time.Now(),
		lastLogTime:     time.Time{},
		lastLogPercent:  -1,
		stepName:        "Step 1: ä¸‹è½½",
		fileName:        src.Remote(),
		object:          o,
	}

	written, err := io.Copy(tempFile, progressReader)
	if err != nil {
		return nil, fmt.Errorf("failed to download to temp file: %w", err)
	}

	// Verify download size
	if written != size {
		return nil, fmt.Errorf("download size mismatch: expected %d, got %d", size, written)
	}

	fs.Infof(o, "Step 1 completed: downloaded %s", fs.SizeSuffix(written))

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
	if _, err := tempFile.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to reset file pointer: %w", err)
	}

	// ç¬¬äºŒæ­¥ï¼šè®¡ç®—æœ¬åœ°æ–‡ä»¶çš„SHA1
	fs.Infof(o, "Calculating local file SHA1...")
	hasher := sha1.New()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("failed to calculate SHA1: %w", err)
	}
	sha1sum := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(o, "Local file SHA1: %s", sha1sum)

	// Set SHA1 to Object
	if o != nil {
		o.sha1sum = sha1sum
	}

	// Reset file pointer to beginning for upload
	if _, err := tempFile.Seek(0, io.SeekStart); err != nil {
		return nil, fmt.Errorf("failed to reset file pointer: %w", err)
	}

	// Step 3: Upload from local file to 115
	fs.Debugf(o, "Step 2: Upload from local to 115...")

	// é‡æ–°åˆå§‹åŒ–ä¸Šä¼ ï¼ˆä½¿ç”¨æ­£ç¡®çš„SHA1ï¼‰
	newUI, err := f.initUploadOpenAPI(ctx, size, o.remote, "", sha1sum, "", "", "")
	if err != nil {
		return nil, fmt.Errorf("failed to reinitialize upload: %w", err)
	}

	// Check if instant upload succeeded
	if newUI.GetStatus() == 2 {
		fs.Debugf(o, "Two-step transfer instant upload successful!")
		// Return special error to indicate instant upload success
		// Due to function signature limitations, we need to modify the calling method
		return nil, fmt.Errorf("INSTANT_UPLOAD_SUCCESS:%s", newUI.GetFileID())
	}

	// åˆ›å»ºOSSå®¢æˆ·ç«¯
	ossClient, err := f.newOSSClient()
	if err != nil {
		return nil, fmt.Errorf("failed to create OSS client: %w", err)
	}

	// ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åˆ›å»ºæ ‡å‡†çš„åˆ†ç‰‡å†™å…¥å™¨
	return f.newChunkWriterWithClient(ctx, src, newUI, tempFile, o, ossClient, options...)
}

// smartStreamHashTransfer115 115ç½‘ç›˜æ™ºèƒ½æµå¼å“ˆå¸Œä¼ è¾“
// æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼šå°æ–‡ä»¶å†…å­˜ç¼“å­˜ï¼Œå¤§æ–‡ä»¶æµå¼å“ˆå¸Œè®¡ç®—
func (f *Fs) smartStreamHashTransfer115(ctx context.Context, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	fileSize := src.Size()

	// æ™ºèƒ½ç­–ç•¥é€‰æ‹©
	if fileSize <= 100*1024*1024 { // 100MBä»¥ä¸‹ä½¿ç”¨å†…å­˜ç¼“å­˜
		fs.Infof(f, "ğŸ“ å°æ–‡ä»¶ä½¿ç”¨å†…å­˜ç¼“å­˜æ¨¡å¼: %s", fs.SizeSuffix(fileSize))
		return f.memoryHashTransfer115(ctx, src, remote, options...)
	} else {
		fs.Infof(f, "ğŸŒŠ å¤§æ–‡ä»¶ä½¿ç”¨æµå¼å“ˆå¸Œæ¨¡å¼: %s", fs.SizeSuffix(fileSize))
		return f.streamHashTransfer115(ctx, src, remote, options...)
	}
}

// memoryHashTransfer115 115ç½‘ç›˜å°æ–‡ä»¶å†…å­˜ç¼“å­˜å“ˆå¸Œä¼ è¾“
func (f *Fs) memoryHashTransfer115(ctx context.Context, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	// è½¬æ¢ä¸ºfs.Objectä»¥ä¾¿è°ƒç”¨Openæ–¹æ³•
	srcObj, ok := src.(fs.Object)
	if !ok {
		return nil, fmt.Errorf("source is not a valid fs.Object")
	}

	// æ‰“å¼€æºæ–‡ä»¶
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file: %w", err)
	}
	defer func() {
		if srcReader != nil {
			if closeErr := srcReader.Close(); closeErr != nil {
				fs.Debugf(f, "âš ï¸ å…³é—­æºæ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
		}
	}()

	// è¯»å–æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜å¹¶è®¡ç®—SHA1
	data, err := io.ReadAll(srcReader)
	if err != nil {
		return nil, fmt.Errorf("failed to read file data: %w", err)
	}

	// è®¡ç®—SHA1å“ˆå¸Œï¼ˆ115ç½‘ç›˜éœ€è¦SHA1ï¼‰
	hasher := sha1.New()
	hasher.Write(data)
	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil)) // 115ç½‘ç›˜éœ€è¦å¤§å†™SHA1

	fs.Infof(f, "ğŸ“Š å†…å­˜å“ˆå¸Œè®¡ç®—å®Œæˆ: %s, SHA1: %s", fs.SizeSuffix(int64(len(data))), sha1Hash)

	// å°è¯•ç§’ä¼ 
	leaf, dirID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºå¯¹è±¡ç”¨äºä¸Šä¼ 
	o := &Object{
		fs:      f,
		remote:  remote,
		size:    int64(len(data)),
		sha1sum: sha1Hash,
	}

	// å°è¯•å“ˆå¸Œä¸Šä¼ ï¼ˆç§’ä¼ ï¼‰
	gotIt, _, _, cleanup, err := f.tryHashUpload(ctx, bytes.NewReader(data), src, o, leaf, dirID, int64(len(data)), options...)
	if cleanup != nil {
		defer cleanup()
	}

	if err == nil && gotIt {
		fs.Infof(f, "ğŸš€ å†…å­˜å“ˆå¸Œè®¡ç®—åç§’ä¼ æˆåŠŸï¼")
		return o, nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®ç›´æ¥ä¸Šä¼ 
	fs.Infof(f, "â¬†ï¸ ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®ä¸Šä¼ ")
	return f.uploadFromMemory115(ctx, data, o, options...)
}

// streamHashTransfer115 115ç½‘ç›˜å¤§æ–‡ä»¶æµå¼å“ˆå¸Œä¼ è¾“
func (f *Fs) streamHashTransfer115(ctx context.Context, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	// è½¬æ¢ä¸ºfs.Objectä»¥ä¾¿è°ƒç”¨Openæ–¹æ³•
	srcObj, ok := src.(fs.Object)
	if !ok {
		return nil, fmt.Errorf("source is not a valid fs.Object")
	}

	// ç¬¬ä¸€éï¼šæµå¼è®¡ç®—SHA1å“ˆå¸Œ
	fs.Infof(f, "ğŸ”„ ç¬¬ä¸€éï¼šæµå¼è®¡ç®—æ–‡ä»¶å“ˆå¸Œ...")

	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file for hash calculation: %w", err)
	}
	defer srcReader.Close()

	// æµå¼è®¡ç®—SHA1ï¼Œä¸ä¿å­˜æ•°æ®
	hasher := sha1.New()
	// ä½¿ç”¨rcloneæ ‡å‡†çš„ç¼“å†²åŒºå¤§å°ï¼Œé¿å…é‡å¤åˆ†é…
	buffer := make([]byte, 64*1024) // 64KBç¼“å†²åŒºï¼Œç¬¦åˆrcloneæ ‡å‡†

	for {
		n, err := srcReader.Read(buffer)
		if n > 0 {
			hasher.Write(buffer[:n])
		}
		if err == io.EOF {
			break
		}
		if err != nil {
			return nil, fmt.Errorf("failed to read data for hash calculation: %w", err)
		}
	}

	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil)) // 115ç½‘ç›˜éœ€è¦å¤§å†™SHA1
	fs.Infof(f, "ğŸ“Š æµå¼å“ˆå¸Œè®¡ç®—å®Œæˆ: SHA1: %s", sha1Hash)

	// å°è¯•ç§’ä¼ 
	leaf, dirID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºå¯¹è±¡ç”¨äºä¸Šä¼ 
	o := &Object{
		fs:      f,
		remote:  remote,
		size:    src.Size(),
		sha1sum: sha1Hash,
	}

	// å°è¯•ç§’ä¼ ï¼šç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„SHA1å“ˆå¸Œè°ƒç”¨initUploadOpenAPI
	fs.Infof(f, "ğŸ” å°è¯•ä½¿ç”¨å·²è®¡ç®—SHA1è¿›è¡Œç§’ä¼ ...")
	ui, err := f.initUploadOpenAPI(ctx, src.Size(), leaf, dirID, sha1Hash, "", "", "")
	if err != nil {
		return nil, fmt.Errorf("failed to init upload for instant upload: %w", err)
	}

	// æ£€æŸ¥ç§’ä¼ çŠ¶æ€
	if ui.GetStatus() == 2 {
		fs.Infof(f, "ğŸš€ æµå¼å“ˆå¸Œè®¡ç®—åç§’ä¼ æˆåŠŸï¼")
		// è®¾ç½®å¯¹è±¡çš„å…ƒæ•°æ®
		o.id = ui.GetFileID()
		o.pickCode = ui.GetPickCode()
		o.hasMetaData = true
		return o, nil
	}

	// å¤„ç†éœ€è¦äºŒæ¬¡éªŒè¯çš„æƒ…å†µï¼ˆçŠ¶æ€7ï¼‰
	if ui.GetStatus() == 7 {
		fs.Infof(f, "ğŸ” éœ€è¦äºŒæ¬¡éªŒè¯ï¼Œè®¡ç®—ç­¾å...")
		signKey := ui.GetSignKey()
		signCheckRange := ui.GetSignCheck()

		// è®¡ç®—æŒ‡å®šèŒƒå›´çš„SHA1
		signVal, err := f.calculateRangeHashFromSource(ctx, srcObj, signCheckRange)
		if err != nil {
			return nil, fmt.Errorf("failed to calculate range hash for verification: %w", err)
		}

		// é‡æ–°æäº¤å¸¦ç­¾åçš„è¯·æ±‚
		ui, err = f.initUploadOpenAPI(ctx, src.Size(), leaf, dirID, sha1Hash, "", signKey, signVal)
		if err != nil {
			return nil, fmt.Errorf("failed to retry upload with signature: %w", err)
		}

		// å†æ¬¡æ£€æŸ¥ç§’ä¼ çŠ¶æ€
		if ui.GetStatus() == 2 {
			fs.Infof(f, "ğŸš€ äºŒæ¬¡éªŒè¯åç§’ä¼ æˆåŠŸï¼")
			o.id = ui.GetFileID()
			o.pickCode = ui.GetPickCode()
			o.hasMetaData = true
			return o, nil
		}
	}

	// ç§’ä¼ å¤±è´¥ï¼Œç¬¬äºŒéï¼šé‡æ–°ä¸‹è½½å¹¶æµå¼ä¸Šä¼ 
	fs.Infof(f, "â¬†ï¸ ç§’ä¼ å¤±è´¥ï¼Œç¬¬äºŒéï¼šé‡æ–°ä¸‹è½½å¹¶æµå¼ä¸Šä¼ ")
	return f.streamUploadWithHash115(ctx, srcObj, o, nil, options...)
}

// uploadFromMemory115 115ç½‘ç›˜ä»å†…å­˜æ•°æ®ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadFromMemory115(ctx context.Context, data []byte, o *Object, options ...fs.OpenOption) (fs.Object, error) {
	// ä½¿ç”¨ç°æœ‰çš„ä¸Šä¼ é€»è¾‘ï¼Œä¼ å…¥å†…å­˜æ•°æ®
	return f.upload(ctx, bytes.NewReader(data), o, o.remote, options...)
}

// smartStreamHashTransferWithReader115 115ç½‘ç›˜æ™ºèƒ½æµå¼å“ˆå¸Œä¼ è¾“ï¼ˆä½¿ç”¨ç°æœ‰Readerï¼‰
// æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼Œé¿å…é‡å¤ä¸‹è½½
func (f *Fs) smartStreamHashTransferWithReader115(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	fileSize := src.Size()

	// æ™ºèƒ½ç­–ç•¥é€‰æ‹©
	if fileSize <= 100*1024*1024 { // 100MBä»¥ä¸‹ä½¿ç”¨å†…å­˜ç¼“å­˜
		fs.Infof(f, "ğŸ“ å°æ–‡ä»¶ä½¿ç”¨å†…å­˜ç¼“å­˜æ¨¡å¼: %s", fs.SizeSuffix(fileSize))
		return f.memoryHashTransferWithReader115(ctx, in, src, remote, options...)
	} else {
		fs.Infof(f, "ğŸŒŠ å¤§æ–‡ä»¶ä½¿ç”¨æµå¼å“ˆå¸Œæ¨¡å¼: %s", fs.SizeSuffix(fileSize))
		return f.streamHashTransferWithReader115(ctx, in, src, remote, options...)
	}
}

// memoryHashTransferWithReader115 115ç½‘ç›˜å°æ–‡ä»¶å†…å­˜ç¼“å­˜å“ˆå¸Œä¼ è¾“ï¼ˆä½¿ç”¨ç°æœ‰Readerï¼‰
func (f *Fs) memoryHashTransferWithReader115(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	fs.Infof(f, "ğŸ”„ ä¼˜åŒ–å†…å­˜å“ˆå¸Œï¼šä½¿ç”¨ç°æœ‰æ•°æ®æµï¼Œé¿å…é‡å¤ä¸‹è½½ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(src.Size()))

	// è¯»å–æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜å¹¶è®¡ç®—SHA1
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("failed to read file data: %w", err)
	}

	// è®¡ç®—SHA1å“ˆå¸Œï¼ˆ115ç½‘ç›˜éœ€è¦SHA1ï¼‰
	hasher := sha1.New()
	hasher.Write(data)
	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil)) // 115ç½‘ç›˜éœ€è¦å¤§å†™SHA1

	fs.Infof(f, "ğŸ“Š å†…å­˜å“ˆå¸Œè®¡ç®—å®Œæˆ: %s, SHA1: %s", fs.SizeSuffix(int64(len(data))), sha1Hash)

	// å°è¯•ç§’ä¼ 
	leaf, dirID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºå¯¹è±¡ç”¨äºä¸Šä¼ 
	o := &Object{
		fs:      f,
		remote:  remote,
		size:    int64(len(data)),
		sha1sum: sha1Hash,
	}

	// å°è¯•å“ˆå¸Œä¸Šä¼ ï¼ˆç§’ä¼ ï¼‰
	gotIt, _, _, cleanup, err := f.tryHashUpload(ctx, bytes.NewReader(data), src, o, leaf, dirID, int64(len(data)), options...)
	if cleanup != nil {
		defer cleanup()
	}

	if err == nil && gotIt {
		fs.Infof(f, "ğŸš€ å†…å­˜å“ˆå¸Œè®¡ç®—åç§’ä¼ æˆåŠŸï¼")
		return o, nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®ç›´æ¥ä¸Šä¼ 
	fs.Infof(f, "â¬†ï¸ ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®ä¸Šä¼ ")
	return f.uploadFromMemory115(ctx, data, o, options...)
}

// streamHashTransferWithReader115 115ç½‘ç›˜å¤§æ–‡ä»¶æµå¼å“ˆå¸Œä¼ è¾“ï¼ˆä½¿ç”¨ç°æœ‰Readerï¼‰
func (f *Fs) streamHashTransferWithReader115(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ”„ ä¼˜åŒ–æµå¼å“ˆå¸Œï¼šä½¿ç”¨ç°æœ‰æ•°æ®æµï¼Œé¿å…é‡å¤ä¸‹è½½ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºç¼“å­˜æ•°æ®å’Œè®¡ç®—å“ˆå¸Œ
	tempFile, err := os.CreateTemp("", "rclone-115-stream-*.tmp")
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// åˆ›å»ºSHA1è®¡ç®—å™¨
	hasher := sha1.New()

	// ä½¿ç”¨MultiWriteråŒæ—¶å†™å…¥ä¸´æ—¶æ–‡ä»¶å’ŒSHA1è®¡ç®—å™¨
	multiWriter := io.MultiWriter(tempFile, hasher)

	// ä»ç°æœ‰çš„Readerè¯»å–æ•°æ®ï¼ŒåŒæ—¶è®¡ç®—å“ˆå¸Œå’Œç¼“å­˜
	_, err = io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("failed to stream data for hash calculation and caching: %w", err)
	}

	// è®¡ç®—SHA1å“ˆå¸Œ
	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil)) // 115ç½‘ç›˜éœ€è¦å¤§å†™SHA1
	fs.Infof(f, "ğŸ“Š æµå¼å“ˆå¸Œè®¡ç®—å®Œæˆ: SHA1: %s", sha1Hash)

	// å°è¯•ç§’ä¼ 
	leaf, dirID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºå¯¹è±¡ç”¨äºä¸Šä¼ 
	o := &Object{
		fs:      f,
		remote:  remote,
		size:    fileSize,
		sha1sum: sha1Hash,
	}

	// å°è¯•ç§’ä¼ ï¼šç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„SHA1å“ˆå¸Œè°ƒç”¨initUploadOpenAPI
	fs.Infof(f, "ğŸ” å°è¯•ä½¿ç”¨å·²è®¡ç®—SHA1è¿›è¡Œç§’ä¼ ...")
	ui, err := f.initUploadOpenAPI(ctx, fileSize, leaf, dirID, sha1Hash, "", "", "")
	if err != nil {
		return nil, fmt.Errorf("failed to init upload for instant upload: %w", err)
	}

	// æ£€æŸ¥ç§’ä¼ çŠ¶æ€
	if ui.GetStatus() == 2 {
		fs.Infof(f, "ğŸš€ æµå¼å“ˆå¸Œè®¡ç®—åç§’ä¼ æˆåŠŸï¼")
		// è®¾ç½®å¯¹è±¡çš„å…ƒæ•°æ®
		o.id = ui.GetFileID()
		o.pickCode = ui.GetPickCode()
		o.hasMetaData = true
		return o, nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜çš„ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
	fs.Infof(f, "â¬†ï¸ ç§’ä¼ å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜æ–‡ä»¶ä¸Šä¼ ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰")

	// é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("failed to seek temp file: %w", err)
	}

	// ä½¿ç”¨ç¼“å­˜æ–‡ä»¶è¿›è¡Œä¸Šä¼ 
	return f.upload(ctx, tempFile, o, o.remote, options...)
}

// streamUploadWithHash115 115ç½‘ç›˜ä½¿ç”¨å·²è®¡ç®—å“ˆå¸Œè¿›è¡Œæµå¼ä¸Šä¼ 
func (f *Fs) streamUploadWithHash115(ctx context.Context, srcObj fs.Object, o *Object, in io.Reader, options ...fs.OpenOption) (fs.Object, error) {
	// å¦‚æœæ²¡æœ‰è¾“å…¥æµï¼Œé‡æ–°æ‰“å¼€æºæ–‡ä»¶
	if in == nil {
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to reopen source file for upload: %w", err)
		}
		defer srcReader.Close()
		in = srcReader
	}

	// ä½¿ç”¨ç°æœ‰çš„ä¸Šä¼ é€»è¾‘
	return f.upload(ctx, in, o, o.remote, options...)
}

// calculateRangeHashFromSource ä»æºå¯¹è±¡è®¡ç®—æŒ‡å®šèŒƒå›´çš„SHA1å“ˆå¸Œ
// ä¿®å¤ï¼šä½¿ç”¨Rangeè¯·æ±‚åªä¸‹è½½æŒ‡å®šèŒƒå›´ï¼Œé¿å…å®Œæ•´ä¸‹è½½
func (f *Fs) calculateRangeHashFromSource(ctx context.Context, srcObj fs.Object, rangeSpec string) (string, error) {
	var start, end int64
	// OpenAPI range is "start-end" (inclusive)
	if n, err := fmt.Sscanf(rangeSpec, "%d-%d", &start, &end); err != nil || n != 2 {
		return "", fmt.Errorf("invalid range spec format %q: %w", rangeSpec, err)
	}
	if start < 0 || end < start {
		return "", fmt.Errorf("invalid range spec values %q", rangeSpec)
	}
	length := end - start + 1

	fs.Debugf(f, "ğŸ” è®¡ç®—äºŒæ¬¡éªŒè¯èŒƒå›´SHA1: èŒƒå›´=%s, å­—èŠ‚=%d-%d, é•¿åº¦=%s",
		rangeSpec, start, end, fs.SizeSuffix(length))

	// å…³é”®ä¿®å¤ï¼šä½¿ç”¨Rangeè¯·æ±‚åªä¸‹è½½æŒ‡å®šèŒƒå›´
	rangeOption := &fs.RangeOption{Start: start, End: end}
	srcReader, err := srcObj.Open(ctx, rangeOption)
	if err != nil {
		return "", fmt.Errorf("failed to open source file with range %s: %w", rangeSpec, err)
	}
	defer srcReader.Close()

	// ç›´æ¥è¯»å–Rangeè¯·æ±‚è¿”å›çš„æ•°æ®
	hasher := sha1.New()
	bytesRead, err := io.Copy(hasher, srcReader)
	if err != nil {
		return "", fmt.Errorf("failed to read range data for hash: %w", err)
	}

	fs.Debugf(f, "âœ… äºŒæ¬¡éªŒè¯èŒƒå›´SHA1è®¡ç®—å®Œæˆ: å®é™…è¯»å–=%s, é¢„æœŸé•¿åº¦=%s",
		fs.SizeSuffix(bytesRead), fs.SizeSuffix(length))

	if bytesRead != length {
		fs.Debugf(f, "âš ï¸ è¯»å–é•¿åº¦ä¸åŒ¹é…ï¼Œä½†ç»§ç»­è®¡ç®—SHA1: å®é™…=%d, é¢„æœŸ=%d", bytesRead, length)
	}

	sha1Hash := fmt.Sprintf("%X", hasher.Sum(nil))
	fs.Debugf(f, "ğŸ” äºŒæ¬¡éªŒè¯SHA1: %s (èŒƒå›´: %s)", sha1Hash, rangeSpec)

	return sha1Hash, nil
}
