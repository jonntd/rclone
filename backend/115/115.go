package _115

import (
	"bytes"
	"context"
	"crypto/rand"
	"crypto/sha256"
	"encoding/base64"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net"
	"net/http"
	"net/url"
	"os"
	"path"
	"path/filepath"
	"reflect"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/cenkalti/backoff/v4"
	"github.com/rclone/rclone/backend/115/api"
	"github.com/rclone/rclone/backend/115/crypto" // Keep for traditional calls
	"github.com/rclone/rclone/backend/115/dircache"
	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/accounting"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	"github.com/rclone/rclone/fs/fshttp"
	"github.com/rclone/rclone/fs/hash"
	"github.com/rclone/rclone/lib/cache"
	"github.com/rclone/rclone/lib/encoder"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/rest"
	"golang.org/x/oauth2"
)

// Constants
const (
	domain             = "www.115.com"
	traditionalRootURL = "https://webapi.115.com"
	openAPIRootURL     = "https://proapi.115.com"
	passportRootURL    = "https://passportapi.115.com"
	qrCodeAPIRootURL   = "https://qrcodeapi.115.com"
	hnQrCodeAPIRootURL = "https://hnqrcodeapi.115.com"     // For confirm step
	defaultAppID       = "100195123"                       // Provided App ID
	tradUserAgent      = "Mozilla/5.0 115Browser/27.0.7.5" // Keep for traditional login mimicry?
	defaultUserAgent   = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"

	// ğŸ”§ ä¼˜åŒ–çš„QPSé…ç½® - åŸºäº115ç½‘ç›˜APIç‰¹æ€§å’Œç¨³å®šæ€§è€ƒè™‘
	defaultGlobalMinSleep = fs.Duration(200 * time.Millisecond) // 5 QPS - é€šç”¨APIè°ƒç”¨é¢‘ç‡
	traditionalMinSleep   = fs.Duration(500 * time.Millisecond) // ~2 QPS - æå‡ä¼ ç»ŸAPIå“åº”é€Ÿåº¦
	uploadMinSleep        = fs.Duration(333 * time.Millisecond) // ~3 QPS - ä¸Šä¼ APIä¸“ç”¨ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
	downloadURLMinSleep   = fs.Duration(500 * time.Millisecond) // ~2 QPS - ä¸‹è½½URL APIä¸“ç”¨ï¼Œé¿å…åçˆ¬
	maxSleep              = 2 * time.Second
	decayConstant         = 2 // bigger for slower decay, exponential

	defaultConTimeout = fs.Duration(15 * time.Second)  // å‡å°‘è¿æ¥è¶…æ—¶ï¼Œå¿«é€Ÿå¤±è´¥é‡è¯•
	defaultTimeout    = fs.Duration(120 * time.Second) // å‡å°‘IOè¶…æ—¶ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…

	maxUploadSize       = 115 * fs.Gibi // 115 GiB from https://proapi.115.com/app/uploadinfo (or OpenAPI equivalent)
	maxUploadParts      = 10000         // Part number must be an integer between 1 and 10000, inclusive.
	defaultChunkSize    = 10 * fs.Mebi  // ä»5MBæé«˜åˆ°10MBï¼Œå‡å°‘åˆ†ç‰‡æ•°é‡ï¼Œæå‡æ€§èƒ½
	minChunkSize        = 100 * fs.Kibi
	maxChunkSize        = 5 * fs.Gibi  // Max part size for OSS
	defaultUploadCutoff = 50 * fs.Mebi // é™ä½åˆ°50MBï¼Œè®©æ›´å¤šæ–‡ä»¶ä½¿ç”¨å¤šçº¿ç¨‹ä¸Šä¼ 
	defaultNohashSize   = 100 * fs.Mebi
	StreamUploadLimit   = 5 * fs.Gibi // Max size for sample/streamed upload (traditional)
	maxUploadCutoff     = 5 * fs.Gibi // maximum allowed size for singlepart uploads (OSS PutObject limit)

	tokenRefreshWindow = 10 * time.Minute // Refresh token 10 minutes before expiry
	pkceVerifierLength = 64               // Length for PKCE code verifier
)

// TraditionalRequest is the standard 115.com request structure for traditional API
// ... existing code ...

// Register with Fs
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "115",
		Description: "115 drive (supports Open API)",
		NewFs:       NewFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name: "cookie",
			Help: `Provide the login cookie in the format "UID=...; CID=...; SEID=...;".
Required for initial login to obtain API tokens.
Example: "UID=123; CID=abc; SEID=def;"`,
			Required:  true,
			Sensitive: true,
		}, {
			Name:      "share_code",
			Help:      "Share code from share link (for accessing shared files via traditional API).",
			Sensitive: true,
		}, {
			Name:      "receive_code",
			Help:      "Password from share link (for accessing shared files via traditional API).",
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Default:  defaultUserAgent,
			Advanced: true,
			Help: fmt.Sprintf(`HTTP user agent. Primarily used for initial login mimicry.
Defaults to "%s".`, defaultUserAgent),
		}, {
			Name: "root_folder_id",
			Help: `ID of the root folder.
Leave blank normally (uses the drive root '0').
Fill in for rclone to use a non root folder as its starting point.`,
			Advanced:  true,
			Sensitive: true,
		}, {
			Name:     "app_id",
			Default:  defaultAppID,
			Advanced: true,
			Help: fmt.Sprintf(`Custom App ID for authentication.
Defaults to "%s". Only change this if you have a specific reason to use a different App ID.`, defaultAppID),
		}, {
			Name:     "list_chunk",
			Default:  1150, // Max limit for OpenAPI file list
			Help:     "Size of listing chunk.",
			Advanced: true,
		}, {
			Name:     "censored_only",
			Default:  false,
			Help:     "Only show files that are censored (only applies to traditional API calls).",
			Advanced: true,
		}, {
			Name:     "pacer_min_sleep",
			Default:  defaultGlobalMinSleep,
			Help:     "Minimum time to sleep between API calls (controls global QPS, default 5 QPS).",
			Advanced: true,
		}, {
			Name:     "contimeout",
			Default:  defaultConTimeout,
			Help:     "Connect timeout.",
			Advanced: true,
		}, {
			Name:     "timeout",
			Default:  defaultTimeout,
			Help:     "IO idle timeout.",
			Advanced: true,
		}, {
			Name:     "upload_hash_only",
			Default:  false,
			Advanced: true,
			Help: `Attempt hash-based upload (ç§’ä¼ ) only. Skip uploading if the server doesn't have the file.
Requires SHA1 hash to be available or calculable.`,
		}, {
			Name:     "only_stream",
			Default:  false,
			Advanced: true,
			Help:     `Use traditional streamed upload (sample upload) for all files up to 5GiB. Fails for larger files.`,
		}, {
			Name:     "fast_upload",
			Default:  false,
			Advanced: true,
			Help: `Upload strategy:
- Files <= nohash_size: Use traditional streamed upload.
- Files > nohash_size: Attempt hash-based upload (ç§’ä¼ ).
- If ç§’ä¼  fails and size <= 5GiB: Use traditional streamed upload.
- If ç§’ä¼  fails and size > 5GiB: Use multipart upload.`,
		}, {
			Name:     "hash_memory_limit",
			Help:     "Files bigger than this will be cached on disk to calculate hash if required.",
			Default:  fs.SizeSuffix(10 * 1024 * 1024),
			Advanced: true,
		}, {
			Name: "upload_cutoff",
			Help: `Cutoff for switching to multipart upload.
Any files larger than this will be uploaded in chunks using the OSS multipart API.
Minimum is 0, maximum is 5 GiB.`,
			Default:  defaultUploadCutoff,
			Advanced: true,
		}, {
			Name:     "nohash_size",
			Help:     `Files smaller than this size will use traditional streamed upload if fast_upload is enabled or if hash upload is not attempted/fails. Max is 5GiB.`,
			Default:  defaultNohashSize,
			Advanced: true,
		}, {
			Name: "chunk_size",
			Help: `Chunk size for multipart uploads.
Rclone will automatically increase the chunk size for large files to stay below the 10,000 parts limit.
Minimum is 100 KiB, maximum is 5 GiB.`,
			Default:  defaultChunkSize,
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     `Maximum number of parts in a multipart upload.`,
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name: "upload_concurrency",
			Help: `Concurrency for multipart uploads.
Number of chunks to upload in parallel. Set to 0 to disable multipart uploads.
Minimum is 1, maximum is 32.`,
			Default:  8, // ä»4æé«˜åˆ°8ï¼Œæå‡ä¸Šä¼ æ€§èƒ½
			Advanced: true,
		}, {
			Name:     "internal",
			Help:     `Use the internal OSS endpoint for uploads (requires appropriate network access).`,
			Default:  false,
			Advanced: true,
		}, {
			Name:     "dual_stack",
			Help:     `Use a dual-stack (IPv4/IPv6) OSS endpoint for uploads.`,
			Default:  false,
			Advanced: true,
		}, {
			Name:     "no_check",
			Default:  false,
			Advanced: true,
			Help:     "Disable post-upload check (avoids extra API call but reduces certainty).",
		}, {
			Name:     "no_buffer",
			Default:  false,
			Advanced: true,
			Help:     "Skip disk buffering for uploads.",
		}, {
			Name:     config.ConfigEncoding,
			Help:     config.ConfigEncodingHelp,
			Advanced: true,
			Default: (encoder.EncodeLtGt |
				encoder.EncodeDoubleQuote |
				encoder.EncodeLeftSpace |
				encoder.EncodeCtl |
				encoder.EncodeRightSpace |
				encoder.EncodeInvalidUtf8),
		}},
	})
}

func checkUploadChunkSize(cs fs.SizeSuffix) error {
	if cs < minChunkSize {
		return fmt.Errorf("%s is less than %s", cs, minChunkSize)
	}
	if cs > maxChunkSize {
		return fmt.Errorf("%s is greater than %s", cs, maxChunkSize)
	}
	return nil
}

func (f *Fs) setUploadChunkSize(cs fs.SizeSuffix) (old fs.SizeSuffix, err error) {
	err = checkUploadChunkSize(cs)
	if err == nil {
		old, f.opt.ChunkSize = f.opt.ChunkSize, cs
	}
	return
}

func checkUploadCutoff(cs fs.SizeSuffix) error {
	if cs > maxUploadCutoff {
		return fmt.Errorf("%s is greater than %s", cs, maxUploadCutoff)
	}
	return nil
}

func checkUploadConcurrency(concurrency int) error {
	if concurrency < 1 {
		return fmt.Errorf("upload_concurrency must be at least 1, got %d", concurrency)
	}
	if concurrency > 32 {
		return fmt.Errorf("upload_concurrency must be at most 32, got %d", concurrency)
	}
	return nil
}

func (f *Fs) setUploadCutoff(cs fs.SizeSuffix) (old fs.SizeSuffix, err error) {
	err = checkUploadCutoff(cs)
	if err == nil {
		old, f.opt.UploadCutoff = f.opt.UploadCutoff, cs
	}
	return
}

// Options defines the configuration of this backend
type Options struct {
	Cookie              string               `config:"cookie"` // Single cookie string now
	ShareCode           string               `config:"share_code"`
	ReceiveCode         string               `config:"receive_code"`
	UserAgent           string               `config:"user_agent"`
	RootFolderID        string               `config:"root_folder_id"`
	ListChunk           int                  `config:"list_chunk"`
	CensoredOnly        bool                 `config:"censored_only"`
	PacerMinSleep       fs.Duration          `config:"pacer_min_sleep"` // Global pacer setting
	ConTimeout          fs.Duration          `config:"contimeout"`
	Timeout             fs.Duration          `config:"timeout"`
	HashMemoryThreshold fs.SizeSuffix        `config:"hash_memory_limit"`
	UploadHashOnly      bool                 `config:"upload_hash_only"`
	OnlyStream          bool                 `config:"only_stream"`
	FastUpload          bool                 `config:"fast_upload"`
	UploadCutoff        fs.SizeSuffix        `config:"upload_cutoff"`
	NohashSize          fs.SizeSuffix        `config:"nohash_size"`
	ChunkSize           fs.SizeSuffix        `config:"chunk_size"`
	MaxUploadParts      int                  `config:"max_upload_parts"`
	UploadConcurrency   int                  `config:"upload_concurrency"`
	Internal            bool                 `config:"internal"`
	DualStack           bool                 `config:"dual_stack"`
	NoCheck             bool                 `config:"no_check"`
	NoBuffer            bool                 `config:"no_buffer"` // Skip disk buffering for uploads
	Enc                 encoder.MultiEncoder `config:"encoding"`
	AppID               string               `config:"app_id"` // Custom App ID for authentication
}

// Fs represents a remote 115 drive
type Fs struct {
	name          string
	originalName  string // Original config name without modifications
	root          string
	opt           Options
	features      *fs.Features
	tradClient    *rest.Client // Client for traditional (cookie, encrypted) API calls
	openAPIClient *rest.Client // Client for OpenAPI (token) calls
	dirCache      *dircache.DirCache
	globalPacer   *fs.Pacer // Controls overall QPS
	tradPacer     *fs.Pacer // Controls QPS for traditional calls only (subset of global)
	downloadPacer *fs.Pacer // Controls QPS for download URL API calls (ä¸“é—¨ç”¨äºè·å–ä¸‹è½½URLçš„APIè°ƒç”¨)
	uploadPacer   *fs.Pacer // Controls QPS for upload related API calls (ä¸“é—¨ç”¨äºä¸Šä¼ ç›¸å…³çš„APIè°ƒç”¨)
	rootFolder    string    // path of the absolute root
	rootFolderID  string
	appVer        string // parsed from user-agent; used in traditional calls
	userID        string // User ID from cookie/token
	userkey       string // User key from traditional uploadinfo (needed for traditional upload init signature)
	isShare       bool   // mark it is from shared or not
	fileObj       *fs.Object
	m             configmap.Mapper // config map for saving tokens

	// Token management
	tokenMu      sync.Mutex
	accessToken  string
	refreshToken string
	tokenExpiry  time.Time
	codeVerifier string // For PKCE
	tokenRenewer *oauthutil.Renew
	loginMu      sync.Mutex

	// è·¯å¾„ç¼“å­˜ä¼˜åŒ–
	pathCache *PathCache

	// BadgerDBæŒä¹…åŒ–ç¼“å­˜ç³»ç»Ÿ
	pathResolveCache *cache.BadgerCache // è·¯å¾„è§£æç¼“å­˜ (å·²å®ç°)
	dirListCache     *cache.BadgerCache // ç›®å½•åˆ—è¡¨ç¼“å­˜
	metadataCache    *cache.BadgerCache // æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜
	fileIDCache      *cache.BadgerCache // æ–‡ä»¶IDéªŒè¯ç¼“å­˜

	// ç¼“å­˜æ—¶é—´é…ç½®
	cacheConfig CacheConfig115

	// HTTPè¿æ¥æ± ä¼˜åŒ–
	httpClient *http.Client
}

// CacheConfig115 115ç½‘ç›˜ç¼“å­˜æ—¶é—´é…ç½®
type CacheConfig115 struct {
	PathResolveCacheTTL time.Duration // è·¯å¾„è§£æç¼“å­˜TTLï¼Œé»˜è®¤10åˆ†é’Ÿ
	DirListCacheTTL     time.Duration // ç›®å½•åˆ—è¡¨ç¼“å­˜TTLï¼Œé»˜è®¤5åˆ†é’Ÿ
	MetadataCacheTTL    time.Duration // æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜TTLï¼Œé»˜è®¤30åˆ†é’Ÿ
	FileIDCacheTTL      time.Duration // æ–‡ä»¶IDéªŒè¯ç¼“å­˜TTLï¼Œé»˜è®¤15åˆ†é’Ÿ
}

// DefaultCacheConfig115 è¿”å›115ç½‘ç›˜ä¼˜åŒ–çš„ç¼“å­˜é…ç½®
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šç»Ÿä¸€TTLç­–ç•¥ï¼Œé¿å…ç¼“å­˜ä¸ä¸€è‡´é—®é¢˜
func DefaultCacheConfig115() CacheConfig115 {
	unifiedTTL := 5 * time.Minute // ç»Ÿä¸€TTLç­–ç•¥ï¼Œä¸123ç½‘ç›˜ä¿æŒä¸€è‡´
	return CacheConfig115{
		PathResolveCacheTTL: unifiedTTL, // ä»15åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œç»Ÿä¸€TTL
		DirListCacheTTL:     unifiedTTL, // ä»10åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œç»Ÿä¸€TTL
		MetadataCacheTTL:    unifiedTTL, // ä»60åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œé¿å…é•¿æœŸç¼“å­˜ä¸ä¸€è‡´
		FileIDCacheTTL:      unifiedTTL, // ä»30åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œç»Ÿä¸€TTL
	}
}

// PathCacheEntry è·¯å¾„ç¼“å­˜æ¡ç›®
type PathCacheEntry struct {
	ID        string
	IsDir     bool
	Timestamp time.Time
	// LRUé“¾è¡¨èŠ‚ç‚¹
	prev, next *PathCacheEntry
	key        string // å­˜å‚¨keyç”¨äºä»mapä¸­åˆ é™¤
}

// PathCache LRUè·¯å¾„ç¼“å­˜å®ç°ï¼ŒO(1)æ—¶é—´å¤æ‚åº¦
type PathCache struct {
	cache   map[string]*PathCacheEntry
	mu      sync.RWMutex
	ttl     time.Duration
	maxSize int
	// LRUåŒå‘é“¾è¡¨
	head, tail *PathCacheEntry
}

// NewPathCache åˆ›å»ºæ–°çš„è·¯å¾„ç¼“å­˜
func NewPathCache(ttl time.Duration, maxSize int) *PathCache {
	pc := &PathCache{
		cache:   make(map[string]*PathCacheEntry),
		ttl:     ttl,
		maxSize: maxSize,
	}

	// åˆå§‹åŒ–LRUé“¾è¡¨çš„å“¨å…µèŠ‚ç‚¹
	pc.head = &PathCacheEntry{}
	pc.tail = &PathCacheEntry{}
	pc.head.next = pc.tail
	pc.tail.prev = pc.head

	return pc
}

// Get ä»ç¼“å­˜è·å–è·¯å¾„ä¿¡æ¯ï¼Œå®ç°LRUè®¿é—®
func (pc *PathCache) Get(path string) (*PathCacheEntry, bool) {
	pc.mu.Lock()
	defer pc.mu.Unlock()

	entry, exists := pc.cache[path]
	if !exists {
		return nil, false
	}

	// æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
	if time.Since(entry.Timestamp) > pc.ttl {
		// è¿‡æœŸçš„æ¡ç›®éœ€è¦ä»ç¼“å­˜ä¸­åˆ é™¤
		pc.removeFromList(entry)
		delete(pc.cache, path)
		return nil, false
	}

	// ç§»åŠ¨åˆ°é“¾è¡¨å¤´éƒ¨ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
	pc.moveToHead(entry)

	return entry, true
}

// addToHead å°†èŠ‚ç‚¹æ·»åŠ åˆ°é“¾è¡¨å¤´éƒ¨
func (pc *PathCache) addToHead(entry *PathCacheEntry) {
	entry.prev = pc.head
	entry.next = pc.head.next
	pc.head.next.prev = entry
	pc.head.next = entry
}

// removeFromList ä»é“¾è¡¨ä¸­ç§»é™¤èŠ‚ç‚¹
func (pc *PathCache) removeFromList(entry *PathCacheEntry) {
	if entry.prev != nil {
		entry.prev.next = entry.next
	}
	if entry.next != nil {
		entry.next.prev = entry.prev
	}
}

// moveToHead å°†èŠ‚ç‚¹ç§»åŠ¨åˆ°é“¾è¡¨å¤´éƒ¨
func (pc *PathCache) moveToHead(entry *PathCacheEntry) {
	pc.removeFromList(entry)
	pc.addToHead(entry)
}

// removeTail ç§»é™¤é“¾è¡¨å°¾éƒ¨èŠ‚ç‚¹
func (pc *PathCache) removeTail() *PathCacheEntry {
	lastEntry := pc.tail.prev
	if lastEntry == pc.head {
		return nil // é“¾è¡¨ä¸ºç©º
	}
	pc.removeFromList(lastEntry)
	return lastEntry
}

// Put å°†è·¯å¾„ä¿¡æ¯æ”¾å…¥ç¼“å­˜
func (pc *PathCache) Put(path, id string, isDir bool) {
	pc.mu.Lock()
	defer pc.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
	if existingEntry, exists := pc.cache[path]; exists {
		// æ›´æ–°ç°æœ‰æ¡ç›®å¹¶ç§»åŠ¨åˆ°å¤´éƒ¨
		existingEntry.ID = id
		existingEntry.IsDir = isDir
		existingEntry.Timestamp = time.Now()
		pc.moveToHead(existingEntry)
		return
	}

	// å¦‚æœç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€æ—§çš„æ¡ç›®
	if len(pc.cache) >= pc.maxSize {
		tailEntry := pc.removeTail()
		if tailEntry != nil && tailEntry.key != "" {
			delete(pc.cache, tailEntry.key)
		}
	}

	// åˆ›å»ºæ–°æ¡ç›®
	newEntry := &PathCacheEntry{
		ID:        id,
		IsDir:     isDir,
		Timestamp: time.Now(),
		key:       path,
	}

	// æ·»åŠ åˆ°ç¼“å­˜å’Œé“¾è¡¨å¤´éƒ¨
	pc.cache[path] = newEntry
	pc.addToHead(newEntry)
}

// Clear æ¸…ç©ºç¼“å­˜
func (pc *PathCache) Clear() {
	pc.mu.Lock()
	defer pc.mu.Unlock()

	// æ¸…ç©ºmap
	pc.cache = make(map[string]*PathCacheEntry)

	// é‡æ–°åˆå§‹åŒ–é“¾è¡¨
	pc.head.next = pc.tail
	pc.tail.prev = pc.head
}

// Delete åˆ é™¤æŒ‡å®šè·¯å¾„çš„ç¼“å­˜æ¡ç›®
func (pc *PathCache) Delete(path string) {
	pc.mu.Lock()
	defer pc.mu.Unlock()

	if entry, exists := pc.cache[path]; exists {
		delete(pc.cache, path)
		pc.removeFromList(entry)
	}
}

// ClearOldEntries æ¸…ç†æŒ‡å®šæ—¶é—´ä¹‹å‰çš„ç¼“å­˜æ¡ç›®
func (pc *PathCache) ClearOldEntries(maxAge time.Duration) {
	pc.mu.Lock()
	defer pc.mu.Unlock()

	cutoffTime := time.Now().Add(-maxAge)
	toDelete := make([]string, 0)

	// æ‰¾å‡ºéœ€è¦åˆ é™¤çš„æ¡ç›®
	for path, entry := range pc.cache {
		if entry.Timestamp.Before(cutoffTime) {
			toDelete = append(toDelete, path)
		}
	}

	// åˆ é™¤è¿‡æœŸæ¡ç›®
	for _, path := range toDelete {
		if entry, exists := pc.cache[path]; exists {
			delete(pc.cache, path)
			pc.removeFromList(entry)
		}
	}

	if len(toDelete) > 0 {
		fs.Debugf(nil, "æ¸…ç†äº† %d ä¸ªè¿‡æœŸçš„ç¼“å­˜æ¡ç›®", len(toDelete))
	}
}

// AsyncCacheUpdate å¼‚æ­¥æ›´æ–°ç¼“å­˜ï¼Œé¿å…é˜»å¡ä¸»è¦æ“ä½œ
func (f *Fs) AsyncCacheUpdate(ctx context.Context, path, id string, isDir bool) {
	select {
	case <-ctx.Done():
		fs.Debugf(f, "AsyncCacheUpdate cancelled for path: %s", path)
		return
	default:
		go func() {
			defer func() {
				if r := recover(); r != nil {
					fs.Debugf(f, "AsyncCacheUpdate panic for path %s: %v", path, r)
				}
			}()

			// ä½¿ç”¨å¸¦è¶…æ—¶çš„contexté˜²æ­¢goroutineé•¿æ—¶é—´è¿è¡Œ
			updateCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
			defer cancel()

			select {
			case <-updateCtx.Done():
				fs.Debugf(f, "AsyncCacheUpdate timeout for path: %s", path)
				return
			case <-ctx.Done():
				fs.Debugf(f, "AsyncCacheUpdate cancelled during execution for path: %s", path)
				return
			default:
				// æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼ˆé¿å…åœ¨æ¸…ç†åæ›´æ–°è„æ•°æ®ï¼‰
				if f.pathCache != nil {
					f.pathCache.Put(path, id, isDir)
					fs.Debugf(f, "AsyncCacheUpdate completed for path: %s", path)
				}
			}
		}()
	}
}

// AsyncDirCacheUpdate å¼‚æ­¥æ›´æ–°ç›®å½•ç¼“å­˜
func (f *Fs) AsyncDirCacheUpdate(ctx context.Context, path, id string) {
	select {
	case <-ctx.Done():
		fs.Debugf(f, "AsyncDirCacheUpdate cancelled for path: %s", path)
		return
	default:
		go func() {
			defer func() {
				if r := recover(); r != nil {
					fs.Debugf(f, "AsyncDirCacheUpdate panic for path %s: %v", path, r)
				}
			}()

			// ä½¿ç”¨å¸¦è¶…æ—¶çš„contexté˜²æ­¢goroutineé•¿æ—¶é—´è¿è¡Œ
			updateCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
			defer cancel()

			select {
			case <-updateCtx.Done():
				fs.Debugf(f, "AsyncDirCacheUpdate timeout for path: %s", path)
				return
			case <-ctx.Done():
				fs.Debugf(f, "AsyncDirCacheUpdate cancelled during execution for path: %s", path)
				return
			default:
				// æ£€æŸ¥ç›®å½•ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
				if f.dirCache != nil {
					f.dirCache.Put(path, id)
					fs.Debugf(f, "AsyncDirCacheUpdate completed for path: %s", path)
				}
			}
		}()
	}
}

// BatchFindRequest æ‰¹é‡æŸ¥æ‰¾è¯·æ±‚
type BatchFindRequest struct {
	PathID string
	Leaf   string
}

// BatchFindResult æ‰¹é‡æŸ¥æ‰¾ç»“æœ
type BatchFindResult struct {
	Request BatchFindRequest
	ID      string
	Found   bool
	IsDir   bool
	Error   error
}

// BatchFindLeaf æ‰¹é‡æŸ¥æ‰¾å¶èŠ‚ç‚¹ï¼Œä¼˜åŒ–å¤šä¸ªæŸ¥æ‰¾æ“ä½œ
func (f *Fs) BatchFindLeaf(ctx context.Context, requests []BatchFindRequest) ([]BatchFindResult, error) {
	results := make([]BatchFindResult, len(requests))

	// æŒ‰çˆ¶ç›®å½•IDåˆ†ç»„è¯·æ±‚
	groupedRequests := make(map[string][]int)
	for i, req := range requests {
		groupedRequests[req.PathID] = append(groupedRequests[req.PathID], i)
	}

	// å¯¹æ¯ä¸ªçˆ¶ç›®å½•æ‰§è¡Œä¸€æ¬¡listAllæ“ä½œ
	for pathID, indices := range groupedRequests {
		// åˆ›å»ºæŸ¥æ‰¾æ˜ å°„
		leafMap := make(map[string][]int)
		for _, idx := range indices {
			leaf := requests[idx].Leaf
			leafMap[leaf] = append(leafMap[leaf], idx)
		}

		// æ‰§è¡Œå•æ¬¡listAllæ“ä½œ
		_, err := f.listAll(ctx, pathID, f.opt.ListChunk, false, false, func(item *api.File) bool {
			decodedName := f.opt.Enc.ToStandardName(item.FileNameBest())
			if indices, exists := leafMap[decodedName]; exists {
				for _, idx := range indices {
					results[idx] = BatchFindResult{
						Request: requests[idx],
						ID:      item.ID(),
						Found:   true,
						IsDir:   item.IsDir(),
						Error:   nil,
					}
				}
				delete(leafMap, decodedName) // é¿å…é‡å¤å¤„ç†
			}
			return len(leafMap) == 0 // å¦‚æœæ‰€æœ‰é¡¹ç›®éƒ½æ‰¾åˆ°äº†å°±åœæ­¢
		})

		if err != nil {
			// å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œæ ‡è®°æ‰€æœ‰ç›¸å…³ç»“æœä¸ºé”™è¯¯
			for _, idx := range indices {
				if results[idx].Error == nil { // åªè®¾ç½®å°šæœªè®¾ç½®çš„é”™è¯¯
					results[idx] = BatchFindResult{
						Request: requests[idx],
						Found:   false,
						Error:   err,
					}
				}
			}
		}

		// æ ‡è®°æœªæ‰¾åˆ°çš„é¡¹ç›®
		for _, indices := range leafMap {
			for _, idx := range indices {
				if results[idx].Error == nil {
					results[idx] = BatchFindResult{
						Request: requests[idx],
						Found:   false,
						Error:   nil,
					}
				}
			}
		}
	}

	return results, nil
}

// isAPILimitError æ£€æŸ¥é”™è¯¯æ˜¯å¦ä¸ºAPIé™åˆ¶é”™è¯¯
func isAPILimitError(err error) bool {
	if err == nil {
		return false
	}
	errStr := err.Error()
	return strings.Contains(errStr, "770004") ||
		strings.Contains(errStr, "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™")
}

// isNetworkError æ£€æŸ¥æ˜¯å¦ä¸ºç½‘ç»œç›¸å…³é”™è¯¯
func isNetworkError(err error) bool {
	if err == nil {
		return false
	}
	errStr := strings.ToLower(err.Error())
	return strings.Contains(errStr, "connection refused") ||
		strings.Contains(errStr, "connection reset") ||
		strings.Contains(errStr, "connection timeout") ||
		strings.Contains(errStr, "network is unreachable") ||
		strings.Contains(errStr, "no route to host") ||
		strings.Contains(errStr, "dns") ||
		strings.Contains(errStr, "dial tcp") ||
		strings.Contains(errStr, "i/o timeout")
}

// isTemporaryError æ£€æŸ¥æ˜¯å¦ä¸ºä¸´æ—¶é”™è¯¯
func isTemporaryError(err error) bool {
	if err == nil {
		return false
	}

	// æ£€æŸ¥æ˜¯å¦å®ç°äº†Temporaryæ¥å£
	if temp, ok := err.(interface{ Temporary() bool }); ok {
		return temp.Temporary()
	}

	// æ£€æŸ¥é”™è¯¯å­—ç¬¦ä¸²ä¸­çš„ä¸´æ—¶é”™è¯¯æŒ‡ç¤º
	errStr := strings.ToLower(err.Error())
	return strings.Contains(errStr, "temporary") ||
		strings.Contains(errStr, "try again") ||
		strings.Contains(errStr, "service unavailable") ||
		strings.Contains(errStr, "internal server error")
}

// isSevereAPILimit æ£€æŸ¥æ˜¯å¦ä¸ºä¸¥é‡çš„APIé™åˆ¶é”™è¯¯
func isSevereAPILimit(err error) bool {
	if err == nil {
		return false
	}
	errStr := err.Error()
	// ä¸¥é‡çš„APIé™åˆ¶é€šå¸¸åŒ…å«ç‰¹å®šçš„é”™è¯¯ç æˆ–æ¶ˆæ¯
	return strings.Contains(errStr, "770004") || // ä¸¥é‡é™åˆ¶é”™è¯¯ç 
		strings.Contains(errStr, "è´¦å·å·²è¢«é™åˆ¶") ||
		strings.Contains(errStr, "è®¿é—®é¢‘ç‡è¿‡é«˜")
}

// clearCacheGradually æ ¹æ®é”™è¯¯ä¸¥é‡ç¨‹åº¦æ¸è¿›å¼æ¸…ç†ç¼“å­˜
func (f *Fs) clearCacheGradually(err error, affectedPath string) {
	if !isAPILimitError(err) {
		return
	}

	if isSevereAPILimit(err) {
		fs.Debugf(f, "ä¸¥é‡APIé™åˆ¶é”™è¯¯ï¼Œæ¸…ç†æ‰€æœ‰ç¼“å­˜: %v", err)
		f.dirCache.ResetRoot()
		f.pathCache.Clear()

		// æ¸…ç†æ‰€æœ‰BadgerDBæŒä¹…åŒ–ç¼“å­˜
		caches := []*cache.BadgerCache{
			f.pathResolveCache,
			f.dirListCache,
			f.metadataCache,
			f.fileIDCache,
		}

		for i, c := range caches {
			if c != nil {
				if clearErr := c.Clear(); clearErr != nil {
					fs.Debugf(f, "æ¸…ç†BadgerDBç¼“å­˜%då¤±è´¥: %v", i, clearErr)
				}
			}
		}
		fs.Debugf(f, "å·²æ¸…ç†æ‰€æœ‰BadgerDBæŒä¹…åŒ–ç¼“å­˜")
	} else {
		fs.Debugf(f, "è½»å¾®APIé™åˆ¶é”™è¯¯ï¼Œé‡‡ç”¨æ¸è¿›å¼ç¼“å­˜æ¸…ç†: %v", err)

		// åªæ¸…ç†å¯èƒ½å—å½±å“çš„ç‰¹å®šè·¯å¾„ç¼“å­˜
		if affectedPath != "" {
			// æ¸…ç†ç‰¹å®šè·¯å¾„çš„ç¼“å­˜
			f.pathCache.Delete(affectedPath)

			// æ¸…ç†BadgerDBä¸­å¯¹åº”çš„ç¼“å­˜
			if f.pathResolveCache != nil {
				cacheKey := fmt.Sprintf("path_%s", affectedPath)
				if delErr := f.pathResolveCache.Delete(cacheKey); delErr != nil {
					fs.Debugf(f, "æ¸…ç†BadgerDBè·¯å¾„ç¼“å­˜å¤±è´¥ %s: %v", affectedPath, delErr)
				}
			}

			// æ¸…ç†çˆ¶ç›®å½•çš„ç¼“å­˜
			parentPath := path.Dir(affectedPath)
			if parentPath != "." && parentPath != "/" {
				f.pathCache.Delete(parentPath)

				// æ¸…ç†BadgerDBä¸­çˆ¶ç›®å½•çš„ç¼“å­˜
				if f.pathResolveCache != nil {
					parentCacheKey := fmt.Sprintf("path_%s", parentPath)
					if delErr := f.pathResolveCache.Delete(parentCacheKey); delErr != nil {
						fs.Debugf(f, "æ¸…ç†BadgerDBçˆ¶ç›®å½•ç¼“å­˜å¤±è´¥ %s: %v", parentPath, delErr)
					}
				}
			}
		}

		// æ¸…ç†æœ€è¿‘5åˆ†é’Ÿçš„ç¼“å­˜æ¡ç›®ï¼ˆä¿ç•™è¾ƒè€çš„ç¨³å®šç¼“å­˜ï¼‰
		f.pathCache.ClearOldEntries(5 * time.Minute)

		// ä¸æ¸…ç†æ ¹ç›®å½•ç¼“å­˜ï¼Œé¿å…å®Œå…¨é‡å»º
		fs.Debugf(f, "ä¿ç•™æ ¹ç›®å½•ç¼“å­˜ï¼Œé¿å…å®Œå…¨é‡å»º")
	}
}

// cachePathResolution ç¼“å­˜è·¯å¾„è§£æç»“æœåˆ°BadgerDB
func (f *Fs) cachePathResolution(path, fileID string, isDir bool) {
	if f.pathResolveCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("path_%s", path)
	cacheValue := map[string]interface{}{
		"file_id": fileID,
		"is_dir":  isDir,
	}

	if err := f.pathResolveCache.Set(cacheKey, cacheValue, f.cacheConfig.PathResolveCacheTTL); err != nil {
		fs.Debugf(f, "ç¼“å­˜è·¯å¾„è§£æå¤±è´¥ %s: %v", path, err)
	} else {
		fs.Debugf(f, "å·²ç¼“å­˜è·¯å¾„è§£æ %s -> %s (dir: %v), TTL=%v", path, fileID, isDir, f.cacheConfig.PathResolveCacheTTL)
	}
}

// getCachedPathResolution ä»BadgerDBè·å–ç¼“å­˜çš„è·¯å¾„è§£æç»“æœ
func (f *Fs) getCachedPathResolution(path string) (string, bool, bool) {
	if f.pathResolveCache == nil {
		return "", false, false
	}

	cacheKey := fmt.Sprintf("path_%s", path)
	var cacheValue map[string]interface{}

	found, err := f.pathResolveCache.Get(cacheKey, &cacheValue)
	if err != nil {
		fs.Debugf(f, "è·å–ç¼“å­˜è·¯å¾„è§£æå¤±è´¥ %s: %v", path, err)
		return "", false, false
	}

	if !found {
		return "", false, false
	}

	fileID, ok1 := cacheValue["file_id"].(string)
	isDir, ok2 := cacheValue["is_dir"].(bool)

	if !ok1 || !ok2 {
		fs.Debugf(f, "ç¼“å­˜è·¯å¾„è§£ææ•°æ®æ ¼å¼é”™è¯¯ %s", path)
		return "", false, false
	}

	fs.Debugf(f, "ä»BadgerDBç¼“å­˜è·å–è·¯å¾„è§£æ %s -> %s (dir: %v)", path, fileID, isDir)
	return fileID, isDir, true
}

// DirListCacheEntry ç›®å½•åˆ—è¡¨ç¼“å­˜æ¡ç›®
type DirListCacheEntry struct {
	FileList   []api.File `json:"file_list"`
	ParentID   string     `json:"parent_id"`
	TotalCount int        `json:"total_count"`
	CachedAt   time.Time  `json:"cached_at"`
}

// DownloadURLCacheEntry ä¸‹è½½URLç¼“å­˜æ¡ç›®
type DownloadURLCacheEntry struct {
	URL       string    `json:"url"`
	ExpiresAt time.Time `json:"expires_at"`
	FileID    string    `json:"file_id"`
	PickCode  string    `json:"pick_code"`
	CachedAt  time.Time `json:"cached_at"`
}

// MetadataCacheEntry æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜æ¡ç›®
type MetadataCacheEntry struct {
	FileID   string    `json:"file_id"`
	Name     string    `json:"name"`
	Size     int64     `json:"size"`
	ModTime  time.Time `json:"mod_time"`
	IsDir    bool      `json:"is_dir"`
	SHA1     string    `json:"sha1"`
	PickCode string    `json:"pick_code"`
	CachedAt time.Time `json:"cached_at"`
}

// FileIDCacheEntry æ–‡ä»¶IDéªŒè¯ç¼“å­˜æ¡ç›®
type FileIDCacheEntry struct {
	FileID   string    `json:"file_id"`
	Exists   bool      `json:"exists"`
	IsDir    bool      `json:"is_dir"`
	CachedAt time.Time `json:"cached_at"`
}

// getDirListFromCache ä»ç¼“å­˜è·å–ç›®å½•åˆ—è¡¨
func (f *Fs) getDirListFromCache(parentID string) (*DirListCacheEntry, bool) {
	if f.dirListCache == nil {
		return nil, false
	}

	cacheKey := fmt.Sprintf("dirlist_%s", parentID)
	var entry DirListCacheEntry

	found, err := f.dirListCache.Get(cacheKey, &entry)
	if err != nil {
		fs.Debugf(f, "è·å–ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return nil, false
	}

	if found {
		fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å‘½ä¸­: parentID=%s, æ–‡ä»¶æ•°=%d", parentID, len(entry.FileList))
		return &entry, true
	}

	return nil, false
}

// saveDirListToCache ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜
func (f *Fs) saveDirListToCache(parentID string, fileList []api.File) {
	if f.dirListCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("dirlist_%s", parentID)
	entry := DirListCacheEntry{
		FileList:   fileList,
		ParentID:   parentID,
		TotalCount: len(fileList),
		CachedAt:   time.Now(),
	}

	// ä½¿ç”¨é…ç½®çš„ç›®å½•åˆ—è¡¨ç¼“å­˜TTL
	if err := f.dirListCache.Set(cacheKey, entry, f.cacheConfig.DirListCacheTTL); err != nil {
		fs.Debugf(f, "ä¿å­˜ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜: parentID=%s, æ–‡ä»¶æ•°=%d, TTL=%v", parentID, len(fileList), f.cacheConfig.DirListCacheTTL)
	}
}

// clearDirListCache æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜
func (f *Fs) clearDirListCache(parentID string) {
	if f.dirListCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("dirlist_%s", parentID)
	if err := f.dirListCache.Delete(cacheKey); err != nil {
		fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜: parentID=%s", parentID)
	}
}

// getMetadataFromCache ä»ç¼“å­˜è·å–æ–‡ä»¶å…ƒæ•°æ®
func (f *Fs) getMetadataFromCache(fileID string) (*MetadataCacheEntry, bool) {
	if f.metadataCache == nil {
		return nil, false
	}

	cacheKey := fmt.Sprintf("metadata_%s", fileID)
	var entry MetadataCacheEntry

	found, err := f.metadataCache.Get(cacheKey, &entry)
	if err != nil {
		fs.Debugf(f, "è·å–æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return nil, false
	}

	if found {
		fs.Debugf(f, "æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜å‘½ä¸­: fileID=%s", fileID)
		return &entry, true
	}

	return nil, false
}

// saveMetadataToCache ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®åˆ°ç¼“å­˜
func (f *Fs) saveMetadataToCache(fileID, name, sha1, pickCode string, size int64, modTime time.Time, isDir bool) {
	if f.metadataCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("metadata_%s", fileID)
	entry := MetadataCacheEntry{
		FileID:   fileID,
		Name:     name,
		Size:     size,
		ModTime:  modTime,
		IsDir:    isDir,
		SHA1:     sha1,
		PickCode: pickCode,
		CachedAt: time.Now(),
	}

	// ä½¿ç”¨é…ç½®çš„æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜TTL
	if err := f.metadataCache.Set(cacheKey, entry, f.cacheConfig.MetadataCacheTTL); err != nil {
		fs.Debugf(f, "ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®åˆ°ç¼“å­˜: fileID=%s, name=%s, TTL=%v", fileID, name, f.cacheConfig.MetadataCacheTTL)
	}
}

// clearMetadataCache æ¸…ç†æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜
func (f *Fs) clearMetadataCache(fileID string) {
	if f.metadataCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("metadata_%s", fileID)
	if err := f.metadataCache.Delete(cacheKey); err != nil {
		fs.Debugf(f, "æ¸…ç†æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜: fileID=%s", fileID)
	}
}

// getFileIDFromCache ä»ç¼“å­˜è·å–æ–‡ä»¶IDéªŒè¯ç»“æœ
func (f *Fs) getFileIDFromCache(fileID string) (bool, bool, bool) {
	if f.fileIDCache == nil {
		return false, false, false
	}

	cacheKey := fmt.Sprintf("file_id_%s", fileID)
	var entry FileIDCacheEntry

	found, err := f.fileIDCache.Get(cacheKey, &entry)
	if err != nil {
		fs.Debugf(f, "è·å–æ–‡ä»¶IDéªŒè¯ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return false, false, false
	}

	if found {
		fs.Debugf(f, "æ–‡ä»¶IDéªŒè¯ç¼“å­˜å‘½ä¸­: fileID=%s, exists=%v, isDir=%v", fileID, entry.Exists, entry.IsDir)
		return entry.Exists, entry.IsDir, true
	}

	return false, false, false
}

// saveFileIDToCache ä¿å­˜æ–‡ä»¶IDéªŒè¯ç»“æœåˆ°ç¼“å­˜
func (f *Fs) saveFileIDToCache(fileID string, exists, isDir bool) {
	if f.fileIDCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("file_id_%s", fileID)
	entry := FileIDCacheEntry{
		FileID:   fileID,
		Exists:   exists,
		IsDir:    isDir,
		CachedAt: time.Now(),
	}

	// ä½¿ç”¨é…ç½®çš„æ–‡ä»¶IDéªŒè¯ç¼“å­˜TTL
	if err := f.fileIDCache.Set(cacheKey, entry, f.cacheConfig.FileIDCacheTTL); err != nil {
		fs.Debugf(f, "ä¿å­˜æ–‡ä»¶IDéªŒè¯ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜æ–‡ä»¶IDéªŒè¯åˆ°ç¼“å­˜: fileID=%s, exists=%v, isDir=%v, TTL=%v", fileID, exists, isDir, f.cacheConfig.FileIDCacheTTL)
	}
}

// clearFileIDCache æ¸…ç†æ–‡ä»¶IDéªŒè¯ç¼“å­˜
func (f *Fs) clearFileIDCache(fileID string) {
	if f.fileIDCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("file_id_%s", fileID)
	if err := f.fileIDCache.Delete(cacheKey); err != nil {
		fs.Debugf(f, "æ¸…ç†æ–‡ä»¶IDéªŒè¯ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†æ–‡ä»¶IDéªŒè¯ç¼“å­˜: fileID=%s", fileID)
	}
}

// invalidateRelatedCaches115 æ™ºèƒ½ç¼“å­˜å¤±æ•ˆ - æ ¹æ®æ“ä½œç±»å‹æ¸…ç†ç›¸å…³ç¼“å­˜
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šç®€åŒ–ç¼“å­˜å¤±æ•ˆé€»è¾‘ï¼Œæé«˜å¯é æ€§
func (f *Fs) invalidateRelatedCaches115(path string, operation string) {

	// ğŸš€ ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼šå¯¹æ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½è¿›è¡Œå…¨é¢ç¼“å­˜æ¸…ç†
	// è™½ç„¶ä¼šå½±å“ä¸€äº›æ€§èƒ½ï¼Œä½†ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œé¿å…å¤æ‚çš„æ¡ä»¶åˆ¤æ–­

	switch operation {
	case "upload", "put", "mkdir", "delete", "remove", "rmdir", "rename", "move":
		// ç»Ÿä¸€å¤„ç†ï¼šæ¸…ç†å½“å‰è·¯å¾„å’Œçˆ¶è·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
		f.clearAllRelatedCaches115(path)

		// é¢å¤–æ¸…ç†çˆ¶ç›®å½•ç¼“å­˜
		parentPath := filepath.Dir(path)
		if parentPath != "." && parentPath != "/" {
			f.clearAllRelatedCaches115(parentPath)
		}

		fs.Debugf(f, "å·²æ¸…ç†è·¯å¾„ %s å’Œçˆ¶è·¯å¾„ %s çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜", path, parentPath)
	}
}

// clearAllRelatedCaches115 æ¸…ç†æŒ‡å®šè·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šæ–°å¢ç»Ÿä¸€ç¼“å­˜æ¸…ç†å‡½æ•°
func (f *Fs) clearAllRelatedCaches115(path string) {
	// æ¸…ç†å†…å­˜è·¯å¾„ç¼“å­˜
	f.pathCache.Clear()

	// æ¸…ç†æ‰€æœ‰BadgerDBæŒä¹…åŒ–ç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šå…¨éƒ¨æ¸…ç†ï¼‰
	caches := []*cache.BadgerCache{
		f.pathResolveCache,
		f.dirListCache,
		f.metadataCache,
		f.fileIDCache,
	}

	for _, c := range caches {
		if c != nil {
			c.Clear()
		}
	}

	// æ¸…ç†dircache
	f.dirCache.ResetRoot()
}

// Object describes a 115 object
type Object struct {
	fs          *Fs
	remote      string
	hasMetaData bool
	id          string
	parent      string
	size        int64
	sha1sum     string
	pickCode    string
	modTime     time.Time
	durl        *api.DownloadURL // link to download the object
	durlMu      *sync.Mutex
}
type ApiResponse struct {
	State   bool                `json:"state"`   // Indicates success or failure
	Message string              `json:"message"` // Optional message
	Code    int                 `json:"code"`    // Status code
	Data    map[string]FileInfo `json:"data"`    // Map where keys are file IDs (strings) and values are FileInfo objects
}

// FileInfo represents the details of a single file.
// The key for this object in the parent map (ApiResponse.Data) is the file ID.
type FileInfo struct {
	FileName string      `json:"file_name"` // Name of the file
	FileSize int64       `json:"file_size"` // Size of the file in bytes (using int64 for potentially large files)
	PickCode string      `json:"pick_code"` // File pick code (extraction code)
	SHA1     string      `json:"sha1"`      // SHA1 hash of the file content
	URL      DownloadURL `json:"url"`       // Object containing the download URL
}
type DownloadURL struct {
	URL string `json:"url"` // The file download address
}

// retryErrorCodes is a slice of HTTP status codes that we will retry
var retryErrorCodes = []int{
	429, // Too Many Requests.
	500, // Internal Server Error
	502, // Bad Gateway
	503, // Service Unavailable
	504, // Gateway Timeout
	509, // Bandwidth Limit Exceeded
}

// UnifiedErrorClassifier ç»Ÿä¸€é”™è¯¯åˆ†ç±»å™¨
type UnifiedErrorClassifier struct {
	// é”™è¯¯åˆ†ç±»ç»Ÿè®¡
	ServerOverloadCount int64
	URLExpiredCount     int64
	NetworkTimeoutCount int64
	RateLimitCount      int64
	UnknownErrorCount   int64
}

// ClassifyError ç»Ÿä¸€é”™è¯¯åˆ†ç±»æ–¹æ³•
func (c *UnifiedErrorClassifier) ClassifyError(err error) string {
	if err == nil {
		return "no_error"
	}

	errStr := strings.ToLower(err.Error())

	// æœåŠ¡å™¨è¿‡è½½é”™è¯¯
	if strings.Contains(errStr, "502") || strings.Contains(errStr, "bad gateway") ||
		strings.Contains(errStr, "503") || strings.Contains(errStr, "service unavailable") ||
		strings.Contains(errStr, "504") || strings.Contains(errStr, "gateway timeout") {
		c.ServerOverloadCount++
		return "server_overload"
	}

	// URLè¿‡æœŸé”™è¯¯
	if strings.Contains(errStr, "download url is invalid") || strings.Contains(errStr, "expired") ||
		strings.Contains(errStr, "urlè¿‡æœŸ") || strings.Contains(errStr, "é“¾æ¥å¤±æ•ˆ") {
		c.URLExpiredCount++
		return "url_expired"
	}

	// ç½‘ç»œè¶…æ—¶é”™è¯¯
	if strings.Contains(errStr, "timeout") || strings.Contains(errStr, "deadline exceeded") ||
		strings.Contains(errStr, "context canceled") || strings.Contains(errStr, "connection reset") {
		c.NetworkTimeoutCount++
		return "network_timeout"
	}

	// é™æµé”™è¯¯
	if strings.Contains(errStr, "429") || strings.Contains(errStr, "too many requests") ||
		strings.Contains(errStr, "rate limit") || strings.Contains(errStr, "770004") ||
		strings.Contains(errStr, "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™") {
		c.RateLimitCount++
		return "rate_limit"
	}

	// è®¤è¯é”™è¯¯
	if strings.Contains(errStr, "401") || strings.Contains(errStr, "unauthorized") ||
		strings.Contains(errStr, "token") || strings.Contains(errStr, "è®¤è¯å¤±è´¥") {
		return "auth_error"
	}

	// æƒé™é”™è¯¯
	if strings.Contains(errStr, "403") || strings.Contains(errStr, "forbidden") ||
		strings.Contains(errStr, "permission denied") {
		return "permission_error"
	}

	// èµ„æºä¸å­˜åœ¨é”™è¯¯
	if strings.Contains(errStr, "404") || strings.Contains(errStr, "not found") ||
		strings.Contains(errStr, "file not found") {
		return "not_found"
	}

	c.UnknownErrorCount++
	return "unknown"
}

// shouldRetry checks if a request should be retried based on the response, error, and API type.
// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€é”™è¯¯åˆ†ç±»å’Œé‡è¯•ç­–ç•¥
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	if fserrors.ContextError(ctx, &err) {
		return false, err
	}

	// Check HTTP response status codes first
	if resp != nil {
		switch resp.StatusCode {
		case 429: // Too Many Requests
			fs.Debugf(nil, "HTTP 429 Too Many Requests, retrying with delay")
			return true, pacer.RetryAfterError(err, 15*time.Second)
		case 500, 502, 503, 504: // Server errors
			fs.Debugf(nil, "Server error %d, retrying: %v", resp.StatusCode, err)
			return true, pacer.RetryAfterError(err, 5*time.Second)
		case 408: // Request Timeout
			fs.Debugf(nil, "Request timeout, retrying: %v", err)
			return true, pacer.RetryAfterError(err, 3*time.Second)
		}
	}

	// Check for specific error types and messages
	if err != nil {
		// Check for API rate limit by error message
		if isAPILimitError(err) {
			fs.Debugf(nil, "API rate limit detected, retrying: %v", err)
			return true, pacer.RetryAfterError(err, 10*time.Second)
		}

		// Check for network-related errors
		if isNetworkError(err) {
			fs.Debugf(nil, "Network error detected, retrying: %v", err)
			return true, pacer.RetryAfterError(err, 2*time.Second)
		}

		// Check for temporary errors
		if isTemporaryError(err) {
			fs.Debugf(nil, "Temporary error detected, retrying: %v", err)
			return true, pacer.RetryAfterError(err, 1*time.Second)
		}
	}

	// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€é”™è¯¯åˆ†ç±»å™¨
	classifier := &UnifiedErrorClassifier{}
	errorType := classifier.ClassifyError(err)

	// åŸºäºé”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
	switch errorType {
	case "server_overload", "network_timeout":
		// æœåŠ¡å™¨è¿‡è½½å’Œç½‘ç»œè¶…æ—¶é”™è¯¯åº”è¯¥é‡è¯•
		return true, err

	case "rate_limit":
		// é™æµé”™è¯¯åº”è¯¥é‡è¯•ï¼Œä½†ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
		return true, fserrors.NewErrorRetryAfter(30 * time.Second)

	case "url_expired":
		// URLè¿‡æœŸé”™è¯¯åº”è¯¥é‡è¯•ï¼ˆä¼šè§¦å‘URLåˆ·æ–°ï¼‰
		return true, err

	case "auth_error":
		// è®¤è¯é”™è¯¯ä¸é‡è¯•ï¼Œè®©ä¸Šå±‚å¤„ç†tokenåˆ·æ–°
		var apiErr *api.TokenError
		if errors.As(err, &apiErr) {
			return false, err
		}
		// å…¶ä»–è®¤è¯é”™è¯¯ä¹Ÿä¸é‡è¯•
		return false, err

	case "permission_error", "not_found":
		// æƒé™é”™è¯¯å’Œèµ„æºä¸å­˜åœ¨é”™è¯¯ä¸é‡è¯•
		return false, err

	default:
		// æœªçŸ¥é”™è¯¯ä½¿ç”¨rcloneæ ‡å‡†é‡è¯•é€»è¾‘
		if fserrors.ShouldRetry(err) {
			return true, err
		}

		// HTTPçŠ¶æ€ç é‡è¯•ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
		return fserrors.ShouldRetryHTTP(resp, retryErrorCodes), err
	}
}

// ------------------------------------------------------------
// Authentication and Client Setup
// ------------------------------------------------------------

// Credential holds the parsed cookie values needed for initial login
type Credential struct {
	UID  string
	CID  string
	SEID string
	KID  string // Keep KID as it might be used implicitly by the web API calls
}

// Valid reports whether the credential is valid.
func (cr *Credential) Valid() error {
	if cr == nil {
		return errors.New("nil credential")
	}
	// KID is optional/sometimes empty, SEID seems required for login mimicry
	if cr.UID == "" || cr.CID == "" || cr.SEID == "" {
		return errors.New("missing UID, CID, or SEID in cookie")
	}
	return nil
}

// FromCookie loads credential from cookie string
func (cr *Credential) FromCookie(cookieStr string) *Credential {
	for _, item := range strings.Split(cookieStr, ";") {
		kv := strings.SplitN(strings.TrimSpace(item), "=", 2)
		if len(kv) != 2 {
			continue
		}
		key := strings.TrimSpace(strings.ToUpper(kv[0]))
		val := strings.TrimSpace(kv[1])
		switch key {
		case "UID":
			cr.UID = val
		case "CID":
			cr.CID = val
		case "SEID":
			cr.SEID = val
		case "KID":
			cr.KID = val
		}
	}
	return cr
}

// Cookie turns the credential into a list of http cookie for the traditional client
func (cr *Credential) Cookie() []*http.Cookie {
	cookies := []*http.Cookie{
		{Name: "UID", Value: cr.UID, Domain: domain, Path: "/", HttpOnly: true},
		{Name: "CID", Value: cr.CID, Domain: domain, Path: "/", HttpOnly: true},
		{Name: "SEID", Value: cr.SEID, Domain: domain, Path: "/", HttpOnly: true},
	}
	// Add KID only if it's present
	if cr.KID != "" {
		cookies = append(cookies, &http.Cookie{Name: "KID", Value: cr.KID, Domain: domain, Path: "/", HttpOnly: true})
	}
	return cookies
}

// UserID parses userID from UID field
func (cr *Credential) UserID() string {
	userID, _, _ := strings.Cut(cr.UID, "_")
	return userID
}

// getHTTPClient makes an http client according to the options with optimized connection pool
func getHTTPClient(ctx context.Context, opt *Options) *http.Client {
	t := fshttp.NewTransportCustom(ctx, func(t *http.Transport) {
		t.TLSHandshakeTimeout = time.Duration(opt.ConTimeout)
		t.ResponseHeaderTimeout = time.Duration(opt.Timeout)

		// ä¼˜åŒ–è¿æ¥æ± é…ç½®
		t.MaxIdleConns = 100                 // æœ€å¤§ç©ºé—²è¿æ¥æ•°
		t.MaxIdleConnsPerHost = 20           // æ¯ä¸ªä¸»æœºçš„æœ€å¤§ç©ºé—²è¿æ¥æ•°
		t.MaxConnsPerHost = 50               // æ¯ä¸ªä¸»æœºçš„æœ€å¤§è¿æ¥æ•°
		t.IdleConnTimeout = 90 * time.Second // ç©ºé—²è¿æ¥è¶…æ—¶
		t.DisableKeepAlives = false          // å¯ç”¨Keep-Alive
		t.ForceAttemptHTTP2 = true           // å¼ºåˆ¶å°è¯•HTTP/2

		// ä¼˜åŒ–è¶…æ—¶è®¾ç½®
		t.DialContext = (&net.Dialer{
			Timeout:   30 * time.Second,
			KeepAlive: 30 * time.Second,
		}).DialContext
		t.ExpectContinueTimeout = 1 * time.Second
	})

	return &http.Client{
		Transport: t,
		Timeout:   time.Duration(opt.Timeout) + 60*time.Second, // å¢åŠ ç¼“å†²æ—¶é—´åˆ°60ç§’ï¼Œæ”¯æŒè·¨äº‘ç›˜å¤§æ–‡ä»¶ä¼ è¾“
	}
}

// getTradHTTPClient creates an HTTP client with traditional UserAgent
func getTradHTTPClient(ctx context.Context, opt *Options) *http.Client {
	// Create a new context with the traditional UserAgent
	newCtx, ci := fs.AddConfig(ctx)
	ci.UserAgent = tradUserAgent
	return getHTTPClient(newCtx, opt)
}

// getOpenAPIHTTPClient creates an HTTP client with default UserAgent
func getOpenAPIHTTPClient(ctx context.Context, opt *Options) *http.Client {
	// Create a new context with the default UserAgent
	newCtx, ci := fs.AddConfig(ctx)
	ci.UserAgent = defaultUserAgent
	return getHTTPClient(newCtx, opt)
}

// errorHandler parses a non 2xx error response into an error (Generic, might need adjustment per API)
func errorHandler(resp *http.Response) error {
	// Attempt to decode as OpenAPI error first
	openAPIErr := new(api.OpenAPIBase)
	bodyBytes, readErr := rest.ReadBody(resp) // Read body once
	if readErr != nil {
		fs.Debugf(nil, "Couldn't read error response body: %v", readErr)
		// Fallback to status code if body read fails
		return api.NewTokenError(fmt.Sprintf("HTTP error %d (%s)", resp.StatusCode, resp.Status))
	}

	decodeErr := json.Unmarshal(bodyBytes, &openAPIErr)
	if decodeErr == nil && !openAPIErr.State {
		// Successfully decoded as OpenAPI error
		err := openAPIErr.Err()
		// Check for specific token-related errors
		if openAPIErr.ErrCode() == 401 || openAPIErr.ErrCode() == 100001 || strings.Contains(openAPIErr.ErrMsg(), "token") { // Example codes
			return api.NewTokenError(err.Error(), true) // Assume token error needs refresh/relogin
		}
		return err
	}

	// Attempt to decode as Traditional error
	tradErr := new(api.TraditionalBase)
	decodeErr = json.Unmarshal(bodyBytes, &tradErr)
	if decodeErr == nil && !tradErr.State {
		// Successfully decoded as Traditional error
		return tradErr.Err()
	}

	// Fallback if JSON decoding fails or state is true (but status code != 2xx)
	fs.Debugf(nil, "Couldn't decode error response: %v. Body: %s", decodeErr, string(bodyBytes))
	return api.NewTokenError(fmt.Sprintf("HTTP error %d (%s): %s", resp.StatusCode, resp.Status, string(bodyBytes)))
}

// generatePKCE generates a code_verifier and code_challenge
func generatePKCE() (verifier, challenge string, err error) {
	verifierBytes := make([]byte, pkceVerifierLength)
	_, err = rand.Read(verifierBytes)
	if err != nil {
		return "", "", fmt.Errorf("failed to generate random verifier: %w", err)
	}
	// Use URL-safe base64 encoding without padding
	verifier = base64.RawURLEncoding.EncodeToString(verifierBytes)
	// Calculate SHA256 hash
	hash := sha256.Sum256([]byte(verifier))
	// Base64 encode the hash
	challenge = base64.RawURLEncoding.EncodeToString(hash[:])
	return verifier, challenge, nil
}

// login performs the initial authentication flow to get tokens
func (f *Fs) login(ctx context.Context) error {
	// Use a global mutex for login to prevent multiple concurrent login attempts
	// which could overwhelm the API and cause authentication failures
	f.loginMu.Lock()
	defer f.loginMu.Unlock()

	// Check if login is needed (avoid nested locking)
	needLogin := func() bool {
		f.tokenMu.Lock()
		defer f.tokenMu.Unlock()
		return f.accessToken == "" || time.Now().After(f.tokenExpiry)
	}()

	if !needLogin {
		fs.Debugf(f, "Token is already valid after waiting for login mutex, skipping login")
		return nil
	}

	// Parse cookie and setup clients
	if err := f.setupLoginEnvironment(ctx); err != nil {
		return err
	}

	fs.Debugf(f, "Starting login process for user %s", f.userID)

	// Generate PKCE
	var challenge string
	var err error
	if f.codeVerifier, challenge, err = generatePKCE(); err != nil {
		return fmt.Errorf("failed to generate PKCE codes: %w", err)
	}
	fs.Debugf(f, "Generated PKCE challenge")

	// Request device code
	loginUID, err := f.getAuthDeviceCode(ctx, challenge)
	if err != nil {
		return err
	}

	// Mimic QR scan confirmation
	if err := f.simulateQRCodeScan(ctx, loginUID); err != nil {
		// Only log errors from these steps, don't fail
		fs.Logf(f, "QR code scan simulation steps had errors (continuing anyway): %v", err)
	}

	// Exchange device code for access token
	if err := f.exchangeDeviceCodeForToken(ctx, loginUID); err != nil {
		return err
	}

	// Get userkey for traditional uploads if needed
	if f.userkey == "" {
		fs.Debugf(f, "Fetching userkey using traditional API...")
		if err := f.getUploadBasicInfo(ctx); err != nil {
			// Log error but don't fail login, userkey is only for traditional upload init
			fs.Logf(f, "Failed to get userkey (needed for some traditional uploads): %v", err)
		} else {
			fs.Debugf(f, "Successfully fetched userkey.")
		}
	}

	return nil
}

// setupLoginEnvironment parses cookies and sets up HTTP clients
func (f *Fs) setupLoginEnvironment(ctx context.Context) error {
	// Parse cookie
	cred := (&Credential{}).FromCookie(f.opt.Cookie)
	if err := cred.Valid(); err != nil {
		return fmt.Errorf("invalid cookie provided: %w", err)
	}
	f.userID = cred.UserID() // Set userID early

	// Setup clients (needed for the login calls)
	// Create separate clients for each API type with different User-Agents
	tradHTTPClient := getTradHTTPClient(ctx, &f.opt)
	openAPIHTTPClient := getOpenAPIHTTPClient(ctx, &f.opt)

	// Traditional client (uses cookie)
	f.tradClient = rest.NewClient(tradHTTPClient).
		SetRoot(traditionalRootURL).
		SetCookie(cred.Cookie()...).
		SetErrorHandler(errorHandler)

	// OpenAPI client (will have token set later)
	f.openAPIClient = rest.NewClient(openAPIHTTPClient).
		SetRoot(openAPIRootURL).
		SetErrorHandler(errorHandler)

	return nil
}

// getAuthDeviceCode calls the authDeviceCode API to start the login process
func (f *Fs) getAuthDeviceCode(ctx context.Context, challenge string) (string, error) {
	// Use configured AppID if provided, otherwise use default
	clientID := f.opt.AppID

	authData := url.Values{
		"client_id":             {clientID},
		"code_challenge":        {challenge},
		"code_challenge_method": {"sha256"}, // Use SHA256
	}
	authOpts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL, // Use passport API domain
		Path:         "/open/authDeviceCode",
		Parameters:   authData, // Send as query parameters for POST? Docs say body, let's try body.
		Body:         strings.NewReader(authData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}
	var authResp api.AuthDeviceCodeResp
	// This initial call uses the traditional client *but doesn't need encryption*
	// It still needs the cookie and pacing.
	err := f.CallTraditionalAPI(ctx, &authOpts, nil, &authResp, true) // Pass skipEncrypt=true
	if err != nil {
		return "", fmt.Errorf("authDeviceCode failed: %w", err)
	}
	if authResp.Data == nil || authResp.Data.UID == "" {
		return "", fmt.Errorf("authDeviceCode returned empty data: %v", authResp)
	}
	loginUID := authResp.Data.UID
	fs.Debugf(f, "authDeviceCode successful, login UID: %s", loginUID)
	return loginUID, nil
}

// simulateQRCodeScan mimics the QR code scan and confirm process
func (f *Fs) simulateQRCodeScan(ctx context.Context, loginUID string) error {
	// Call QR scan API
	scanErr := f.callQRScanAPI(ctx, loginUID)

	// Call QR confirm API - still proceed if scan had an error
	confirmErr := f.callQRConfirmAPI(ctx, loginUID)

	// Add a small delay after mimic steps, just in case
	time.Sleep(1 * time.Second)

	// Return an error if both steps failed, otherwise continue
	if scanErr != nil && confirmErr != nil {
		return fmt.Errorf("both scan and confirm steps failed: scan: %v, confirm: %v", scanErr, confirmErr)
	}
	return nil
}

// callQRScanAPI calls the QR scan API
func (f *Fs) callQRScanAPI(ctx context.Context, loginUID string) error {
	scanPayload := map[string]string{"uid": loginUID}
	scanOpts := rest.Opts{
		Method:     "GET",
		RootURL:    qrCodeAPIRootURL,
		Path:       "/api/2.0/prompt.php",
		Parameters: url.Values{"uid": []string{loginUID}}, // Send as query params
	}
	var scanResp api.TraditionalBase // Use base struct, don't care about response data much
	fs.Debugf(f, "Calling login_qrcode_scan...")
	err := f.CallTraditionalAPI(ctx, &scanOpts, scanPayload, &scanResp, true)
	if err != nil {
		return fmt.Errorf("login_qrcode_scan failed: %w", err)
	}
	fs.Debugf(f, "login_qrcode_scan call successful (State: %v)", scanResp.State)
	return nil
}

// callQRConfirmAPI calls the QR confirm API
func (f *Fs) callQRConfirmAPI(ctx context.Context, loginUID string) error {
	confirmPayload := map[string]string{"uid": loginUID, "key": loginUID, "client": "0"} // Key seems to be same as uid?
	confirmOpts := rest.Opts{
		Method:     "GET",
		RootURL:    hnQrCodeAPIRootURL,
		Path:       "/api/2.0/slogin.php",
		Parameters: url.Values{"uid": []string{loginUID}, "key": []string{loginUID}, "client": []string{"0"}}, // Send as query params
	}
	var confirmResp api.TraditionalBase
	fs.Debugf(f, "Calling login_qrcode_scan_confirm...")
	err := f.CallTraditionalAPI(ctx, &confirmOpts, confirmPayload, &confirmResp, true) // Needs encryption? Assume yes.
	if err != nil {
		return fmt.Errorf("login_qrcode_scan_confirm failed: %w", err)
	}
	fs.Debugf(f, "login_qrcode_scan_confirm call successful (State: %v)", confirmResp.State)
	return nil
}

// exchangeDeviceCodeForToken gets the access token using the device code
func (f *Fs) exchangeDeviceCodeForToken(ctx context.Context, loginUID string) error {
	tokenData := url.Values{
		"uid":           {loginUID},
		"code_verifier": {f.codeVerifier},
	}
	tokenOpts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL,
		Path:         "/open/deviceCodeToToken",
		Body:         strings.NewReader(tokenData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}
	var tokenResp api.DeviceCodeTokenResp
	// This call also uses traditional client but no encryption needed.
	err := f.CallTraditionalAPI(ctx, &tokenOpts, nil, &tokenResp, true) // skipEncrypt=true
	if err != nil {
		return fmt.Errorf("deviceCodeToToken failed: %w", err)
	}
	if tokenResp.Data == nil || tokenResp.Data.AccessToken == "" {
		return fmt.Errorf("deviceCodeToToken returned empty data: %v", tokenResp)
	}

	// Store tokens with proper locking
	f.tokenMu.Lock()
	f.accessToken = tokenResp.Data.AccessToken
	f.refreshToken = tokenResp.Data.RefreshToken
	f.tokenExpiry = time.Now().Add(time.Duration(tokenResp.Data.ExpiresIn) * time.Second)
	f.tokenMu.Unlock()

	fs.Debugf(f, "Successfully obtained access token, expires at %v", f.tokenExpiry)

	return nil
}

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	f.tokenMu.Lock()

	// Check if token is already valid when we acquire the lock
	// This handles the case where another thread just refreshed the token
	if !refreshTokenExpired && !forceRefresh && isTokenStillValid(f) {
		fs.Debugf(f, "Token is still valid after acquiring lock, skipping refresh")
		f.tokenMu.Unlock()
		return nil
	}

	// Check if we need to perform a full login instead of a refresh
	if shouldPerformFullLogin(f, refreshTokenExpired) {
		f.tokenMu.Unlock()
		err := f.login(ctx) // login handles its own locking
		if err != nil {
			return err
		}
		// Save the token after successful login
		f.saveToken(ctx, f.m)
		return nil
	}

	// Check if token is still valid and refresh not forced
	if !forceRefresh && isTokenStillValid(f) {
		f.tokenMu.Unlock()
		return nil
	}

	// Prepare for token refresh
	refreshToken := f.refreshToken // Make a local copy of the refresh token
	f.tokenMu.Unlock()             // Unlock before making API call

	// Perform the actual token refresh
	result, err := f.performTokenRefresh(ctx, refreshToken)
	if err != nil {
		return err // Error already formatted with context
	}

	// Update the tokens with new values
	f.updateTokens(result)

	// Save the refreshed token to config
	f.saveToken(ctx, f.m)

	return nil
}

// shouldPerformFullLogin determines if we should skip refresh and do a full login
func shouldPerformFullLogin(f *Fs, refreshTokenExpired bool) bool {
	// Skip directly to re-login if refresh token expired
	if refreshTokenExpired {
		fs.Debugf(f, "Token refresh skipped, going directly to re-login due to expired refresh token")
		return true
	}

	// Re-login if no tokens available
	if f.accessToken == "" || f.refreshToken == "" {
		fs.Debugf(f, "No token found, attempting login.")
		return true
	}

	return false
}

// isTokenStillValid checks if the current token is still valid
func isTokenStillValid(f *Fs) bool {
	return time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow))
}

// performTokenRefresh handles the actual API call to refresh the token
func (f *Fs) performTokenRefresh(ctx context.Context, refreshToken string) (*api.RefreshTokenResp, error) {
	// Ensure client exists
	if err := f.ensureOpenAPIClient(ctx); err != nil {
		return nil, err
	}

	// Set up and make the refresh request
	refreshResp, err := f.callRefreshTokenAPI(ctx, refreshToken)
	if err != nil {
		return handleRefreshError(f, ctx, err)
	}

	// Validate the response
	if refreshResp.Data == nil || refreshResp.Data.AccessToken == "" {
		// Log detailed information about the empty response
		fs.Errorf(f, "Refresh token response empty or invalid. Full response: %#v", refreshResp)
		// Log OpenAPI base information (state, code, message)
		fs.Errorf(f, "Response state: %v, code: %d, message: %q",
			refreshResp.State, refreshResp.Code, refreshResp.Message)

		fs.Errorf(f, "Refresh token response empty, attempting re-login.")

		// Re-lock before checking token again to avoid race condition
		f.tokenMu.Lock()
		// Check if another thread has already refreshed the token
		if f.accessToken != "" && time.Now().Before(f.tokenExpiry) {
			fs.Debugf(f, "Token was refreshed by another thread while waiting")
			f.tokenMu.Unlock()
			return nil, nil
		}
		f.tokenMu.Unlock()

		loginErr := f.login(ctx)
		if loginErr != nil {
			return nil, fmt.Errorf("re-login failed after empty refresh response: %w", loginErr)
		}
		fs.Debugf(f, "Re-login successful after empty refresh response.")
		return nil, nil // Re-login successful, no need to update tokens
	}

	return refreshResp, nil
}

// ensureOpenAPIClient ensures the OpenAPI client is initialized
func (f *Fs) ensureOpenAPIClient(ctx context.Context) error {
	if f.openAPIClient != nil {
		return nil
	}

	// Setup client if it doesn't exist
	newCtx, ci := fs.AddConfig(ctx)
	ci.UserAgent = f.opt.UserAgent
	httpClient := getHTTPClient(newCtx, &f.opt)
	f.openAPIClient = rest.NewClient(httpClient).
		SetRoot(openAPIRootURL).
		SetErrorHandler(errorHandler)

	return nil
}

// callRefreshTokenAPI makes the actual API call to refresh the token
func (f *Fs) callRefreshTokenAPI(ctx context.Context, refreshToken string) (*api.RefreshTokenResp, error) {
	refreshData := url.Values{
		"refresh_token": {refreshToken},
	}
	opts := rest.Opts{
		Method:       "POST",
		RootURL:      passportRootURL,
		Path:         "/open/refreshToken",
		Body:         strings.NewReader(refreshData.Encode()),
		ExtraHeaders: map[string]string{"Content-Type": "application/x-www-form-urlencoded"},
	}

	var refreshResp api.RefreshTokenResp
	resp, err := f.openAPIClient.Call(ctx, &opts)
	if err != nil {
		return nil, err
	}

	// Read the raw response for logging
	defer fs.CheckClose(resp.Body, &err)
	body, readErr := io.ReadAll(resp.Body)
	if readErr != nil {
		fs.Errorf(f, "Failed to read refresh token response body: %v", readErr)
		return nil, readErr
	}

	// Log the raw response
	fs.Debugf(f, "Raw refresh token response: %s", string(body))

	// Create a new reader for the JSON unmarshal
	err = json.Unmarshal(body, &refreshResp)
	if err != nil {
		fs.Errorf(f, "Failed to parse refresh token response: %v", err)
		return nil, err
	}

	return &refreshResp, nil
}

// handleRefreshError handles errors from the refresh token API call
func handleRefreshError(f *Fs, ctx context.Context, err error) (*api.RefreshTokenResp, error) {
	fs.Errorf(f, "Refresh token failed: %v", err)

	// Check if the error indicates the refresh token itself is expired
	var tokenErr *api.TokenError
	if errors.As(err, &tokenErr) && tokenErr.IsRefreshTokenExpired ||
		strings.Contains(err.Error(), "refresh token expired") {
		fs.Debugf(f, "Refresh token seems expired, attempting full re-login.")
		loginErr := f.login(ctx) // login handles its own locking
		if loginErr != nil {
			return nil, fmt.Errorf("re-login failed after refresh token expired: %w", loginErr)
		}
		fs.Debugf(f, "Re-login successful after refresh token expiry.")
		return nil, nil // Re-login successful
	}

	// Return the original refresh error if it wasn't an expiry issue
	return nil, fmt.Errorf("token refresh failed: %w", err)
}

// updateTokens updates the token values with new values from a refresh response
func (f *Fs) updateTokens(refreshResp *api.RefreshTokenResp) {
	// If we got a nil response, it means we did a re-login instead of a refresh
	if refreshResp == nil {
		return
	}

	f.tokenMu.Lock()
	defer f.tokenMu.Unlock()

	f.accessToken = refreshResp.Data.AccessToken
	// OpenAPI spec says refresh_token might be updated, so store the new one
	if refreshResp.Data.RefreshToken != "" {
		f.refreshToken = refreshResp.Data.RefreshToken
	}
	f.tokenExpiry = time.Now().Add(time.Duration(refreshResp.Data.ExpiresIn) * time.Second)
	fs.Debugf(f, "Token refreshed successfully, new expiry: %v", f.tokenExpiry)
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(_ context.Context, m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "Not saving tokens - nil mapper provided")
		return
	}

	f.tokenMu.Lock()
	defer f.tokenMu.Unlock()

	if f.accessToken == "" || f.refreshToken == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "Not saving tokens - incomplete token information")
		return
	}

	// Create the token structure
	token := &oauth2.Token{
		AccessToken:  f.accessToken,
		TokenType:    "Bearer",
		RefreshToken: f.refreshToken,
		Expiry:       f.tokenExpiry,
	}

	// Save the token directly using oauthutil's method
	// Note: This uses the originalName without brackets to ensure consistency
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Errorf(f, "Failed to save token to config: %v", err)
		return
	}

	fs.Debugf(f, "Saved token to config file using original name %q", f.originalName)
}

// setupTokenRenewer initializes the token renewer to automatically refresh tokens
func (f *Fs) setupTokenRenewer(ctx context.Context, m configmap.Mapper) {
	// Only set up renewer if we have valid tokens
	if f.accessToken == "" || f.refreshToken == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "Not setting up token renewer - incomplete token information")
		return
	}

	// Create a renewal transaction function
	transaction := func() error {
		fs.Debugf(f, "Token renewer triggered, refreshing token")
		// Use non-global function to avoid deadlocks
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Errorf(f, "Failed to refresh token in renewer: %v", err)
			return err
		}

		return nil // saveToken is already called in refreshTokenIfNecessary
	}

	// Create minimal OAuth config
	config := &oauthutil.Config{
		TokenURL: passportRootURL + "/open/refreshToken",
	}

	// Create a token source using the existing token
	token := &oauth2.Token{
		AccessToken:  f.accessToken,
		RefreshToken: f.refreshToken,
		Expiry:       f.tokenExpiry,
		TokenType:    "Bearer",
	}

	// Save token to config so it can be accessed by TokenSource
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Logf(f, "Failed to save token for renewer: %v", err)
		return
	}

	// Create a client with the token source
	_, ts, err := oauthutil.NewClientWithBaseClient(ctx, f.originalName, m, config, fshttp.NewClient(ctx))
	if err != nil {
		fs.Logf(f, "Failed to create token source for renewer: %v", err)
		return
	}

	// Create token renewer that will trigger when the token is about to expire
	f.tokenRenewer = oauthutil.NewRenew(f.originalName, ts, transaction)
	f.tokenRenewer.Start() // Start the renewer immediately
	fs.Debugf(f, "Token renewer initialized and started with original name %q", f.originalName)
}

// CallOpenAPI performs a call to the OpenAPI endpoint.
// It handles token refresh and sets the Authorization header.
// If skipToken is true, it skips adding the Authorization header (used for refresh itself).
func (f *Fs) CallOpenAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	fs.Debugf(f, "ğŸ” CallOpenAPIå¼€å§‹: path=%q, method=%q", opts.Path, opts.Method)

	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// Wrap the entire attempt sequence with the global pacer, returning proper retry signals
	return f.globalPacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		fs.Debugf(f, "ğŸ” CallOpenAPI: è¿›å…¥globalPacer")

		// Ensure token is available and current
		if !skipToken {
			fs.Debugf(f, "ğŸ” CallOpenAPI: å‡†å¤‡token")
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallOpenAPI: prepareTokenForRequestå¤±è´¥: %v", err)
				return false, backoff.Permanent(err)
			}
		}

		// Make the API call
		fs.Debugf(f, "ğŸ” CallOpenAPI: æ‰§è¡ŒAPIè°ƒç”¨")
		resp, apiErr := f.executeOpenAPICall(ctx, opts, request, response)

		// Handle retries for network/server errors
		if retryNeeded, retryErr := shouldRetry(ctx, resp, apiErr); retryNeeded {
			fs.Debugf(f, "pacer: low level retry required for OpenAPI call (error: %v)", retryErr)
			return true, retryErr // Signal globalPacer to retry
		}

		// Handle token errors
		if apiErr != nil {
			if retryAfterTokenRefresh, err := f.handleTokenError(ctx, opts, apiErr, skipToken); retryAfterTokenRefresh {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, backoff.Permanent(err)
			}
			return false, backoff.Permanent(apiErr) // Non-token error, don't retry
		}

		// Check for API-level errors in the response
		if apiErr = f.checkResponseForAPIErrors(response, skipToken, opts); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, backoff.Permanent(err)
			}
			return false, backoff.Permanent(apiErr) // Other API error, don't retry
		}

		fs.Debugf(f, "pacer: OpenAPI call successful")
		return false, nil // Success, don't retry
	})
}

// getPacerForEndpoint æ ¹æ®APIç«¯ç‚¹è¿”å›é€‚å½“çš„è°ƒé€Ÿå™¨
// ğŸ”§ å®ç°å·®å¼‚åŒ–QPSæ§åˆ¶ï¼Œæå‡APIè°ƒç”¨çš„ç¨³å®šæ€§å’Œæ€§èƒ½
func (f *Fs) getPacerForEndpoint(endpoint string) *fs.Pacer {
	switch {
	// ä¸Šä¼ ç›¸å…³API - ä½¿ç”¨ä¸Šä¼ ä¸“ç”¨è°ƒé€Ÿå™¨ (~3 QPS)
	case strings.Contains(endpoint, "/open/upload/init"),
		strings.Contains(endpoint, "/open/upload/resume"),
		strings.Contains(endpoint, "/open/upload/complete"):
		return f.uploadPacer

	// ä¸‹è½½URL API - ä½¿ç”¨ä¸‹è½½ä¸“ç”¨è°ƒé€Ÿå™¨ (~2 QPS)
	case strings.Contains(endpoint, "/open/ufile/downurl"):
		return f.downloadPacer

	// ä¼ ç»ŸAPI - ä½¿ç”¨ä¼ ç»Ÿè°ƒé€Ÿå™¨ (~2 QPS)
	case strings.Contains(endpoint, "webapi.115.com"):
		return f.tradPacer

	// å…¶ä»–OpenAPI - ä½¿ç”¨å…¨å±€è°ƒé€Ÿå™¨ (5 QPS)
	default:
		return f.globalPacer
	}
}

// CallUploadAPI ä¸“é—¨ç”¨äºä¸Šä¼ ç›¸å…³APIè°ƒç”¨çš„å‡½æ•°ï¼Œä½¿ç”¨ä¸“ç”¨è°ƒé€Ÿå™¨
// ğŸ”§ ä¼˜åŒ–ä¸Šä¼ APIè°ƒç”¨é¢‘ç‡ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
func (f *Fs) CallUploadAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	fs.Debugf(f, "ğŸ” CallUploadAPIå¼€å§‹: path=%q, method=%q", opts.Path, opts.Method)

	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// ğŸ”§ ä½¿ç”¨ä¸“ç”¨çš„ä¸Šä¼ è°ƒé€Ÿå™¨ï¼Œè€Œä¸æ˜¯å…¨å±€è°ƒé€Ÿå™¨
	return f.uploadPacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		fs.Debugf(f, "ğŸ” CallUploadAPI: è¿›å…¥uploadPacer (QPSé™åˆ¶: ~3 QPS)")

		// Ensure token is available and current
		if !skipToken {
			fs.Debugf(f, "ğŸ” CallUploadAPI: å‡†å¤‡token")
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallUploadAPI: prepareTokenForRequestå¤±è´¥: %v", err)
				return false, backoff.Permanent(err)
			}
		}

		// Make the actual API call
		fs.Debugf(f, "ğŸ” CallUploadAPI: æ‰§è¡ŒAPIè°ƒç”¨")
		resp, err := f.openAPIClient.CallJSON(ctx, opts, request, response)

		// Check for API-level errors in the response
		if apiErr := f.checkResponseForAPIErrors(response, skipToken, opts); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, backoff.Permanent(err)
			}
			return false, backoff.Permanent(apiErr) // Other API error, don't retry
		}

		fs.Debugf(f, "ğŸ” CallUploadAPI: APIè°ƒç”¨æˆåŠŸ")
		return shouldRetry(ctx, resp, err)
	})
}

// CallDownloadURLAPI ä¸“é—¨ç”¨äºä¸‹è½½URL APIè°ƒç”¨çš„å‡½æ•°ï¼Œä½¿ç”¨ä¸“ç”¨è°ƒé€Ÿå™¨
// ğŸ”§ é˜²æ­¢ä¸‹è½½URL APIè°ƒç”¨é¢‘ç‡è¿‡é«˜ï¼Œé¿å…è§¦å‘115ç½‘ç›˜åçˆ¬æœºåˆ¶
func (f *Fs) CallDownloadURLAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipToken bool) error {
	fs.Debugf(f, "ğŸ” CallDownloadURLAPIå¼€å§‹: path=%q, method=%q", opts.Path, opts.Method)

	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = openAPIRootURL
	}

	// ğŸ”§ ä½¿ç”¨ä¸“ç”¨çš„ä¸‹è½½URLè°ƒé€Ÿå™¨ï¼Œè€Œä¸æ˜¯å…¨å±€è°ƒé€Ÿå™¨
	return f.downloadPacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		fs.Debugf(f, "ğŸ” CallDownloadURLAPI: è¿›å…¥downloadPacer (QPSé™åˆ¶: ~2 QPS)")

		// Ensure token is available and current
		if !skipToken {
			fs.Debugf(f, "ğŸ” CallDownloadURLAPI: å‡†å¤‡token")
			if err := f.prepareTokenForRequest(ctx, opts); err != nil {
				fs.Debugf(f, "ğŸ” CallDownloadURLAPI: prepareTokenForRequestå¤±è´¥: %v", err)
				return false, backoff.Permanent(err)
			}
		}

		// Make the actual API call
		fs.Debugf(f, "ğŸ” CallDownloadURLAPI: æ‰§è¡ŒAPIè°ƒç”¨")
		resp, err := f.openAPIClient.CallJSON(ctx, opts, request, response)

		// Check for API-level errors in the response
		if apiErr := f.checkResponseForAPIErrors(response, skipToken, opts); apiErr != nil {
			if tokenRefreshed, err := f.handleTokenError(ctx, opts, apiErr, skipToken); tokenRefreshed {
				return true, nil // Retry with refreshed token
			} else if err != nil {
				return false, backoff.Permanent(err)
			}
			return false, backoff.Permanent(apiErr) // Other API error, don't retry
		}

		fs.Debugf(f, "ğŸ” CallDownloadURLAPI: APIè°ƒç”¨æˆåŠŸ")
		return shouldRetry(ctx, resp, err)
	})
}

// prepareTokenForRequest ensures a valid token is available and sets it in the request headers
func (f *Fs) prepareTokenForRequest(ctx context.Context, opts *rest.Opts) error {
	// Use double-checked locking pattern to minimize lock contention
	// First check without lock (fast path for common case)
	f.tokenMu.Lock()
	needsRefresh := !isTokenStillValid(f)
	currentToken := f.accessToken
	f.tokenMu.Unlock()

	// If refresh is needed, call refreshTokenIfNecessary which has its own locking
	if needsRefresh {
		refreshErr := f.refreshTokenIfNecessary(ctx, false, false)
		if refreshErr != nil {
			fs.Debugf(f, "Token refresh failed: %v", refreshErr)
			return fmt.Errorf("token refresh failed: %w", refreshErr)
		}

		// Get the refreshed token
		f.tokenMu.Lock()
		currentToken = f.accessToken
		f.tokenMu.Unlock()
	}

	// Validate we have a token before using it
	if currentToken == "" {
		return fmt.Errorf("no valid access token available")
	}

	if opts.ExtraHeaders == nil {
		opts.ExtraHeaders = make(map[string]string)
	}
	opts.ExtraHeaders["Authorization"] = "Bearer " + currentToken
	return nil
}

// executeOpenAPICall makes the actual API call with the provided parameters
func (f *Fs) executeOpenAPICall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	fs.Debugf(f, "ğŸ” executeOpenAPICallå¼€å§‹: path=%q", opts.Path)

	var resp *http.Response
	var err error

	if request != nil && response != nil {
		// Assume standard JSON request/response
		fs.Debugf(f, "ğŸ” executeOpenAPICall: æ ‡å‡†JSONè¯·æ±‚/å“åº”æ¨¡å¼")
		resp, err = f.openAPIClient.CallJSON(ctx, opts, request, response)
	} else if response != nil {
		// Assume GET request with JSON response
		fs.Debugf(f, "ğŸ” executeOpenAPICall: GETè¯·æ±‚JSONå“åº”æ¨¡å¼")
		resp, err = f.openAPIClient.CallJSON(ctx, opts, nil, response)
	} else {
		// Assume call without specific request/response body
		fs.Debugf(f, "ğŸ” executeOpenAPICall: åŸºç¡€è°ƒç”¨æ¨¡å¼")
		var baseResp api.OpenAPIBase
		resp, err = f.openAPIClient.CallJSON(ctx, opts, nil, &baseResp)
		if err == nil {
			err = baseResp.Err() // Check for API-level errors
		}
	}

	if err != nil {
		fs.Debugf(f, "ğŸ” executeOpenAPICallå¤±è´¥: %v", err)
	}

	return resp, err
}

// handleTokenError processes token-related errors and attempts to refresh or re-login
func (f *Fs) handleTokenError(ctx context.Context, opts *rest.Opts, apiErr error, skipToken bool) (bool, error) {
	var tokenErr *api.TokenError
	if errors.As(apiErr, &tokenErr) {
		fs.Debugf(f, "Token error detected: %v (relogin needed: %v)", tokenErr, tokenErr.IsRefreshTokenExpired)
		// Handle token refresh/re-login using refreshTokenIfNecessary
		refreshErr := f.refreshTokenIfNecessary(ctx, tokenErr.IsRefreshTokenExpired, !tokenErr.IsRefreshTokenExpired)
		if refreshErr != nil {
			fs.Debugf(f, "Token refresh/relogin failed: %v", refreshErr)
			return false, fmt.Errorf("token refresh/relogin failed: %w (original: %v)", refreshErr, apiErr)
		}

		// Token was successfully refreshed or re-login succeeded, retry the API call
		fs.Debugf(f, "Token refresh/relogin succeeded, retrying API call")

		// Update the Authorization header with the new token
		if !skipToken {
			// Always get the freshest token right before using it
			f.tokenMu.Lock()
			token := f.accessToken
			f.tokenMu.Unlock()

			if opts.ExtraHeaders == nil {
				opts.ExtraHeaders = make(map[string]string)
			}
			opts.ExtraHeaders["Authorization"] = "Bearer " + token
		}
		return true, nil // Signal retry with the refreshed token
	}
	return false, nil // Not a token error
}

// checkResponseForAPIErrors examines the response for API-level errors using reflection
func (f *Fs) checkResponseForAPIErrors(response any, _ bool, _ *rest.Opts) error {
	if response == nil {
		return nil
	}

	// Use reflection to check for and call an Err() method
	val := reflect.ValueOf(response)
	if val.Kind() == reflect.Ptr && !val.IsNil() {
		method := val.MethodByName("Err")
		if method.IsValid() && method.Type().NumIn() == 0 && method.Type().NumOut() == 1 &&
			method.Type().Out(0).Implements(reflect.TypeOf((*error)(nil)).Elem()) {
			result := method.Call(nil)
			if !result[0].IsNil() {
				// Get the error and check if it's a rate limit error
				err := result[0].Interface().(error)
				if strings.Contains(err.Error(), "rate limit exceeded") {
					fs.Debugf(f, "Rate limit error detected in response: %v", err)
					// Return the error as is - the shouldRetry function will handle it
					// This ensures both CallOpenAPI and CallTraditionalAPI will retry rate limit errors
				}
				return err
			}
		}
	}
	return nil
}

// CallTraditionalAPI performs a call to the traditional (cookie, encrypted) API.
// It uses both the traditional and global pacers.
// If skipEncrypt is true, it skips the request/response encryption (used for some login steps).
func (f *Fs) CallTraditionalAPI(ctx context.Context, opts *rest.Opts, request any, response any, skipEncrypt bool) error {
	// Ensure root URL is set if not provided in opts
	if opts.RootURL == "" {
		opts.RootURL = traditionalRootURL
	}

	// Wrap the entire attempt sequence with the global pacer
	return f.globalPacer.Call(func() (shouldRetryGlobal bool, errGlobal error) {
		// Wait for traditional pacer
		if err := f.enforceTraditionalPacerDelay(); err != nil {
			return false, backoff.Permanent(err)
		}

		// Make the API call (with or without encryption)
		resp, apiErr := f.executeTraditionalAPICall(ctx, opts, request, response, skipEncrypt)

		// Process the result
		return f.processTraditionalAPIResult(ctx, resp, apiErr)
	})
}

// enforceTraditionalPacerDelay ensures we respect the traditional API pacer limits
func (f *Fs) enforceTraditionalPacerDelay() error {
	// Use tradPacer.Call with a dummy function that always succeeds immediately
	// and doesn't retry. This effectively just waits for the pacer's internal timer.
	tradPaceErr := f.tradPacer.Call(func() (bool, error) {
		return false, nil // Dummy call: Success, don't retry this dummy op.
	})
	if tradPaceErr != nil {
		// If waiting for tradPacer was interrupted (e.g., context cancelled)
		fs.Debugf(f, "Context cancelled or error while waiting for traditional pacer: %v", tradPaceErr)
		return tradPaceErr
	}
	return nil
}

// executeTraditionalAPICall makes the actual API call with proper handling of encryption
func (f *Fs) executeTraditionalAPICall(ctx context.Context, opts *rest.Opts, request any, response any, skipEncrypt bool) (*http.Response, error) {
	if skipEncrypt {
		return f.executeUnencryptedCall(ctx, opts, request, response)
	} else {
		return f.executeEncryptedCall(ctx, opts, request, response)
	}
}

// executeUnencryptedCall makes a traditional API call without encryption
func (f *Fs) executeUnencryptedCall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	var resp *http.Response
	var apiErr error

	// Choose the right call pattern based on request/response
	if request != nil && response != nil {
		resp, apiErr = f.tradClient.CallJSON(ctx, opts, request, response)
	} else if response != nil {
		resp, apiErr = f.tradClient.CallJSON(ctx, opts, nil, response)
	} else {
		var baseResp api.TraditionalBase
		resp, apiErr = f.tradClient.CallJSON(ctx, opts, nil, &baseResp)
		if apiErr == nil {
			apiErr = baseResp.Err()
		}
	}

	return resp, apiErr
}

// executeEncryptedCall makes a traditional API call with encryption
func (f *Fs) executeEncryptedCall(ctx context.Context, opts *rest.Opts, request any, response any) (*http.Response, error) {
	var resp *http.Response
	var apiErr error

	if request != nil && response != nil {
		// Handle request+response case with encryption
		apiErr = f.executeEncryptedRequestResponse(ctx, opts, request, response, &resp)
	} else if response != nil {
		// Handle response-only case with encryption
		apiErr = f.executeEncryptedResponseOnly(ctx, opts, response, &resp)
	} else {
		// Simple base response case
		var baseResp api.TraditionalBase
		resp, apiErr = f.tradClient.CallJSON(ctx, opts, nil, &baseResp)
		if apiErr == nil {
			apiErr = baseResp.Err()
		}
	}

	return resp, apiErr
}

// executeEncryptedRequestResponse handles the case where both request and response need encryption
func (f *Fs) executeEncryptedRequestResponse(ctx context.Context, opts *rest.Opts, request any, response any, resp **http.Response) error {
	// Encode request data
	input, marshalErr := json.Marshal(request)
	if marshalErr != nil {
		return fmt.Errorf("failed to marshal traditional request: %w", marshalErr)
	}

	key := crypto.GenerateKey()
	// Prepare multipart parameters
	if opts.MultipartParams == nil {
		opts.MultipartParams = url.Values{}
	}
	opts.MultipartParams.Set("data", crypto.Encode(input, key))
	opts.Body = nil // Clear body if using multipart params

	// Execute call expecting encrypted response
	var info api.StringInfo
	var err error
	*resp, err = f.tradClient.CallJSON(ctx, opts, nil, &info)
	if err != nil {
		return err
	}

	// Process the encrypted response
	return f.processEncryptedResponse(&info, response)
}

// executeEncryptedResponseOnly handles the case where only the response is encrypted
func (f *Fs) executeEncryptedResponseOnly(ctx context.Context, opts *rest.Opts, response any, resp **http.Response) error {
	// Call expecting encrypted response, no request body needed (e.g., GET)
	var info api.StringInfo
	var err error
	*resp, err = f.tradClient.CallJSON(ctx, opts, nil, &info)
	if err != nil {
		return err
	}

	// Process the encrypted response
	return f.processEncryptedResponse(&info, response)
}

// processEncryptedResponse decodes and unmarshals an encrypted response
func (f *Fs) processEncryptedResponse(info *api.StringInfo, response any) error {
	if err := info.Err(); err != nil {
		return err // API level error before decryption
	}

	if info.Data == "" {
		return errors.New("no data received in traditional response")
	}

	// Decode and unmarshal response
	key := crypto.GenerateKey()
	output, decodeErr := crypto.Decode(string(info.Data), key)
	if decodeErr != nil {
		return fmt.Errorf("failed to decode traditional data: %w", decodeErr)
	}

	if unmarshalErr := json.Unmarshal(output, response); unmarshalErr != nil {
		return fmt.Errorf("failed to unmarshal traditional response %q: %w", string(output), unmarshalErr)
	}

	return nil
}

// processTraditionalAPIResult handles the result of a traditional API call
func (f *Fs) processTraditionalAPIResult(ctx context.Context, resp *http.Response, apiErr error) (bool, error) {
	// Check for retryable errors
	retryNeeded, retryErr := shouldRetry(ctx, resp, apiErr)
	if retryNeeded {
		fs.Debugf(f, "pacer: low level retry required for traditional call (error: %v)", retryErr)
		return true, retryErr
	}

	// Handle non-retryable errors
	if apiErr != nil {
		fs.Debugf(f, "pacer: permanent error encountered in traditional call: %v", apiErr)
		// Ensure the error is marked as permanent
		var permanentErr *backoff.PermanentError
		if !errors.As(apiErr, &permanentErr) {
			return false, backoff.Permanent(apiErr)
		}
		return false, apiErr // Already permanent
	}

	// Success
	fs.Debugf(f, "pacer: traditional call successful")
	return false, nil
}

// newFs constructs an Fs from the path, container:path
func NewFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	// Parse config into Options struct
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, err
	}

	// Validate incompatible options
	if opt.FastUpload && opt.OnlyStream {
		return nil, errors.New("fast_upload and only_stream cannot be set simultaneously")
	}
	if opt.FastUpload && opt.UploadHashOnly {
		return nil, errors.New("fast_upload and upload_hash_only cannot be set simultaneously")
	}
	if opt.OnlyStream && opt.UploadHashOnly {
		return nil, errors.New("only_stream and upload_hash_only cannot be set simultaneously")
	}

	// Validate upload parameters
	err = checkUploadChunkSize(opt.ChunkSize)
	if err != nil {
		return nil, fmt.Errorf("115: chunk size: %w", err)
	}
	err = checkUploadCutoff(opt.UploadCutoff)
	if err != nil {
		return nil, fmt.Errorf("115: upload cutoff: %w", err)
	}
	err = checkUploadConcurrency(opt.UploadConcurrency)
	if err != nil {
		return nil, fmt.Errorf("115: upload concurrency: %w", err)
	}
	if opt.NohashSize > StreamUploadLimit {
		fs.Logf(name, "nohash_size (%v) reduced to stream upload limit (%v)", opt.NohashSize, StreamUploadLimit)
		opt.NohashSize = StreamUploadLimit
	}

	// Store the original name before any modifications for config operations
	// Extract the base name without the config override suffix {xxxx}
	originalName := name
	if idx := strings.IndexRune(name, '{'); idx > 0 {
		originalName = name[:idx]
	}

	// Parse root ID from path if present
	if rootID, _, _ := parseRootID(root); rootID != "" {
		name += rootID // Append ID to name for uniqueness
		root = root[strings.Index(root, "}")+1:]
	}

	root = strings.Trim(root, "/")

	f := &Fs{
		name:         name,
		originalName: originalName,
		root:         root,
		opt:          *opt,
		m:            m,
	}

	// Initialize features
	f.features = (&fs.Features{
		DuplicateFiles:          false,
		CanHaveEmptyDirectories: true,
		NoMultiThreading:        true, // Keep true as downloads might still use traditional API
	}).Fill(ctx, f)

	// åˆå§‹åŒ–è·¯å¾„ç¼“å­˜ (10ç§’TTLç”¨äºæµ‹è¯•, æœ€å¤§1000æ¡ç›®)
	f.pathCache = NewPathCache(10*time.Second, 1000)

	// åˆå§‹åŒ–ç¼“å­˜é…ç½®
	f.cacheConfig = DefaultCacheConfig115()

	// åˆå§‹åŒ–BadgerDBæŒä¹…åŒ–ç¼“å­˜ç³»ç»Ÿ
	cacheDir := cache.GetCacheDir("115drive")

	// åˆå§‹åŒ–å¤šä¸ªç¼“å­˜å®ä¾‹
	caches := map[string]**cache.BadgerCache{
		"path_resolve": &f.pathResolveCache,
		"dir_list":     &f.dirListCache,
		"metadata":     &f.metadataCache,
		"file_id":      &f.fileIDCache,
	}

	for name, cachePtr := range caches {
		cache, cacheErr := cache.NewBadgerCache(name, cacheDir)
		if cacheErr != nil {
			fs.Errorf(f, "åˆå§‹åŒ–%sç¼“å­˜å¤±è´¥: %v", name, cacheErr)
			// ç¼“å­˜åˆå§‹åŒ–å¤±è´¥ä¸åº”è¯¥é˜»æ­¢æ–‡ä»¶ç³»ç»Ÿå·¥ä½œï¼Œç»§ç»­æ‰§è¡Œ
		} else {
			*cachePtr = cache
			fs.Debugf(f, "%sç¼“å­˜åˆå§‹åŒ–æˆåŠŸ", name)
		}
	}

	fs.Debugf(f, "BadgerDBç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: %s", cacheDir)

	// åˆå§‹åŒ–ä¼˜åŒ–çš„HTTPå®¢æˆ·ç«¯
	f.httpClient = &http.Client{
		Transport: &http.Transport{
			MaxIdleConns:        100,              // æœ€å¤§ç©ºé—²è¿æ¥æ•°
			MaxIdleConnsPerHost: 10,               // æ¯ä¸ªä¸»æœºçš„æœ€å¤§ç©ºé—²è¿æ¥æ•°
			IdleConnTimeout:     90 * time.Second, // ç©ºé—²è¿æ¥è¶…æ—¶
			DisableCompression:  false,            // å¯ç”¨å‹ç¼©
		},
		Timeout: 30 * time.Second, // è¯·æ±‚è¶…æ—¶
	}

	// Setting appVer (needed for traditional calls)
	re := regexp.MustCompile(`\d+\.\d+\.\d+(\.\d+)?$`)
	if m := re.FindStringSubmatch(tradUserAgent); m == nil {
		fs.Logf(f, "Could not parse app version from User-Agent %q. Using default.", tradUserAgent)
		f.appVer = "27.0.7.5" // Default fallback
	} else {
		f.appVer = m[0]
		fs.Debugf(f, "Using App Version %q from User-Agent %q", f.appVer, tradUserAgent)
	}

	// Initialize pacers
	f.globalPacer = fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(time.Duration(opt.PacerMinSleep)),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))

	f.tradPacer = fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(traditionalMinSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))

	// ğŸ”§ æ·»åŠ ä¸‹è½½URLä¸“ç”¨è°ƒé€Ÿå™¨ï¼Œé˜²æ­¢APIè°ƒç”¨é¢‘ç‡è¿‡é«˜
	// åŸºäº115ç½‘ç›˜APIç‰¹æ€§ï¼Œè®¾ç½®ä¿å®ˆçš„QPSé™åˆ¶é¿å…è§¦å‘åçˆ¬æœºåˆ¶
	f.downloadPacer = fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(downloadURLMinSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))

	// ğŸ”§ æ·»åŠ ä¸Šä¼ ä¸“ç”¨è°ƒé€Ÿå™¨ï¼Œä¼˜åŒ–ä¸Šä¼ APIè°ƒç”¨é¢‘ç‡
	// å¹³è¡¡ä¸Šä¼ æ€§èƒ½å’ŒAPIç¨³å®šæ€§ï¼Œé¿å…ä¸Šä¼ æ—¶é¢‘ç‡è¿‡é«˜å¯¼è‡´é™åˆ¶
	f.uploadPacer = fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(uploadMinSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))

	// Create clients first to ensure they're available for token operations
	// Create separate clients for each API type with different User-Agents
	tradHTTPClient := getTradHTTPClient(ctx, &f.opt)
	openAPIHTTPClient := getOpenAPIHTTPClient(ctx, &f.opt)

	// Traditional client (uses cookie)
	f.tradClient = rest.NewClient(tradHTTPClient).
		SetRoot(traditionalRootURL).
		SetErrorHandler(errorHandler)

	// Add cookie to traditional client if provided
	if f.opt.Cookie != "" {
		cred := (&Credential{}).FromCookie(f.opt.Cookie)
		f.tradClient.SetCookie(cred.Cookie()...)
	}

	// OpenAPI client
	f.openAPIClient = rest.NewClient(openAPIHTTPClient).
		SetRoot(openAPIRootURL).
		SetErrorHandler(errorHandler)

	// Check if we have saved token in config file
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "Token loaded from config: %v (expires at %v)", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if tokenLoaded {
		// Check if token is expired or will expire soon
		if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
			fs.Debugf(f, "Token expired or will expire soon, refreshing now")
			tokenRefreshNeeded = true
		}
	} else if f.opt.Cookie != "" {
		// No token but have cookie, so login
		fs.Debugf(f, "No token found but cookie provided, attempting login")
		err = f.login(ctx)
		if err != nil {
			return nil, fmt.Errorf("initial login failed: %w", err)
		}
		// Save token to config after successful login
		f.saveToken(ctx, m)
		fs.Debugf(f, "Login successful, token saved")
	} else {
		return nil, errors.New("no valid cookie or token found, please configure cookie")
	}

	// Try to refresh the token if needed
	if tokenRefreshNeeded {
		err = f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Debugf(f, "Token refresh failed, attempting full login: %v", err)
			// If refresh fails, try full login
			err = f.login(ctx)
			if err != nil {
				return nil, fmt.Errorf("login failed after token refresh failure: %w", err)
			}
		}
		// Save the refreshed/new token
		f.saveToken(ctx, m)
		fs.Debugf(f, "Token refresh/login successful, token saved")
	}

	// Setup token renewer for automatic refresh
	fs.Debugf(f, "Setting up token renewer")
	f.setupTokenRenewer(ctx, m)

	// Set the root folder ID based on config
	if f.opt.RootFolderID != "" {
		// Use configured root folder ID
		f.rootFolderID = f.opt.RootFolderID
	} else {
		// Use default root folder ID
		f.rootFolderID = "0"
	}

	// Initialize directory cache
	f.dirCache = dircache.New(f.root, f.rootFolderID, f)

	// ğŸ”§ ä¼˜åŒ–ï¼šå¯åŠ¨æ™ºèƒ½ç¼“å­˜é¢„çƒ­ï¼Œå‚è€ƒ123ç½‘ç›˜çš„æˆåŠŸç»éªŒ
	go f.intelligentCachePreheating(ctx)

	// Find the current working directory
	if f.root != "" {
		// Find directory specified by the root path
		err := f.dirCache.FindRoot(ctx, false)
		if err != nil {
			// Assume it is a file or doesn't exist
			newRoot, remote := dircache.SplitPath(f.root)
			// åˆ›å»ºæ–°çš„Fså®ä¾‹ï¼Œé¿å…å¤åˆ¶mutex
			tempF := &Fs{
				name:          f.name,
				originalName:  f.originalName,
				root:          newRoot,
				opt:           f.opt,
				features:      f.features,
				tradClient:    f.tradClient,
				openAPIClient: f.openAPIClient,
				globalPacer:   f.globalPacer,
				tradPacer:     f.tradPacer,
				rootFolder:    f.rootFolder,
				rootFolderID:  f.rootFolderID,
				appVer:        f.appVer,
				userID:        f.userID,
				userkey:       f.userkey,
				isShare:       f.isShare,
				fileObj:       f.fileObj,
				m:             f.m,
				accessToken:   f.accessToken,
				refreshToken:  f.refreshToken,
				tokenExpiry:   f.tokenExpiry,
				codeVerifier:  f.codeVerifier,
				tokenRenewer:  f.tokenRenewer,
				pathCache:     f.pathCache,
				httpClient:    f.httpClient,
			}
			tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
			// Make new Fs which is the parent
			err = tempF.dirCache.FindRoot(ctx, false)
			if err != nil {
				// No root so return old f
				return f, nil
			}
			// Check if it's a file
			_, err := tempF.newObjectWithInfo(ctx, remote, nil)
			if err != nil {
				// æ£€æŸ¥æ˜¯å¦æ˜¯APIé™åˆ¶é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™ç›´æ¥è¿”å›ç›®å½•æ¨¡å¼ï¼Œé¿å…è·¯å¾„æ··ä¹±
				if isAPILimitError(err) {
					return f, nil
				}
				if err == fs.ErrorObjectNotFound {
					// File doesn't exist so return old f
					return f, nil
				}
				return nil, err
			}
			// Copy the features
			f.features.Fill(ctx, tempF)
			// Update the dir cache in the old f
			f.dirCache = tempF.dirCache
			f.root = tempF.root
			// Return an error with an fs which points to the parent
			return f, fs.ErrorIsFile
		}
	}

	return f, nil
}

// Name of the remote (as passed into NewFs)
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (as passed into NewFs)
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("115 %s", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	return f.features
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	// OpenAPI might allow setting modtime, but let's assume not for now
	return fs.ModTimeNotSupported
}

// DirCacheFlush resets the directory cache
func (f *Fs) DirCacheFlush() {
	f.dirCache.ResetRoot()
}

// Hashes returns the supported hash types of the filesystem
func (f *Fs) Hashes() hash.Set {
	return hash.Set(hash.SHA1)
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	fs.Debugf(f, "ğŸ” NewObjectå¼€å§‹: remote=%q", remote)

	if f.fileObj != nil { // Handle case where Fs points to a single file
		fs.Debugf(f, "ğŸ” NewObject: å¤„ç†å•æ–‡ä»¶æ¨¡å¼")
		obj := *f.fileObj
		if obj.Remote() == remote || obj.Remote() == "isFile:"+remote {
			fs.Debugf(f, "ğŸ” NewObject: å•æ–‡ä»¶åŒ¹é…æˆåŠŸ")
			return obj, nil
		}
		fs.Debugf(f, "ğŸ” NewObject: å•æ–‡ä»¶ä¸åŒ¹é…ï¼Œè¿”å›NotFound")
		return nil, fs.ErrorObjectNotFound // If remote doesn't match the single file
	}

	result, err := f.newObjectWithInfo(ctx, remote, nil)
	if err != nil {
		fs.Debugf(f, "ğŸ” NewObjectå¤±è´¥: %v", err)
	}
	return result, err
}

// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	// æ„å»ºç¼“å­˜é”®
	cacheKey := pathID + "/" + leaf

	// é¦–å…ˆæ£€æŸ¥ç¼“å­˜
	if entry, exists := f.pathCache.Get(cacheKey); exists {
		if entry.IsDir {
			return entry.ID, true, nil
		} else {
			// æ‰¾åˆ°çš„æ˜¯æ–‡ä»¶ï¼Œè¿”å›ç©ºIDè¡¨ç¤ºè¿™æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
			return "", true, nil
		}
	}

	// Use listAll which now uses OpenAPI
	found, err = f.listAll(ctx, pathID, f.opt.ListChunk, false, false, func(item *api.File) bool {
		// Compare with decoded name to handle special characters correctly
		decodedName := f.opt.Enc.ToStandardName(item.FileNameBest())
		if decodedName == leaf {
			// æ£€æŸ¥æ‰¾åˆ°çš„é¡¹ç›®æ˜¯å¦ä¸ºç›®å½•
			if item.IsDir() {
				// è¿™æ˜¯ç›®å½•ï¼Œè¿”å›ç›®å½•ID
				foundID = item.ID()
				// Cache the found item's path/ID mapping (only for directories)
				parentPath, ok := f.dirCache.GetInv(pathID)
				if ok {
					itemPath := path.Join(parentPath, leaf)
					f.dirCache.Put(itemPath, foundID)
				}
				// åŒæ—¶æ›´æ–°è·¯å¾„ç¼“å­˜
				f.pathCache.Put(cacheKey, foundID, true)
			} else {
				// è¿™æ˜¯æ–‡ä»¶ï¼Œä¸è¿”å›IDï¼ˆä¿æŒfoundIDä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
				foundID = "" // æ˜ç¡®è®¾ç½®ä¸ºç©ºï¼Œè¡¨ç¤ºæ‰¾åˆ°çš„æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
				// ç¼“å­˜æ–‡ä»¶ä¿¡æ¯
				f.pathCache.Put(cacheKey, item.ID(), false)
			}
			return true // Stop searching
		}
		return false // Continue searching
	})

	// å¦‚æœé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œé‡‡ç”¨æ¸è¿›å¼ç¼“å­˜æ¸…ç†ç­–ç•¥
	if isAPILimitError(err) {
		f.clearCacheGradually(err, "")
	}

	return foundID, found, err
}

// GetDirID finds a directory ID by its absolute path. Used by dircache.
// This implementation uses the OpenAPI list endpoint iteratively, which might be slow.
// The traditional /files/getid is faster but less reliable. We prioritize OpenAPI.
func (f *Fs) GetDirID(ctx context.Context, dir string) (string, error) {
	// Start from the filesystem's root ID
	currentID := f.rootFolderID
	if dir == "" || dir == "/" {
		return currentID, nil
	}

	parts := strings.Split(strings.Trim(dir, "/"), "/")
	for _, part := range parts {
		if part == "" {
			continue
		}
		encodedPart := f.opt.Enc.FromStandardName(part) // Encode each part
		foundID, found, err := f.FindLeaf(ctx, currentID, encodedPart)
		if err != nil {
			// å¦‚æœæ˜¯APIé™åˆ¶é”™è¯¯ï¼Œç«‹å³è¿”å›ï¼Œä¸è¦ç»§ç»­é€’å½’æœç´¢
			if isAPILimitError(err) {
				// é‡‡ç”¨æ¸è¿›å¼ç¼“å­˜æ¸…ç†ç­–ç•¥
				f.clearCacheGradually(err, encodedPart)
				return "", err
			}
			return "", fmt.Errorf("error searching for %q in %q: %w", encodedPart, currentID, err)
		}
		if !found {
			return "", fs.ErrorDirNotFound
		}

		// éªŒè¯æ‰¾åˆ°çš„é¡¹ç›®ç¡®å®æ˜¯ç›®å½•
		// å¦‚æœæ˜¯æ–‡ä»¶ï¼Œåˆ™è¿™ä¸ªè·¯å¾„ä¸åº”è¯¥è¢«å½“ä½œç›®å½•è·¯å¾„å¤„ç†
		if foundID == "" {
			return "", fmt.Errorf("path component %q is a file, not a directory", encodedPart)
		}
		// Check if the found item is actually a directory
		// Need a way to confirm type without full listing again. Assume FindLeaf returns correct type for now.
		// If FindLeaf could return the *api.File, we could check item.IsDir() here.
		// For now, assume FindLeaf only finds directories when called by GetDirID context.
		currentID = foundID
	}
	return currentID, nil
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "Listè°ƒç”¨ï¼Œç›®å½•: %q", dir)

	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		// å¦‚æœæ˜¯APIé™åˆ¶é”™è¯¯ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å¹¶è¿”å›
		if strings.Contains(err.Error(), "770004") ||
			strings.Contains(err.Error(), "å·²è¾¾åˆ°å½“å‰è®¿é—®ä¸Šé™") {
			fs.Debugf(f, "Listé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œç›®å½•: %q, é”™è¯¯: %v", dir, err)
			return nil, err
		}
		fs.Debugf(f, "ListæŸ¥æ‰¾ç›®å½•å¤±è´¥ï¼Œç›®å½•: %q, é”™è¯¯: %v", dir, err)
		return nil, err
	}

	fs.Debugf(f, "Listæ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %q", dirID, dir)
	var iErr error
	_, err = f.listAll(ctx, dirID, f.opt.ListChunk, false, false, func(item *api.File) bool {
		entry, err := f.itemToDirEntry(ctx, path.Join(dir, item.FileNameBest()), item)
		if err != nil {
			iErr = err // Capture error but continue listing
			return false
		}
		if entry != nil {
			entries = append(entries, entry)
		}
		return false
	})
	if err != nil {
		return nil, err // Return listing error
	}
	if iErr != nil {
		return nil, iErr // Return item processing error
	}
	return entries, nil
}

// CreateDir makes a directory with pathID as parent and name leaf. Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	if f.isShare {
		return "", errors.New("unsupported operation: Mkdir on shared filesystem")
	}
	// Use makeDir which now uses OpenAPI
	info, err := f.makeDir(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}
	if info.Data == nil || info.Data.FileID == "" {
		return "", errors.New("Mkdir response did not contain a file ID")
	}
	return info.Data.FileID, nil
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	// Check if destination exists. If not, use PutUnchecked. If yes, use Update.
	existingObj, err := f.NewObject(ctx, src.Remote())
	if err == fs.ErrorObjectNotFound {
		// Not found, so create it using PutUnchecked
		return f.PutUnchecked(ctx, in, src, options...)
	} else if err != nil {
		// An error other than not found
		return nil, err
	}
	// Object exists, so update it
	return existingObj, existingObj.Update(ctx, in, src, options...)
}

// PutUnchecked uploads the object without checking for existence first.
func (f *Fs) PutUnchecked(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	return f.putUnchecked(ctx, in, src, src.Remote(), options...)
}

// putUnchecked uploads the object
func (f *Fs) putUnchecked(ctx context.Context, in io.Reader, src fs.ObjectInfo, remote string, options ...fs.OpenOption) (fs.Object, error) {
	if f.isShare {
		return nil, errors.New("unsupported operation: Put on shared filesystem")
	}

	// Call the main upload function which handles different strategies
	newObj, err := f.upload(ctx, in, src, remote, options...)
	if err != nil {
		return nil, err
	}
	if newObj == nil {
		return nil, errors.New("internal error: upload returned nil object without error")
	}

	o := newObj.(*Object)

	// Post-upload check (optional)
	if !f.opt.NoCheck && !o.hasMetaData {
		fs.Debugf(o, "Running post-upload check...")
		// Attempt to read metadata for the uploaded object to confirm
		err = o.readMetaData(ctx) // This will list the parent directory
		if err != nil {
			// Don't fail the upload, just log a warning
			fs.Logf(o, "Post-upload check failed to read metadata: %v", err)
			// Mark as having metadata anyway to avoid repeated checks
			o.hasMetaData = true
		} else {
			fs.Debugf(o, "Post-upload check successful.")
		}
	}

	// ä¸Šä¼ æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches115(remote, "upload")

	return newObj, nil
}

// MergeDirs merges multiple source directories into the first one.
func (f *Fs) MergeDirs(ctx context.Context, dirs []fs.Directory) (err error) {
	if f.isShare {
		return errors.New("unsupported operation: MergeDirs on shared filesystem")
	}
	if len(dirs) < 2 {
		return nil
	}
	dstDir := dirs[0]
	dstDirID := dstDir.ID()

	for _, srcDir := range dirs[1:] {
		srcDirID := srcDir.ID()
		fs.Debugf(srcDir, "Merging contents into %v", dstDir)

		// List all items in the source directory
		var itemsToMove []*api.File
		_, err = f.listAll(ctx, srcDirID, f.opt.ListChunk, false, false, func(item *api.File) bool {
			itemsToMove = append(itemsToMove, item)
			return false // Collect all items
		})
		if err != nil {
			return fmt.Errorf("MergeDirs list failed on %v: %w", srcDir, err)
		}

		// Move items in chunks
		chunkSize := f.opt.ListChunk // Use list chunk size for move chunks
		for i := 0; i < len(itemsToMove); i += chunkSize {
			end := i + chunkSize
			if end > len(itemsToMove) {
				end = len(itemsToMove)
			}
			chunk := itemsToMove[i:end]
			if len(chunk) == 0 {
				continue
			}

			var idsToMove []string
			for _, item := range chunk {
				idsToMove = append(idsToMove, item.ID())
			}

			fs.Debugf(srcDir, "Moving %d items to %v", len(idsToMove), dstDir)
			if err = f.moveFiles(ctx, idsToMove, dstDirID); err != nil {
				return fmt.Errorf("MergeDirs move failed for %v: %w", srcDir, err)
			}
		}
	}

	// Remove the source directories (now empty)
	var dirsToDelete []string
	for _, srcDir := range dirs[1:] {
		dirsToDelete = append(dirsToDelete, srcDir.ID())
	}

	// Delete directories in chunks
	chunkSize := f.opt.ListChunk
	for i := 0; i < len(dirsToDelete); i += chunkSize {
		end := i + chunkSize
		if end > len(dirsToDelete) {
			end = len(dirsToDelete)
		}
		chunkIDs := dirsToDelete[i:end]
		if len(chunkIDs) == 0 {
			continue
		}

		fs.Debugf(f, "Removing merged source directories: %v", chunkIDs)
		if err = f.deleteFiles(ctx, chunkIDs); err != nil {
			// Log error but continue trying to delete others
			fs.Errorf(f, "MergeDirs failed to rmdir chunk %v: %v", chunkIDs, err)
		}
	}

	// Flush the cache for the source directories
	for _, srcDir := range dirs[1:] {
		f.dirCache.FlushDir(srcDir.Remote())
	}

	return nil // Return nil even if some deletions failed, as merge likely succeeded partially
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	if f.isShare {
		return errors.New("unsupported operation: Mkdir on shared filesystem")
	}
	_, err := f.dirCache.FindDir(ctx, dir, true) // create = true
	if err == nil {
		// åˆ›å»ºç›®å½•æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		f.invalidateRelatedCaches115(dir, "mkdir")
	}
	return err
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	if f.isShare {
		return nil, fs.ErrorCantMove
	}
	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(src, "Can't move - not same remote type")
		return nil, fs.ErrorCantMove
	}
	// Ensure metadata is read for srcObj.id
	err := srcObj.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read source metadata for move: %w", err)
	}
	if srcObj.id == "" {
		return nil, errors.New("cannot move object with empty ID")
	}

	// Find destination parent directory ID, creating if necessary
	dstLeaf, dstParentID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, fmt.Errorf("failed to find destination directory for move: %w", err)
	}

	// Find source parent directory ID
	srcLeaf, srcParentID, err := srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false)
	if err != nil {
		// Log error but proceed, maybe parent doesn't exist in cache but file does
		fs.Logf(src, "Could not find source path in cache for move: %v", err)
		// Attempt to get parent ID from object if available
		if srcObj.parent != "" {
			srcParentID = srcObj.parent
		} else {
			return nil, fmt.Errorf("failed to find source directory for move and object has no parent ID: %w", err)
		}
	}

	// Perform the move if parents differ
	if srcParentID != dstParentID {
		fs.Debugf(srcObj, "Moving %q from %q to %q", srcObj.id, srcParentID, dstParentID)
		err = f.moveFiles(ctx, []string{srcObj.id}, dstParentID)
		if err != nil {
			return nil, fmt.Errorf("server-side move failed: %w", err)
		}
	}

	// Perform rename if names differ
	if srcLeaf != dstLeaf {
		fs.Debugf(srcObj, "Renaming %q to %q", srcLeaf, dstLeaf)
		err = f.renameFile(ctx, srcObj.id, dstLeaf)
		if err != nil {
			// Attempt to move back if rename fails? Or just return error?
			return nil, fmt.Errorf("failed to rename after move: %w", err)
		}
	}

	// Create new object representing the destination
	dstObj := &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: false, // Mark metadata as stale
		id:          srcObj.id,
		parent:      dstParentID, // Update parent ID
		size:        srcObj.size,
		sha1sum:     srcObj.sha1sum,
		pickCode:    srcObj.pickCode,
		modTime:     srcObj.modTime, // Keep original modTime? Or update? Keep for now.
		durlMu:      new(sync.Mutex),
	}

	// Read metadata for the new object to confirm and update details
	err = dstObj.readMetaData(ctx)
	if err != nil {
		// Log error but return the object anyway, metadata might be eventually consistent
		fs.Logf(dstObj, "Failed to read metadata after move: %v", err)
	} else {
		dstObj.hasMetaData = true
	}

	// Flush source directory from cache
	dir, _ := dircache.SplitPath(srcObj.remote)
	srcObj.fs.dirCache.FlushDir(dir)

	// ç§»åŠ¨æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches115(srcObj.remote, "move")
	f.invalidateRelatedCaches115(remote, "move")

	return dstObj, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	if f.isShare {
		return fs.ErrorCantDirMove
	}
	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(srcFs, "Can't move directory - not same remote type")
		return fs.ErrorCantDirMove
	}

	// Use dircache helper to prepare for move
	srcID, srcParentID, srcLeaf, dstParentID, dstLeaf, err := f.dirCache.DirMove(ctx, srcFs.dirCache, srcFs.root, srcRemote, f.root, dstRemote)
	if err != nil {
		return err // Errors like DirExists, CantMoveRoot handled by DirMove helper
	}

	// Perform the move if parents differ
	if srcParentID != dstParentID {
		fs.Debugf(srcFs, "Moving directory %q from %q to %q", srcID, srcParentID, dstParentID)
		err = f.moveFiles(ctx, []string{srcID}, dstParentID)
		if err != nil {
			return fmt.Errorf("server-side directory move failed: %w", err)
		}
	}

	// Perform rename if names differ
	if srcLeaf != dstLeaf {
		fs.Debugf(srcFs, "Renaming directory %q to %q", srcLeaf, dstLeaf)
		err = f.renameFile(ctx, srcID, dstLeaf)
		if err != nil {
			return fmt.Errorf("failed to rename directory after move: %w", err)
		}
	}

	// Flush source directory from cache
	srcFs.dirCache.FlushDir(srcRemote)
	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(src, "Can't copy - not same remote type")
		return nil, fs.ErrorCantCopy
	}

	// Special handling for copying *from* a shared remote (using traditional API)
	if srcObj.fs.isShare {
		if f.isShare {
			// Cannot copy from share to share directly? Assume not supported.
			return nil, errors.New("copying between shared remotes is not supported")
		}
		fs.Debugf(src, "Copying from shared remote using traditional API")
		// Find destination parent ID
		_, dstParentID, err := f.dirCache.FindPath(ctx, remote, true)
		if err != nil {
			return nil, fmt.Errorf("failed to find destination directory for copy: %w", err)
		}
		// Call traditional copyFromShare
		err = f.copyFromShare(ctx, srcObj.fs.opt.ShareCode, srcObj.fs.opt.ReceiveCode, srcObj.id, dstParentID)
		if err != nil {
			return nil, fmt.Errorf("copy from share failed: %w", err)
		}
		// Need to find the new object in the destination to return it
		// The name will be the same as the source object's name
		srcLeaf, _, _ := srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false) // Get original leaf name
		dir, _ := dircache.SplitPath(remote)
		dstPath := path.Join(dir, srcLeaf)       // Construct potential destination path
		newObj, err := f.NewObject(ctx, dstPath) // Find the newly copied object
		if err != nil {
			return nil, fmt.Errorf("failed to find copied object in destination after share copy: %w", err)
		}
		// If the target remote name is different, rename the copied object
		dstLeaf := path.Base(remote)
		if srcLeaf != dstLeaf {
			err = f.renameFile(ctx, newObj.(*Object).id, dstLeaf)
			if err != nil {
				return nil, fmt.Errorf("failed to rename after copy from share: %w", err)
			}
			// Re-fetch the object to get updated metadata/remote path
			finalObj, err := f.NewObject(ctx, remote)
			if err != nil {
				return nil, fmt.Errorf("failed to find renamed object after share copy: %w", err)
			}
			return finalObj, nil
		}
		return newObj, nil
	}

	// --- Standard Copy (Non-Share Source) ---
	if f.isShare {
		// Cannot copy *to* a shared remote
		return nil, errors.New("copying to a shared remote is not supported")
	}

	// Ensure metadata is read for srcObj.id
	err := srcObj.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read source metadata for copy: %w", err)
	}
	if srcObj.id == "" {
		return nil, errors.New("cannot copy object with empty ID")
	}

	// Find destination parent directory ID, creating if necessary
	dstLeaf, dstParentID, err := f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return nil, fmt.Errorf("failed to find destination directory for copy: %w", err)
	}

	// Check if source and destination parent are the same
	srcParentID := srcObj.parent
	if srcParentID == "" { // Try to get from cache if not on object
		_, srcParentID, _ = srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false)
	}
	if srcParentID == dstParentID {
		// API restriction: cannot copy within the same directory
		// We could potentially handle this by copying to a temp dir and moving,
		// but for now, return ErrorCantCopy.
		fs.Debugf(src, "Can't copy - source and destination directory are the same (%q)", srcParentID)
		return nil, fs.ErrorCantCopy
	}

	// Perform the copy using OpenAPI
	fs.Debugf(srcObj, "Copying %q to %q", srcObj.id, dstParentID)
	err = f.copyFiles(ctx, []string{srcObj.id}, dstParentID)
	if err != nil {
		return nil, fmt.Errorf("server-side copy failed: %w", err)
	}

	// Find the newly created object in the destination directory
	// It will initially have the same name as the source object.
	srcLeaf, _, _ := srcObj.fs.dirCache.FindPath(ctx, srcObj.remote, false) // Get original leaf name
	dir, _ := dircache.SplitPath(remote)
	copiedObjPath := path.Join(dir, srcLeaf)       // Construct path where copied object should be
	newObj, err := f.NewObject(ctx, copiedObjPath) // Find the object at that path
	if err != nil {
		return nil, fmt.Errorf("failed to find copied object in destination: %w", err)
	}
	newObjConcrete := newObj.(*Object)

	// Rename the copied object if the target remote name is different
	if srcLeaf != dstLeaf {
		fs.Debugf(newObj, "Renaming copied object to %q", dstLeaf)
		err = f.renameFile(ctx, newObjConcrete.id, dstLeaf)
		if err != nil {
			// Attempt to delete the wrongly named copy? Or just return error?
			_ = f.deleteFiles(ctx, []string{newObjConcrete.id}) // Best effort cleanup
			return nil, fmt.Errorf("failed to rename after copy: %w", err)
		}
		// Update the object's remote path and mark metadata stale
		newObjConcrete.remote = remote
		newObjConcrete.hasMetaData = false
		// Read metadata again to confirm rename and get latest info
		err = newObjConcrete.readMetaData(ctx)
		if err != nil {
			fs.Logf(newObj, "Failed to read metadata after rename: %v", err)
		} else {
			newObjConcrete.hasMetaData = true
		}
	} else {
		// If no rename needed, ensure the returned object has the correct remote path
		newObjConcrete.remote = remote
	}

	return newObjConcrete, nil
}

// purgeCheck removes the root directory. Refuses if check=true and not empty.
func (f *Fs) purgeCheck(ctx context.Context, dir string, check bool) error {
	if f.isShare {
		return errors.New("unsupported operation: Purge/Rmdir on shared filesystem")
	}
	root := path.Join(f.root, dir)
	if root == "" && dir != "" { // Check if trying to delete the effective root specified in config
		// This case needs careful handling. If root_folder_id is set, `dir` might be ""
		// but `f.rootFolderID` is not "0".
		if f.rootFolderID == "0" {
			return errors.New("internal error: attempting to purge root directory")
		}
		// Allow purging the configured root folder ID
	} else if root == "" && dir == "" && f.rootFolderID == "0" {
		// Explicitly prevent purging the absolute root "0"
		return errors.New("refusing to purge the absolute root directory '0'")
	}

	// Find the ID of the directory to purge
	dirID, err := f.dirCache.FindDir(ctx, dir, false) // Don't create
	if err != nil {
		return err // Return DirNotFound or other errors
	}

	if check {
		// Check if directory is empty using listAll with limit 1
		found, listErr := f.listAll(ctx, dirID, 1, false, false, func(item *api.File) bool {
			fs.Debugf(f, "Rmdir check: directory %q contains %q", dir, item.FileNameBest())
			return true // Found an item, stop listing
		})
		if listErr != nil {
			return fmt.Errorf("failed to check if directory %q is empty: %w", dir, listErr)
		}
		if found {
			return fs.ErrorDirectoryNotEmpty
		}
	}

	// Perform the delete using OpenAPI
	fs.Debugf(f, "Purging directory %q (ID: %q)", dir, dirID)
	err = f.deleteFiles(ctx, []string{dirID})
	if err != nil {
		return fmt.Errorf("failed to delete directory %q (ID: %q): %w", dir, dirID, err)
	}

	// Flush the directory from cache
	f.dirCache.FlushDir(dir)
	return nil
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	return f.purgeCheck(ctx, dir, true) // check = true
}

// Purge removes a directory and all its contents.
func (f *Fs) Purge(ctx context.Context, dir string) error {
	return f.purgeCheck(ctx, dir, false) // check = false
}

// About gets quota information (currently uses traditional API).
// TODO: Check if OpenAPI provides a quota endpoint.
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	// Using traditional indexInfo for now
	info, err := f.indexInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get usage info: %w", err)
	}

	usage := &fs.Usage{}
	if totalInfo, ok := info.SpaceInfo["all_total"]; ok {
		usage.Total = fs.NewUsageValue(int64(totalInfo.Size))
	}
	if useInfo, ok := info.SpaceInfo["all_use"]; ok {
		usage.Used = fs.NewUsageValue(int64(useInfo.Size))
	}
	if remainInfo, ok := info.SpaceInfo["all_remain"]; ok {
		usage.Free = fs.NewUsageValue(int64(remainInfo.Size))
	}

	return usage, nil
}

// Shutdown shuts down the fs, closing any background tasks
func (f *Fs) Shutdown(ctx context.Context) error {
	if f.tokenRenewer != nil {
		f.tokenRenewer.Shutdown()
		f.tokenRenewer = nil
	}

	// å…³é—­æ‰€æœ‰BadgerDBç¼“å­˜
	caches := []*cache.BadgerCache{
		f.pathResolveCache,
		f.dirListCache,
		f.metadataCache,
		f.fileIDCache,
	}

	for i, c := range caches {
		if c != nil {
			if err := c.Close(); err != nil {
				fs.Debugf(f, "å…³é—­BadgerDBç¼“å­˜%då¤±è´¥: %v", i, err)
			}
		}
	}

	return nil
}

// itemToDirEntry converts an api.File to an fs.DirEntry
func (f *Fs) itemToDirEntry(ctx context.Context, remote string, item *api.File) (entry fs.DirEntry, err error) {
	if item.IsDir() {
		// Cache the directory ID
		f.dirCache.Put(remote, item.ID())
		d := fs.NewDir(remote, item.ModTime()).SetID(item.ID()).SetParentID(item.ParentID())
		return d, nil
	}
	// It's a file
	entry, err = f.newObjectWithInfo(ctx, remote, item)
	if err == fs.ErrorObjectNotFound {
		return nil, nil // Should not happen if item came from listing
	}
	return entry, err
}

// newObjectWithInfo creates an fs.Object from an api.File or by reading metadata.
func (f *Fs) newObjectWithInfo(ctx context.Context, remote string, info *api.File) (fs.Object, error) {
	fs.Debugf(f, "ğŸ” newObjectWithInfoå¼€å§‹: remote=%q, hasInfo=%v", remote, info != nil)

	o := &Object{
		fs:     f,
		remote: remote,
		durlMu: new(sync.Mutex),
	}
	var err error
	if info != nil {
		// Set metadata from provided info
		err = o.setMetaData(info)
	} else {
		// Read metadata from the backend
		err = o.readMetaData(ctx)
	}
	if err != nil {
		fs.Debugf(f, "ğŸ” newObjectWithInfoå¤±è´¥: %v", err)
		return nil, err
	}
	return o, nil
}

// readMetaDataForPath finds metadata for a specific file path.
func (f *Fs) readMetaDataForPath(ctx context.Context, path string) (info *api.File, err error) {
	fs.Debugf(f, "ğŸ” readMetaDataForPathå¼€å§‹: path=%q", path)

	leaf, dirID, err := f.dirCache.FindPath(ctx, path, false)
	if err != nil {
		fs.Debugf(f, "ğŸ” readMetaDataForPath: FindPathå¤±è´¥: %v", err)
		// æ£€æŸ¥æ˜¯å¦æ˜¯APIé™åˆ¶é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™ç«‹å³è¿”å›ï¼Œé¿å…è·¯å¾„æ··ä¹±
		if isAPILimitError(err) {
			fs.Debugf(f, "readMetaDataForPathé‡åˆ°APIé™åˆ¶é”™è¯¯ï¼Œè·¯å¾„: %q, é”™è¯¯: %v", path, err)
			// é‡‡ç”¨æ¸è¿›å¼ç¼“å­˜æ¸…ç†ç­–ç•¥
			f.clearCacheGradually(err, path)
			return nil, err
		}
		if err == fs.ErrorDirNotFound {
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, "ğŸ” readMetaDataForPath: FindPathæˆåŠŸ, leaf=%q, dirID=%q", leaf, dirID)

	// List the directory and find the leaf
	fs.Debugf(f, "ğŸ” readMetaDataForPath: å¼€å§‹è°ƒç”¨listAll")
	found, err := f.listAll(ctx, dirID, f.opt.ListChunk, true, false, func(item *api.File) bool {
		// Compare with decoded name to handle special characters correctly
		decodedName := f.opt.Enc.ToStandardName(item.FileNameBest())
		if decodedName == leaf {
			fs.Debugf(f, "ğŸ” readMetaDataForPath: æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: %q", decodedName)
			info = item
			return true // Found it
		}
		return false // Keep looking
	})
	if err != nil {
		fs.Debugf(f, "ğŸ” readMetaDataForPath: listAllå¤±è´¥: %v", err)
		return nil, fmt.Errorf("failed to list directory %q to find %q: %w", dirID, leaf, err)
	}
	if !found {
		fs.Debugf(f, "ğŸ” readMetaDataForPath: æœªæ‰¾åˆ°æ–‡ä»¶")
		return nil, fs.ErrorObjectNotFound
	}
	fs.Debugf(f, "ğŸ” readMetaDataForPath: æˆåŠŸæ‰¾åˆ°æ–‡ä»¶å…ƒæ•°æ®")
	return info, nil
}

// createObject creates a placeholder Object struct before upload.
func (f *Fs) createObject(ctx context.Context, remote string, modTime time.Time, size int64) (o *Object, leaf string, dirID string, err error) {
	// Create the parent directory if it doesn't exist
	leaf, dirID, err = f.dirCache.FindPath(ctx, remote, true)
	if err != nil {
		return
	}
	// Temporary Object under construction
	o = &Object{
		fs:          f,
		remote:      remote,
		parent:      dirID,
		size:        size,
		modTime:     modTime,
		hasMetaData: false, // Metadata not set yet
		durlMu:      new(sync.Mutex),
	}
	return o, leaf, dirID, nil
}

// ------------------------------------------------------------
// Command Help & Execution
// ------------------------------------------------------------

var commandHelp = []fs.CommandHelp{{
	Name:  "addurls",
	Short: "Add offline download task for urls (uses traditional API)",
	Long: `This command adds offline download task for urls using the traditional API.

Usage:

    rclone backend addurls 115:dirpath url1 url2

Downloads are saved to the folder "dirpath". If omitted or non-existent,
it defaults to "äº‘ä¸‹è½½". Requires cookie authentication.
This command always exits with code 0; check output for errors.`,
}, {
	Name:  "getid",
	Short: "Get the ID of a file or directory",
	Long: `This command obtains the ID of a file or directory using the OpenAPI.

Usage:

    rclone backend getid 115:path/to/item

Returns the internal ID used by the 115 API.`,
}, {
	Name:  "addshare",
	Short: "Add shared files/dirs from a share link (uses traditional API)",
	Long: `This command adds shared files/dirs from a share link using the traditional API.

Usage:

    rclone backend addshare 115:dirpath share_link

Content from the link is copied to "dirpath". Requires cookie authentication.`,
}, {
	Name:  "stats",
	Short: "Get folder statistics (uses OpenAPI)",
	Long: `This command retrieves statistics for a folder using the OpenAPI.

Usage:

    rclone backend stats 115:path/to/folder

Returns information like total size, file count, folder count, etc.`,
}, {
	Name:  "getdownloadurl",
	Short: "Get the download URL of a file by its path",
	Long: `This command retrieves the download URL of a file using its path.

Usage:

rclone backend getdownloadurl 115:path/to/file

The command returns the download URL for the specified file. Ensure the file path is correct.`,
}, {
	Name:  "getdownloadurlau",
	Short: "Get the download URL of a file by its path",
	Long: `This command retrieves the download URL of a file using its path.

Usage:

rclone backend getdownloadurlau 115:path/to/file VidHub/1.7.24

The command returns the download URL for the specified file. Ensure the file path is correct.`,
},
}

// Command executes backend-specific commands.
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {
	case "addurls":
		if f.isShare {
			return nil, errors.New("addurls unsupported for shared filesystem")
		}
		dir := "" // Default to root or äº‘ä¸‹è½½ handled by API
		if parentDir, ok := opt["dir"]; ok {
			dir = parentDir
		}
		return f.addURLs(ctx, dir, arg) // Uses traditional API
	case "getid":
		path := ""
		if len(arg) > 0 {
			path = arg[0]
		}
		return f.getID(ctx, path) // Uses OpenAPI via listAll/FindDir/NewObject
	case "addshare":
		if f.isShare {
			return nil, errors.New("addshare unsupported for shared filesystem")
		}
		if len(arg) < 1 {
			return nil, errors.New("addshare requires a share link argument")
		}
		shareCode, receiveCode, err := parseShareLink(arg[0])
		if err != nil {
			return nil, err
		}
		dirID, err := f.dirCache.FindDir(ctx, "", true) // Find target dir ID (create if needed)
		if err != nil {
			return nil, err
		}
		return nil, f.copyFromShare(ctx, shareCode, receiveCode, "", dirID) // Uses traditional API
	case "getdownloadurlua":
		path := ""
		ua := ""
		if len(arg) > 0 {
			ua = arg[0]
		}

		return f.getDownloadURLByUA(ctx, path, ua)
	default:
		return nil, fs.ErrorCommandNotFound
	}
}

type FileInfoResponse struct {
	State   bool      `json:"state"`
	Message string    `json:"message"`
	Code    int       `json:"code"`
	Data    *struct { // ç›´æ¥å®šä¹‰åŒ¿åç»“æ„ä½“ï¼ŒåªåŒ…å« PickCode
		PickCode string `json:"pick_code"`
	} `json:"data"`
}

func (f *Fs) GetPickCodeByPath(ctx context.Context, path string) (string, error) {
	// ä½¿ç”¨CallOpenAPIé€šè¿‡pacerè¿›è¡Œè°ƒç”¨
	formData := url.Values{}
	formData.Add("path", path)

	opts := rest.Opts{
		Method:      "POST",
		Path:        "/open/folder/get_info",
		ContentType: "application/x-www-form-urlencoded",
		Body:        strings.NewReader(formData.Encode()),
	}

	var response struct {
		State   int    `json:"state"`
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			PickCode string `json:"pick_code"`
		} `json:"data"`
	}

	err := f.CallOpenAPI(ctx, &opts, nil, &response, false)
	if err != nil {
		return "", fmt.Errorf("è·å–PickCodeå¤±è´¥: %w", err)
	}

	// æ£€æŸ¥APIè¿”å›çš„çŠ¶æ€
	if response.Code != 0 {
		return "", fmt.Errorf("APIè¿”å›é”™è¯¯: %s (Code: %d)", response.Message, response.Code)
	}

	// è¿”å› PickCode
	return response.Data.PickCode, nil
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, UA string) (string, error) {
	pickCode, err := f.GetPickCodeByPath(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to read metadata for file path %q: %w", filePath, err)
	}

	// å¦‚æœæ²¡æœ‰æä¾› UAï¼Œä½¿ç”¨é»˜è®¤å€¼
	if UA == "" {
		UA = defaultUserAgent
	}

	// ä½¿ç”¨CallOpenAPIé€šè¿‡pacerè¿›è¡Œè°ƒç”¨
	opts := rest.Opts{
		Method:      "POST",
		Path:        "/open/ufile/downurl",
		ContentType: "application/x-www-form-urlencoded",
		Body:        strings.NewReader("pick_code=" + pickCode),
		ExtraHeaders: map[string]string{
			"User-Agent": UA,
		},
	}

	var response ApiResponse
	err = f.CallOpenAPI(ctx, &opts, nil, &response, false)
	if err != nil {
		return "", fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
	}

	for _, downInfo := range response.Data {
		if downInfo != (FileInfo{}) {
			fs.Infof(nil, "è·å–åˆ°ä¸‹è½½URL: %s", downInfo.URL.URL)
			return downInfo.URL.URL, nil
		}
	}
	return "", fmt.Errorf("æœªä»APIå“åº”ä¸­è·å–åˆ°ä¸‹è½½URL")
}

// ------------------------------------------------------------
// Object Methods
// ------------------------------------------------------------

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	err := o.readMetaData(ctx)
	if err != nil {
		fs.Logf(o, "failed to read metadata for ModTime: %v", err)
		// Return a zero time instead of Now() as Precision is NotSupported
		return time.Time{}
	}
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// Return size immediately if known, otherwise read metadata
	if o.hasMetaData || o.size > 0 { // Check if size is already populated
		return o.size
	}
	err := o.readMetaData(context.TODO()) // Use TODO context for simplicity here
	if err != nil {
		fs.Logf(o, "failed to read metadata for Size: %v", err)
		return -1 // Indicate error or unknown size
	}
	return o.size
}

// Hash returns the SHA1 checksum
func (o *Object) Hash(ctx context.Context, t hash.Type) (string, error) {
	if t != hash.SHA1 {
		return "", hash.ErrUnsupported
	}
	// Return hash immediately if known, otherwise read metadata
	if o.hasMetaData || o.sha1sum != "" {
		return o.sha1sum, nil
	}
	err := o.readMetaData(ctx)
	if err != nil {
		return "", fmt.Errorf("failed to read metadata for Hash: %w", err)
	}
	return o.sha1sum, nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// Return ID immediately if known, otherwise read metadata
	if o.hasMetaData || o.id != "" {
		return o.id
	}
	// Reading metadata just for ID might be inefficient, but necessary if not cached
	err := o.readMetaData(context.TODO()) // Use TODO context
	if err != nil {
		fs.Logf(o, "failed to read metadata for ID: %v", err)
		return "" // Return empty string on error
	}
	return o.id
}

// ParentID returns the parent ID of the Object
func (o *Object) ParentID() string {
	// Return parent immediately if known, otherwise read metadata
	if o.hasMetaData || o.parent != "" {
		return o.parent
	}
	err := o.readMetaData(context.TODO()) // Use TODO context
	if err != nil {
		fs.Logf(o, "failed to read metadata for ParentID: %v", err)
		return "" // Return empty string on error
	}
	return o.parent
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// open opens the object for reading.
func (o *Object) open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// ğŸ”’ å¹¶å‘å®‰å…¨ä¿®å¤ï¼šä½¿ç”¨äº’æ–¥é”ä¿æŠ¤å¯¹durlçš„è®¿é—®
	o.durlMu.Lock()

	// æ£€æŸ¥URLæœ‰æ•ˆæ€§ï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if o.durl == nil || !o.durl.Valid() {
		o.durlMu.Unlock()
		return nil, errors.New("download URL is invalid or expired")
	}

	// åˆ›å»ºURLçš„æœ¬åœ°å‰¯æœ¬ï¼Œé¿å…åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­è¢«å…¶ä»–çº¿ç¨‹ä¿®æ”¹
	downloadURL := o.durl.URL
	o.durlMu.Unlock()

	// ä½¿ç”¨æœ¬åœ°å‰¯æœ¬åˆ›å»ºè¯·æ±‚ï¼Œé¿å…å¹¶å‘è®¿é—®é—®é¢˜
	req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
	if err != nil {
		return nil, err
	}
	fs.FixRangeOption(options, o.size)
	fs.OpenOptionAddHTTPHeaders(req.Header, options)
	if o.size == 0 {
		// Don't supply range requests for 0 length objects as they always fail
		delete(req.Header, "Range")
	}
	resp, err := o.fs.openAPIClient.Do(req)
	if err != nil {
		return nil, err
	}
	return resp.Body, nil
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// Ensure metadata (specifically pickCode or ID) is available
	err := o.readMetaData(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to read metadata before open: %w", err)
	}

	if o.size == 0 {
		// No need for download URL for 0-byte files
		return io.NopCloser(bytes.NewReader(nil)), nil
	}

	// ğŸ”§ 115ç½‘ç›˜å¹¶å‘ä¸‹è½½ä¼˜åŒ–ï¼šä¿®å¤è¿›åº¦æ˜¾ç¤ºé—®é¢˜
	if o.size > 100*1024*1024 && o.shouldUseConcurrentDownload(ctx, options) {
		fs.Infof(o, "ğŸš€ 115ç½‘ç›˜å¤§æ–‡ä»¶æ£€æµ‹ï¼Œå¯ç”¨å¹¶å‘ä¸‹è½½ä¼˜åŒ–(å¹¶å‘æ•°=2): %s", fs.SizeSuffix(o.size))
		return o.openWithConcurrency(ctx, options...)
	}

	// Get/refresh download URL
	err = o.setDownloadURL(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get download URL: %w", err)
	}
	if !o.durl.Valid() {
		// Attempt refresh again if invalid right after getting it
		fs.Debugf(o, "Download URL invalid immediately after fetching, retrying...")
		time.Sleep(500 * time.Millisecond) // Small delay before retry
		err = o.setDownloadURL(ctx)
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL on retry: %w", err)
		}
		if !o.durl.Valid() {
			return nil, errors.New("failed to obtain a valid download URL")
		}
	}

	// Open the URL
	return o.open(ctx, options...)
}

// Update the object with new content.
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	if o.fs.isShare {
		return errors.New("unsupported operation: Update on shared filesystem")
	}
	if src.Size() < 0 {
		return errors.New("refusing to update with unknown size")
	}

	// Start the token renewer if we have a valid one
	if o.fs.tokenRenewer != nil {
		o.fs.tokenRenewer.Start()
		defer o.fs.tokenRenewer.Stop()
	}

	// Ensure metadata is read for the existing object
	err := o.readMetaData(ctx)
	if err != nil {
		return fmt.Errorf("failed to read metadata of existing object for update: %w", err)
	}
	oldID := o.id // Keep track of the old ID

	// Upload the new content using putUnchecked (handles creation logic)
	// This will place the new file at the same remote path.
	newObj, err := o.fs.putUnchecked(ctx, in, src, o.remote, options...)
	if err != nil {
		return fmt.Errorf("upload during update failed: %w", err)
	}

	newO := newObj.(*Object)

	// If the upload resulted in a *new* file ID (not an overwrite), delete the old one.
	if oldID != "" && newO.id != oldID {
		fs.Debugf(o, "Update created new object %q, removing old object %q", newO.id, oldID)
		err = o.fs.deleteFiles(ctx, []string{oldID})
		if err != nil {
			// Log error but don't fail the update, the new file is there
			fs.Errorf(o, "Failed to remove old version %q after update: %v", oldID, err)
		}
	} else {
		fs.Debugf(o, "Update likely overwrote existing object %q", oldID)
	}

	// Replace the metadata of the original object `o` with the new object's data
	*o = *newO

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	if o.fs.isShare {
		return errors.New("unsupported operation: Remove on shared filesystem")
	}
	// Ensure metadata (ID) is read
	err := o.readMetaData(ctx)
	if err != nil {
		// If object not found, Remove should succeed
		if errors.Is(err, fs.ErrorObjectNotFound) {
			return nil
		}
		return fmt.Errorf("failed to read metadata before remove: %w", err)
	}
	if o.id == "" {
		return errors.New("cannot remove object with empty ID")
	}

	err = o.fs.deleteFiles(ctx, []string{o.id})
	if err != nil {
		return fmt.Errorf("failed to delete object %q: %w", o.id, err)
	}

	// åˆ é™¤æ–‡ä»¶æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	o.fs.invalidateRelatedCaches115(o.remote, "remove")

	return nil
}

// setMetaData updates the object's metadata from an api.File struct.
func (o *Object) setMetaData(info *api.File) error {
	if info == nil {
		return errors.New("cannot set metadata from nil info")
	}
	if info.IsDir() {
		// This indicates we tried to create an Object for a directory path
		return fs.ErrorIsDir
	}
	o.id = info.ID()
	o.parent = info.ParentID()
	o.size = info.FileSizeBest()
	o.sha1sum = strings.ToLower(info.Sha1Best())
	o.pickCode = info.PickCodeBest()
	o.modTime = info.ModTime()
	o.hasMetaData = true
	return nil
}

// setMetaDataFromCallBack updates metadata after an upload callback.
func (o *Object) setMetaDataFromCallBack(data *api.CallbackData) error {
	if data == nil {
		return errors.New("cannot set metadata from nil callback data")
	}
	// Assume size and modTime are already set from the source info
	o.id = data.FileID
	o.parent = data.CID // Callback provides parent CID
	o.pickCode = data.PickCode
	o.sha1sum = strings.ToLower(data.Sha)
	// Update size from callback if available and different?
	if data.FileSize > 0 {
		o.size = int64(data.FileSize)
	}
	// ModTime is usually the upload time, keep the original source ModTime
	o.hasMetaData = true
	return nil
}

// readMetaData gets the metadata if it hasn't already been fetched.
func (o *Object) readMetaData(ctx context.Context) error {
	if o.hasMetaData {
		return nil
	}

	// Use the path-based lookup
	info, err := o.fs.readMetaDataForPath(ctx, o.remote)
	if err != nil {
		fs.Debugf(o.fs, "ğŸ” readMetaDataå¤±è´¥: %v", err)
		return err // fs.ErrorObjectNotFound or other errors
	}

	err = o.setMetaData(info)
	if err != nil {
		fs.Debugf(o.fs, "ğŸ” readMetaData: setMetaDataå¤±è´¥: %v", err)
	}
	return err
}

// setDownloadURL ensures a valid download URL is available with optimized concurrent access.
func (o *Object) setDownloadURL(ctx context.Context) error {
	// ğŸ”’ å¹¶å‘å®‰å…¨ä¿®å¤ï¼šå¿«é€Ÿè·¯å¾„ä¹Ÿéœ€è¦é”ä¿æŠ¤
	o.durlMu.Lock()

	// æ£€æŸ¥URLæ˜¯å¦å·²ç»æœ‰æ•ˆï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if o.durl != nil && o.durl.Valid() {
		o.durlMu.Unlock()
		return nil
	}

	// URLæ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œéœ€è¦è·å–æ–°çš„URLï¼ˆç»§ç»­æŒæœ‰é”ï¼‰
	// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ‰€æœ‰æ£€æŸ¥
	if o.durl != nil && o.durl.Valid() {
		fs.Debugf(o, "Download URL was fetched by another goroutine")
		o.durlMu.Unlock()
		return nil
	}
	// æ³¨æ„ï¼šè¿™é‡Œä¸é‡Šæ”¾é”ï¼Œç»§ç»­åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡ŒURLè·å–

	var err error
	var newURL *api.DownloadURL

	// Add timeout context for URL fetching to prevent hanging
	urlCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	if o.fs.isShare {
		// Use traditional share download API
		if o.id == "" {
			return errors.New("cannot get share download URL without file ID")
		}

		newURL, err = o.fs.getDownloadURLFromShare(urlCtx, o.id)
	} else {
		// Use OpenAPI download URL endpoint
		if o.pickCode == "" {
			// If pickCode is missing, try getting it from metadata first
			metaErr := o.readMetaData(urlCtx)
			if metaErr != nil || o.pickCode == "" {
				return fmt.Errorf("cannot get download URL without pick code (metadata read error: %v)", metaErr)
			}
		}

		newURL, err = o.fs.getDownloadURL(urlCtx, o.pickCode)
	}

	if err != nil {
		o.durl = nil      // Clear invalid URL
		o.durlMu.Unlock() // ğŸ”’ é”™è¯¯è·¯å¾„é‡Šæ”¾é”

		// æ£€æŸ¥æ˜¯å¦æ˜¯502é”™è¯¯
		if strings.Contains(err.Error(), "502") || strings.Contains(err.Error(), "Bad Gateway") {

			return fmt.Errorf("server overload (502) when fetching download URL: %w", err)
		}

		// Check if it's a context timeout
		if errors.Is(err, context.DeadlineExceeded) {
			return fmt.Errorf("timeout fetching download URL: %w", err)
		}

		return fmt.Errorf("failed to get download URL: %w", err)
	}

	o.durl = newURL
	if !o.durl.Valid() {
		// This might happen if the link expires immediately or is invalid
		fs.Logf(o, "Fetched download URL is invalid or expired immediately: %s", o.durl.URL)
		// æ¸…é™¤æ— æ•ˆçš„URLï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°è·å–
		o.durl = nil
		o.durlMu.Unlock() // ğŸ”’ æ— æ•ˆURLè·¯å¾„é‡Šæ”¾é”
		return fmt.Errorf("fetched download URL is invalid or expired immediately")
	} else {
		fs.Debugf(o, "Successfully fetched download URL")
	}
	o.durlMu.Unlock() // ğŸ”’ æˆåŠŸè·¯å¾„é‡Šæ”¾é”
	return nil
}

// setDownloadURLWithForce sets the download URL for the object with optional cache bypass
func (o *Object) setDownloadURLWithForce(ctx context.Context, forceRefresh bool) error {
	// ğŸ”’ å¹¶å‘å®‰å…¨ä¿®å¤ï¼šå¿«é€Ÿè·¯å¾„ä¹Ÿéœ€è¦é”ä¿æŠ¤
	o.durlMu.Lock()

	// å¦‚æœä¸æ˜¯å¼ºåˆ¶åˆ·æ–°ï¼Œæ£€æŸ¥URLæ˜¯å¦å·²ç»æœ‰æ•ˆï¼ˆåœ¨é”ä¿æŠ¤ä¸‹ï¼‰
	if !forceRefresh && o.durl != nil && o.durl.Valid() {
		o.durlMu.Unlock()
		return nil
	}

	// URLæ— æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œæˆ–è€…å¼ºåˆ¶åˆ·æ–°ï¼Œéœ€è¦è·å–æ–°çš„URLï¼ˆç»§ç»­æŒæœ‰é”ï¼‰
	// Double-check: ç¡®ä¿åœ¨é”ä¿æŠ¤ä¸‹è¿›è¡Œæ‰€æœ‰æ£€æŸ¥
	if !forceRefresh && o.durl != nil && o.durl.Valid() {
		fs.Debugf(o, "Download URL was fetched by another goroutine")
		o.durlMu.Unlock()
		return nil
	}
	// æ³¨æ„ï¼šè¿™é‡Œä¸é‡Šæ”¾é”ï¼Œç»§ç»­åœ¨é”ä¿æŠ¤ä¸‹æ‰§è¡ŒURLè·å–

	var err error
	var newURL *api.DownloadURL

	// Add timeout context for URL fetching to prevent hanging
	urlCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	if o.fs.isShare {
		// Use traditional share download API
		if o.id == "" {
			o.durlMu.Unlock()
			return errors.New("cannot get share download URL without file ID")
		}

		newURL, err = o.fs.getDownloadURLFromShare(urlCtx, o.id)
	} else {
		// Use OpenAPI download URL endpoint
		if o.pickCode == "" {
			// If pickCode is missing, try getting it from metadata first
			metaErr := o.readMetaData(urlCtx)
			if metaErr != nil || o.pickCode == "" {
				o.durlMu.Unlock()
				return fmt.Errorf("cannot get download URL without pick code (metadata read error: %v)", metaErr)
			}
		}

		newURL, err = o.fs.getDownloadURLWithForce(urlCtx, o.pickCode, forceRefresh)
	}

	if err != nil {
		o.durl = nil      // Clear invalid URL
		o.durlMu.Unlock() // ğŸ”’ é”™è¯¯è·¯å¾„é‡Šæ”¾é”

		// æ£€æŸ¥æ˜¯å¦æ˜¯502é”™è¯¯
		if strings.Contains(err.Error(), "502") || strings.Contains(err.Error(), "Bad Gateway") {
			return fmt.Errorf("server overload (502) when fetching download URL: %w", err)
		}

		// Check if it's a context timeout
		if errors.Is(err, context.DeadlineExceeded) {
			return fmt.Errorf("timeout fetching download URL: %w", err)
		}

		return fmt.Errorf("failed to get download URL: %w", err)
	}

	o.durl = newURL
	if !o.durl.Valid() {
		// This might happen if the link expires immediately or is invalid
		fs.Logf(o, "Fetched download URL is invalid or expired immediately: %s", o.durl.URL)
		// æ¸…é™¤æ— æ•ˆçš„URLï¼Œå¼ºåˆ¶ä¸‹æ¬¡é‡æ–°è·å–
		o.durl = nil
		o.durlMu.Unlock() // ğŸ”’ æ— æ•ˆURLè·¯å¾„é‡Šæ”¾é”
		return fmt.Errorf("fetched download URL is invalid or expired immediately")
	} else {
		fs.Debugf(o, "Successfully fetched download URL")
	}
	o.durlMu.Unlock() // ğŸ”’ æˆåŠŸè·¯å¾„é‡Šæ”¾é”
	return nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs              = (*Fs)(nil)
	_ fs.Purger          = (*Fs)(nil)
	_ fs.Copier          = (*Fs)(nil)
	_ fs.Mover           = (*Fs)(nil)
	_ fs.DirMover        = (*Fs)(nil)
	_ fs.DirCacheFlusher = (*Fs)(nil)
	_ fs.MergeDirser     = (*Fs)(nil)
	_ fs.PutUncheckeder  = (*Fs)(nil)
	_ fs.Abouter         = (*Fs)(nil)
	_ fs.Commander       = (*Fs)(nil)
	_ fs.Object          = (*Object)(nil)
	_ fs.ObjectInfo      = (*Object)(nil)
	_ fs.IDer            = (*Object)(nil)
	_ fs.ParentIDer      = (*Object)(nil)
	_ fs.Shutdowner      = (*Fs)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, m configmap.Mapper) bool {
	// Try to load the token using oauthutil's method instead of
	// directly accessing the config file
	token, err := oauthutil.GetToken(f.originalName, m)
	if err != nil {
		fs.Debugf(f, "Failed to get token from config: %v", err)
		return false
	}

	if token == nil || token.AccessToken == "" || token.RefreshToken == "" {
		fs.Debugf(f, "Token from config is incomplete")
		return false
	}

	// Extract token components
	f.accessToken = token.AccessToken
	f.refreshToken = token.RefreshToken
	f.tokenExpiry = token.Expiry

	// Check if we got valid token data
	if f.accessToken == "" || f.refreshToken == "" {
		return false
	}

	fs.Debugf(f, "Loaded token from config file, expires at %v", f.tokenExpiry)
	return true
}

// rclone backend getdownloadurlua "116:/ç”µå½±/2025_ç”µå½±/ç‹¬è£è€… (2012) {tmdb-76493} 23.8GB.mkv" "VidHub/1.7.2"

// downloadWithConcurrency 115ç½‘ç›˜å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½å®ç°
// ğŸ”§ ä¼˜åŒ–ï¼šåŸºäº123ç½‘ç›˜çš„å¹¶å‘ä¸‹è½½æ¶æ„ï¼Œä¸“é—¨ä¸º115ç½‘ç›˜ä¼˜åŒ–
func (f *Fs) downloadWithConcurrency(ctx context.Context, srcObj *Object, tempFile *os.File, fileSize int64) (int64, error) {
	// ğŸš€ ç½‘ç»œå‚æ•°è°ƒä¼˜ï¼šä¼˜åŒ–TCPè¿æ¥å‚æ•°
	f.optimizeNetworkParameters(ctx)

	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å‚æ•°
	chunkSize := f.calculateDownloadChunkSize(fileSize)
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	maxConcurrency := f.calculateDownloadConcurrency(fileSize)

	fs.Infof(f, "ğŸ“Š 115ç½‘ç›˜å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	fs.Infof(f, "ğŸš€ 115ç½‘ç›˜å¼€å§‹å¹¶å‘ä¸‹è½½: %s (%s)", srcObj.remote, fs.SizeSuffix(fileSize))

	// ä½¿ç”¨å¹¶å‘ä¸‹è½½å®ç°
	downloadStartTime := time.Now()
	err := f.downloadChunksConcurrently(ctx, srcObj, tempFile, chunkSize, numChunks, int64(maxConcurrency))
	downloadDuration := time.Since(downloadStartTime)
	if err != nil {
		return 0, fmt.Errorf("115ç½‘ç›˜å¹¶å‘ä¸‹è½½å¤±è´¥: %w", err)
	}

	// éªŒè¯ä¸‹è½½å®Œæ•´æ€§
	stat, err := tempFile.Stat()
	if err != nil {
		return 0, fmt.Errorf("è·å–ä¸‹è½½æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	actualSize := stat.Size()
	if actualSize != fileSize {
		return 0, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(actualSize))
	}

	// ğŸ”§ ä¼˜åŒ–ï¼šè¯¦ç»†æ€§èƒ½ç»Ÿè®¡å’Œç›‘æ§
	avgSpeed := float64(actualSize) / downloadDuration.Seconds() / 1024 / 1024 // MB/s

	// æ€§èƒ½è¯„ä¼°
	performanceLevel := f.evaluateDownloadPerformance(avgSpeed, maxConcurrency)

	fs.Infof(f, "âœ… 115ç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s | ç”¨æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s | æ€§èƒ½: %s",
		fs.SizeSuffix(actualSize), downloadDuration, avgSpeed, performanceLevel)

	// æ€§èƒ½å»ºè®®
	f.suggestPerformanceOptimizations(avgSpeed, maxConcurrency, fileSize)

	return actualSize, nil
}

// DownloadProgress 115ç½‘ç›˜ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
type DownloadProgress struct {
	totalChunks     int64
	completedChunks int64
	totalBytes      int64
	downloadedBytes int64
	startTime       time.Time
	lastUpdateTime  time.Time
	chunkSizes      map[int64]int64         // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„å¤§å°
	chunkTimes      map[int64]time.Duration // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„ä¸‹è½½æ—¶é—´
	peakSpeed       float64                 // å³°å€¼é€Ÿåº¦ MB/s
	mu              sync.RWMutex
}

// NewDownloadProgress åˆ›å»ºæ–°çš„ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
func NewDownloadProgress(totalChunks, totalBytes int64) *DownloadProgress {
	return &DownloadProgress{
		totalChunks:    totalChunks,
		totalBytes:     totalBytes,
		startTime:      time.Now(),
		lastUpdateTime: time.Now(),
		chunkSizes:     make(map[int64]int64),
		chunkTimes:     make(map[int64]time.Duration),
		peakSpeed:      0,
	}
}

// UpdateChunkProgress æ›´æ–°åˆ†ç‰‡ä¸‹è½½è¿›åº¦
func (dp *DownloadProgress) UpdateChunkProgress(chunkIndex, chunkSize int64, chunkDuration time.Duration) {
	dp.mu.Lock()
	defer dp.mu.Unlock()

	// å¦‚æœè¿™ä¸ªåˆ†ç‰‡è¿˜æ²¡æœ‰è®°å½•ï¼Œå¢åŠ å®Œæˆè®¡æ•°
	if _, exists := dp.chunkSizes[chunkIndex]; !exists {
		dp.completedChunks++
		dp.downloadedBytes += chunkSize
		dp.chunkSizes[chunkIndex] = chunkSize
		dp.chunkTimes[chunkIndex] = chunkDuration
		dp.lastUpdateTime = time.Now()

		// è®¡ç®—å¹¶æ›´æ–°å³°å€¼é€Ÿåº¦
		if chunkDuration.Seconds() > 0 {
			chunkSpeed := float64(chunkSize) / chunkDuration.Seconds() / 1024 / 1024 // MB/s
			if chunkSpeed > dp.peakSpeed {
				dp.peakSpeed = chunkSpeed
			}
		}
	}
}

// GetProgressInfo è·å–å½“å‰è¿›åº¦ä¿¡æ¯
func (dp *DownloadProgress) GetProgressInfo() (percentage float64, avgSpeed float64, peakSpeed float64, eta time.Duration, completed, total int64, downloadedBytes, totalBytes int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	elapsed := time.Since(dp.startTime)
	percentage = float64(dp.completedChunks) / float64(dp.totalChunks) * 100

	if elapsed.Seconds() > 0 {
		avgSpeed = float64(dp.downloadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	peakSpeed = dp.peakSpeed

	if avgSpeed > 0 && dp.downloadedBytes > 0 {
		remainingBytes := dp.totalBytes - dp.downloadedBytes
		etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
		eta = time.Duration(etaSeconds) * time.Second
	}

	return percentage, avgSpeed, peakSpeed, eta, dp.completedChunks, dp.totalChunks, dp.downloadedBytes, dp.totalBytes
}

// downloadChunksConcurrently 115ç½‘ç›˜å¹¶å‘ä¸‹è½½æ–‡ä»¶åˆ†ç‰‡ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) downloadChunksConcurrently(ctx context.Context, srcObj *Object, tempFile *os.File, chunkSize, numChunks, maxConcurrency int64) error {
	// åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
	progress := NewDownloadProgress(numChunks, srcObj.Size())

	// ğŸ”§ å…³é”®ä¿®å¤ï¼šè·å–å½“å‰ä¼ è¾“çš„Accountå¯¹è±¡ï¼Œç”¨äºæŠ¥å‘Šå®é™…ä¸‹è½½è¿›åº¦
	var currentAccount *accounting.Account
	remoteName := srcObj.Remote() // ä½¿ç”¨æ ‡å‡†Remote()æ–¹æ³•è·å–è·¯å¾„

	if stats := accounting.GlobalStats(); stats != nil {
		// å°è¯•ç²¾ç¡®åŒ¹é…
		currentAccount = stats.GetInProgressAccount(remoteName)
		if currentAccount == nil {
			// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
			allAccounts := stats.ListInProgressAccounts()
			for _, accountName := range allAccounts {
				if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
					currentAccount = stats.GetInProgressAccount(accountName)
					if currentAccount != nil {
						fs.Debugf(f, "115ç½‘ç›˜æ‰¾åˆ°åŒ¹é…çš„Account: %s -> %s", remoteName, accountName)
						break
					}
				}
			}
		} else {
			fs.Debugf(f, "115ç½‘ç›˜æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„Account: %s", remoteName)
		}
	}

	if currentAccount == nil {
		fs.Debugf(f, "115ç½‘ç›˜æœªæ‰¾åˆ°Accountå¯¹è±¡ï¼Œè¿›åº¦æ˜¾ç¤ºå¯èƒ½ä¸å‡†ç¡®: %s", remoteName)
	}

	// å¯åŠ¨è¿›åº¦æ›´æ–°åç¨‹ï¼ˆæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯ï¼‰
	progressCtx, cancelProgress := context.WithCancel(ctx)
	defer cancelProgress()

	go f.updateAccountExtraInfo(progressCtx, progress, remoteName, currentAccount)

	// åˆ›å»ºå·¥ä½œæ± 
	semaphore := make(chan struct{}, maxConcurrency)
	errChan := make(chan error, numChunks)
	var wg sync.WaitGroup

	for i := int64(0); i < numChunks; i++ {
		wg.Add(1)
		go func(chunkIndex int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := chunkIndex * chunkSize
			end := start + chunkSize - 1
			if end >= srcObj.Size() {
				end = srcObj.Size() - 1
			}
			actualChunkSize := end - start + 1

			// ä¸‹è½½åˆ†ç‰‡ï¼ˆå¸¦è¿›åº¦è·Ÿè¸ªï¼‰
			chunkStartTime := time.Now()
			err := f.downloadChunk(ctx, srcObj, tempFile, start, end, chunkIndex)
			chunkDuration := time.Since(chunkStartTime)

			if err != nil {
				errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", chunkIndex, err)
				return
			}

			// æ›´æ–°è¿›åº¦ï¼ˆåŒ…å«ä¸‹è½½æ—¶é—´ï¼‰
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, chunkDuration)

			// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šè°ƒç”¨å…±äº«çš„è¿›åº¦æ˜¾ç¤ºå‡½æ•°
			f.displayUnifiedDownloadProgress(progress, "115ç½‘ç›˜", srcObj.remote)

		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	for err := range errChan {
		if err != nil {
			return err
		}
	}

	return nil
}

// updateAccountExtraInfo æ›´æ–°Accountçš„é¢å¤–è¿›åº¦ä¿¡æ¯ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) updateAccountExtraInfo(ctx context.Context, progress *DownloadProgress, remoteName string, account *accounting.Account) {
	ticker := time.NewTicker(2 * time.Second) // æ¯2ç§’æ›´æ–°ä¸€æ¬¡Accountçš„é¢å¤–ä¿¡æ¯
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ¸…é™¤é¢å¤–ä¿¡æ¯
			if account != nil {
				account.SetExtraInfo("")
			}
			return
		case <-ticker.C:
			if account == nil {
				// å°è¯•é‡æ–°è·å–Accountå¯¹è±¡
				if stats := accounting.GlobalStats(); stats != nil {
					// å°è¯•ç²¾ç¡®åŒ¹é…
					account = stats.GetInProgressAccount(remoteName)
					if account == nil {
						// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
						allAccounts := stats.ListInProgressAccounts()
						for _, accountName := range allAccounts {
							if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
								account = stats.GetInProgressAccount(accountName)
								if account != nil {
									break
								}
							}
						}
					}
				}
				if account == nil {
					continue // å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·³è¿‡è¿™æ¬¡æ›´æ–°
				}
			}

			_, _, _, _, completed, total, _, _ := progress.GetProgressInfo()

			// ğŸ”§ é›†æˆï¼šæ„å»ºé¢å¤–ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ˜¾ç¤ºåˆ†ç‰‡è¿›åº¦
			if completed > 0 && total > 0 {
				extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completed, total)
				account.SetExtraInfo(extraInfo)
			}
		}
	}
}

// displayDownloadProgress æ˜¾ç¤º115ç½‘ç›˜ä¸‹è½½è¿›åº¦ï¼ˆä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ç®€åŒ–è¾“å‡ºï¼‰
func (f *Fs) displayDownloadProgress(ctx context.Context, progress *DownloadProgress, fileName string) {
	// ğŸ”§ ä¼˜åŒ–ï¼šå¤§å¹…å‡å°‘ç‹¬ç«‹è¿›åº¦æ˜¾ç¤ºï¼Œä¸»è¦ä¾èµ–rcloneæ ‡å‡†è¿›åº¦
	ticker := time.NewTicker(10 * time.Second) // æ¯10ç§’æ›´æ–°ä¸€æ¬¡ï¼Œä»…ç”¨äºé‡è¦çŠ¶æ€
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ˜¾ç¤ºæœ€ç»ˆå®ŒæˆçŠ¶æ€
			percentage, avgSpeed, peakSpeed, _, completed, total, downloadedBytes, totalBytes := progress.GetProgressInfo()
			fs.Infof(f, "ğŸ“¥ 115ç½‘ç›˜ä¸‹è½½å®Œæˆ: %s | %d/%dåˆ†ç‰‡ (%.1f%%) | %s/%s | å¹³å‡: %.2f MB/s | å³°å€¼: %.2f MB/s",
				fileName, completed, total, percentage,
				fs.SizeSuffix(downloadedBytes), fs.SizeSuffix(totalBytes), avgSpeed, peakSpeed)
			return
		case <-ticker.C:
			// ğŸ”§ ä¼˜åŒ–ï¼šåªåœ¨ç‰¹æ®Šæƒ…å†µä¸‹æ˜¾ç¤ºç‹¬ç«‹è¿›åº¦ï¼ˆå¦‚é”™è¯¯æ¢å¤ç­‰ï¼‰
			// æ­£å¸¸æƒ…å†µä¸‹ä¾èµ–rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤º
		}
	}
}

// displayUnifiedDownloadProgress ç»Ÿä¸€çš„ä¸‹è½½è¿›åº¦æ˜¾ç¤ºå‡½æ•°
func (f *Fs) displayUnifiedDownloadProgress(progress *DownloadProgress, networkName, fileName string) {
	percentage, avgSpeed, peakSpeed, eta, completed, total, downloadedBytes, totalBytes := progress.GetProgressInfo()

	// è®¡ç®—ETAæ˜¾ç¤º
	etaStr := "ETA: -"
	if eta > 0 {
		etaStr = fmt.Sprintf("ETA: %v", eta.Round(time.Second))
	} else if avgSpeed > 0 {
		etaStr = "ETA: è®¡ç®—ä¸­..."
	}

	// åˆ›å»ºè¿›åº¦æ¡
	progressBar := f.createProgressBar(percentage)

	// ç»Ÿä¸€çš„è¿›åº¦æ˜¾ç¤ºæ ¼å¼
	fs.Infof(f, "ğŸ“¥ %sä¸‹è½½è¿›åº¦: %s", networkName, fileName)
	fs.Infof(f, "   %s %.1f%% | %d/%dåˆ†ç‰‡ | %s/%s",
		progressBar, percentage, completed, total,
		fs.SizeSuffix(downloadedBytes), fs.SizeSuffix(totalBytes))
	fs.Infof(f, "   å¹³å‡é€Ÿåº¦: %.2f MB/s | å³°å€¼é€Ÿåº¦: %.2f MB/s | %s",
		avgSpeed, peakSpeed, etaStr)
}

// createProgressBar åˆ›å»ºè¿›åº¦æ¡å­—ç¬¦ä¸²
func (f *Fs) createProgressBar(percentage float64) string {
	const barLength = 30
	filled := int(percentage / 100 * barLength)
	if filled > barLength {
		filled = barLength
	}

	bar := "["
	for i := 0; i < barLength; i++ {
		if i < filled {
			bar += "â–ˆ"
		} else {
			bar += "â–‘"
		}
	}
	bar += "]"
	return bar
}

// downloadChunk 115ç½‘ç›˜ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ†ç‰‡
func (f *Fs) downloadChunk(ctx context.Context, srcObj *Object, tempFile *os.File, start, end, chunkIndex int64) error {
	rangeOption := &fs.RangeOption{Start: start, End: end}
	expectedSize := end - start + 1

	// é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯•5æ¬¡ï¼Œå¤„ç†URLè¿‡æœŸé—®é¢˜
	maxRetries := 5
	var lastErr error

	for retry := 0; retry < maxRetries; retry++ {
		// åœ¨æ¯æ¬¡é‡è¯•å‰å¼ºåˆ¶åˆ·æ–°URL
		if retry > 0 {
			fs.Debugf(f, "115ç½‘ç›˜åˆ†ç‰‡ %d é‡è¯• %d/%d: å¼ºåˆ¶åˆ·æ–°ä¸‹è½½URL", chunkIndex, retry, maxRetries)

			// ğŸ”§ ä¿®å¤ï¼šå¼ºåˆ¶åˆ·æ–°ä¸‹è½½URLï¼Œé¿å…é‡è¯•æ—¶ä»ç„¶è·å–è¿‡æœŸURL
			fs.Debugf(f, "115ç½‘ç›˜åˆ†ç‰‡é‡è¯•ï¼Œå¼ºåˆ¶åˆ·æ–°ä¸‹è½½URL: pickCode=%s", srcObj.pickCode)

			// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ–¹æ³•ï¼Œè·³è¿‡ç¼“å­˜ç›´æ¥è·å–æ–°URL
			err := srcObj.setDownloadURLWithForce(ctx, true)
			if err != nil {
				lastErr = fmt.Errorf("åˆ·æ–°ä¸‹è½½URLå¤±è´¥: %w", err)
				time.Sleep(time.Duration(retry) * time.Second) // å¢åŠ å»¶è¿Ÿ
				continue
			}

			// éªŒè¯æ–°URLæ˜¯å¦æœ‰æ•ˆ
			if !srcObj.durl.Valid() {
				lastErr = fmt.Errorf("åˆ·æ–°åçš„URLä»ç„¶æ— æ•ˆ")
				time.Sleep(time.Duration(retry) * time.Second)
				continue
			}

			// é€’å¢å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„é‡è¯•
			time.Sleep(time.Duration(retry) * 500 * time.Millisecond)
		}

		// å°è¯•æ‰“å¼€åˆ†ç‰‡
		chunkReader, err := srcObj.open(ctx, rangeOption)
		if err != nil {
			lastErr = err

			// ğŸ”§ ä¼˜åŒ–ï¼šæ™ºèƒ½é”™è¯¯åˆ†ç±»å’Œå¤„ç†ç­–ç•¥
			errorType := f.classifyDownloadError(err)
			retryDelay := f.calculateRetryDelay(errorType, retry)

			switch errorType {
			case "server_overload": // 502, 503ç­‰æœåŠ¡å™¨è¿‡è½½é”™è¯¯
				time.Sleep(retryDelay)
				continue

			case "url_expired": // URLè¿‡æœŸé”™è¯¯
				// URLè¿‡æœŸé”™è¯¯ä¸éœ€è¦å»¶è¿Ÿï¼Œç«‹å³é‡è¯•
				continue

			case "network_timeout": // ç½‘ç»œè¶…æ—¶é”™è¯¯
				time.Sleep(retryDelay)
				continue

			case "rate_limit": // 429é™æµé”™è¯¯
				time.Sleep(retryDelay)
				continue

			default: // å…¶ä»–é”™è¯¯
				time.Sleep(retryDelay)
				continue
			}

			// æ£€æŸ¥æ˜¯å¦æ˜¯å…¶ä»–æœåŠ¡å™¨é”™è¯¯ï¼ˆ5xxï¼‰
			if strings.Contains(err.Error(), "50") {
				time.Sleep(time.Duration(retry+1) * 2 * time.Second)
				continue
			}

			// å…¶ä»–é”™è¯¯ç›´æ¥è¿”å›
			return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡å¤±è´¥: %w", err)
		}

		// è¯»å–åˆ†ç‰‡æ•°æ®
		chunkData, err := io.ReadAll(chunkReader)
		chunkReader.Close()
		if err != nil {
			lastErr = err
			continue
		}

		// æ£€æµ‹HTMLé”™è¯¯å“åº”ï¼ˆé€šå¸¸æ˜¯79å­—èŠ‚å·¦å³çš„é”™è¯¯é¡µé¢ï¼‰
		if len(chunkData) < 1000 && isHTMLErrorResponse(chunkData) {
			lastErr = fmt.Errorf("æ”¶åˆ°HTMLé”™è¯¯å“åº”ï¼Œå¯èƒ½æ˜¯URLè¿‡æœŸ: å¤§å°=%d", len(chunkData))
			continue
		}

		// éªŒè¯åˆ†ç‰‡å¤§å°
		if int64(len(chunkData)) != expectedSize {
			// å¦‚æœæ˜¯æœ€åä¸€ä¸ªåˆ†ç‰‡ï¼Œå¯èƒ½å¤§å°ä¸è¶³
			if chunkIndex == (srcObj.Size()+expectedSize-1)/expectedSize-1 {
				// æœ€åä¸€ä¸ªåˆ†ç‰‡ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯åˆç†çš„å‰©ä½™å¤§å°
				remainingSize := srcObj.Size() - start
				if int64(len(chunkData)) == remainingSize {
					// æœ€ååˆ†ç‰‡å¤§å°æ­£ç¡®
				} else {
					lastErr = fmt.Errorf("æœ€ååˆ†ç‰‡å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", remainingSize, len(chunkData))
					continue
				}
			} else {
				lastErr = fmt.Errorf("åˆ†ç‰‡å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, len(chunkData))
				continue
			}
		}

		// å†™å…¥ä¸´æ—¶æ–‡ä»¶çš„æ­£ç¡®ä½ç½®
		_, err = tempFile.WriteAt(chunkData, start)
		if err != nil {
			return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		// æˆåŠŸå®Œæˆ
		return nil
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
	return fmt.Errorf("åˆ†ç‰‡ %d ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡: %w", chunkIndex, maxRetries, lastErr)
}

// isHTMLErrorResponse æ£€æµ‹æ˜¯å¦æ˜¯HTMLé”™è¯¯å“åº”
func isHTMLErrorResponse(data []byte) bool {
	if len(data) == 0 {
		return false
	}

	// è½¬æ¢ä¸ºå°å†™å­—ç¬¦ä¸²è¿›è¡Œæ£€æŸ¥
	content := strings.ToLower(string(data))

	// æ£€æŸ¥å¸¸è§çš„HTMLé”™è¯¯å“åº”ç‰¹å¾
	htmlIndicators := []string{
		"<html",
		"<!doctype",
		"<head>",
		"<title>",
		"error",
		"404",
		"403",
		"500",
		"expired",
		"invalid",
	}

	for _, indicator := range htmlIndicators {
		if strings.Contains(content, indicator) {
			return true
		}
	}

	return false
}

// calculateDownloadChunkSize 115ç½‘ç›˜æ™ºèƒ½è®¡ç®—ä¸‹è½½åˆ†ç‰‡å¤§å°
// ğŸ”§ ä¼˜åŒ–ï¼šåŸºäºæ–‡ä»¶å¤§å°å’Œå¹¶å‘æ•°åŠ¨æ€è°ƒæ•´åˆ†ç‰‡å¤§å°
func (f *Fs) calculateDownloadChunkSize(fileSize int64) int64 {
	// ğŸš€ æ–°çš„æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥ï¼šå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œä¸‹è½½æ•ˆç‡
	switch {
	case fileSize < 50*1024*1024: // <50MB
		return 8 * 1024 * 1024 // 8MBåˆ†ç‰‡ï¼Œå°æ–‡ä»¶ä½¿ç”¨è¾ƒå°åˆ†ç‰‡
	case fileSize < 200*1024*1024: // <200MB
		return 16 * 1024 * 1024 // 16MBåˆ†ç‰‡
	case fileSize < 1*1024*1024*1024: // <1GB
		return 32 * 1024 * 1024 // 32MBåˆ†ç‰‡
	case fileSize < 5*1024*1024*1024: // <5GB
		return 64 * 1024 * 1024 // 64MBåˆ†ç‰‡
	case fileSize < 20*1024*1024*1024: // <20GB
		return 128 * 1024 * 1024 // 128MBåˆ†ç‰‡ï¼Œå¤§æ–‡ä»¶ä½¿ç”¨æ›´å¤§åˆ†ç‰‡
	default: // >20GB
		return 256 * 1024 * 1024 // 256MBåˆ†ç‰‡ï¼Œè¶…å¤§æ–‡ä»¶ä½¿ç”¨æœ€å¤§åˆ†ç‰‡
	}
}

// calculateDownloadConcurrency 115ç½‘ç›˜æ™ºèƒ½è®¡ç®—ä¸‹è½½å¹¶å‘æ•°
// ğŸ”§ ä¼˜åŒ–ï¼šå®ç°åŠ¨æ€å¹¶å‘æ§åˆ¶ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
func (f *Fs) calculateDownloadConcurrency(fileSize int64) int {
	// ğŸš€ æ–°çš„æ™ºèƒ½å¹¶å‘ç­–ç•¥ï¼šåŸºäºæ–‡ä»¶å¤§å°å’Œç½‘ç»œçŠ¶å†µåŠ¨æ€è°ƒæ•´
	baseConcurrency := f.getBaseConcurrency(fileSize)

	// ğŸ”§ ç½‘ç»œçŠ¶å†µè‡ªé€‚åº”è°ƒæ•´
	adjustedConcurrency := f.adjustConcurrencyByNetworkCondition(baseConcurrency)

	return adjustedConcurrency
}

// getBaseConcurrency è·å–åŸºç¡€å¹¶å‘æ•°
func (f *Fs) getBaseConcurrency(fileSize int64) int {
	// ğŸ”§ ä¸123ç½‘ç›˜ä¿æŒä¸€è‡´ï¼šæœ€å¤š4ä¸ªå¹¶å‘ä¸‹è½½
	// åŸºäºæ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
	switch {
	case fileSize < 100*1024*1024: // <100MB
		return 2 // å°æ–‡ä»¶ä½¿ç”¨2å¹¶å‘
	case fileSize < 500*1024*1024: // <500MB
		return 3 // ä¸­ç­‰æ–‡ä»¶ä½¿ç”¨3å¹¶å‘
	default: // >500MB
		return 4 // å¤§æ–‡ä»¶ä½¿ç”¨4å¹¶å‘ï¼ˆä¸123ç½‘ç›˜ä¸€è‡´ï¼‰
	}
}

// adjustConcurrencyByNetworkCondition æ ¹æ®ç½‘ç»œçŠ¶å†µè°ƒæ•´å¹¶å‘æ•°
func (f *Fs) adjustConcurrencyByNetworkCondition(baseConcurrency int) int {
	// ğŸ”§ ä¸123ç½‘ç›˜ä¿æŒä¸€è‡´ï¼šæœ€å¤š4ä¸ªå¹¶å‘ä¸‹è½½
	// è®¾ç½®æœ€å¤§å¹¶å‘é™åˆ¶ï¼Œé¿å…è¿‡åº¦å¹¶å‘å¯¼è‡´æœåŠ¡å™¨å‹åŠ›å’ŒURLè¿‡æœŸ
	maxConcurrency := 4
	if baseConcurrency > maxConcurrency {
		return maxConcurrency
	}

	// è®¾ç½®æœ€å°å¹¶å‘ï¼Œç¡®ä¿åŸºæœ¬æ€§èƒ½
	minConcurrency := 2
	if baseConcurrency < minConcurrency {
		return minConcurrency
	}

	return baseConcurrency
}

// calculateUploadConcurrency 115ç½‘ç›˜ä¸Šä¼ å¹¶å‘æ•°è®¡ç®—
// æ³¨æ„ï¼šåˆ†ç‰‡ä¸Šä¼ å§‹ç»ˆä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼ä»¥ç¡®ä¿SHA1éªŒè¯é€šè¿‡
func (f *Fs) calculateUploadConcurrency(fileSize int64) int {
	// 115ç½‘ç›˜åˆ†ç‰‡ä¸Šä¼ å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹æ¨¡å¼ï¼Œç¡®ä¿ä¸Šä¼ ç¨³å®šæ€§
	// è¿™æ˜¯ä¸ºäº†é¿å…å¤šçº¿ç¨‹ä¸Šä¼ æ—¶çš„SHA1éªŒè¯å¤±è´¥é—®é¢˜
	return 1
}

// classifyDownloadError æ™ºèƒ½åˆ†ç±»ä¸‹è½½é”™è¯¯ç±»å‹
func (f *Fs) classifyDownloadError(err error) string {
	errStr := strings.ToLower(err.Error())

	// æœåŠ¡å™¨è¿‡è½½é”™è¯¯
	if strings.Contains(errStr, "502") || strings.Contains(errStr, "bad gateway") ||
		strings.Contains(errStr, "503") || strings.Contains(errStr, "service unavailable") {
		return "server_overload"
	}

	// URLè¿‡æœŸé”™è¯¯
	if strings.Contains(errStr, "download url is invalid") || strings.Contains(errStr, "expired") {
		return "url_expired"
	}

	// ç½‘ç»œè¶…æ—¶é”™è¯¯
	if strings.Contains(errStr, "timeout") || strings.Contains(errStr, "deadline exceeded") {
		return "network_timeout"
	}

	// é™æµé”™è¯¯
	if strings.Contains(errStr, "429") || strings.Contains(errStr, "too many requests") ||
		strings.Contains(errStr, "rate limit") {
		return "rate_limit"
	}

	// è¿æ¥é”™è¯¯
	if strings.Contains(errStr, "connection") || strings.Contains(errStr, "network") {
		return "network_error"
	}

	return "unknown"
}

// calculateRetryDelay æ ¹æ®é”™è¯¯ç±»å‹å’Œé‡è¯•æ¬¡æ•°è®¡ç®—å»¶è¿Ÿæ—¶é—´
func (f *Fs) calculateRetryDelay(errorType string, retryCount int) time.Duration {
	baseDelay := time.Second

	switch errorType {
	case "server_overload":
		// æœåŠ¡å™¨è¿‡è½½ä½¿ç”¨æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§30ç§’
		delay := time.Duration(1<<uint(retryCount)) * baseDelay
		if delay > 30*time.Second {
			delay = 30 * time.Second
		}
		return delay

	case "url_expired":
		// URLè¿‡æœŸç«‹å³é‡è¯•
		return 0

	case "network_timeout":
		// ç½‘ç»œè¶…æ—¶ä½¿ç”¨çº¿æ€§å¢é•¿ï¼Œæœ€å¤§10ç§’
		delay := time.Duration(retryCount+1) * baseDelay
		if delay > 10*time.Second {
			delay = 10 * time.Second
		}
		return delay

	case "rate_limit":
		// é™æµé”™è¯¯ä½¿ç”¨è¾ƒé•¿å»¶è¿Ÿ
		delay := time.Duration(retryCount+1) * 5 * time.Second
		if delay > 60*time.Second {
			delay = 60 * time.Second
		}
		return delay

	default:
		// å…¶ä»–é”™è¯¯ä½¿ç”¨æ ‡å‡†æŒ‡æ•°é€€é¿
		delay := time.Duration(1<<uint(retryCount)) * baseDelay
		if delay > 15*time.Second {
			delay = 15 * time.Second
		}
		return delay
	}
}

// optimizeNetworkParameters ä¼˜åŒ–ç½‘ç»œå‚æ•°ä»¥æå‡ä¸‹è½½æ€§èƒ½
func (f *Fs) optimizeNetworkParameters(ctx context.Context) {
	// ğŸ”§ ç½‘ç»œå‚æ•°è°ƒä¼˜ï¼šè®¾ç½®æ›´ä¼˜çš„HTTPå®¢æˆ·ç«¯å‚æ•°
	ci := fs.GetConfig(ctx)

	// ç½‘ç»œå‚æ•°ä¼˜åŒ–å»ºè®®ï¼ˆé™é»˜å¤„ç†ï¼‰
	_ = ci.Transfers
	_ = ci.Timeout
	_ = ci.LowLevelRetries
}

// evaluateDownloadPerformance è¯„ä¼°ä¸‹è½½æ€§èƒ½ç­‰çº§
func (f *Fs) evaluateDownloadPerformance(avgSpeed float64, concurrency int) string {
	// ğŸ”§ æ€§èƒ½ç­‰çº§è¯„ä¼°æ ‡å‡†
	switch {
	case avgSpeed >= 50: // >=50MB/s
		return "ä¼˜ç§€"
	case avgSpeed >= 30: // >=30MB/s
		return "è‰¯å¥½"
	case avgSpeed >= 15: // >=15MB/s
		return "ä¸€èˆ¬"
	case avgSpeed >= 5: // >=5MB/s
		return "è¾ƒæ…¢"
	default: // <5MB/s
		return "å¾ˆæ…¢"
	}
}

// suggestPerformanceOptimizations æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®
func (f *Fs) suggestPerformanceOptimizations(avgSpeed float64, concurrency int, fileSize int64) {
	suggestions := []string{}

	// åŸºäºé€Ÿåº¦çš„å»ºè®®
	if avgSpeed < 15 {
		suggestions = append(suggestions, "ç½‘ç»œé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥")

		if concurrency < 8 {
			suggestions = append(suggestions, "å¯ä»¥å°è¯•å¢åŠ å¹¶å‘æ•°ä»¥æå‡æ€§èƒ½")
		}

		if fileSize > 5*1024*1024*1024 { // >5GB
			suggestions = append(suggestions, "å¤§æ–‡ä»¶å»ºè®®åœ¨ç½‘ç»œçŠ¶å†µè‰¯å¥½æ—¶ä¸‹è½½")
		}
	}

	// åŸºäºå¹¶å‘æ•°çš„å»ºè®®
	if concurrency < 4 && fileSize > 1*1024*1024*1024 { // >1GB
		suggestions = append(suggestions, "å¤§æ–‡ä»¶å¯ä»¥è€ƒè™‘å¢åŠ å¹¶å‘æ•°")
	}

	// è¾“å‡ºå»ºè®®
	if len(suggestions) > 0 {
		fs.Infof(f, "ğŸ’¡ 115ç½‘ç›˜æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
		for i, suggestion := range suggestions {
			fs.Infof(f, "   %d. %s", i+1, suggestion)
		}
	}
}

// intelligentCachePreheating æ™ºèƒ½ç¼“å­˜é¢„çƒ­ï¼Œå‚è€ƒ123ç½‘ç›˜çš„æˆåŠŸç»éªŒ
func (f *Fs) intelligentCachePreheating(ctx context.Context) {
	// å»¶è¿Ÿå¯åŠ¨ï¼Œé¿å…å½±å“åˆå§‹åŒ–æ€§èƒ½
	time.Sleep(5 * time.Second)

	// 1. é¢„çƒ­æ ¹ç›®å½•ä¿¡æ¯
	f.preheatRootDirectory(ctx)

	// 2. é¢„çƒ­å¸¸ç”¨è·¯å¾„ï¼ˆå¦‚æœæœ‰å†å²è®¿é—®è®°å½•ï¼‰
	f.preheatFrequentPaths(ctx)

	// 3. é¢„çƒ­æœ€è¿‘è®¿é—®çš„ç›®å½•
	f.preheatRecentDirectories(ctx)

}

// preheatRootDirectory é¢„çƒ­æ ¹ç›®å½•ä¿¡æ¯
func (f *Fs) preheatRootDirectory(ctx context.Context) {
	defer func() {
		if r := recover(); r != nil {
			fs.Debugf(f, "æ ¹ç›®å½•é¢„çƒ­å‡ºç°å¼‚å¸¸: %v", r)
		}
	}()

	// é¢„çƒ­æ ¹ç›®å½•çš„åŸºæœ¬ä¿¡æ¯
	if f.dirCache != nil {
		_, err := f.dirCache.RootID(ctx, false)
		if err == nil {
			fs.Debugf(f, "ğŸ”¥ æ ¹ç›®å½•ä¿¡æ¯é¢„çƒ­æˆåŠŸ")
		} else {
			fs.Debugf(f, "æ ¹ç›®å½•ä¿¡æ¯é¢„çƒ­å¤±è´¥: %v", err)
		}
	}
}

// preheatFrequentPaths é¢„çƒ­å¸¸ç”¨è·¯å¾„
func (f *Fs) preheatFrequentPaths(ctx context.Context) {
	defer func() {
		if r := recover(); r != nil {
			fs.Debugf(f, "å¸¸ç”¨è·¯å¾„é¢„çƒ­å‡ºç°å¼‚å¸¸: %v", r)
		}
	}()

	// é¢„çƒ­ä¸€äº›å¸¸è§çš„ç›®å½•è·¯å¾„
	commonPaths := []string{
		"",          // æ ¹ç›®å½•
		"Documents", // æ–‡æ¡£ç›®å½•
		"Downloads", // ä¸‹è½½ç›®å½•
		"Pictures",  // å›¾ç‰‡ç›®å½•
		"Videos",    // è§†é¢‘ç›®å½•
	}

	for _, path := range commonPaths {
		select {
		case <-ctx.Done():
			return
		default:
			if f.dirCache != nil {
				_, err := f.dirCache.FindDir(ctx, path, false)
				if err == nil {
					fs.Debugf(f, "ğŸ”¥ å¸¸ç”¨è·¯å¾„é¢„çƒ­æˆåŠŸ: %s", path)
				}
			}
			// é¿å…è¿‡äºé¢‘ç¹çš„APIè°ƒç”¨
			time.Sleep(1 * time.Second)
		}
	}
}

// preheatRecentDirectories é¢„çƒ­æœ€è¿‘è®¿é—®çš„ç›®å½•
func (f *Fs) preheatRecentDirectories(ctx context.Context) {
	defer func() {
		if r := recover(); r != nil {
			fs.Debugf(f, "æœ€è¿‘ç›®å½•é¢„çƒ­å‡ºç°å¼‚å¸¸: %v", r)
		}
	}()

	// ä»ç¼“å­˜ä¸­è·å–æœ€è¿‘è®¿é—®çš„è·¯å¾„è¿›è¡Œé¢„çƒ­
	if f.pathResolveCache != nil {
		// è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é€»è¾‘ï¼Œæ¯”å¦‚ä»ç¼“å­˜ä¸­è¯»å–æœ€è¿‘è®¿é—®çš„è·¯å¾„
		fs.Debugf(f, "ğŸ”¥ æœ€è¿‘ç›®å½•é¢„çƒ­é€»è¾‘å¾…å®ç°")
	}
}

// CacheStatistics ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
type CacheStatistics struct {
	PathResolveHits   int64   `json:"path_resolve_hits"`
	PathResolveMisses int64   `json:"path_resolve_misses"`
	DirListHits       int64   `json:"dir_list_hits"`
	DirListMisses     int64   `json:"dir_list_misses"`
	DownloadURLHits   int64   `json:"download_url_hits"`
	DownloadURLMisses int64   `json:"download_url_misses"`
	MetadataHits      int64   `json:"metadata_hits"`
	MetadataMisses    int64   `json:"metadata_misses"`
	FileIDHits        int64   `json:"file_id_hits"`
	FileIDMisses      int64   `json:"file_id_misses"`
	TotalHits         int64   `json:"total_hits"`
	TotalMisses       int64   `json:"total_misses"`
	HitRate           float64 `json:"hit_rate"`
}

// getCacheStatistics è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
func (f *Fs) getCacheStatistics() *CacheStatistics {
	stats := &CacheStatistics{}

	// è¿™é‡Œå¯ä»¥ä»å„ä¸ªç¼“å­˜å®ä¾‹æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
	// å½“å‰å…ˆè¿”å›åŸºç¡€ç»“æ„ï¼Œåç»­å¯ä»¥æ‰©å±•

	stats.TotalHits = stats.PathResolveHits + stats.DirListHits + stats.DownloadURLHits + stats.MetadataHits + stats.FileIDHits
	stats.TotalMisses = stats.PathResolveMisses + stats.DirListMisses + stats.DownloadURLMisses + stats.MetadataMisses + stats.FileIDMisses

	if stats.TotalHits+stats.TotalMisses > 0 {
		stats.HitRate = float64(stats.TotalHits) / float64(stats.TotalHits+stats.TotalMisses) * 100
	}

	return stats
}

// logCacheStatistics è®°å½•ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
func (f *Fs) logCacheStatistics() {
	stats := f.getCacheStatistics()

	fs.Infof(f, "ğŸ“Š 115ç½‘ç›˜ç¼“å­˜ç»Ÿè®¡:")
	fs.Infof(f, "   è·¯å¾„è§£æ: å‘½ä¸­=%d, æœªå‘½ä¸­=%d", stats.PathResolveHits, stats.PathResolveMisses)
	fs.Infof(f, "   ç›®å½•åˆ—è¡¨: å‘½ä¸­=%d, æœªå‘½ä¸­=%d", stats.DirListHits, stats.DirListMisses)
	fs.Infof(f, "   ä¸‹è½½URL: å‘½ä¸­=%d, æœªå‘½ä¸­=%d", stats.DownloadURLHits, stats.DownloadURLMisses)
	fs.Infof(f, "   å…ƒæ•°æ®: å‘½ä¸­=%d, æœªå‘½ä¸­=%d", stats.MetadataHits, stats.MetadataMisses)
	fs.Infof(f, "   æ–‡ä»¶ID: å‘½ä¸­=%d, æœªå‘½ä¸­=%d", stats.FileIDHits, stats.FileIDMisses)
	fs.Infof(f, "   æ€»è®¡: å‘½ä¸­=%d, æœªå‘½ä¸­=%d, å‘½ä¸­ç‡=%.2f%%", stats.TotalHits, stats.TotalMisses, stats.HitRate)
}

// ErrorStatistics é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
type ErrorStatistics struct {
	ServerOverloadErrors int64   `json:"server_overload_errors"`
	URLExpiredErrors     int64   `json:"url_expired_errors"`
	NetworkTimeoutErrors int64   `json:"network_timeout_errors"`
	RateLimitErrors      int64   `json:"rate_limit_errors"`
	AuthErrors           int64   `json:"auth_errors"`
	PermissionErrors     int64   `json:"permission_errors"`
	NotFoundErrors       int64   `json:"not_found_errors"`
	UnknownErrors        int64   `json:"unknown_errors"`
	TotalErrors          int64   `json:"total_errors"`
	TotalRetries         int64   `json:"total_retries"`
	SuccessfulRetries    int64   `json:"successful_retries"`
	RetrySuccessRate     float64 `json:"retry_success_rate"`
}

// getErrorStatistics è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
func (f *Fs) getErrorStatistics() *ErrorStatistics {
	stats := &ErrorStatistics{}

	// è¿™é‡Œå¯ä»¥ä»é”™è¯¯åˆ†ç±»å™¨æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
	// å½“å‰å…ˆè¿”å›åŸºç¡€ç»“æ„ï¼Œåç»­å¯ä»¥æ‰©å±•

	stats.TotalErrors = stats.ServerOverloadErrors + stats.URLExpiredErrors +
		stats.NetworkTimeoutErrors + stats.RateLimitErrors + stats.AuthErrors +
		stats.PermissionErrors + stats.NotFoundErrors + stats.UnknownErrors

	if stats.TotalRetries > 0 {
		stats.RetrySuccessRate = float64(stats.SuccessfulRetries) / float64(stats.TotalRetries) * 100
	}

	return stats
}

// logErrorStatistics è®°å½•é”™è¯¯ç»Ÿè®¡ä¿¡æ¯
func (f *Fs) logErrorStatistics() {
	stats := f.getErrorStatistics()

	if stats.TotalErrors > 0 {
		fs.Infof(f, "ğŸ“Š 115ç½‘ç›˜é”™è¯¯ç»Ÿè®¡:")
		fs.Infof(f, "   æœåŠ¡å™¨è¿‡è½½: %d", stats.ServerOverloadErrors)
		fs.Infof(f, "   URLè¿‡æœŸ: %d", stats.URLExpiredErrors)
		fs.Infof(f, "   ç½‘ç»œè¶…æ—¶: %d", stats.NetworkTimeoutErrors)
		fs.Infof(f, "   é™æµé”™è¯¯: %d", stats.RateLimitErrors)
		fs.Infof(f, "   è®¤è¯é”™è¯¯: %d", stats.AuthErrors)
		fs.Infof(f, "   æƒé™é”™è¯¯: %d", stats.PermissionErrors)
		fs.Infof(f, "   èµ„æºä¸å­˜åœ¨: %d", stats.NotFoundErrors)
		fs.Infof(f, "   æœªçŸ¥é”™è¯¯: %d", stats.UnknownErrors)
		fs.Infof(f, "   æ€»é”™è¯¯æ•°: %d", stats.TotalErrors)
		fs.Infof(f, "   é‡è¯•æˆåŠŸç‡: %.2f%% (%d/%d)", stats.RetrySuccessRate, stats.SuccessfulRetries, stats.TotalRetries)
	}
}

// shouldUseConcurrentDownload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
func (o *Object) shouldUseConcurrentDownload(ctx context.Context, options []fs.OpenOption) bool {
	// æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œåªå¯¹å¤§æ–‡ä»¶ä½¿ç”¨å¹¶å‘ä¸‹è½½
	if o.size < 100*1024*1024 { // å°äº100MB
		return false
	}

	// æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æŒ‡å®šçš„Rangeé€‰é¡¹
	// æ™ºèƒ½Rangeæ£€æµ‹ï¼šåŒºåˆ†å°Rangeè¯·æ±‚å’Œå¤§Rangeè¯·æ±‚ï¼ˆè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
	for _, option := range options {
		if rangeOpt, ok := option.(*fs.RangeOption); ok {
			// å¦‚æœRangeè¦†ç›–äº†æ•´ä¸ªæ–‡ä»¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
			if rangeOpt.Start == 0 && (rangeOpt.End == -1 || rangeOpt.End >= o.size-1) {
				fs.Debugf(o, "æ£€æµ‹åˆ°å…¨æ–‡ä»¶Rangeé€‰é¡¹ï¼Œå…è®¸å¹¶å‘ä¸‹è½½")
				break
			} else {
				// è®¡ç®—Rangeå¤§å°
				rangeSize := rangeOpt.End - rangeOpt.Start + 1

				// ğŸš€ è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šå¦‚æœRangeè¶³å¤Ÿå¤§ï¼ˆ>=32MBï¼‰ï¼Œå¯ç”¨å¹¶å‘ä¸‹è½½
				// è¿™ä¸»è¦é’ˆå¯¹rcloneå¤šçº¿ç¨‹ä¼ è¾“åœºæ™¯ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„æ•°æ®å—é€šå¸¸è¾ƒå¤§
				if rangeSize >= 32*1024*1024 { // 32MBé˜ˆå€¼
					fs.Infof(o, "ğŸš€ æ£€æµ‹åˆ°å¤§Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œå¯ç”¨å¹¶å‘ä¸‹è½½ä¼˜åŒ–ï¼ˆè·¨äº‘ä¼ è¾“åœºæ™¯ï¼‰",
						rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
					break
				} else {
					fs.Debugf(o, "æ£€æµ‹åˆ°å°Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½",
						rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
					return false
				}
			}
		}
	}

	return true
}

// openWithConcurrency ä½¿ç”¨å¹¶å‘ä¸‹è½½æ‰“å¼€æ–‡ä»¶
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Infof(o, "ğŸš€ 115ç½‘ç›˜å¯åŠ¨å¹¶å‘ä¸‹è½½: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "115_concurrent_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.open(ctx, options...)
	}

	// ğŸ”§ åˆ›å»ºè¿›åº¦æŠ¥å‘Šå™¨ï¼Œè®©rcloneèƒ½çœ‹åˆ°ä¸‹è½½è¿›åº¦
	progressReporter := NewProgressReporter(o.size)

	// ä½¿ç”¨å¹¶å‘ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶æŠ¥å‘Šè¿›åº¦
	downloadedSize, err := o.fs.downloadWithConcurrency(ctx, o, tempFile, o.size)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.open(ctx, options...)
	}

	// éªŒè¯ä¸‹è½½å¤§å°
	if downloadedSize != o.size {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "ä¸‹è½½å¤§å°ä¸åŒ¹é…ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: æœŸæœ›%dï¼Œå®é™…%d", o.size, downloadedSize)
		return o.open(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(o, "âœ… 115ç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(downloadedSize))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œè®©rcloneèƒ½çœ‹åˆ°è¯»å–è¿›åº¦
	return &ConcurrentDownloadReader{
		file:             tempFile,
		tempPath:         tempFile.Name(),
		progressReporter: progressReporter,
		totalSize:        o.size,
	}, nil
}

// ProgressReporter è¿›åº¦æŠ¥å‘Šå™¨ï¼Œç”¨äºå‘rcloneæŠ¥å‘Šä¸‹è½½è¿›åº¦
type ProgressReporter struct {
	totalSize       int64
	transferredSize int64
	mu              sync.Mutex
}

// NewProgressReporter åˆ›å»ºæ–°çš„è¿›åº¦æŠ¥å‘Šå™¨
func NewProgressReporter(totalSize int64) *ProgressReporter {
	return &ProgressReporter{
		totalSize: totalSize,
	}
}

// AddTransferred æ·»åŠ å·²ä¼ è¾“çš„å­—èŠ‚æ•°
func (pr *ProgressReporter) AddTransferred(bytes int64) {
	pr.mu.Lock()
	defer pr.mu.Unlock()
	pr.transferredSize += bytes
}

// GetTransferred è·å–å·²ä¼ è¾“çš„å­—èŠ‚æ•°
func (pr *ProgressReporter) GetTransferred() int64 {
	pr.mu.Lock()
	defer pr.mu.Unlock()
	return pr.transferredSize
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file             *os.File
	tempPath         string
	progressReporter *ProgressReporter
	totalSize        int64
	readBytes        int64
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æŠ¥å‘Šè¯»å–è¿›åº¦
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	n, err = r.file.Read(p)
	if n > 0 {
		r.readBytes += int64(n)
		// è¿™é‡Œrcloneçš„accountingç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦æ˜¾ç¤º
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	err := r.file.Close()
	os.Remove(r.tempPath) // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return err
}
