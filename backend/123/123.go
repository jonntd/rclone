package _123

import (
	"bytes"
	"context"
	"crypto/md5"
	"encoding/json"
	"errors"
	"fmt"
	"hash"
	"io"
	"mime/multipart"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"
	"unicode/utf8"

	"github.com/rclone/rclone/lib/dircache"
	"github.com/rclone/rclone/lib/encoder"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/rest"

	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	"github.com/rclone/rclone/fs/fshttp"
	fshash "github.com/rclone/rclone/fs/hash"
)

const (
	openAPIRootURL     = "https://open-api.123pan.com"
	defaultUserAgent   = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
	tokenRefreshWindow = 10 * time.Minute

	// å®˜æ–¹QPSé™åˆ¶æ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
	// é«˜é¢‘ç‡API (15-20 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	listV2APIMinSleep = 200 * time.Millisecond // ~5 QPS ç”¨äº api/v2/file/list (å®˜æ–¹15 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	fileMoveMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/move (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileInfosMinSleep   = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/infos (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	userInfoMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/user/info (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	accessTokenMinSleep = 300 * time.Millisecond // ~3.3 QPS ç”¨äº api/v1/access_token (å®˜æ–¹8 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä½é¢‘ç‡API (5 QPS) - åŸºäºä¸Šä¼ é€Ÿåº¦åˆ†æå¤§å¹…ä¼˜åŒ–æ€§èƒ½
	// Note: uploadCreateMinSleep, mkdirMinSleep, fileTrashMinSleep removed (unused)

	downloadInfoMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº api/v1/file/download_info (ä¿æŒä¸å˜)

	// æ ¹æ®123ç½‘ç›˜APIé™æµè¡¨ä¼˜åŒ–çš„ä¸Šä¼ åˆ†ç‰‡QPS
	uploadV2SliceMinSleep = 20 * time.Millisecond // 50 QPS ç”¨äº upload/v2/file/slice (APIé™æµè¡¨æœ€é«˜å€¼)

	maxSleep      = 30 * time.Second // 429é€€é¿çš„æœ€å¤§ç¡çœ æ—¶é—´
	decayConstant = 2

	// æ–‡ä»¶ä¸Šä¼ ç›¸å…³å¸¸é‡ - æ ¹æ®123ç½‘ç›˜OpenAPIæ–‡æ¡£
	singleStepUploadLimit = 1024 * 1024 * 1024 // 1GB - V2ç‰ˆæœ¬å•æ­¥ä¸Šä¼ APIçš„æ–‡ä»¶å¤§å°é™åˆ¶
	maxMemoryBufferSize   = 1024 * 1024 * 1024 // 1GB - å†…å­˜ç¼“å†²çš„æœ€å¤§å¤§å°ï¼ˆä»512MBæå‡ï¼‰
	maxFileNameBytes      = 255                // æ–‡ä»¶åçš„æœ€å¤§å­—èŠ‚é•¿åº¦ï¼ˆUTF-8ç¼–ç ï¼‰

	// ä¸Šä¼ ç›¸å…³å¸¸é‡ - ä¼˜åŒ–å¤§æ–‡ä»¶ä¼ è¾“æ€§èƒ½
	defaultChunkSize    = 100 * fs.Mebi // å¢åŠ é»˜è®¤åˆ†ç‰‡å¤§å°åˆ°100MB
	minChunkSize        = 50 * fs.Mebi  // å¢åŠ æœ€å°åˆ†ç‰‡å¤§å°åˆ°50MB
	maxChunkSize        = 500 * fs.Mebi // è®¾ç½®æœ€å¤§åˆ†ç‰‡å¤§å°ä¸º500MB
	defaultUploadCutoff = 100 * fs.Mebi // é™ä½åˆ†ç‰‡ä¸Šä¼ é˜ˆå€¼
	maxUploadParts      = 10000

	// æ–‡ä»¶åéªŒè¯ç›¸å…³å¸¸é‡
	maxFileNameLength = 256          // 123ç½‘ç›˜æ–‡ä»¶åæœ€å¤§é•¿åº¦ï¼ˆåŒ…æ‹¬æ‰©å±•åï¼‰
	invalidChars      = `"\/:*?|><\` // 123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
	replacementChar   = "_"          // ç”¨äºæ›¿æ¢éæ³•å­—ç¬¦çš„å®‰å…¨å­—ç¬¦
	// é…ç½®éªŒè¯ç›¸å…³å¸¸é‡
	MaxListChunk          = 10000 // æœ€å¤§åˆ—è¡¨å—å¤§å°
	MaxUploadPartsLimit   = 10000 // æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°é™åˆ¶
	MaxConcurrentLimit    = 100   // æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	DefaultListChunk      = 1000  // é»˜è®¤åˆ—è¡¨å—å¤§å°
	DefaultMaxUploadParts = 1000  // é»˜è®¤æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°

	// Simplified network constants
	DefaultConcurrency = 4   // Default concurrency for uploads
	DefaultChunkSize   = 100 // Default chunk size in MB

	// Cache management constants
	PreloadQueueCapacity    = 1000 // é¢„åŠ è½½é˜Ÿåˆ—å®¹é‡
	MaxHotFiles             = 100  // æœ€å¤§çƒ­ç‚¹æ–‡ä»¶æ•°
	HotFileCleanupBatchSize = 20   // çƒ­ç‚¹æ–‡ä»¶æ¸…ç†æ‰¹æ¬¡å¤§å°
)

// Options å®šä¹‰æ­¤åç«¯çš„é…ç½®é€‰é¡¹
// ç¬¦åˆrcloneæ ‡å‡†è®¾è®¡åŸåˆ™çš„é…ç½®ç»“æ„
type Options struct {
	// 123ç½‘ç›˜ç‰¹å®šçš„è®¤è¯é…ç½®
	ClientID     string `config:"client_id"`
	ClientSecret string `config:"client_secret"`
	Token        string `config:"token"`
	UserAgent    string `config:"user_agent"`
	RootFolderID string `config:"root_folder_id"`

	// 123ç½‘ç›˜ç‰¹æœ‰çš„é…ç½®é€‰é¡¹
	MaxUploadParts int `config:"max_upload_parts"`

	// 123ç½‘ç›˜ç‰¹å®šçš„QPSæ§åˆ¶é€‰é¡¹
	UploadPacerMinSleep   fs.Duration `config:"upload_pacer_min_sleep"`
	DownloadPacerMinSleep fs.Duration `config:"download_pacer_min_sleep"`
	StrictPacerMinSleep   fs.Duration `config:"strict_pacer_min_sleep"`

	// ç¼–ç é…ç½®
	Enc encoder.MultiEncoder `config:"encoding"` // æ–‡ä»¶åç¼–ç è®¾ç½®
}

// CachedURL ç¼“å­˜çš„ä¸‹è½½URLä¿¡æ¯
type CachedURL struct {
	URL       string    // ä¸‹è½½URL
	ExpiresAt time.Time // è¿‡æœŸæ—¶é—´
}

// Fs è¡¨ç¤ºè¿œç¨‹123ç½‘ç›˜é©±åŠ¨å™¨å®ä¾‹
type Fs struct {
	name         string       // æ­¤è¿œç¨‹å®ä¾‹çš„åç§°
	originalName string       // æœªä¿®æ”¹çš„åŸå§‹é…ç½®åç§°
	root         string       // å½“å‰æ“ä½œçš„æ ¹è·¯å¾„
	opt          Options      // è§£æåçš„é…ç½®é€‰é¡¹
	features     *fs.Features // å¯é€‰åŠŸèƒ½ç‰¹æ€§

	// APIå®¢æˆ·ç«¯å’Œèº«ä»½éªŒè¯ç›¸å…³
	rst          *rest.Client     // rcloneæ ‡å‡†restå®¢æˆ·ç«¯
	token        string           // ç¼“å­˜çš„è®¿é—®ä»¤ç‰Œ
	tokenExpiry  time.Time        // ç¼“å­˜è®¿é—®ä»¤ç‰Œçš„è¿‡æœŸæ—¶é—´
	tokenMu      sync.Mutex       // ä¿æŠ¤tokenå’ŒtokenExpiryçš„äº’æ–¥é”
	tokenRenewer *oauthutil.Renew // ä»¤ç‰Œè‡ªåŠ¨æ›´æ–°å™¨

	// é…ç½®å’ŒçŠ¶æ€ä¿¡æ¯
	m            configmap.Mapper // é…ç½®æ˜ å°„å™¨
	rootFolderID string           // æ ¹æ–‡ä»¶å¤¹çš„ID

	// ç¼“å­˜ç³»ç»Ÿ
	parentDirCache   map[int64]time.Time  // ç¼“å­˜å·²éªŒè¯çš„çˆ¶ç›®å½•IDå’ŒéªŒè¯æ—¶é—´
	downloadURLCache map[string]CachedURL // ä¸‹è½½URLç¼“å­˜
	cacheMu          sync.RWMutex         // ä¿æŠ¤ç¼“å­˜çš„è¯»å†™é”

	// æ€§èƒ½å’Œé€Ÿç‡é™åˆ¶ - æ ¹æ®123ç½‘ç›˜APIé™æµè¡¨ç²¾ç¡®ä¼˜åŒ–çš„è°ƒé€Ÿå™¨
	listPacer     *fs.Pacer // æ–‡ä»¶åˆ—è¡¨APIçš„è°ƒé€Ÿå™¨ï¼ˆ10 QPSï¼‰
	strictPacer   *fs.Pacer // ä¸¥æ ¼APIï¼ˆå¦‚user/info, move, deleteï¼‰çš„è°ƒé€Ÿå™¨ï¼ˆ10 QPSï¼‰
	uploadPacer   *fs.Pacer // ä¸Šä¼ åˆ†ç‰‡APIçš„è°ƒé€Ÿå™¨ï¼ˆ50 QPS - æœ€é«˜ï¼‰
	downloadPacer *fs.Pacer // ä¸‹è½½APIçš„è°ƒé€Ÿå™¨ï¼ˆ10 QPSï¼‰
	batchPacer    *fs.Pacer // æ‰¹é‡æ“ä½œAPIçš„è°ƒé€Ÿå™¨ï¼ˆ5 QPS - æœ€ä½ï¼‰
	tokenPacer    *fs.Pacer // ä»¤ç‰Œç›¸å…³APIçš„è°ƒé€Ÿå™¨ï¼ˆ1 QPS - æœ€ä¸¥æ ¼ï¼‰

	// ç›®å½•ç¼“å­˜
	dirCache *dircache.DirCache
}

// ProgressReadCloser åŒ…è£…ReadCloserä»¥æä¾›è¿›åº¦è·Ÿè¸ªå’Œèµ„æºç®¡ç†
type ProgressReadCloser struct {
	io.ReadCloser
	fs              *Fs
	totalSize       int64
	transferredSize int64
	startTime       time.Time // ä¸‹è½½å¼€å§‹æ—¶é—´
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (prc *ProgressReadCloser) Read(p []byte) (n int, err error) {
	n, err = prc.ReadCloser.Read(p)
	if n > 0 {
		prc.transferredSize += int64(n)
		// rcloneå†…ç½®çš„accountingä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦è·Ÿè¸ª
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£ï¼ŒåŒæ—¶æ¸…ç†èµ„æº
func (prc *ProgressReadCloser) Close() error {
	// å…³é—­åº•å±‚ReadCloser
	return prc.ReadCloser.Close()
}

// Object æè¿°123ç½‘ç›˜ä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å¯¹è±¡
type Object struct {
	fs          *Fs       // çˆ¶æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	remote      string    // è¿œç¨‹è·¯å¾„
	hasMetaData bool      // æ˜¯å¦å·²åŠ è½½å…ƒæ•°æ®
	id          string    // æ–‡ä»¶/æ–‡ä»¶å¤¹ID
	size        int64     // æ–‡ä»¶å¤§å°
	md5sum      string    // MD5å“ˆå¸Œå€¼
	modTime     time.Time // ä¿®æ”¹æ—¶é—´
	isDir       bool      // æ˜¯å¦ä¸ºç›®å½•
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file     *os.File
	tempPath string
}

// Read å®ç°io.Readeræ¥å£
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	var err error
	if r.file != nil {
		err = r.file.Close()
		r.file = nil
	}
	if r.tempPath != "" {
		if removeErr := os.Remove(r.tempPath); removeErr != nil {
			fs.Debugf(nil, "âŒ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", r.tempPath, removeErr)
		} else {
			fs.Debugf(nil, "âœ… åˆ é™¤ä¸´æ—¶æ–‡ä»¶æˆåŠŸ: %s", r.tempPath)
		}
		r.tempPath = ""
	}
	return err
}

// æ–‡ä»¶åéªŒè¯ç›¸å…³çš„å…¨å±€å˜é‡
var (
	// invalidCharsRegex ç”¨äºåŒ¹é…123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
	invalidCharsRegex = regexp.MustCompile(`["\/:*?|><\\]`)
)

func validateFileName(name string) error {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„è·¯å¾„æ£€æŸ¥
	if strings.Contains(name, "/") || strings.Contains(name, "\\") {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½åŒ…å«è·¯å¾„åˆ†éš”ç¬¦")
	}

	// åŸºç¡€æ£€æŸ¥ï¼šç©ºå€¼å’Œç©ºæ ¼
	if strings.TrimSpace(name) == "" {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½ä¸ºç©ºæˆ–å…¨éƒ¨æ˜¯ç©ºæ ¼")
	}

	// UTF-8æœ‰æ•ˆæ€§æ£€æŸ¥
	if !utf8.ValidString(name) {
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«æ— æ•ˆçš„UTF-8å­—ç¬¦")
	}

	// 123ç½‘ç›˜ç‰¹å®šçš„é•¿åº¦æ£€æŸ¥ï¼ˆå­—ç¬¦æ•°ï¼‰
	runeCount := utf8.RuneCountInString(name)
	if runeCount > maxFileNameLength {
		return fmt.Errorf("æ–‡ä»¶åé•¿åº¦è¶…è¿‡é™åˆ¶ï¼šå½“å‰%dä¸ªå­—ç¬¦ï¼Œæœ€å¤§å…è®¸%dä¸ªå­—ç¬¦",
			runeCount, maxFileNameLength)
	}

	// å­—èŠ‚é•¿åº¦æ£€æŸ¥
	byteLength := len([]byte(name))
	if byteLength > maxFileNameBytes {
		return fmt.Errorf("æ–‡ä»¶åå­—èŠ‚é•¿åº¦è¶…è¿‡%d: %d", maxFileNameBytes, byteLength)
	}

	// ç¦ç”¨å­—ç¬¦æ£€æŸ¥
	if invalidCharsRegex.MatchString(name) {
		invalidFound := invalidCharsRegex.FindAllString(name, -1)
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«ä¸å…è®¸çš„å­—ç¬¦ï¼š%vï¼Œ123ç½‘ç›˜ä¸å…è®¸ä½¿ç”¨ä»¥ä¸‹å­—ç¬¦ï¼š%s",
			invalidFound, invalidChars)
	}

	return nil
}

// cleanFileName æ¸…ç†æ–‡ä»¶åï¼Œå°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºå®‰å…¨å­—ç¬¦
// åŒæ—¶ç¡®ä¿æ–‡ä»¶åé•¿åº¦ä¸è¶…è¿‡é™åˆ¶
func cleanFileName(name string) string {
	if name == "" {
		return "æœªå‘½åæ–‡ä»¶"
	}

	// æ›¿æ¢éæ³•å­—ç¬¦
	cleaned := invalidCharsRegex.ReplaceAllString(name, replacementChar)

	// å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†
	if utf8.RuneCountInString(cleaned) > maxFileNameLength {
		// ä¿ç•™æ–‡ä»¶æ‰©å±•å
		ext := filepath.Ext(cleaned)
		nameWithoutExt := strings.TrimSuffix(cleaned, ext)

		// è®¡ç®—å¯ç”¨çš„æ–‡ä»¶åé•¿åº¦ï¼ˆæ€»é•¿åº¦ - æ‰©å±•åé•¿åº¦ï¼‰
		maxNameLength := maxFileNameLength - utf8.RuneCountInString(ext)
		if maxNameLength < 1 {
			// å¦‚æœæ‰©å±•åå¤ªé•¿ï¼Œåªä¿ç•™éƒ¨åˆ†æ‰©å±•å
			maxNameLength = maxFileNameLength - 10 // ä¿ç•™è‡³å°‘10ä¸ªå­—ç¬¦ç»™æ–‡ä»¶å
			maxNameLength = max(maxNameLength, 1)
			extRunes := []rune(ext)
			if len(extRunes) > 10 {
				ext = string(extRunes[:10])
			}
		}

		// æˆªæ–­æ–‡ä»¶åä¸»ä½“éƒ¨åˆ†
		nameRunes := []rune(nameWithoutExt)
		if len(nameRunes) > maxNameLength {
			nameWithoutExt = string(nameRunes[:maxNameLength])
		}

		cleaned = nameWithoutExt + ext
	}

	return cleaned
}

// validateAndCleanFileName éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶åçš„ä¾¿æ·å‡½æ•°
// å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
func validateAndCleanFileName(name string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(name); err == nil {
		return name, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(name)

	// è¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
	warning = fmt.Errorf("æ–‡ä»¶åå·²è‡ªåŠ¨æ¸…ç†ï¼šåŸå§‹åç§° '%s' -> æ¸…ç†ååç§° '%s'", name, cleanedName)
	return cleanedName, warning
}

// generateUniqueFileName ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª
func (f *Fs) generateUniqueFileName(ctx context.Context, parentFileID int64, baseName string) (string, error) {
	// é¦–å…ˆæ£€æŸ¥åŸå§‹æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
	exists, err := f.fileExists(ctx, parentFileID, baseName)
	if err != nil {
		return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
	}

	if !exists {
		return baseName, nil
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
	ext := filepath.Ext(baseName)
	nameWithoutExt := strings.TrimSuffix(baseName, ext)

	for i := 1; i <= 999; i++ {
		newName := fmt.Sprintf("%s (%d)%s", nameWithoutExt, i, ext)

		// éªŒè¯æ–°æ–‡ä»¶åé•¿åº¦
		if len([]byte(newName)) > maxFileNameBytes {
			// å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­åŸºç¡€åç§°
			suffixBytes := make([]byte, 0, 20) // é¢„åˆ†é…è¶³å¤Ÿçš„å®¹é‡
			suffixBytes = fmt.Appendf(suffixBytes, " (%d)%s", i, ext)
			maxBaseLength := maxFileNameBytes - len(suffixBytes)
			if maxBaseLength > 0 {
				truncatedBase := string([]byte(nameWithoutExt)[:maxBaseLength])
				newName = fmt.Sprintf("%s (%d)%s", truncatedBase, i, ext)
			} else {
				// å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œä½¿ç”¨ç®€åŒ–åç§°
				newName = fmt.Sprintf("file_%d%s", i, ext)
			}
		}

		exists, err := f.fileExists(ctx, parentFileID, newName)
		if err != nil {
			return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
		}

		if !exists {
			fs.Debugf(f, "ğŸ“ ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
			return newName, nil
		}
	}

	// å¦‚æœå°è¯•äº†999æ¬¡éƒ½æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€åç§°ï¼Œä½¿ç”¨æ—¶é—´æˆ³
	timestamp := time.Now().Unix()
	newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, timestamp, ext)
	fs.Debugf(f, "â° ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
	return newName, nil
}

// fileExists æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExists(ctx context.Context, parentFileID int64, fileName string) (bool, error) {
	// ä½¿ç”¨ListFile APIæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
	response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", 0)
	if err != nil {
		return false, err
	}

	for _, file := range response.Data.FileList {
		// ğŸ”§ æ£€æŸ¥æ–‡ä»¶ååŒ¹é…ä¸”æ–‡ä»¶æœ‰æ•ˆï¼ˆä¸åœ¨å›æ”¶ç«™ä¸”æœªè¢«å®¡æ ¸é©³å›ï¼‰
		if file.Filename == fileName && isValidFile(file) {
			return true, nil
		}
	}

	return false, nil
}

// verifyParentFileID éªŒè¯çˆ¶ç›®å½•IDæ˜¯å¦å­˜åœ¨ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
func (f *Fs) verifyParentFileID(ctx context.Context, parentFileID int64) (bool, error) {
	// æ£€æŸ¥ç¼“å­˜ï¼Œå¦‚æœæœ€è¿‘éªŒè¯è¿‡ï¼ˆ5åˆ†é’Ÿå†…ï¼‰ï¼Œç›´æ¥è¿”å›æˆåŠŸ
	f.cacheMu.RLock()
	if lastVerified, exists := f.parentDirCache[parentFileID]; exists {
		if time.Since(lastVerified) < 5*time.Minute {
			f.cacheMu.RUnlock()
			fs.Debugf(f, "ğŸ“ çˆ¶ç›®å½•ID %d ç¼“å­˜éªŒè¯é€šè¿‡ï¼ˆä¸Šæ¬¡éªŒè¯: %vï¼‰", parentFileID, lastVerified.Format("15:04:05"))
			return true, nil
		}
	}
	f.cacheMu.RUnlock()

	fs.Debugf(f, "ğŸ” éªŒè¯çˆ¶ç›®å½•ID: %d", parentFileID)

	// ç›´æ¥æ‰§è¡ŒAPIéªŒè¯ï¼Œä½¿ç”¨ListFile APIï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
	response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", 0)
	if err != nil {
		fs.Debugf(f, "âŒ éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %v", parentFileID, err)
		return false, err
	}

	isValid := response.Code == 0
	if !isValid {
		fs.Debugf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼ŒAPIè¿”å›: code=%d, message=%s", parentFileID, response.Code, response.Message)
	} else {
		// éªŒè¯æˆåŠŸï¼Œæ›´æ–°ç¼“å­˜
		f.cacheMu.Lock()
		f.parentDirCache[parentFileID] = time.Now()
		f.cacheMu.Unlock()
		fs.Debugf(f, "âœ… çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	return isValid, nil
}

// getCorrectParentFileID è·å–ä¸Šä¼ APIè®¤å¯çš„æ­£ç¡®çˆ¶ç›®å½•ID
// ç»Ÿä¸€ç‰ˆæœ¬ï¼šä¸ºå•æ­¥ä¸Šä¼ å’Œåˆ†ç‰‡ä¸Šä¼ æä¾›ä¸€è‡´çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
func (f *Fs) getCorrectParentFileID(ctx context.Context, cachedParentID int64) (int64, error) {
	fs.Debugf(f, "ğŸ”§ ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥ï¼Œç¼“å­˜ID: %d", cachedParentID)

	// Strategy 1: Re-acquire directory structure
	fs.Debugf(f, "ğŸ”„ é‡æ–°è·å–ç›®å½•ç»“æ„")

	// Reset dirCache only, keep other valid caches
	if f.dirCache != nil {
		fs.Debugf(f, "ğŸ”„ é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…é™¤è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", cachedParentID)
		f.dirCache.ResetRoot()
	}

	// Strategy 2: Re-verify original ID (may recover after cache cleanup)
	fs.Debugf(f, "ğŸ” é‡æ–°éªŒè¯åŸå§‹ç›®å½•ID: %d", cachedParentID)
	exists, err := f.verifyParentFileID(ctx, cachedParentID)
	if err == nil && exists {
		fs.Debugf(f, "âœ… ç¼“å­˜æ¸…ç†ååŸå§‹ç›®å½•ID %d éªŒè¯æˆåŠŸ", cachedParentID)
		return cachedParentID, nil
	}

	// Strategy 3: Try to verify if root directory is available
	fs.Debugf(f, "ğŸ  å°è¯•éªŒè¯æ ¹ç›®å½• (parentFileID: 0)")
	rootExists, err := f.verifyParentFileID(ctx, 0)
	if err == nil && rootExists {
		fs.Debugf(f, "âœ… æ ¹ç›®å½•éªŒè¯æˆåŠŸï¼Œç”¨ä½œå›é€€æ–¹æ¡ˆ")
		return 0, nil
	}

	// Strategy 4: Force fallback to root directory
	fs.Debugf(f, "âŒ æ‰€æœ‰éªŒè¯ç­–ç•¥å¤±è´¥ï¼Œå¼ºåˆ¶ä½¿ç”¨æ ¹ç›®å½• (parentFileID: 0)")
	return 0, nil
}

// getParentID è·å–æ–‡ä»¶æˆ–ç›®å½•çš„çˆ¶ç›®å½•ID
func (f *Fs) getParentID(ctx context.Context, fileID int64) (int64, error) {
	fs.Debugf(f, "ğŸ” å°è¯•è·å–æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID", fileID)

	// ç›´æ¥é€šè¿‡APIæŸ¥è¯¢ï¼Œå› ä¸ºç¼“å­˜æœç´¢åŠŸèƒ½æœªå®ç°

	// å¦‚æœç¼“å­˜ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡APIæŸ¥è¯¢
	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…APIè·å–æ–‡ä»¶ä¿¡æ¯
	var response struct {
		Code int `json:"code"`
		Data struct {
			ParentFileID int64 `json:"parentFileId"`
		} `json:"data"`
		Message string `json:"message"`
	}

	url := fmt.Sprintf("/api/v1/file/info?fileId=%d", fileID)
	err := f.makeAPICallWithRest(ctx, url, "GET", nil, &response)
	if err != nil {
		return 0, fmt.Errorf("è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return 0, fmt.Errorf("APIè¿”å›é”™è¯¯: %s", response.Message)
	}

	fs.Debugf(f, "âœ… é€šè¿‡APIè·å–åˆ°æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID: %d", fileID, response.Data.ParentFileID)
	return response.Data.ParentFileID, nil
}

// isValidFile æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆï¼ˆä¸åœ¨å›æ”¶ç«™ä¸”æœªè¢«å®¡æ ¸é©³å›ï¼‰
// æ ¹æ®123äº‘ç›˜APIæ–‡æ¡£ï¼Œå¿…é¡»è¿‡æ»¤ trashed=1 å’Œ status>=100 çš„æ–‡ä»¶
func isValidFile(file FileListInfoRespDataV2) bool {
	return file.Trashed == 0 && file.Status < 100
}

// fileExistsInDirectory æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExistsInDirectory(ctx context.Context, parentID int64, fileName string) (bool, int64, error) {
	fs.Debugf(f, "ğŸ” æ£€æŸ¥ç›®å½• %d ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶: %s", parentID, fileName)

	// ä½¿ç”¨ListFile APIæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
	response, err := f.ListFile(ctx, int(parentID), 100, "", "", 0)
	if err != nil {
		return false, 0, err
	}

	if response.Code != 0 {
		return false, 0, fmt.Errorf("ListFile APIé”™è¯¯: code=%d, message=%s", response.Code, response.Message)
	}

	for _, file := range response.Data.FileList {
		// ğŸ”§ æ£€æŸ¥æ–‡ä»¶ååŒ¹é…ä¸”ä¸åœ¨å›æ”¶ç«™ä¸”æœªè¢«å®¡æ ¸é©³å›
		if file.Filename == fileName && file.Trashed == 0 && file.Status < 100 {
			fs.Debugf(f, "âœ… æ‰¾åˆ°æ–‡ä»¶ %sï¼ŒID: %d (trashed=%d, status=%d)", fileName, file.FileID, file.Trashed, file.Status)
			return true, int64(file.FileID), nil
		}
	}

	fs.Debugf(f, "âŒ ç›®å½• %d ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: %s", parentID, fileName)
	return false, 0, nil
}

// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†è·¯å¾„å¤„ç†åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè¦æ±‚
// dircacheè¦æ±‚ï¼šè·¯å¾„ä¸åº”è¯¥ä»¥/å¼€å¤´æˆ–ç»“å°¾
func normalizePath(path string) string {
	if path == "" {
		return ""
	}

	// ä½¿ç”¨filepath.Cleanè¿›è¡ŒåŸºç¡€è·¯å¾„æ¸…ç†
	path = filepath.Clean(path)

	// å¤„ç†filepath.Cleançš„ç‰¹æ®Šè¿”å›å€¼
	if path == "." {
		return ""
	}

	// ç§»é™¤å¼€å¤´çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimPrefix(path, "/")
	path = strings.TrimPrefix(path, "\\") // å¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦

	// ç§»é™¤ç»“å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimSuffix(path, "/")
	path = strings.TrimSuffix(path, "\\")

	// å°†åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆæ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦ï¼‰
	path = strings.ReplaceAll(path, "\\", "/")

	// æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç»“æœä¸ä»¥/å¼€å¤´æˆ–ç»“å°¾
	path = strings.Trim(path, "/")

	return path
}

// getCachedDownloadURL è·å–ç¼“å­˜çš„ä¸‹è½½URL
func (f *Fs) getCachedDownloadURL(fileID string) (string, bool) {
	f.cacheMu.RLock()
	defer f.cacheMu.RUnlock()

	cached, exists := f.downloadURLCache[fileID]
	if !exists {
		return "", false
	}

	// æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆæ ¹æ®123ç½‘ç›˜APIæ–‡æ¡£ï¼Œä¸‹è½½URLæœ‰æ•ˆæœŸä¸º2å°æ—¶ï¼‰
	if time.Now().After(cached.ExpiresAt) {
		// è¿‡æœŸäº†ï¼Œéœ€è¦æ¸…ç†
		go func() {
			f.cacheMu.Lock()
			delete(f.downloadURLCache, fileID)
			f.cacheMu.Unlock()
		}()
		return "", false
	}

	fs.Debugf(f, "â¬‡ï¸ ä½¿ç”¨ç¼“å­˜çš„ä¸‹è½½URL: fileID=%s", fileID)
	return cached.URL, true
}

// setCachedDownloadURL è®¾ç½®ä¸‹è½½URLç¼“å­˜
func (f *Fs) setCachedDownloadURL(fileID, url string) {
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	// ä¸‹è½½URLç¼“å­˜1.5å°æ—¶ï¼ˆå……åˆ†åˆ©ç”¨2å°æ—¶æœ‰æ•ˆæœŸï¼Œç•™0.5å°æ—¶å®‰å…¨è¾¹é™…ï¼‰
	f.downloadURLCache[fileID] = CachedURL{
		URL:       url,
		ExpiresAt: time.Now().Add(90 * time.Minute),
	}

	fs.Debugf(f, "ğŸ’¾ ç¼“å­˜ä¸‹è½½URL: fileID=%s, æœ‰æ•ˆæœŸ90åˆ†é’Ÿ", fileID)
}

// hasFileExtension ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶æ‰©å±•åï¼ˆä»…ç”¨äºè·¯å¾„åˆå§‹åŒ–æ—¶çš„å¯å‘å¼åˆ¤æ–­ï¼‰
// æ³¨æ„ï¼š123ç½‘ç›˜APIçš„Typeå­—æ®µå·²ç»å¾ˆå‡†ç¡®ï¼Œè¿™ä¸ªå‡½æ•°åªåœ¨æ— æ³•è°ƒç”¨APIæ—¶ä½¿ç”¨
func hasFileExtension(filename string) bool {
	if filename == "" {
		return false
	}
	// ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•å
	return strings.Contains(filename, ".") && !strings.HasSuffix(filename, ".")
}

// isRemoteSource æ£€æŸ¥æºå¯¹è±¡æ˜¯å¦æ¥è‡ªè¿œç¨‹äº‘ç›˜ï¼ˆéæœ¬åœ°æ–‡ä»¶ï¼‰
// ä½¿ç”¨rcloneæ ‡å‡†çš„Features.IsLocalæ–¹æ³•è¿›è¡Œæ£€æµ‹
func (f *Fs) isRemoteSource(src fs.ObjectInfo) bool {
	srcFs := src.Fs()
	if srcFs == nil {
		fs.Debugf(f, "ğŸ” isRemoteSource: srcFsä¸ºnilï¼Œè¿”å›false")
		return false
	}

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•ï¼šæ£€æŸ¥Features.IsLocal
	features := srcFs.Features()
	if features != nil && features.IsLocal {
		fs.Debugf(f, "ğŸ  rcloneæ ‡å‡†æ£€æµ‹: æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ (%s)", srcFs.Name())
		return false
	}

	// éæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå³ä¸ºè¿œç¨‹æº
	fs.Debugf(f, "â˜ï¸ rcloneæ ‡å‡†æ£€æµ‹: è¿œç¨‹æ–‡ä»¶ç³»ç»Ÿ (%s) - å¯ç”¨è·¨äº‘ä¼ è¾“æ¨¡å¼", srcFs.Name())
	return true
}

// init å‡½æ•°ç”¨äºå‘Fsæ³¨å†Œ123ç½‘ç›˜åç«¯
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "123",
		Description: "123ç½‘ç›˜é©±åŠ¨å™¨",
		NewFs:       newFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name:     "client_id",
			Help:     "123ç½‘ç›˜APIå®¢æˆ·ç«¯IDã€‚",
			Required: true,
		}, {
			Name:      "client_secret",
			Help:      "123ç½‘ç›˜APIå®¢æˆ·ç«¯å¯†é’¥ã€‚",
			Required:  true,
			Sensitive: true,
		}, {
			Name:      "token",
			Help:      "OAuthè®¿é—®ä»¤ç‰Œï¼ˆJSONæ ¼å¼ï¼‰ã€‚é€šå¸¸ç•™ç©ºã€‚",
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Help:     "APIè¯·æ±‚çš„User-Agentå¤´ã€‚",
			Default:  defaultUserAgent,
			Advanced: true,
		}, {
			Name:     "root_folder_id",
			Help:     "æ ¹æ–‡ä»¶å¤¹çš„IDã€‚ç•™ç©ºè¡¨ç¤ºæ ¹ç›®å½•ã€‚",
			Default:  "0",
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ ä¸­çš„æœ€å¤§åˆ†ç‰‡æ•°ã€‚",
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name:     "upload_pacer_min_sleep",
			Help:     "ä¸Šä¼ APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚è®¾ç½®æ›´é«˜çš„å€¼å¯ä»¥é¿å…429é”™è¯¯ã€‚",
			Default:  uploadV2SliceMinSleep,
			Advanced: true,
		}, {
			Name:     "download_pacer_min_sleep",
			Help:     "ä¸‹è½½APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  downloadInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "strict_pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆmove, deleteç­‰ï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  fileMoveMinSleep,
			Advanced: true,
		}, {
			Name:     config.ConfigEncoding,
			Help:     config.ConfigEncodingHelp,
			Advanced: true,
			// é»˜è®¤åŒ…å«Slashå’ŒInvalidUtf8ç¼–ç ï¼Œé€‚é…ä¸­æ–‡æ–‡ä»¶åå’Œç‰¹æ®Šå­—ç¬¦
			Default: (encoder.EncodeCtl |
				encoder.EncodeLeftSpace |
				encoder.EncodeRightSpace |
				encoder.EncodeSlash | // æ–°å¢ï¼šé»˜è®¤ç¼–ç æ–œæ 
				encoder.EncodeInvalidUtf8), // æ–°å¢ï¼šç¼–ç æ— æ•ˆUTF-8å­—ç¬¦
		}},
	})
}

var commandHelp = []fs.CommandHelp{{
	Name:  "media-sync",
	Short: "åŒæ­¥åª’ä½“åº“å¹¶åˆ›å»ºä¼˜åŒ–çš„.strmæ–‡ä»¶",
	Long: `å°†123ç½‘ç›˜ä¸­çš„è§†é¢‘æ–‡ä»¶åŒæ­¥åˆ°æœ¬åœ°ç›®å½•ï¼Œåˆ›å»ºå¯¹åº”çš„.strmæ–‡ä»¶ã€‚
.strmæ–‡ä»¶å°†åŒ…å«ä¼˜åŒ–çš„fileIdæ ¼å¼ï¼Œæ”¯æŒç›´æ¥æ’­æ”¾å’Œåª’ä½“åº“ç®¡ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend media-sync 123:Movies /local/media/movies
rclone backend media-sync 123:Videos /local/media/videos -o min-size=200M -o strm-format=true

æ”¯æŒçš„è§†é¢‘æ ¼å¼: mp4, mkv, avi, mov, wmv, flv, webm, m4v, 3gp, ts, m2ts
.strmæ–‡ä»¶å†…å®¹æ ¼å¼: 123://fileId (å¯é€šè¿‡strm-formaté€‰é¡¹è°ƒæ•´)`,
	Opts: map[string]string{
		"min-size":    "æœ€å°æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼Œå°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†è¢«å¿½ç•¥ (é»˜è®¤: 100M)",
		"strm-format": ".strmæ–‡ä»¶å†…å®¹æ ¼å¼: true(ä¼˜åŒ–æ ¼å¼)/false(è·¯å¾„æ ¼å¼) (é»˜è®¤: trueï¼Œå…¼å®¹: fileid/path)",
		"include":     "åŒ…å«çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš” (é»˜è®¤: mp4,mkv,avi,mov,wmv,flv,webm,m4v,3gp,ts,m2ts)",
		"exclude":     "æ’é™¤çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš”",
		"update-mode": "æ›´æ–°æ¨¡å¼: full/incremental (é»˜è®¤: full)",
		"sync-delete": "åŒæ­¥åˆ é™¤: false(ä»…åˆ›å»º.strmæ–‡ä»¶)/true(åˆ é™¤å­¤ç«‹æ–‡ä»¶å’Œç©ºç›®å½•) (é»˜è®¤: trueï¼Œç±»ä¼¼rclone sync)",
		"dry-run":     "é¢„è§ˆæ¨¡å¼ï¼Œæ˜¾ç¤ºå°†è¦åˆ›å»ºçš„æ–‡ä»¶ä½†ä¸å®é™…åˆ›å»º (true/false)",
		"target-path": "ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœä¸åœ¨å‚æ•°ä¸­æŒ‡å®šåˆ™å¿…é¡»é€šè¿‡æ­¤é€‰é¡¹æä¾›",
	},
}, {
	Name:  "get-download-url",
	Short: "é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL",
	Long: `é€šè¿‡123ç½‘ç›˜çš„fileIdæˆ–.strmæ–‡ä»¶å†…å®¹è·å–å®é™…çš„ä¸‹è½½URLã€‚
æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œç‰¹åˆ«é€‚ç”¨äºåª’ä½“æœåŠ¡å™¨å’Œ.strmæ–‡ä»¶å¤„ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend get-download-url 123: "123://17995550"
rclone backend get-download-url 123: "17995550"
rclone backend get-download-url 123: "/path/to/file.mp4"
rclone backend get-download-url 123: "123://17995550" -o user-agent="Custom-UA"

è¾“å…¥æ ¼å¼æ”¯æŒ:
- 123://fileId æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
- çº¯fileId
- æ–‡ä»¶è·¯å¾„ (è‡ªåŠ¨è§£æä¸ºfileId)`,
	Opts: map[string]string{
		"user-agent": "è‡ªå®šä¹‰User-Agentå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰",
	},
},
}

// Command æ‰§è¡Œåç«¯ç‰¹å®šçš„å‘½ä»¤ã€‚
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {

	case "media-sync":
		// ğŸ¬ æ–°å¢ï¼šåª’ä½“åº“åŒæ­¥åŠŸèƒ½
		return f.mediaSyncCommand(ctx, arg, opt)

	case "get-download-url":
		// ğŸ”— æ–°å¢ï¼šé€šè¿‡fileIdè·å–ä¸‹è½½URL
		return f.getDownloadURLCommand(ctx, arg, opt)

	default:
		return nil, fs.ErrorCommandNotFound
	}
}

type ListRequest struct {
	ParentFileID int    `json:"parentFileId"`
	Limit        int    `json:"limit"`
	SearchData   string `json:"searchData,omitempty"`
	SearchMode   string `json:"searchMode,omitempty"`
	LastFileID   int    `json:"lastFileID,omitempty"`
}

type ListResponse struct {
	Code    int                   `json:"code"`
	Message string                `json:"message"`
	Data    GetFileListRespDataV2 `json:"data"` // æ›´æ”¹ä¸ºç‰¹å®šç±»å‹
}

// GetFileListRespDataV2 è¡¨ç¤ºæ–‡ä»¶åˆ—è¡¨å“åº”çš„æ•°æ®ç»“æ„
type GetFileListRespDataV2 struct {
	// -1ä»£è¡¨æœ€åä¸€é¡µï¼ˆæ— éœ€å†ç¿»é¡µæŸ¥è¯¢ï¼‰, å…¶ä»–ä»£è¡¨ä¸‹ä¸€é¡µå¼€å§‹çš„æ–‡ä»¶idï¼Œæºå¸¦åˆ°è¯·æ±‚å‚æ•°ä¸­
	LastFileId int64 `json:"lastFileId"`
	// æ–‡ä»¶åˆ—è¡¨
	FileList []FileListInfoRespDataV2 `json:"fileList"`
}

// FileListInfoRespDataV2 è¡¨ç¤ºæ¥è‡ªAPIå“åº”çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹é¡¹ç›®
type FileListInfoRespDataV2 struct {
	// æ–‡ä»¶ID
	FileID int64 `json:"fileID"`
	// æ–‡ä»¶å
	Filename string `json:"filename"`
	// 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	Type int `json:"type"`
	// æ–‡ä»¶å¤§å°
	Size int64 `json:"size"`
	// md5
	Etag string `json:"etag"`
	// æ–‡ä»¶å®¡æ ¸çŠ¶æ€, å¤§äº 100 ä¸ºå®¡æ ¸é©³å›æ–‡ä»¶
	Status int `json:"status"`
	// ç›®å½•ID
	ParentFileID int64 `json:"parentFileID"`
	// æ–‡ä»¶åˆ†ç±», 0-æœªçŸ¥ 1-éŸ³é¢‘ 2-è§†é¢‘ 3-å›¾ç‰‡
	Category int `json:"category"`
	// å›æ”¶ç«™æ ‡è¯†, 0-æ­£å¸¸ 1-åœ¨å›æ”¶ç«™ (é‡è¦ï¼šå¿…é¡»è¿‡æ»¤trashed=1çš„æ–‡ä»¶)
	Trashed int `json:"trashed"`
}

func (f *Fs) ListFile(ctx context.Context, parentFileID, limit int, searchData, searchMode string, lastFileID int) (*ListResponse, error) {
	fs.Debugf(f, "ğŸ“‹ è°ƒç”¨ListFileï¼Œå‚æ•°ï¼šparentFileID=%d, limit=%d, lastFileID=%d", parentFileID, limit, lastFileID)

	// ç§»é™¤ç¼“å­˜æŸ¥è¯¢ï¼Œç›´æ¥è°ƒç”¨API

	// æ„é€ æŸ¥è¯¢å‚æ•°
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", limit))

	if searchData != "" {
		params.Add("searchData", searchData)
	}
	if searchMode != "" {
		params.Add("searchMode", searchMode)
	}
	if lastFileID != 0 {
		params.Add("lastFileID", fmt.Sprintf("%d", lastFileID))
	}

	// æ„é€ å¸¦æŸ¥è¯¢å‚æ•°çš„ç«¯ç‚¹
	endpoint := "/api/v2/file/list?" + params.Encode()
	fs.Debugf(f, "ğŸŒ APIç«¯ç‚¹: %s%s", openAPIRootURL, endpoint)

	// ç›´æ¥ä½¿ç”¨v2 APIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var result ListResponse
	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &result)
	if err != nil {
		fs.Debugf(f, "âŒ ListFile APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "âœ… ListFile APIå“åº”: code=%d, message=%s, fileCount=%d", result.Code, result.Message, len(result.Data.FileList))

	// ç§»é™¤ç¼“å­˜ä¿å­˜ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

	return &result, nil
}

func (f *Fs) pathToFileID(ctx context.Context, filePath string) (string, error) {
	fs.Debugf(f, "ğŸ” pathToFileIDå¼€å§‹: filePath=%s", filePath)

	// æ ¹ç›®å½•
	if filePath == "/" {
		return "0", nil
	}
	if len(filePath) > 1 && strings.HasSuffix(filePath, "/") {
		filePath = filePath[:len(filePath)-1]
	}

	// ç§»é™¤è·¯å¾„æ˜ å°„ç¼“å­˜æŸ¥è¯¢ï¼Œç›´æ¥è§£æè·¯å¾„
	currentID := "0"
	parts := strings.Split(filePath, "/")
	if len(parts) > 0 && parts[0] == "" { // å¦‚æœè·¯å¾„ä»¥/å¼€å¤´ï¼Œç§»é™¤ç©ºå­—ç¬¦ä¸²
		parts = parts[1:]
	}
	for _, part := range parts {
		if part == "" {
			continue
		}
		findPart := false
		next := "0"           // æ ¹æ®APIä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºnext
		maxIterations := 1000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š1000æ¬¡è¿­ä»£
		iteration := 0
		for {
			iteration++
			if iteration > maxIterations {
				return "", fmt.Errorf("æŸ¥æ‰¾è·¯å¾„ %s è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", part, maxIterations)
			}
			parentFileID, err := strconv.Atoi(currentID)
			if err != nil {
				return "", fmt.Errorf("invalid parentFileId: %s", currentID)
			}

			lastFileIDInt, err := strconv.Atoi(next)
			if err != nil {
				return "", fmt.Errorf("invalid next token: %s", next)
			}

			var response *ListResponse
			// ä½¿ç”¨åˆ—è¡¨è°ƒé€Ÿå™¨è¿›è¡Œé€Ÿç‡é™åˆ¶ï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
			err = f.listPacer.Call(func() (bool, error) {
				response, err = f.ListFile(ctx, parentFileID, 100, "", "", lastFileIDInt)
				if err != nil {
					return shouldRetry(ctx, nil, err)
				}
				return false, nil
			})
			if err != nil {
				return "", fmt.Errorf("list file failed: %w", err)
			}
			if response.Code != 200 && response.Code != 0 {
				return "", fmt.Errorf("API returned error code %d: %s", response.Code, response.Message)
			}
			infoList := response.Data.FileList
			if len(infoList) == 0 {
				break
			}
			for _, item := range infoList {
				if item.Filename == part {
					currentID = strconv.FormatInt(item.FileID, 10)
					findPart = true

					// è®°å½•æ‰¾åˆ°çš„é¡¹ç›®ç±»å‹ä¿¡æ¯ï¼Œç”¨äºåç»­ç¼“å­˜
					isDir := (item.Type == 1) // Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹

					// âœ… ç›´æ¥ä½¿ç”¨123ç½‘ç›˜APIçš„Typeå­—æ®µï¼Œæ— éœ€é¢å¤–åˆ¤æ–­
					// APIçš„Typeå­—æ®µå·²ç»å¾ˆå‡†ç¡®ï¼š0=æ–‡ä»¶ï¼Œ1=æ–‡ä»¶å¤¹
					fs.Debugf(f, "âœ… 123ç½‘ç›˜APIç±»å‹: '%s' Type=%d, isDir=%v", part, item.Type, isDir)

					fs.Debugf(f, "pathToFileIDæ‰¾åˆ°é¡¹ç›®: %s -> ID=%s, Type=%d, isDir=%v", part, currentID, item.Type, isDir)

					// ç§»é™¤è·¯å¾„ç±»å‹ç¼“å­˜ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

					break
				}
			}
			if findPart {
				break
			}

			nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
			if nextRaw == "-1" {
				break
			} else {
				next = nextRaw
				// æ— éœ€æ‰‹åŠ¨ç¡çœ  - è°ƒé€Ÿå™¨å¤„ç†é€Ÿç‡é™åˆ¶
			}
		}
		if !findPart {
			return "", fs.ErrorObjectNotFound
		}
	}

	if currentID == "0" && filePath != "/" {
		return "", fs.ErrorObjectNotFound
	}

	// ç¼“å­˜è·¯å¾„æ˜ å°„ç»“æœå·²åœ¨è·¯å¾„è§£æå¾ªç¯ä¸­æ­£ç¡®å¤„ç†

	fs.Debugf(f, " pathToFileIDç»“æœ: filePath=%s -> currentID=%s", filePath, currentID)
	return currentID, nil
}

type FileDetail struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

type FileInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		List []FileDetail `json:"list"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

type FileDetailResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID       int64  `json:"fileID"`
		Filename     string `json:"filename"`
		Type         int    `json:"type"`
		Size         int64  `json:"size"`
		ETag         string `json:"etag"`
		Status       int    `json:"status"`
		ParentFileID int64  `json:"parentFileID"`
		CreateAt     string `json:"createAt"`
		Trashed      int    `json:"trashed"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

// FileInfo è¡¨ç¤º'list'æ•°ç»„ä¸­å•ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
type FileInfo struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

// FileInfosResponse æ˜¯APIå“åº”çš„é¡¶çº§ç»“æ„

// å®šä¹‰Dataç»“æ„ä½“ï¼Œæ˜ å°„dataå­—æ®µçš„å†…å®¹
type Data struct {
	FileList []FileInfo `json:"fileList"`
}

// å®šä¹‰FileInfosResponseç»“æ„ä½“ï¼Œæ˜ å°„æ•´ä¸ªå“åº”JSON
type FileInfosResponse struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     Data   `json:"data"`
	XTraceID string `json:"x-traceID"`
}

// å®šä¹‰FileInfoRequestç»“æ„ä½“ï¼Œç”¨äºå‘é€è¯·æ±‚çš„payload
type FileInfoRequest struct {
	FileIDs []int64 `json:"fileIDs"`
}

type DownloadInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		DownloadURL string `json:"downloadUrl"` // ç›´é“¾åœ°å€
		ExpireTime  string `json:"expireTime"`  // è¿‡æœŸæ—¶é—´
	} `json:"data"`
}

type UploadCreateResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID      int64  `json:"fileID"`
		PreuploadID string `json:"preuploadID"`
		Reuse       bool   `json:"reuse"`
		SliceSize   int64  `json:"sliceSize"`
	} `json:"data"`
}

type UserInfoResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		UID            int64  `json:"uid"`
		Username       string `json:"username"`
		DisplayName    string `json:"displayName"`
		HeadImage      string `json:"headImage"`
		Passport       string `json:"passport"`
		Mail           string `json:"mail"`
		SpaceUsed      int64  `json:"spaceUsed"`
		SpacePermanent int64  `json:"spacePermanent"`
		SpaceTemp      int64  `json:"spaceTemp"`
		SpaceTempExpr  int64  `json:"spaceTempExpr"`
		Vip            bool   `json:"vip"`
		DirectTraffic  int64  `json:"directTraffic"`
		IsHideUID      bool   `json:"isHideUID"`
	} `json:"data"`
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, userAgent string) (string, error) {
	// Ensure token is valid before making API calls
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return "", err
	}

	// Record User-Agent usage
	if userAgent != "" {
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰User-Agent: %s", userAgent)
	} else {
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨é»˜è®¤User-Agent")
	}

	if filePath == "" {
		filePath = f.root
	}

	fileID, err := f.pathToFileID(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to get file ID for path %q: %w", filePath, err)
	}

	fs.Debugf(f, "ğŸ“¥ é€šè¿‡è·¯å¾„è·å–ä¸‹è½½URL: path=%s, fileId=%s", filePath, fileID)

	// Use the standard getDownloadURL method
	return f.getDownloadURL(ctx, fileID)
}

// getDownloadURLCommand é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL
func (f *Fs) getDownloadURLCommand(ctx context.Context, args []string, opt map[string]string) (any, error) {
	if len(args) < 1 {
		return nil, fmt.Errorf("éœ€è¦æä¾›fileIdã€123://fileIdæ ¼å¼æˆ–æ–‡ä»¶è·¯å¾„")
	}

	input := args[0]
	fs.Debugf(f, "ğŸ”— å¤„ç†ä¸‹è½½URLè¯·æ±‚: %s", input)

	var fileID string
	var err error

	// Parse input format
	if fileID, found := strings.CutPrefix(input, "123://"); found {
		// 123://fileId format (from .strm files)
		fs.Debugf(f, "ğŸ“± è§£æ.strmæ ¼å¼: fileId=%s", fileID)
	} else if strings.HasPrefix(input, "/") {
		// File path format, needs conversion to fileId
		fs.Debugf(f, "ğŸ“‚ è§£ææ–‡ä»¶è·¯å¾„: %s", input)
		fileID, err = f.pathToFileID(ctx, input)
		if err != nil {
			return nil, fmt.Errorf("path to fileId conversion failed %q: %w", input, err)
		}
		fs.Debugf(f, "âœ… è·¯å¾„è½¬æ¢æˆåŠŸ: %s -> %s", input, fileID)
	} else {
		// Assume pure fileId
		fileID = input
		fs.Debugf(f, "ğŸ†” ä½¿ç”¨çº¯æ–‡ä»¶ID: %s", fileID)
	}

	// éªŒè¯fileIdæ ¼å¼
	if fileID == "" {
		return nil, fmt.Errorf("æ— æ•ˆçš„fileId: %s", input)
	}

	// Get download URL
	userAgent := opt["user-agent"]
	if userAgent != "" {
		// If custom UA provided, use getDownloadURLByUA method
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URL: fileId=%s, UA=%s", fileID, userAgent)

		// Need to convert fileID back to path since getDownloadURLByUA needs path parameter
		var filePath string
		if strings.HasPrefix(input, "/") {
			filePath = input // If original input is path, use directly
		} else {
			// If original input is fileID, we can't easily convert back to path, so use standard method
			fs.Debugf(f, "âš ï¸ è‡ªå®šä¹‰UAä»…æ”¯æŒè·¯å¾„è¾“å…¥ï¼Œæ–‡ä»¶IDè¾“å…¥å°†ä½¿ç”¨æ ‡å‡†æ–¹æ³•")
			downloadURL, err := f.getDownloadURL(ctx, fileID)
			if err != nil {
				return nil, fmt.Errorf("failed to get download URL: %w", err)
			}
			fs.Debugf(f, "âœ… æˆåŠŸè·å–ä¸‹è½½URL: fileId=%s (æ ‡å‡†æ–¹æ³•)", fileID)
			return downloadURL, nil
		}

		downloadURL, err := f.getDownloadURLByUA(ctx, filePath, userAgent)
		if err != nil {
			return nil, fmt.Errorf("ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URLå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "âœ… ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URLæˆåŠŸ: fileId=%s", fileID)
		return downloadURL, nil
	} else {
		downloadURL, err := f.getDownloadURL(ctx, fileID)
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL: %w", err)
		}
		fs.Debugf(f, "âœ… è·å–ä¸‹è½½URLæˆåŠŸ: fileId=%s", fileID)
		return downloadURL, nil
	}
}

// validateDuration éªŒè¯æ—¶é—´é…ç½®ä¸ä¸ºè´Ÿæ•°
func validateDuration(fieldName string, value time.Duration) error {
	if value < 0 {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºè´Ÿæ•°", fieldName)
	}
	return nil
}

// validateOptions éªŒè¯é…ç½®é€‰é¡¹çš„æœ‰æ•ˆæ€§
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»“æ„åŒ–éªŒè¯æ–¹æ³•ï¼Œå‡å°‘é‡å¤ä»£ç 
func validateOptions(opt *Options) error {
	var errors []string

	// Validate required authentication info
	if opt.ClientID == "" {
		errors = append(errors, "client_id cannot be empty")
	}
	if opt.ClientSecret == "" {
		errors = append(errors, "client_secret cannot be empty")
	}

	// Validate numeric ranges
	if opt.MaxUploadParts <= 0 || opt.MaxUploadParts > MaxUploadPartsLimit {
		errors = append(errors, fmt.Sprintf("max_upload_parts must be between 1-%d", MaxUploadPartsLimit))
	}

	// éªŒè¯123ç½‘ç›˜ç‰¹å®šçš„æ—¶é—´é…ç½®
	if err := validateDuration("upload_pacer_min_sleep", time.Duration(opt.UploadPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("download_pacer_min_sleep", time.Duration(opt.DownloadPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("strict_pacer_min_sleep", time.Duration(opt.StrictPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯UserAgenté•¿åº¦
	if opt.UserAgent != "" && len(opt.UserAgent) > 200 {
		errors = append(errors, "user_agent é•¿åº¦ä¸èƒ½è¶…è¿‡ 200 å­—ç¬¦")
	}

	// éªŒè¯RootFolderIDæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
	if opt.RootFolderID != "" {
		if _, err := parseFileID(opt.RootFolderID); err != nil {
			errors = append(errors, "root_folder_id å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•°å­—")
		}
	}

	// è¿”å›èšåˆçš„é”™è¯¯ä¿¡æ¯
	if len(errors) > 0 {
		return fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %s", strings.Join(errors, "; "))
	}

	return nil
}

// createPacer åˆ›å»ºpacerçš„å·¥å‚å‡½æ•°ï¼Œå‡å°‘é‡å¤é…ç½®ä»£ç 
func createPacer(ctx context.Context, minSleep, maxSleep time.Duration, decayConstant float64) *fs.Pacer {
	return fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(minSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))
}

// normalizeOptions æ ‡å‡†åŒ–å’Œä¿®æ­£é…ç½®é€‰é¡¹
func normalizeOptions(opt *Options) {
	// è®¾ç½®123ç½‘ç›˜ç‰¹æœ‰é…ç½®çš„é»˜è®¤å€¼
	if opt.MaxUploadParts <= 0 {
		opt.MaxUploadParts = DefaultMaxUploadParts
	}

	// è®¾ç½®123ç½‘ç›˜ç‰¹å®šçš„paceré»˜è®¤å€¼
	if time.Duration(opt.UploadPacerMinSleep) <= 0 {
		opt.UploadPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.DownloadPacerMinSleep) <= 0 {
		opt.DownloadPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.StrictPacerMinSleep) <= 0 {
		opt.StrictPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}

	// è®¾ç½®é»˜è®¤UserAgent
	if opt.UserAgent == "" {
		opt.UserAgent = "rclone/" + fs.Version
	}
}

// shouldRetry æ ¹æ®å“åº”å’Œé”™è¯¯ç¡®å®šæ˜¯å¦é‡è¯•APIè°ƒç”¨ï¼ˆæœ¬åœ°å®ç°ï¼Œæ›¿ä»£commonåŒ…ï¼‰
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„é‡è¯•åˆ¤æ–­
	return fserrors.ShouldRetry(err), err
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡ŒAPIè°ƒç”¨ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶
// è¿™æ˜¯æ¨èçš„APIè°ƒç”¨æ–¹æ³•ï¼Œæ›¿ä»£ç›´æ¥ä½¿ç”¨HTTPå®¢æˆ·ç«¯
func (f *Fs) makeAPICallWithRest(ctx context.Context, endpoint string, method string, reqBody any, respBody any) error {
	fs.Debugf(f, "ğŸŒ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è¿›è¡ŒAPIè°ƒç”¨: %s %s", method, endpoint)

	// Safety check: ensure rest client is initialized
	if f.rst == nil {
		fs.Debugf(f, "ğŸ”§ Restå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆ›å»º")
		// Recreate rest client using rclone standard HTTP client
		client := fshttp.NewClient(context.Background())
		f.rst = rest.NewClient(client).SetRoot(openAPIRootURL)
		fs.Debugf(f, "âœ… Restå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºæˆåŠŸ")
	}

	// Check token validity with reduced frequency
	if f.token == "" || time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		err := f.refreshTokenIfNecessary(ctx, false, false)
		if err != nil {
			return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
		}
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - æ ¹æ®å®˜æ–¹æ–‡æ¡£åªæœ‰ä¸¤ä¸ªAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		// åªæœ‰åˆ†ç‰‡ä¸Šä¼ å’Œå•æ­¥ä¸Šä¼ ä½¿ç”¨ä¸Šä¼ åŸŸåï¼ˆå®˜æ–¹æ–‡æ¡£æ˜ç¡®è¯´æ˜ï¼‰
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		// å…¶ä»–APIï¼ˆåŒ…æ‹¬createã€upload_completeï¼‰ä½¿ç”¨æ ‡å‡†APIåŸŸå
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err := pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.CallJSON(ctx, &opts, reqBody, respBody)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "ğŸ” æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "âœ… tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		return shouldRetry(ctx, resp, err)
	})

	if err != nil {
		return fmt.Errorf("APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// makeAPICallWithRestMultipartToDomain ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨åˆ°æŒ‡å®šåŸŸå
func (f *Fs) makeAPICallWithRestMultipartToDomain(ctx context.Context, domain string, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ“¤ å‘åŸŸåå‘èµ·multipart APIè°ƒç”¨: %s %s %s", method, domain, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ä½¿ç”¨è°ƒé€Ÿå™¨è¿›è¡ŒAPIè°ƒç”¨
	return pacer.Call(func() (bool, error) {
		opts := rest.Opts{
			Method:  method,
			RootURL: domain,
			Path:    endpoint,
			Body:    body,
			ExtraHeaders: map[string]string{
				"Content-Type": contentType,
			},
		}

		// æ·»åŠ å¿…è¦çš„è®¤è¯å¤´
		opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
		opts.ExtraHeaders["Platform"] = "open_platform"
		opts.ExtraHeaders["User-Agent"] = f.opt.UserAgent

		var resp *http.Response
		var err error
		resp, err = f.rst.Call(ctx, &opts)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
			if fserrors.ShouldRetry(err) {
				fs.Debugf(f, "ğŸ”„ APIè°ƒç”¨å¤±è´¥ï¼Œå°†é‡è¯•: %v", err)
				return true, err
			}
			return false, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
		}
		defer resp.Body.Close()

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode >= 400 {
			body, _ := io.ReadAll(resp.Body)

			// ç‰¹æ®Šå¤„ç†401é”™è¯¯ - tokenè¿‡æœŸ
			if resp.StatusCode == http.StatusUnauthorized {
				fs.Debugf(f, "ğŸ” æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
				// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
				refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
				if refreshErr != nil {
					fs.Errorf(f, "âŒ åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
					return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
				}
				// æ›´æ–°Authorizationå¤´
				opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
				fs.Debugf(f, "âœ… tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
				return true, nil // é‡è¯•
			}

			return false, fmt.Errorf("APIè¿”å›é”™è¯¯çŠ¶æ€ %d: %s", resp.StatusCode, string(body))
		}

		// è§£æå“åº”JSON
		if respBody != nil {
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”JSONå¤±è´¥: %w", err)
			}
		}

		return false, nil
	})
}

// getPacerForEndpoint æ ¹æ®APIç«¯ç‚¹è¿”å›é€‚å½“çš„è°ƒé€Ÿå™¨
// åŸºäºå®˜æ–¹APIé™æµæ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
// æ›´æ–°æ—¶é—´ï¼š2025-07-04ï¼Œä½¿ç”¨æœ€æ–°å®˜æ–¹QPSé™åˆ¶è¡¨
func (f *Fs) getPacerForEndpoint(endpoint string) *fs.Pacer {
	switch {
	// é«˜é¢‘ç‡API (15-20 QPS)
	case strings.Contains(endpoint, "/api/v2/file/list"):
		return f.listPacer // ~14 QPS (å®˜æ–¹15 QPS)
	case strings.Contains(endpoint, "/upload/v2/file/get_upload_url"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"):
		return f.downloadPacer // ~16 QPS (å®˜æ–¹20 QPS)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS)
	case strings.Contains(endpoint, "/api/v1/file/move"),
		strings.Contains(endpoint, "/api/v1/file/name"),
		strings.Contains(endpoint, "/api/v1/file/infos"),
		strings.Contains(endpoint, "/api/v1/user/info"):
		return f.listPacer // ~8 QPS (å®˜æ–¹10 QPS)
	case strings.Contains(endpoint, "/api/v1/access_token"):
		return f.downloadPacer // ~6 QPS (å®˜æ–¹8 QPS)

	// ä½é¢‘ç‡API (5 QPS)
	case strings.Contains(endpoint, "/upload/v1/file/create"),
		strings.Contains(endpoint, "/upload/v2/file/create"),
		strings.Contains(endpoint, "/upload/v1/file/mkdir"),
		strings.Contains(endpoint, "/api/v1/file/trash"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"),
		strings.Contains(endpoint, "/api/v1/file/download_info"):
		return f.strictPacer // ~4 QPS (å®˜æ–¹5 QPS)

	// æœ€ä½é¢‘ç‡API (1 QPS)
	case strings.Contains(endpoint, "/api/v1/file/delete"),
		strings.Contains(endpoint, "/api/v1/file/list"),
		strings.Contains(endpoint, "/api/v1/video/transcode/list"):
		return f.strictPacer // ~0.8 QPS (å®˜æ–¹1 QPS) - ä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶

	// v2åˆ†ç‰‡ä¸Šä¼ API (ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨ä¸“ç”¨çš„uploadPacer)
	case strings.Contains(endpoint, "/upload/v2/file/slice"):
		return f.uploadPacer // ~5 QPS (åŸºäºå®˜æ–¹æ–‡æ¡£å’Œå®é™…æµ‹è¯•)

	// ä¸Šä¼ åŸŸåAPI (ç‰¹æ®Šå¤„ç†)
	case strings.Contains(endpoint, "/upload/v2/file/domain"):
		return f.strictPacer // ä¿å®ˆå¤„ç†ï¼Œé¿å…é¢‘ç¹è°ƒç”¨

	default:
		// For safety, unknown endpoints default to strictest pacer
		fs.Debugf(f, "âš ï¸ æœªçŸ¥APIç«¯ç‚¹ï¼Œä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶: %s", endpoint)
		return f.strictPacer // Strictest limits
	}
}

// Use rclone standard hash verification instead

// parseFileID é€šç”¨çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
func parseFileID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseFileIDWithContext å¸¦ä¸Šä¸‹æ–‡çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
func parseFileIDWithContext(idStr, context string) (int64, error) {
	id, err := parseFileID(idStr)
	if err != nil {
		return 0, fmt.Errorf("%s: %w", context, err)
	}
	return id, nil
}

// parseParentID è§£æçˆ¶ç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºçˆ¶ç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseParentID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„çˆ¶ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseDirID è§£æç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseDirID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// getFileInfo æ ¹æ®IDè·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
func (f *Fs) getFileInfo(ctx context.Context, fileID string) (*FileListInfoRespDataV2, error) {
	fs.Debugf(f, "ğŸ“‹ getFileInfoå¼€å§‹: fileID=%s", fileID)

	// éªŒè¯æ–‡ä»¶ID
	_, err := parseFileIDWithContext(fileID, "è·å–æ–‡ä»¶ä¿¡æ¯")
	if err != nil {
		fs.Debugf(f, "âŒ getFileInfoæ–‡ä»¶IDéªŒè¯å¤±è´¥: %v", err)
		return nil, err
	}

	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…API - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response FileDetailResponse
	endpoint := fmt.Sprintf("/api/v1/file/detail?fileID=%s", fileID)

	fs.Debugf(f, "ğŸŒ getFileInfoè°ƒç”¨API: %s", endpoint)
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		fs.Debugf(f, "âŒ getFileInfo APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "ğŸ“‹ getFileInfo APIå“åº”: code=%d, message=%s", response.Code, response.Message)
	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ‡å‡†æ ¼å¼
	fileInfo := &FileListInfoRespDataV2{
		FileID:       response.Data.FileID,
		Filename:     response.Data.Filename,
		Type:         response.Data.Type,
		Size:         response.Data.Size,
		Etag:         response.Data.ETag,
		Status:       response.Data.Status,
		ParentFileID: response.Data.ParentFileID,
		Category:     0, // è¯¦æƒ…å“åº”ä¸­æœªæä¾›
	}

	fs.Debugf(f, "âœ… getFileInfoæˆåŠŸ: fileID=%s, filename=%s, type=%d, size=%d", fileID, fileInfo.Filename, fileInfo.Type, fileInfo.Size)
	return fileInfo, nil
}

// getDownloadURL è·å–æ–‡ä»¶çš„ä¸‹è½½URL
func (f *Fs) getDownloadURL(ctx context.Context, fileID string) (string, error) {
	// é¦–å…ˆæ£€æŸ¥ç¼“å­˜
	if cachedURL, found := f.getCachedDownloadURL(fileID); found {
		return cachedURL, nil
	}

	// ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨APIè·å–
	fs.Debugf(f, "ğŸ” 123ç½‘ç›˜è·å–ä¸‹è½½URL: fileID=%s", fileID)

	var response DownloadInfoResponse
	endpoint := fmt.Sprintf("/api/v1/file/download_info?fileID=%s", fileID)

	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	downloadURL := response.Data.DownloadURL
	fs.Debugf(f, "â¬‡ï¸ 123ç½‘ç›˜ä¸‹è½½URLè·å–æˆåŠŸ: fileID=%s", fileID)

	// ç¼“å­˜ä¸‹è½½URL
	f.setCachedDownloadURL(fileID, downloadURL)

	return downloadURL, nil
}

// createDirectory åˆ›å»ºæ–°ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™è¿”å›ç°æœ‰ç›®å½•çš„ID
func (f *Fs) createDirectory(ctx context.Context, parentID, name string) (string, error) {
	// éªŒè¯å‚æ•°
	if name == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if parentID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯ç›®å½•å
	if err := validateFileName(name); err != nil {
		return "", fmt.Errorf("ç›®å½•åéªŒè¯å¤±è´¥: %w", err)
	}

	// å°†çˆ¶ç›®å½•IDè½¬æ¢ä¸ºint64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return "", err
	}

	// é¦–å…ˆæ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„APIè°ƒç”¨
	fs.Debugf(f, "ğŸ” æ£€æŸ¥ç›®å½• '%s' æ˜¯å¦å·²å­˜åœ¨äºçˆ¶ç›®å½• %s", name, parentID)
	existingID, found, err := f.FindLeaf(ctx, parentID, name)
	if err != nil {
		fs.Debugf(f, "âš ï¸ æ£€æŸ¥ç°æœ‰ç›®å½•æ—¶å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
	} else if found {
		fs.Debugf(f, "âœ… ç›®å½• '%s' å·²å­˜åœ¨ï¼ŒID: %s", name, existingID)
		return existingID, nil
	}

	// å¦‚æœå¸¸è§„æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼ˆé˜²æ­¢ç¼“å­˜é—®é¢˜ï¼‰
	if !found && err == nil {
		fs.Debugf(f, "ğŸ”„ å¸¸è§„æŸ¥æ‰¾æœªæ‰¾åˆ°ç›®å½• '%s'ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾", name)
		existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
		if err != nil {
			fs.Debugf(f, "âš ï¸ å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
		} else if found {
			fs.Debugf(f, "âœ… å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°ç›®å½• '%s'ï¼ŒID: %s", name, existingID)
			return existingID, nil
		}
	}

	// ç›®å½•ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»º
	fs.Debugf(f, "ğŸ“ ç›®å½• '%s' ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º", name)
	reqBody := map[string]any{
		"parentID": strconv.FormatInt(parentFileID, 10),
		"name":     name,
	}

	// è¿›è¡ŒAPIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    any    `json:"data"` // ä½¿ç”¨anyå› ä¸ºä¸ç¡®å®šå…·ä½“ç»“æ„
	}

	err = f.makeAPICallWithRest(ctx, "/upload/v1/file/mkdir", "POST", reqBody, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯
		if response.Code == 1 && strings.Contains(response.Message, "å·²ç»æœ‰åŒåæ–‡ä»¶å¤¹") {
			fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼Œå¼ºåˆ¶æ¸…ç†ç¼“å­˜åæŸ¥æ‰¾ç°æœ‰ç›®å½•ID", name)

			// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨åŒæ­¥
			time.Sleep(100 * time.Millisecond)

			// æŸ¥æ‰¾ç°æœ‰ç›®å½•çš„ID - ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼
			existingID, found, err := f.findLeafWithForceRefresh(ctx, parentID, name)
			if err != nil {
				return "", fmt.Errorf("æŸ¥æ‰¾ç°æœ‰ç›®å½•å¤±è´¥: %w", err)
			}
			if !found {
				// å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼ˆå¯èƒ½æ˜¯APIå»¶è¿Ÿï¼‰
				fs.Debugf(f, "ç¬¬ä¸€æ¬¡æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•é‡è¯•æŸ¥æ‰¾ç›®å½• '%s'", name)
				for retry := 0; retry < 3; retry++ {
					time.Sleep(time.Duration(retry+1) * 200 * time.Millisecond)
					existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
					if err == nil && found {
						break
					}
					fs.Debugf(f, "é‡è¯•ç¬¬%dæ¬¡æŸ¥æ‰¾ç›®å½• '%s': found=%v, err=%v", retry+1, name, found, err)
				}

				if !found {
					return "", fmt.Errorf("ç›®å½•å·²å­˜åœ¨ä½†æ— æ³•æ‰¾åˆ°å…¶IDï¼Œå¯èƒ½æ˜¯APIåŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜")
				}
			}

			fs.Debugf(f, "æ‰¾åˆ°ç°æœ‰ç›®å½•ID: %s", existingID)
			return existingID, nil
		}
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// åˆ›å»ºæˆåŠŸï¼Œä½†APIå¯èƒ½ä¸è¿”å›ç›®å½•IDï¼Œéœ€è¦é€šè¿‡æŸ¥æ‰¾è·å–
	fs.Debugf(f, "âœ… ç›®å½• '%s' åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹æŸ¥æ‰¾æ–°ç›®å½•ID", name)

	// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨ç«¯åŒæ­¥å®Œæˆ
	time.Sleep(200 * time.Millisecond)

	// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•çš„IDï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
	var newDirID string
	var foundNew bool
	var errNew error

	// å°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼Œå¤„ç†æœåŠ¡å™¨ç«¯åŒæ­¥å»¶è¿Ÿ
	maxRetries := 5
	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// é€’å¢ç­‰å¾…æ—¶é—´ï¼š200ms, 400ms, 600ms, 800ms
			waitTime := time.Duration(attempt*200) * time.Millisecond
			fs.Debugf(f, "ç¬¬%dæ¬¡é‡è¯•æŸ¥æ‰¾æ–°ç›®å½• '%s'ï¼Œç­‰å¾… %v", attempt+1, name, waitTime)
			time.Sleep(waitTime)
		}

		// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼Œè·³è¿‡ç¼“å­˜ç›´æ¥ä»APIè·å–æœ€æ–°æ•°æ®
		newDirID, foundNew, errNew = f.findLeafWithForceRefresh(ctx, parentID, name)
		if errNew != nil {
			fs.Debugf(f, "ç¬¬%dæ¬¡æŸ¥æ‰¾æ–°ç›®å½•å¤±è´¥: %v", attempt+1, errNew)
			continue
		}

		if foundNew {
			fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æˆåŠŸæ‰¾åˆ°æ–°åˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", attempt+1, name, newDirID)
			break
		}

		fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æœªæ‰¾åˆ°æ–°ç›®å½• '%s'", attempt+1, name)
	}

	// æœ€ç»ˆæ£€æŸ¥ç»“æœ
	if errNew != nil {
		return "", fmt.Errorf("æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•å¤±è´¥ï¼Œå·²é‡è¯•%dæ¬¡: %w", maxRetries, errNew)
	}
	if !foundNew {
		return "", fmt.Errorf("æ–°åˆ›å»ºçš„ç›®å½•æœªæ‰¾åˆ°ï¼Œå·²é‡è¯•%dæ¬¡ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨åŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜", maxRetries)
	}

	fs.Debugf(f, "âœ… æˆåŠŸåˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", name, newDirID)
	return newDirID, nil
}

// createUpload åˆ›å»ºä¸Šä¼ ä¼šè¯
func (f *Fs) createUpload(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " åˆ†ç‰‡ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		// Unified handling strategy: if pre-verification fails, try to get correct parent ID
		fs.Debugf(f, "Chunked upload: pre-verification found parent ID %d does not exist, trying to get correct parent ID", parentFileID)

		// Use unified parent ID repair strategy
		realParentFileID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("failed to re-acquire correct parent ID: %w", err)
		}

		if realParentFileID != parentFileID {
			fs.Debugf(f, "Chunked upload: found correct parent ID: %d (original ID: %d), using correct ID to continue upload", realParentFileID, parentFileID)
			parentFileID = realParentFileID
		} else {
			fs.Debugf(f, "Chunked upload: using cached parent ID %d after cleanup to continue attempt", parentFileID)
		}
	} else {
		fs.Debugf(f, "Chunked upload: parent ID %d pre-verification passed", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä½¿ç”¨å®˜æ–¹APIæ–‡æ¡£çš„æ­£ç¡®å‚æ•°å
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸º123 APIåŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API - createä½¿ç”¨æ ‡å‡†APIåŸŸå
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		return nil, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	// Unified parent ID verification logic: detect parentFileID not exist error, use unified repair strategy
	if response.Code == 1 && strings.Contains(response.Message, "parentFileIDä¸å­˜åœ¨") {
		fs.Debugf(f, "Parent ID %d does not exist, cleaning cache", parentFileID)

		// Use unified parent ID repair strategy consistent with single-step upload
		fs.Debugf(f, "Using unified parent ID repair strategy")
		correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("failed to get correct parent ID: %w", err)
		}

		// If got different parent ID, retry creating upload session
		if correctParentID != parentFileID {
			fs.Debugf(f, "Found correct parent ID: %d (original ID: %d), retrying upload session creation", correctParentID, parentFileID)

			// Rebuild request with correct directory ID
			reqBody["parentFileID"] = correctParentID

			// Retry API call
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("retry API call with correct directory ID failed: %w", err)
			}
		} else {
			// Even if same ID, retry because cache has been cleaned
			fs.Debugf(f, "Using cached parent ID %d after cleanup, retrying upload session creation", correctParentID)
			reqBody["parentFileID"] = correctParentID
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("retry API call after cache cleanup failed: %w", err)
			}
		}
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Debug response data
	fs.Debugf(f, "Upload session created successfully: FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	return &response, nil
}

// uploadFile ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹
func (f *Fs) uploadFile(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64) error {
	return f.uploadFileWithPath(ctx, in, createResp, size, "")
}

// uploadFileWithPath ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileWithPath(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64, filePath string) error {
	// ç¡®å®šå—å¤§å°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		// ä½¿ç”¨rcloneå…¨å±€é…ç½®çš„å¤šçº¿ç¨‹åˆ†ç‰‡å¤§å°
		chunkSize = int64(fs.GetConfig(ctx).MultiThreadChunkSize)
	}

	// ç¡®ä¿å—å¤§å°åœ¨é™åˆ¶èŒƒå›´å†…
	if chunkSize < int64(minChunkSize) {
		chunkSize = int64(minChunkSize)
	}
	if chunkSize > int64(maxChunkSize) {
		chunkSize = int64(maxChunkSize)
	}

	// å¯¹äºå°æ–‡ä»¶ï¼Œå…¨éƒ¨è¯»å…¥å†…å­˜ï¼ˆæ— éœ€æ–­ç‚¹ç»­ä¼ ï¼‰
	if size <= chunkSize {
		data, err := io.ReadAll(in)
		if err != nil {
			return fmt.Errorf("failed to read file data: %w", err)
		}

		if int64(len(data)) != size {
			return fmt.Errorf("size mismatch: expected %d, got %d", size, len(data))
		}

		// å•éƒ¨åˆ†ä¸Šä¼ 
		return f.uploadSinglePart(ctx, createResp.Data.PreuploadID, data)
	}

	// å¤§æ–‡ä»¶çš„å¤šéƒ¨åˆ†ä¸Šä¼ ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
	return f.uploadMultiPart(ctx, in, createResp.Data.PreuploadID, size, chunkSize)
}

// uploadSinglePart å•éƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadSinglePart(ctx context.Context, preuploadID string, data []byte) error {
	// è®¡ç®—åˆ†ç‰‡MD5
	chunkHash := fmt.Sprintf("%x", md5.Sum(data))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err := f.uploadPartWithMultipart(ctx, preuploadID, 1, chunkHash, data)
	if err != nil {
		return fmt.Errorf("failed to upload part: %w", err)
	}

	// å®Œæˆä¸Šä¼  - ä½¿ç”¨å¸¦æ–‡ä»¶å¤§å°çš„ç‰ˆæœ¬ä»¥æ”¯æŒå¢å¼ºéªŒè¯
	_, err = f.completeUploadWithResultAndSize(ctx, preuploadID, int64(len(data)))
	return err
}

// uploadMultiPart ç®€åŒ–çš„å¤šéƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
func (f *Fs) uploadMultiPart(ctx context.Context, in io.Reader, preuploadID string, size, chunkSize int64) error {
	// é˜²æ­¢é™¤é›¶é”™è¯¯
	if chunkSize <= 0 {
		return fmt.Errorf("æ— æ•ˆçš„åˆ†ç‰‡å¤§å°: %d", chunkSize)
	}

	uploadNums := (size + chunkSize - 1) / chunkSize

	// é™åˆ¶åˆ†ç‰‡æ•°é‡
	if uploadNums > int64(f.opt.MaxUploadParts) {
		return fmt.Errorf("file too large: would require %d parts, max allowed is %d", uploadNums, f.opt.MaxUploadParts)
	}

	fs.Debugf(f, "ğŸ“¤ å¼€å§‹åˆ†ç‰‡ä¸Šä¼ : %dä¸ªåˆ†ç‰‡ï¼Œæ¯ç‰‡%s", uploadNums, fs.SizeSuffix(chunkSize))

	buffer := make([]byte, chunkSize)

	// ç®€åŒ–çš„åˆ†ç‰‡ä¸Šä¼ å¾ªç¯ï¼Œæ— æ–­ç‚¹ç»­ä¼ 
	for partIndex := int64(0); partIndex < uploadNums; partIndex++ {
		partNumber := partIndex + 1

		// è¯»å–å—æ•°æ®
		var partData []byte
		if partIndex == uploadNums-1 {
			// æœ€åä¸€åˆ†ç‰‡ - è¯»å–å‰©ä½™æ•°æ®
			remaining := size - partIndex*chunkSize
			partData = make([]byte, remaining)
			_, err := io.ReadFull(in, partData)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
		} else {
			// å¸¸è§„åˆ†ç‰‡
			_, err := io.ReadFull(in, buffer)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
			partData = buffer
		}

		// è®¡ç®—åˆ†ç‰‡MD5
		chunkHash := fmt.Sprintf("%x", md5.Sum(partData))

		// ä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶ä¸Šä¼ åˆ†ç‰‡
		err := f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkHash, partData)
		if err != nil {
			return fmt.Errorf("failed to upload part %d: %w", partNumber, err)
		}

		fs.Debugf(f, "å·²ä¸Šä¼ åˆ†ç‰‡ %d/%d", partNumber, uploadNums)
	}

	// å®Œæˆä¸Šä¼ 
	_, err := f.completeUploadWithResultAndSize(ctx, preuploadID, size)
	if err != nil {
		return err
	}

	return nil
}

// uploadPartWithMultipart ä½¿ç”¨æ­£ç¡®çš„multipart/form-dataæ ¼å¼ä¸Šä¼ åˆ†ç‰‡
// ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€é‡è¯•ç®¡ç†å™¨ï¼Œé¿å…å¤šå±‚é‡è¯•åµŒå¥—
func (f *Fs) uploadPartWithMultipart(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	if len(data) == 0 {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "ğŸ”§ å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(data))

	// ç®€åŒ–é‡è¯•é€»è¾‘ - ä½¿ç”¨rcloneæ ‡å‡†çš„fs.Paceré‡è¯•æœºåˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		err := f.uploadPartDirectly(ctx, preuploadID, partNumber, chunkHash, data)
		return fserrors.ShouldRetry(err), err
	})
}

// uploadPartDirectly ç›´æ¥ä¸Šä¼ åˆ†ç‰‡ï¼Œä¸åŒ…å«é‡è¯•é€»è¾‘
// å…³é”®ä¿®å¤ï¼šå°†é‡è¯•é€»è¾‘ä»ä¸šåŠ¡é€»è¾‘ä¸­åˆ†ç¦»
func (f *Fs) uploadPartDirectly(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	// ä¿®å¤ï¼šç§»é™¤ uploadPacer.Callï¼Œç›´æ¥æ‰§è¡Œï¼Œé‡è¯•ç”±ç»Ÿä¸€ç®¡ç†å™¨å¤„ç†
	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
	}

	// åˆ›å»ºmultipartè¡¨å•æ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	writer.WriteField("preuploadID", preuploadID)
	writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
	writer.WriteField("sliceMD5", chunkHash)

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
	}

	_, err = part.Write(data)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	err = writer.Close()
	if err != nil {
		return fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// ä¿®å¤å¡ä½é—®é¢˜ï¼šä¸ºåˆ†ç‰‡ä¸Šä¼ æ·»åŠ è¶…æ—¶æ§åˆ¶
	uploadCtx, cancel := context.WithTimeout(ctx, 5*time.Minute) // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š5åˆ†é’Ÿ
	defer cancel()

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè°ƒç”¨åˆ†ç‰‡ä¸Šä¼ API
	err = f.makeAPICallWithRestMultipartToDomain(uploadCtx, uploadDomain, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "âœ… åˆ†ç‰‡ %d ä¸Šä¼ æˆåŠŸ", partNumber)
	return nil
}

// uploadPartWithRetry å¸¦å¢å¼ºé‡è¯•æœºåˆ¶çš„åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) uploadPartWithRetry(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	maxRetries := 3
	baseDelay := 1 * time.Second

	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// æŒ‡æ•°é€€é¿å»¶è¿Ÿ
			delay := time.Duration(attempt) * baseDelay
			fs.Debugf(f, "ğŸ”„ åˆ†ç‰‡ %d é‡è¯• %d/%dï¼Œç­‰å¾… %v", partNumber, attempt, maxRetries-1, delay)
			time.Sleep(delay)
		}

		err := f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkHash, data)
		if err == nil {
			return nil
		}

		// æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
		if !fserrors.ShouldRetry(err) {
			fs.Debugf(f, "âŒ åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥ï¼Œä¸å¯é‡è¯•: %v", partNumber, err)
			return err
		}

		fs.Debugf(f, "âš ï¸ åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥ï¼Œå°†é‡è¯•: %v", partNumber, err)
	}

	return fmt.Errorf("åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡", partNumber, maxRetries)
}

// UploadCompleteResult ä¸Šä¼ å®Œæˆç»“æœ
type UploadCompleteResult struct {
	FileID int64  `json:"fileID"`
	Etag   string `json:"etag"`
}

// completeUploadWithResultAndSize simplified upload completion with standard retry logic
func (f *Fs) completeUploadWithResultAndSize(ctx context.Context, preuploadID string, fileSize int64) (*UploadCompleteResult, error) {
	fs.Debugf(f, "ğŸ” å®Œæˆä¸Šä¼ éªŒè¯ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))

	reqBody := map[string]any{
		"preuploadID": preuploadID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			Completed bool   `json:"completed"`
			FileID    int64  `json:"fileID"`
			Etag      string `json:"etag"`
		} `json:"data"`
	}

	// Use simple polling strategy with reasonable defaults
	maxRetries := 300 // 10 minutes max
	interval := 2 * time.Second

	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			select {
			case <-ctx.Done():
				return nil, fmt.Errorf("upload verification cancelled: %w", ctx.Err())
			case <-time.After(interval):
			}
		}

		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			fs.Debugf(f, "âŒ ä¸Šä¼ å®ŒæˆAPIè°ƒç”¨å¤±è´¥ (å°è¯• %d/%d): %v", attempt+1, maxRetries, err)

			// Use standard retry logic
			if retry, _ := shouldRetry(ctx, nil, err); retry {
				continue
			}
			return nil, fmt.Errorf("upload verification failed: %w", err)
		}

		if response.Code == 0 && response.Data.Completed {
			fs.Debugf(f, "âœ… ä¸Šä¼ éªŒè¯æˆåŠŸå®Œæˆ (å°è¯•: %d/%d)", attempt+1, maxRetries)
			return &UploadCompleteResult{
				FileID: response.Data.FileID,
				Etag:   response.Data.Etag,
			}, nil
		}

		if response.Code == 0 && !response.Data.Completed {
			fs.Debugf(f, "ğŸ”„ æœåŠ¡å™¨è¿”å›completed=falseï¼Œç»§ç»­è½®è¯¢ (å°è¯• %d/%d)", attempt+1, maxRetries)
			continue
		}

		// Handle API errors
		if response.Code != 0 {
			fs.Debugf(f, "âŒ ä¸Šä¼ éªŒè¯å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
			return nil, fmt.Errorf("upload verification failed: API error %d: %s", response.Code, response.Message)
		}
	}

	return nil, fmt.Errorf("upload verification timeout after %d attempts", maxRetries)
}

// findPathSafe å®‰å…¨çš„FindPathè°ƒç”¨ï¼Œè‡ªåŠ¨å¤„ç†æ–‡ä»¶è·¯å¾„vsç›®å½•è·¯å¾„
func (f *Fs) findPathSafe(ctx context.Context, remote string, create bool) (leaf, directoryID string, err error) {
	fs.Debugf(f, "ğŸ”§ findPathSafe: å¤„ç†è·¯å¾„ '%s', create=%v", remote, create)

	// å¦‚æœè·¯å¾„çœ‹èµ·æ¥åƒæ–‡ä»¶åï¼Œå…ˆå°è¯•æŸ¥æ‰¾æ˜¯å¦å­˜åœ¨åŒåæ–‡ä»¶
	if hasFileExtension(remote) {
		fs.Debugf(f, "ğŸ”§ findPathSafe: è·¯å¾„ '%s' çœ‹èµ·æ¥æ˜¯æ–‡ä»¶åï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨", remote)

		// åˆ†ç¦»ç›®å½•å’Œæ–‡ä»¶å
		directory, filename := dircache.SplitPath(remote)

		if directory != "" {
			// æŸ¥æ‰¾çˆ¶ç›®å½•
			parentDirID, err := f.dirCache.FindDir(ctx, directory, create)
			if err == nil {
				// çˆ¶ç›®å½•å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
				foundID, found, err := f.FindLeaf(ctx, parentDirID, filename)
				if err == nil && found {
					fs.Debugf(f, "ğŸ”§ findPathSafe: æ–‡ä»¶ '%s' å­˜åœ¨ï¼ŒID=%sï¼Œè¿”å›çˆ¶ç›®å½•ä¿¡æ¯", filename, foundID)
					return filename, parentDirID, nil
				}
				fs.Debugf(f, "ğŸ”§ findPathSafe: æ–‡ä»¶ '%s' ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ ‡å‡†ç›®å½•å¤„ç†", filename)
			}
		}
	}

	// ä½¿ç”¨æ ‡å‡†çš„FindPathå¤„ç†
	fs.Debugf(f, "ğŸ”§ findPathSafe: ä½¿ç”¨æ ‡å‡†FindPathå¤„ç†è·¯å¾„ '%s'", remote)
	return f.dirCache.FindPath(ctx, remote, create)
}

// streamingPut ç®€åŒ–çš„ä¸Šä¼ å…¥å£ï¼Œæ”¯æŒä¸¤ç§ä¸»è¦è·¯å¾„ï¼šå°æ–‡ä»¶ç›´æ¥ä¸Šä¼ å’Œå¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) streamingPut(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ : %s", fileName)

	// ç®€åŒ–çš„è·¨äº‘ä¼ è¾“æ£€æµ‹ï¼šåªæ£€æµ‹æ˜¯å¦ä¸ºè¿œç¨‹æº
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		fs.Debugf(f, "â˜ï¸ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s â†’ 123Pan (å¤§å°: %s)", src.Fs().Name(), fs.SizeSuffix(src.Size()))
		return f.handleCrossCloudTransfer(ctx, in, src, parentFileID, fileName)
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šæ ¹æ®123ç½‘ç›˜APIè§„èŒƒ
	// â‰¤ 1GB: ä½¿ç”¨å•æ­¥ä¸Šä¼ API
	// > 1GB: ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ API
	fileSize := src.Size()
	if fileSize <= singleStepUploadLimit { // 1GB threshold per API spec
		fs.Debugf(f, "ğŸ“‹ ç»Ÿä¸€ä¸Šä¼ ç­–ç•¥é€‰æ‹©: å•æ­¥ä¸Šä¼  - File â‰¤ 1GB (%s), using single-step upload API", fs.SizeSuffix(fileSize))
		return f.directSmallFileUpload(ctx, in, src, parentFileID, fileName)
	} else {
		fs.Debugf(f, "ğŸ“‹ ç»Ÿä¸€ä¸Šä¼ ç­–ç•¥é€‰æ‹©: åˆ†ç‰‡ä¸Šä¼  - File > 1GB (%s), using chunked upload API", fs.SizeSuffix(fileSize))
		return f.directLargeFileUpload(ctx, in, src, parentFileID, fileName)
	}
}

// directSmallFileUpload ç›´æ¥ä¸Šä¼ å°æ–‡ä»¶ï¼ˆâ‰¤1GBï¼‰ï¼Œä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) directSmallFileUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()

	// å¯¹äºè¾ƒå°çš„æ–‡ä»¶ï¼ˆâ‰¤100MBï¼‰ï¼Œç›´æ¥è¯»å–åˆ°å†…å­˜
	if fileSize <= 100*1024*1024 {
		// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
		data, err := io.ReadAll(in)
		if err != nil {
			return nil, fmt.Errorf("failed to read file data: %w", err)
		}

		// Calculate MD5
		hasher := md5.New()
		hasher.Write(data)
		md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

		fs.Debugf(f, "ğŸ“ å°æ–‡ä»¶åŠ è½½åˆ°å†…å­˜, å¤§å°: %d, MD5: %s", len(data), md5Hash)
		return f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
	}

	// å¯¹äºè¾ƒå¤§çš„æ–‡ä»¶ï¼ˆ100MB-1GBï¼‰ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶é¿å…å†…å­˜å‹åŠ›
	fs.Debugf(f, "ğŸ“ å¤§å‹å•æ­¥æ–‡ä»¶ (%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå•æ­¥ä¸Šä¼ ", fs.SizeSuffix(fileSize))
	return f.uploadLargeFileWithTempFileForSingleStep(ctx, in, src, parentFileID, fileName)
}

// directLargeFileUpload ç›´æ¥åˆ†ç‰‡ä¸Šä¼ å¤§æ–‡ä»¶ï¼ˆ>1GBï¼‰ï¼Œä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ API
func (f *Fs) directLargeFileUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()

	// For very large files, use chunked streaming
	if fileSize > singleStepUploadLimit {
		fs.Debugf(f, "ğŸ“¦ è¶…å¤§æ–‡ä»¶ (%s)ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“", fs.SizeSuffix(fileSize))

		// Validate source object for chunked upload
		srcObj, ok := src.(fs.Object)
		if !ok {
			fs.Debugf(f, "âš ï¸ æ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
			return f.uploadLargeFileWithTempFile(ctx, in, src, parentFileID, fileName)
		}

		return f.uploadFileInChunksSimplified(ctx, srcObj, parentFileID, fileName)
	}

	// For moderately large files, use temp file strategy
	fs.Debugf(f, "ğŸ“ å¤§æ–‡ä»¶ (%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥", fs.SizeSuffix(fileSize))
	return f.uploadLargeFileWithTempFile(ctx, in, src, parentFileID, fileName)
}

// uploadLargeFileWithTempFile ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å¤„ç†å¤§æ–‡ä»¶ä¸Šä¼ 
func (f *Fs) uploadLargeFileWithTempFile(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸ“ å¤§æ–‡ä»¶ (%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥", fs.SizeSuffix(src.Size()))

	// Create temp file with safe resource management
	tempFile, err := os.CreateTemp("", "rclone-123pan-*")
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	defer func() {
		// Safe resource cleanup
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "âš ï¸ å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// Write to temp file while calculating MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("failed to write to temp file: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "âœ… ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆ, å†™å…¥: %d bytes, MD5: %s", written, md5Hash)

	// Seek back to beginning
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("failed to seek temp file: %w", err)
	}

	// Upload with known MD5
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// uploadLargeFileWithTempFileForSingleStep ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å¤„ç†å¤§æ–‡ä»¶çš„å•æ­¥ä¸Šä¼ ï¼ˆ100MB-1GBï¼‰
func (f *Fs) uploadLargeFileWithTempFileForSingleStep(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "Large single-step file (%s), using temp file for single-step upload", fs.SizeSuffix(src.Size()))

	// Create temp file with safe resource management
	tempFile, err := os.CreateTemp("", "rclone-123pan-singlestep-*")
	if err != nil {
		return nil, fmt.Errorf("failed to create temp file: %w", err)
	}
	defer func() {
		// Safe resource cleanup
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "Failed to close temp file: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "Failed to remove temp file: %v", removeErr)
			}
		}
	}()

	// Write to temp file while calculating MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("failed to write to temp file: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "Temp file buffering complete for single-step upload, written: %d bytes, MD5: %s", written, md5Hash)

	// Seek back to beginning
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("failed to seek temp file: %w", err)
	}

	// Read all data for single-step upload
	data, err := io.ReadAll(tempFile)
	if err != nil {
		return nil, fmt.Errorf("failed to read temp file data: %w", err)
	}

	// Use single-step upload API
	return f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
}

// uploadFileInChunksSimplified ç®€åŒ–çš„åˆ†ç‰‡ä¸Šä¼ å¤„ç†è¶…å¤§æ–‡ä»¶
func (f *Fs) uploadFileInChunksSimplified(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fileSize := srcObj.Size()
	fs.Infof(f, "ğŸ“¦ åˆå§‹åŒ–å¤§æ–‡ä»¶ä¸Šä¼  (%s) - å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°...", fs.SizeSuffix(fileSize))

	// è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ - 123ç½‘ç›˜APIè¦æ±‚æä¾›etag
	fs.Debugf(f, "ğŸ” ä¸ºå¤§æ–‡ä»¶ä¸Šä¼ è®¡ç®—MD5å“ˆå¸Œ...")
	md5Hash, err := srcObj.Hash(ctx, fshash.MD5)
	if err != nil {
		return nil, fmt.Errorf("failed to compute MD5 hash: %w", err)
	}
	if md5Hash == "" {
		return nil, fmt.Errorf("MD5 hash is required for 123 API but could not be computed")
	}
	fs.Debugf(f, "âœ… MD5å“ˆå¸Œè®¡ç®—å®Œæˆ: %s", md5Hash)

	// Create upload session with MD5 hash
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("failed to create upload session: %w", err)
	}

	// ä¸¥æ ¼æŒ‰ç…§123ç½‘ç›˜APIè¿”å›çš„åˆ†ç‰‡å¤§å°è¿›è¡Œåˆ†ç‰‡
	// æ ¹æ®OpenAPIæ–‡æ¡£ï¼ŒsliceSizeæ˜¯æœåŠ¡å™¨å†³å®šçš„ï¼Œå®¢æˆ·ç«¯ä¸èƒ½è‡ªå®šä¹‰
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize <= 0 {
		// å¦‚æœAPIæ²¡æœ‰è¿”å›åˆ†ç‰‡å¤§å°ï¼Œä½¿ç”¨é»˜è®¤å€¼
		apiSliceSize = int64(defaultChunkSize)
		fs.Debugf(f, "âš ï¸ APIæœªè¿”å›åˆ†ç‰‡å¤§å°ï¼Œä½¿ç”¨é»˜è®¤å€¼: %s", fs.SizeSuffix(apiSliceSize))
	} else {
		fs.Debugf(f, "ğŸ“‹ ä½¿ç”¨123ç½‘ç›˜APIæŒ‡å®šçš„åˆ†ç‰‡å¤§å°: %s for file size: %s",
			fs.SizeSuffix(apiSliceSize), fs.SizeSuffix(fileSize))
	}

	// If instant upload triggered, return success
	if createResp.Data.Reuse {
		fs.Debugf(f, "ğŸš€ åˆ†ç‰‡ä¸Šä¼ è¿‡ç¨‹ä¸­è§¦å‘ç§’ä¼ ")
		return f.createObject(fileName, createResp.Data.FileID, fileSize, "", time.Now(), false), nil
	}

	// Start chunked streaming upload using optimal chunk size
	return f.uploadFileInChunks(ctx, srcObj, createResp, fileSize, apiSliceSize, fileName)
}

// handleCrossCloudTransfer ç»Ÿä¸€çš„è·¨äº‘ä¼ è¾“å¤„ç†å‡½æ•°
// é‡‡ç”¨å†…éƒ¨ä¸¤æ­¥ä¼ è¾“ç­–ç•¥ï¼Œå½»åº•è§£å†³è·¨äº‘ä¼ è¾“é—®é¢˜
func (f *Fs) handleCrossCloudTransfer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸ”„ å¼€å§‹å†…éƒ¨ä¸¤æ­¥ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// Use internal two-step transfer to avoid cross-cloud transfer issues
	fs.Debugf(f, "ğŸ“¥ æ­¥éª¤1: ä¸‹è½½åˆ°æœ¬åœ°, ğŸ“¤ æ­¥éª¤2: ä»æœ¬åœ°ä¸Šä¼ åˆ°123")

	return f.internalTwoStepTransfer(ctx, in, src, parentFileID, fileName)
}

// internalTwoStepTransfer å†…éƒ¨ä¸¤æ­¥ä¼ è¾“ï¼šä¸‹è½½åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åä¸Šä¼ åˆ°123ç½‘ç›˜
// è¿™æ˜¯å¤„ç†è·¨äº‘ä¼ è¾“æœ€å¯é çš„æ–¹æ³•ï¼Œé¿å…äº†æµå¼ä¼ è¾“çš„å¤æ‚æ€§
func (f *Fs) internalTwoStepTransfer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸ”„ å¼€å§‹å†…éƒ¨ä¸¤æ­¥ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// Step 1: ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
	fs.Debugf(f, "ğŸ“¥ æ­¥éª¤1: ä»æºä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶...")

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶
	tempFile, err := os.CreateTemp("", "rclone-123-transfer-*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// è·å–æºæ–‡ä»¶çš„Reader
	var srcReader io.ReadCloser
	if in != nil {
		// å¦‚æœå·²ç»æœ‰Readerï¼Œç›´æ¥ä½¿ç”¨
		srcReader = io.NopCloser(in)
	} else {
		// å¦åˆ™æ‰“å¼€æºæ–‡ä»¶
		if srcObj, ok := src.(fs.Object); ok {
			srcReader, err = srcObj.Open(ctx)
			if err != nil {
				return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
			}
		} else {
			return nil, fmt.Errorf("æºå¯¹è±¡ä¸æ”¯æŒOpenæ“ä½œ")
		}
	}
	defer srcReader.Close()

	// ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, srcReader)
	if err != nil {
		return nil, fmt.Errorf("ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
	}

	// è®¡ç®—MD5
	md5sum := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "âœ… æ­¥éª¤1å®Œæˆ: å·²ä¸‹è½½ %s, MD5: %s", fs.SizeSuffix(written), md5sum)

	// Step 2: ä»ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ åˆ°123ç½‘ç›˜
	fs.Debugf(f, "ğŸ“¤ æ­¥éª¤2: ä»æœ¬åœ°ä¸Šä¼ åˆ°123...")

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹
	_, err = tempFile.Seek(0, 0)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	// åˆ›å»ºæœ¬åœ°æ–‡ä»¶ä¿¡æ¯å¯¹è±¡
	localSrc := &localFileInfo{
		name:    fileName,
		size:    written,
		modTime: src.ModTime(ctx),
	}

	// ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å‡½æ•°ï¼Œæ­¤æ—¶æºå·²ç»æ˜¯æœ¬åœ°æ•°æ®
	return f.putWithKnownMD5(ctx, tempFile, localSrc, parentFileID, fileName, md5sum)
}

// localFileInfo æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ç»“æ„ä½“ï¼Œç”¨äºå†…éƒ¨ä¸¤æ­¥ä¼ è¾“
type localFileInfo struct {
	name    string
	size    int64
	modTime time.Time
}

func (l *localFileInfo) String() string {
	return l.name
}

func (l *localFileInfo) Remote() string {
	return l.name
}

func (l *localFileInfo) ModTime(ctx context.Context) time.Time {
	return l.modTime
}

func (l *localFileInfo) Size() int64 {
	return l.size
}

func (l *localFileInfo) Fs() fs.Info {
	return nil // æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ä¸éœ€è¦Fs
}

func (l *localFileInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	return "", fmt.Errorf("hash not supported for local file info") // æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ä¸æ”¯æŒHash
}

func (l *localFileInfo) Storable() bool {
	return true
}

// Removed unused function: unifiedUploadWithMD5

// ç§»é™¤CrossCloudMD5Cacheï¼Œä½¿ç”¨rcloneæ ‡å‡†ç¼“å­˜æœºåˆ¶

// ç§»é™¤getCachedMD5å’ŒcacheMD5å‡½æ•°ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰MD5ç¼“å­˜

// putWithKnownMD5 å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ 
// è¿™æ˜¯æœ€é«˜æ•ˆçš„è·¯å¾„ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨123ç½‘ç›˜çš„ç§’ä¼ åŠŸèƒ½
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) putWithKnownMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	return f.putWithKnownMD5WithOptions(ctx, in, src, parentFileID, fileName, md5Hash, false)
}

// putWithKnownMD5WithOptions å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ ï¼Œæ”¯æŒè·³è¿‡å•æ­¥ä¸Šä¼ é€‰é¡¹
func (f *Fs) putWithKnownMD5WithOptions(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string, skipSingleStep bool) (*Object, error) {
	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆå°è¯•å•æ­¥ä¸Šä¼ APIï¼ˆé™¤éæ˜ç¡®è·³è¿‡ï¼‰
	fileSize := src.Size()
	if !skipSingleStep && fileSize < singleStepUploadLimit && fileSize > 0 && fileSize <= maxMemoryBufferSize {
		fs.Debugf(f, "ğŸ“ æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", fileSize, singleStepUploadLimit)

		// è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
		data, err := io.ReadAll(in)
		if err != nil {
			fs.Debugf(f, "âŒ è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
		} else if int64(len(data)) == fileSize {
			// å°è¯•å•æ­¥ä¸Šä¼ 
			result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
			if err != nil {
				fs.Debugf(f, "âŒ å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
				// é‡æ–°åˆ›å»ºreaderç”¨äºä¼ ç»Ÿä¸Šä¼ 
				in = bytes.NewReader(data)
			} else {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
				return result, nil
			}
		}
	}

	// ä¼ ç»Ÿä¸Šä¼ æµç¨‹ï¼šä½¿ç”¨å·²çŸ¥çš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ç›¸åŒMD5çš„æ–‡ä»¶
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ /å»é‡ï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™æ˜¯æœ€ç†æƒ³çš„æƒ…å†µï¼šæ— éœ€å®é™…ä¼ è¾“æ•°æ®
	if createResp.Data.Reuse {
		fs.Debugf(f, "ğŸš€ æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œéœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®
	fs.Debugf(f, "ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®")
	err = f.uploadFile(ctx, in, createResp, src.Size())
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	// æ³¨æ„ï¼šä½¿ç”¨å®é™…çš„æ–‡ä»¶åè€Œä¸æ˜¯src.Remote()ï¼Œå› ä¸ºsrc.Remote()å¯èƒ½åŒ…å«.partialåç¼€
	// è€Œå®é™…ä¸Šä¼ åˆ°123ç½‘ç›˜çš„æ–‡ä»¶åæ˜¯fileNameï¼ˆå·²å»é™¤.partialåç¼€ï¼‰
	return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
}

// uploadFileInChunks ç®€åŒ–çš„åˆ†ç‰‡æµå¼ä¸Šä¼ ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
func (f *Fs) uploadFileInChunks(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	fs.Debugf(f, "ğŸŒŠ å¼€å§‹åˆ†ç‰‡æµå¼ä¸Šä¼ : %dä¸ªåˆ†ç‰‡", totalChunks)

	// ä½¿ç”¨å•çº¿ç¨‹ä¸Šä¼ ï¼Œç®€åŒ–é€»è¾‘
	return f.uploadChunksSingleThreaded(ctx, srcObj, createResp, fileSize, chunkSize, fileName)
}

// uploadChunksSingleThreaded ç®€åŒ–çš„å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) uploadChunksSingleThreaded(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	overallHasher := md5.New()

	fs.Debugf(f, "ä½¿ç”¨å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")

	// ç®€åŒ–çš„åˆ†ç‰‡ä¸Šä¼ å¾ªç¯ï¼Œæ— æ–­ç‚¹ç»­ä¼ 
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1

		// ä¸Šä¼ åˆ†ç‰‡
		if err := f.uploadSingleChunkWithStream(ctx, srcObj, overallHasher, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks); err != nil {
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		fs.Debugf(f, "âœ… åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ å¹¶è¿”å›ç»“æœ
	return f.finalizeChunkedUpload(ctx, createResp, overallHasher, fileSize, fileName, preuploadID)
}

// finalizeChunkedUpload å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¹¶è¿”å›ç»“æœ
func (f *Fs) finalizeChunkedUpload(ctx context.Context, _ *UploadCreateResp, hasher hash.Hash, fileSize int64, fileName, preuploadID string) (*Object, error) {
	// è®¡ç®—æœ€ç»ˆMD5
	finalMD5 := fmt.Sprintf("%x", hasher.Sum(nil))

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "âœ… åˆ†ç‰‡æµå¼ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return f.createObject(fileName, result.FileID, fileSize, finalMD5, time.Now(), false), nil
}

// uploadSingleChunkWithStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
// è¿™æ˜¯åˆ†ç‰‡æµå¼ä¼ è¾“çš„æ ¸å¿ƒï¼šè¾¹ä¸‹è½½è¾¹ä¸Šä¼ ï¼Œåªéœ€è¦åˆ†ç‰‡å¤§å°çš„ä¸´æ—¶å­˜å‚¨
func (f *Fs) uploadSingleChunkWithStream(ctx context.Context, srcObj fs.Object, overallHasher io.Writer, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) error {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "ğŸŒŠ å¼€å§‹æµå¼ä¸Šä¼ åˆ†ç‰‡ %d/%d (åç§»: %d, å¤§å°: %d)",
		partNumber, totalChunks, chunkStart, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡èŒƒå›´çš„æºæ–‡ä»¶æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æºæ–‡ä»¶æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæ­¤åˆ†ç‰‡ï¼ˆåªéœ€è¦åˆ†ç‰‡å¤§å°çš„å­˜å‚¨ï¼‰
	tempFile, err := os.CreateTemp("", fmt.Sprintf("rclone-123pan-chunk-%d-*", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// åˆ›å»ºåˆ†ç‰‡MD5è®¡ç®—å™¨
	chunkHasher := md5.New()

	// è¾¹è¯»è¾¹å†™ï¼šåŒæ—¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ã€åˆ†ç‰‡MD5å’Œæ•´ä½“MD5
	multiWriter := io.MultiWriter(tempFile, chunkHasher, overallHasher)

	// æµå¼ä¼ è¾“åˆ†ç‰‡æ•°æ®
	written, err := io.Copy(multiWriter, chunkReader)
	if err != nil {
		return fmt.Errorf("æµå¼ä¼ è¾“åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
	}

	if written != actualChunkSize {
		return fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, written)
	}

	// é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶åˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return fmt.Errorf("é‡æ–°å®šä½åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// æ³¨æ„ï¼šæ–°çš„uploadPartWithMultipartæ–¹æ³•å†…éƒ¨ä¼šè·å–ä¸Šä¼ åŸŸåï¼Œè¿™é‡Œä¸å†éœ€è¦

	// è¯»å–ä¸´æ—¶æ–‡ä»¶æ•°æ®ç”¨äºä¸Šä¼ 
	chunkData, err := io.ReadAll(tempFile)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	chunkMD5 := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// ä½¿ç”¨å¢å¼ºé‡è¯•çš„multipartä¸Šä¼ æ–¹æ³•
	err = f.uploadPartWithRetry(ctx, preuploadID, partNumber, chunkMD5, chunkData)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}
	fs.Debugf(f, "âœ… åˆ†ç‰‡ %d æµå¼ä¸Šä¼ æˆåŠŸï¼Œå¤§å°: %d, MD5: %s", partNumber, written, chunkMD5)

	return nil
}

// singleStepUpload ä½¿ç”¨123äº‘ç›˜çš„å•æ­¥ä¸Šä¼ APIä¸Šä¼ å°æ–‡ä»¶ï¼ˆ<1GBï¼‰
// è¿™ä¸ªAPIä¸“é—¨ä¸ºå°æ–‡ä»¶è®¾è®¡ï¼Œä¸€æ¬¡HTTPè¯·æ±‚å³å¯å®Œæˆä¸Šä¼ ï¼Œæ•ˆç‡æ›´é«˜
func (f *Fs) singleStepUpload(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	startTime := time.Now()
	fs.Debugf(f, "ğŸ“¤ ä½¿ç”¨å•æ­¥ä¸Šä¼ APIï¼Œæ–‡ä»¶: %s, å¤§å°: %d bytes, MD5: %s", fileName, len(data), md5Hash)

	// ä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥ç§’ä¼ ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“
	fs.Debugf(f, "ğŸ” å•æ­¥ä¸Šä¼ : ä¼˜å…ˆæ£€æŸ¥ç§’ä¼ ...")
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, int64(len(data)))
	if err != nil {
		fs.Debugf(f, "âŒ ç§’ä¼ æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å•æ­¥ä¸Šä¼ : %v", err)
	} else if isInstant {
		duration := time.Since(startTime)
		fs.Debugf(f, "ğŸš€ å•æ­¥ä¸Šä¼ : ç§’ä¼ æˆåŠŸ! FileID: %d, è€—æ—¶: %v, é€Ÿåº¦: ç¬æ—¶", createResp.Data.FileID, duration)
		return f.createObjectFromUpload(fileName, int64(len(data)), md5Hash, createResp.Data.FileID, time.Now()), nil
	}

	fs.Debugf(f, "âš ï¸ ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…å•æ­¥ä¸Šä¼ ...")

	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " å•æ­¥ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		fs.Debugf(f, "âš ï¸ å•æ­¥ä¸Šä¼ : é¢„éªŒè¯å‘ç°çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œä½†ç»§ç»­å°è¯• (å¯èƒ½çš„APIå·®å¼‚)", parentFileID)
		// Don't return error immediately, let API call make final decision as different APIs may have different validation logic
	} else {
		fs.Debugf(f, "âœ… å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•ID %d é¢„éªŒè¯é€šè¿‡", parentFileID)
	}

	// éªŒè¯æ–‡ä»¶å¤§å°é™åˆ¶
	if len(data) > singleStepUploadLimit {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡å•æ­¥ä¸Šä¼ é™åˆ¶%d bytes: %d bytes", singleStepUploadLimit, len(data))
	}

	// ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åéªŒè¯å’Œæ¸…ç†
	cleanedFileName, warning := f.validateAndCleanFileNameUnified(fileName)
	if warning != nil {
		fs.Logf(f, "âš ï¸ æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// ä½¿ç”¨æ ‡å‡†APIè°ƒç”¨è¿›è¡Œå•æ­¥ä¸Šä¼ 
	var uploadResp struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			FileID    int64 `json:"fileID"`
			Completed bool  `json:"completed"`
		} `json:"data"`
	}

	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return nil, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
	}

	// åˆ›å»ºmultipartè¡¨å•æ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	writer.WriteField("parentFileID", fmt.Sprintf("%d", parentFileID))
	writer.WriteField("filename", fileName)
	writer.WriteField("etag", md5Hash)
	writer.WriteField("size", fmt.Sprintf("%d", len(data)))

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	fileWriter, err := writer.CreateFormFile("file", fileName)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæ–‡ä»¶è¡¨å•å­—æ®µå¤±è´¥: %w", err)
	}

	_, err = fileWriter.Write(data)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	err = writer.Close()
	if err != nil {
		return nil, fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
	err = f.makeAPICallWithRestMultipartToDomain(ctx, uploadDomain, "/upload/v2/file/single/create", "POST", &buf, writer.FormDataContentType(), &uploadResp)
	if err != nil {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	// æ£€æŸ¥APIå“åº”ç 
	if uploadResp.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if uploadResp.Code == 1 && strings.Contains(uploadResp.Message, "parentFileIDä¸å­˜åœ¨") {
			fs.Debugf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œæ¸…ç†ç¼“å­˜", parentFileID)
			// Remove cache cleanup

			// Unified parent ID verification logic: try to get correct parent ID
			fs.Debugf(f, "ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥")
			correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
			if err != nil {
				return nil, fmt.Errorf("failed to get correct parent ID: %w", err)
			}

			// If got different parent ID, retry single-step upload
			if correctParentID != parentFileID {
				fs.Debugf(f, "âœ… æ‰¾åˆ°æ­£ç¡®çš„çˆ¶ç›®å½•ID: %d (åŸå§‹ID: %d)ï¼Œé‡è¯•å•æ­¥ä¸Šä¼ ", correctParentID, parentFileID)
				return f.singleStepUpload(ctx, data, correctParentID, fileName, md5Hash)
			}

			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
		}
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸Šä¼ å®Œæˆ
	if !uploadResp.Data.Completed {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ æœªå®Œæˆï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ")
	}

	duration := time.Since(startTime)
	speed := float64(len(data)) / duration.Seconds() / 1024 / 1024 // MB/s

	// ä¼˜åŒ–ï¼šæ ¹æ®ä¸Šä¼ é€Ÿåº¦å’Œæ—¶é—´åˆ¤æ–­ä¸Šä¼ ç±»å‹å¹¶æä¾›è¯¦ç»†æ—¥å¿—
	if duration < 5*time.Second && speed > 100 {
		fs.Debugf(f, "ğŸš€ å•æ­¥ä¸Šä¼ : ç–‘ä¼¼ç§’ä¼ æˆåŠŸ! FileID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	} else {
		fs.Debugf(f, "âœ… å•æ­¥ä¸Šä¼ : å®é™…ä¸Šä¼ æˆåŠŸ, FileID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	}

	// è¿”å›Object
	return f.createObject(fileName, uploadResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
}

// validateAndCleanFileNameUnified ç»Ÿä¸€çš„éªŒè¯å’Œæ¸…ç†å…¥å£ç‚¹
// è¿™æ˜¯æ¨èçš„æ–‡ä»¶åå¤„ç†æ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ä¸Šä¼ è·¯å¾„çš„ä¸€è‡´æ€§
func (f *Fs) validateAndCleanFileNameUnified(fileName string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(fileName); err == nil {
		return fileName, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(fileName)

	// éªŒè¯æ¸…ç†åçš„æ–‡ä»¶å
	if err := validateFileName(cleanedName); err != nil {
		// å¦‚æœæ¸…ç†åä»ç„¶æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åç§°
		cleanedName = "cleaned_file"
		warning = fmt.Errorf("åŸå§‹æ–‡ä»¶å '%s' æ— æ³•æ¸…ç†ä¸ºæœ‰æ•ˆåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§° '%s'", fileName, cleanedName)
	} else {
		warning = fmt.Errorf("æ–‡ä»¶å '%s' åŒ…å«æ— æ•ˆå­—ç¬¦ï¼Œå·²æ¸…ç†ä¸º '%s'", fileName, cleanedName)
	}

	return cleanedName, warning
}

// getUploadDomain è·å–ä¸Šä¼ åŸŸåï¼Œæ”¯æŒåŠ¨æ€è·å–å’Œç¼“å­˜
func (f *Fs) getUploadDomain(ctx context.Context) (string, error) {

	// å°è¯•åŠ¨æ€è·å–ä¸Šä¼ åŸŸå - ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„
	var response struct {
		Code    int      `json:"code"`
		Data    []string `json:"data"` // ä¿®æ­£ï¼šdataæ˜¯å­—ç¬¦ä¸²æ•°ç»„
		Message string   `json:"message"`
		TraceID string   `json:"x-traceID"`
	}

	// ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„è°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	err := f.makeAPICallWithRest(ctx, "/upload/v2/file/domain", "GET", nil, &response)
	if err == nil && response.Code == 0 && len(response.Data) > 0 {
		domain := response.Data[0] // å–ç¬¬ä¸€ä¸ªåŸŸå
		fs.Debugf(f, "âœ… åŠ¨æ€è·å–ä¸Šä¼ åŸŸåæˆåŠŸ: %s", domain)

		fs.Debugf(f, "âœ… ä¸Šä¼ åŸŸåè·å–æˆåŠŸï¼Œä¸ä½¿ç”¨ç¼“å­˜: %s", domain)

		return domain, nil
	}

	// å¦‚æœåŠ¨æ€è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå
	fs.Debugf(f, "âš ï¸ åŠ¨æ€è·å–ä¸Šä¼ åŸŸåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå: %v", err)
	defaultDomains := []string{
		"https://openapi-upload.123242.com",
		"https://openapi-upload.123pan.com", // å¤‡ç”¨åŸŸå
	}

	// ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„åŸŸå
	for _, domain := range defaultDomains {
		// è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨é»˜è®¤ä¸Šä¼ åŸŸå: %s", domain)
		return domain, nil
	}

	return defaultDomains[0], nil
}

// deleteFile é€šè¿‡ç§»åŠ¨åˆ°å›æ”¶ç«™æ¥åˆ é™¤æ–‡ä»¶
func (f *Fs) deleteFile(ctx context.Context, fileID int64) error {
	fs.Debugf(f, "ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)

	reqBody := map[string]any{
		"fileIDs": []int64{fileID},
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/trash", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("åˆ é™¤æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ é™¤å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "âœ… æˆåŠŸåˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)
	return nil
}

// moveFile å°†æ–‡ä»¶ç§»åŠ¨åˆ°ä¸åŒçš„ç›®å½•
func (f *Fs) moveFile(ctx context.Context, fileID, toParentFileID int64) error {
	fs.Debugf(f, "ğŸ“¦ ç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)

	reqBody := map[string]any{
		"fileIDs":        []int64{fileID},
		"toParentFileID": toParentFileID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/move", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("ç§»åŠ¨æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("ç§»åŠ¨å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "âœ… æˆåŠŸç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)
	return nil
}

// renameFile é‡å‘½åæ–‡ä»¶ - ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI
func (f *Fs) renameFile(ctx context.Context, fileID int64, fileName string) error {
	fs.Debugf(f, "ğŸ“ é‡å‘½åæ–‡ä»¶%dä¸º: %s", fileID, fileName)

	// éªŒè¯æ–°æ–‡ä»¶å
	if err := validateFileName(fileName); err != nil {
		return fmt.Errorf("é‡å‘½åæ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}
	time.Sleep(2 * time.Second)
	// ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI: PUT /api/v1/file/name
	reqBody := map[string]any{
		"fileId":   fileID,
		"fileName": fileName,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// Use standard API call with built-in retry
	err := f.makeAPICallWithRest(ctx, "/api/v1/file/name", "PUT", reqBody, &response)
	if err != nil {
		return fmt.Errorf("rename file failed: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("rename file API error %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "âœ… æ–‡ä»¶é‡å‘½åæˆåŠŸ: %d -> %s", fileID, fileName)
	return nil
}

// handlePartialFileConflict å¤„ç†partialæ–‡ä»¶é‡å‘½åå†²çª
// å½“partialæ–‡ä»¶å°è¯•é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶åæ—¶å‘ç”Ÿå†²çªæ—¶è°ƒç”¨
func (f *Fs) handlePartialFileConflict(ctx context.Context, fileID int64, partialName string, parentID int64, targetName string) (*Object, error) {
	fs.Debugf(f, "ğŸ”§ å¤„ç†partialæ–‡ä»¶å†²çª: %s -> %s (çˆ¶ç›®å½•: %d)", partialName, targetName, parentID)

	// ç§»é™¤.partialåç¼€è·å–åŸå§‹æ–‡ä»¶å
	originalName := strings.TrimSuffix(partialName, ".partial")
	if targetName == "" {
		targetName = originalName
	}

	// æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
	exists, existingFileID, err := f.fileExistsInDirectory(ctx, parentID, targetName)
	if err != nil {
		fs.Debugf(f, "âš ï¸ æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if exists {
		fs.Debugf(f, "âš ï¸ ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼ŒID: %d", existingFileID)

		// å¦‚æœå­˜åœ¨çš„æ–‡ä»¶å°±æ˜¯å½“å‰æ–‡ä»¶ï¼ˆå¯èƒ½å·²ç»é‡å‘½åæˆåŠŸï¼‰
		if existingFileID == fileID {
			fs.Debugf(f, "æ–‡ä»¶å·²æˆåŠŸé‡å‘½åï¼Œè¿”å›æˆåŠŸå¯¹è±¡")
			return f.createObject(targetName, fileID, -1, "", time.Now(), false), nil
		}

		// å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶åå†²çªï¼Œç”Ÿæˆå”¯ä¸€åç§°")
		uniqueName, err := f.generateUniqueFileName(ctx, parentID, targetName)
		if err != nil {
			fs.Debugf(f, "âŒ ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
			return nil, fmt.Errorf("ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %w", err)
		}

		fs.Logf(f, "âš ï¸ Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", targetName, uniqueName)
		targetName = uniqueName
	}

	// å°è¯•é‡å‘½åpartialæ–‡ä»¶ä¸ºç›®æ ‡åç§°
	fs.Debugf(f, "ğŸ“ é‡å‘½åpartialæ–‡ä»¶: %d -> %s", fileID, targetName)
	err = f.renameFile(ctx, fileID, targetName)
	if err != nil {
		// å¦‚æœé‡å‘½åä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²ç»æœ‰æ­£ç¡®åç§°
		if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
			strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
			fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œæ£€æŸ¥æ–‡ä»¶å½“å‰çŠ¶æ€")

			// å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰æ­£ç¡®åç§°
			exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, parentID, targetName)
			if checkErr == nil && exists && existingFileID == fileID {
				fs.Debugf(f, "æ–‡ä»¶å®é™…å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæˆåŠŸ")
				return &Object{
					fs:          f,
					remote:      targetName,
					hasMetaData: true,
					id:          strconv.FormatInt(fileID, 10),
					size:        -1, // éœ€è¦é‡æ–°è·å–
					modTime:     time.Now(),
					isDir:       false,
				}, nil
			}
		}

		return nil, fmt.Errorf("partialæ–‡ä»¶é‡å‘½åå¤±è´¥: %w", err)
	}

	fs.Debugf(f, "âœ… Partialæ–‡ä»¶é‡å‘½åæˆåŠŸ: %s", targetName)
	return &Object{
		fs:          f,
		remote:      targetName,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        -1, // éœ€è¦é‡æ–°è·å–
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// getUserInfo è·å–ç”¨æˆ·ä¿¡æ¯åŒ…æ‹¬å­˜å‚¨é…é¢ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
func (f *Fs) getUserInfo(ctx context.Context) (*UserInfoResp, error) {
	var response UserInfoResp

	// ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯æ–¹æ³•ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶å’Œé‡è¯•æœºåˆ¶
	err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
	if err != nil {
		return nil, err
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	return &response, nil
}

// checkInstantUpload ç»Ÿä¸€çš„ç§’ä¼ æ£€æŸ¥å‡½æ•°
func (f *Fs) checkInstantUpload(ctx context.Context, parentFileID int64, fileName, md5Hash string, fileSize int64) (*UploadCreateResp, bool, error) {
	fs.Debugf(f, "ğŸ” æ­£å¸¸ç§’ä¼ æ£€æŸ¥æµç¨‹: æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ç›¸åŒæ–‡ä»¶...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, false, fmt.Errorf("failed to create upload session: %w", err)
	}

	if createResp.Data.Reuse {
		fs.Debugf(f, "ğŸš€ ç§’ä¼ æˆåŠŸ: æœåŠ¡å™¨å·²æœ‰ç›¸åŒæ–‡ä»¶ï¼Œæ— éœ€ä¸Šä¼ æ•°æ® - Size: %s, MD5: %s", fs.SizeSuffix(fileSize), md5Hash)
		return createResp, true, nil
	}

	fs.Debugf(f, "â¬†ï¸ ç§’ä¼ æ£€æŸ¥å®Œæˆ: éœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ® - Size: %s", fs.SizeSuffix(fileSize))
	return createResp, false, nil
}

// createObjectFromUpload ä»ä¸Šä¼ å“åº”åˆ›å»ºObjectå¯¹è±¡
func (f *Fs) createObjectFromUpload(fileName string, fileSize int64, md5Hash string, fileID int64, modTime time.Time) *Object {
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          fmt.Sprintf("%d", fileID),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       false,
	}
}

// createObject é€šç”¨çš„Objectåˆ›å»ºå‡½æ•°ï¼Œç»Ÿä¸€æ‰€æœ‰Objectåˆ›å»ºé€»è¾‘
func (f *Fs) createObject(remote string, fileID int64, size int64, md5Hash string, modTime time.Time, isDir bool) *Object {
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        size,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       isDir,
	}
}

// newFs ä»è·¯å¾„æ„é€ Fsï¼Œæ ¼å¼ä¸º container:path
func newFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	fs.Debugf(nil, "ğŸš€ 123ç½‘ç›˜newFsè°ƒç”¨: name=%s, root=%s", name, root)

	// Parse config into Options struct
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, err
	}

	// Validate and normalize options
	if err := validateOptions(opt); err != nil {
		return nil, err
	}
	normalizeOptions(opt)

	// Parse root path
	originalName := name
	normalizedRoot := strings.Trim(root, "/")

	// Create HTTP client and rest client
	client := fshttp.NewClient(ctx)

	// Create Fs object
	f := &Fs{
		name:             name,
		originalName:     originalName,
		root:             normalizedRoot,
		opt:              *opt,
		m:                m,
		parentDirCache:   make(map[int64]time.Time),  // åˆå§‹åŒ–çˆ¶ç›®å½•ç¼“å­˜
		downloadURLCache: make(map[string]CachedURL), // åˆå§‹åŒ–ä¸‹è½½URLç¼“å­˜
		rootFolderID:     opt.RootFolderID,
		rst:              rest.NewClient(client).SetRoot(openAPIRootURL),
	}

	// Initialize pacers
	f.listPacer = createPacer(ctx, listV2APIMinSleep, maxSleep, decayConstant)
	strictPacerSleep := fileMoveMinSleep
	if opt.StrictPacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.StrictPacerMinSleep)
	}
	f.strictPacer = createPacer(ctx, strictPacerSleep, maxSleep, decayConstant)
	uploadPacerSleep := uploadV2SliceMinSleep // ä½¿ç”¨ä¸Šä¼ APIçš„é»˜è®¤é™åˆ¶
	if opt.UploadPacerMinSleep > 0 {
		uploadPacerSleep = time.Duration(opt.UploadPacerMinSleep)
	}
	f.uploadPacer = createPacer(ctx, uploadPacerSleep, maxSleep, decayConstant)
	downloadPacerSleep := downloadInfoMinSleep // ä½¿ç”¨ä¸‹è½½APIçš„é»˜è®¤é™åˆ¶
	if opt.DownloadPacerMinSleep > 0 {
		downloadPacerSleep = time.Duration(opt.DownloadPacerMinSleep)
	}
	f.downloadPacer = createPacer(ctx, downloadPacerSleep, maxSleep, decayConstant)

	// æ ¹æ®123ç½‘ç›˜APIé™æµè¡¨åˆå§‹åŒ–æ–°å¢çš„pacer
	f.batchPacer = createPacer(ctx, 200*time.Millisecond, maxSleep, decayConstant) // 5 QPS for batch operations
	f.tokenPacer = createPacer(ctx, 1*time.Second, maxSleep, decayConstant)        // 1 QPS for token operations

	// ç§»é™¤å¤æ‚çš„ç¼“å­˜å’Œç»Ÿä¸€ç»„ä»¶åˆå§‹åŒ–ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶

	// Initialize features
	f.features = (&fs.Features{
		ReadMimeType:            true,
		CanHaveEmptyDirectories: true,
		BucketBased:             false,
		SetTier:                 false,
		GetTier:                 false,
		DuplicateFiles:          false,
		ReadMetadata:            true,
		WriteMetadata:           false,
		UserMetadata:            false,
		PartialUploads:          false,
		NoMultiThreading:        false,
		SlowModTime:             true,
		SlowHash:                false,
	}).Fill(ctx, f)

	// Initialize directory cache
	f.dirCache = dircache.New(normalizedRoot, f.rootFolderID, f)

	// Initialize authentication
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %vï¼ˆè¿‡æœŸæ—¶é—´ %vï¼‰", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if !tokenLoaded {
		tokenRefreshNeeded = true
	} else if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		tokenRefreshNeeded = true
	}

	if tokenRefreshNeeded {
		if err := f.refreshTokenIfNecessary(ctx, true, false); err != nil {
			return nil, fmt.Errorf("åˆå§‹åŒ–è®¤è¯å¤±è´¥: %w", err)
		}
	}

	// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨123ç½‘ç›˜APIç²¾ç¡®åˆ¤æ–­æ–‡ä»¶ç±»å‹
	if normalizedRoot != "" && hasFileExtension(normalizedRoot) {
		fs.Debugf(f, "ğŸ”§ 123ç½‘ç›˜APIåˆ¤æ–­: æ ¹è·¯å¾„ '%s' çœ‹èµ·æ¥æ˜¯æ–‡ä»¶åï¼Œå°è¯•ç²¾ç¡®åˆ¤æ–­", normalizedRoot)

		// å°è¯•é€šè¿‡FindLeaf + getFileInfoç²¾ç¡®åˆ¤æ–­
		directory, filename := dircache.SplitPath(normalizedRoot)
		if directory != "" {
			// åˆ›å»ºä¸´æ—¶FsæŒ‡å‘çˆ¶ç›®å½•
			tempF := &Fs{
				name:          f.name,
				originalName:  f.originalName,
				root:          directory,
				opt:           f.opt,
				features:      f.features,
				rst:           f.rst,
				token:         f.token,
				tokenExpiry:   f.tokenExpiry,
				tokenRenewer:  f.tokenRenewer,
				m:             f.m,
				rootFolderID:  f.rootFolderID,
				listPacer:     f.listPacer,
				uploadPacer:   f.uploadPacer,
				downloadPacer: f.downloadPacer,
				strictPacer:   f.strictPacer,
			}
			tempF.dirCache = dircache.New(directory, f.rootFolderID, tempF)

			// æŸ¥æ‰¾çˆ¶ç›®å½•
			err = tempF.dirCache.FindRoot(ctx, false)
			if err == nil {
				// çˆ¶ç›®å½•å­˜åœ¨ï¼Œè·å–çˆ¶ç›®å½•ID
				parentDirID, err := tempF.dirCache.RootID(ctx, false)
				if err == nil && parentDirID != "" {
					// ä½¿ç”¨FindLeafæŸ¥æ‰¾æ–‡ä»¶
					foundID, found, err := tempF.FindLeaf(ctx, parentDirID, filename)
					if err == nil && found {
						fs.Debugf(f, "ğŸ”§ 123ç½‘ç›˜APIç¡®è®¤: æ–‡ä»¶ '%s' å­˜åœ¨ï¼ŒID=%sï¼Œä½¿ç”¨æ ‡å‡†æ–‡ä»¶å¤„ç†é€»è¾‘", filename, foundID)
						// æ‰¾åˆ°äº†æ–‡ä»¶ï¼Œä¿®æ­£rootä¸ºçˆ¶ç›®å½•ï¼Œç„¶åå¼ºåˆ¶è¿›å…¥æ–‡ä»¶å¤„ç†é€»è¾‘
						fs.Debugf(f, "ğŸ”§ 123ç½‘ç›˜ä¿®æ­£root: ä» '%s' ä¿®æ­£ä¸ºçˆ¶ç›®å½• '%s'", normalizedRoot, directory)
						f.root = directory
						f.dirCache = dircache.New(directory, f.rootFolderID, f)
						err = errors.New("API confirmed file path")
					} else {
						fs.Debugf(f, "ğŸ”§ 123ç½‘ç›˜APIæŸ¥è¯¢å¤±è´¥: æ–‡ä»¶ '%s' ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†å¤„ç†", filename)
						// æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
						err = f.dirCache.FindRoot(ctx, false)
					}
				} else {
					// æ— æ³•è·å–çˆ¶ç›®å½•IDï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
					err = f.dirCache.FindRoot(ctx, false)
				}
			} else {
				// çˆ¶ç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
				err = f.dirCache.FindRoot(ctx, false)
			}
		} else {
			// æ–‡ä»¶åœ¨æ ¹ç›®å½•ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†
			err = f.dirCache.FindRoot(ctx, false)
		}
	} else {
		// Find the root directory
		err = f.dirCache.FindRoot(ctx, false)
	}

	if err != nil {
		// Assume it is a file
		newRoot, remote := dircache.SplitPath(root)
		// Create new Fs instance to avoid copying mutex
		tempF := &Fs{
			name:          f.name,
			originalName:  f.originalName,
			root:          newRoot,
			opt:           f.opt,
			features:      f.features,
			rst:           f.rst,
			token:         f.token,
			tokenExpiry:   f.tokenExpiry,
			tokenRenewer:  f.tokenRenewer,
			m:             f.m,
			rootFolderID:  f.rootFolderID,
			listPacer:     f.listPacer,
			uploadPacer:   f.uploadPacer,
			downloadPacer: f.downloadPacer,
			strictPacer:   f.strictPacer,
		}
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// return an error with an fs which points to the parent
		return tempF, fs.ErrorIsFile
	}
	return f, nil
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	fs.Debugf(f, "ğŸ” è°ƒç”¨NewObject: %s", remote)

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "è§„èŒƒåŒ–åçš„è¿œç¨‹è·¯å¾„: %s -> %s", remote, normalizedRemote)

	// Get the full path
	var fullPath string
	if f.root == "" {
		fullPath = normalizedRemote
	} else if normalizedRemote == "" {
		// å½“remoteä¸ºç©ºæ—¶ï¼ŒfullPathå°±æ˜¯rootæœ¬èº«
		fullPath = f.root
	} else {
		fullPath = normalizePath(f.root + "/" + normalizedRemote)
	}

	if fullPath == "" {
		fullPath = "/"
	}

	// Get file ID
	fileID, err := f.pathToFileID(ctx, fullPath)
	if err != nil {
		if err == fs.ErrorObjectNotFound {
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, " NewObject pathToFileIDç»“æœ: fullPath=%s -> fileID=%s", fullPath, fileID)

	// Get file details
	fileInfo, err := f.getFileInfo(ctx, fileID)
	if err != nil {
		return nil, err
	}

	// Check if it's a file (not directory)
	// Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	isDir := (fileInfo.Type == 1)

	// âœ… ç›´æ¥ä½¿ç”¨123ç½‘ç›˜APIçš„Typeå­—æ®µï¼Œæ— éœ€é¢å¤–åˆ¤æ–­
	// APIçš„Typeå­—æ®µå·²ç»å¾ˆå‡†ç¡®ï¼š0=æ–‡ä»¶ï¼Œ1=æ–‡ä»¶å¤¹
	fs.Debugf(f, "âœ… 123ç½‘ç›˜NewObject APIç±»å‹: '%s' Type=%d, isDir=%v", fileInfo.Filename, fileInfo.Type, isDir)

	if isDir {
		return nil, fs.ErrorNotAFile
	}

	// ç¡®å®šæ­£ç¡®çš„remoteè·¯å¾„
	objectRemote := remote
	if objectRemote == "" && f.root != "" {
		// å½“remoteä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºremote
		objectRemote = fileInfo.Filename
		fs.Debugf(f, " NewObjectè®¾ç½®remote: %s -> %s", remote, objectRemote)
	}

	fileIDInt, _ := strconv.ParseInt(fileID, 10, 64)
	o := f.createObject(objectRemote, fileIDInt, fileInfo.Size, fileInfo.Etag, time.Now(), false)

	return o, nil
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	return fs.ModTimeNotSupported
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	fs.Debugf(f, "ğŸ“¤ è°ƒç”¨Put: %s", src.Remote())

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(src.Remote())
	fs.Debugf(f, "Putæ“ä½œè§„èŒƒåŒ–è·¯å¾„: %s -> %s", src.Remote(), normalizedRemote)

	// Use dircache to find parent directory
	fs.Debugf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•è·¯å¾„: %s", normalizedRemote)
	leaf, parentID, err := f.findPathSafe(ctx, normalizedRemote, true)
	if err != nil {
		fs.Errorf(f, "âŒ æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %v", err)
		return nil, err
	}
	fileName := leaf
	fs.Debugf(f, "æ‰¾åˆ°çˆ¶ç›®å½•ID: %s, æ–‡ä»¶å: %s", parentID, fileName)

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(fileName)
	if warning != nil {
		fs.Logf(f, "âš ï¸ æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨ï¼Œé¿å…é‡å¤åˆ›å»º
	fs.Debugf(f, "ğŸ”§ Putæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: %s", normalizedRemote)
	existingObj, err := f.NewObject(ctx, normalizedRemote)
	if err == nil {
		// æ–‡ä»¶å·²å­˜åœ¨ï¼Œä½¿ç”¨Updateæ›´æ–°è€Œä¸æ˜¯åˆ›å»ºæ–°æ–‡ä»¶
		fs.Debugf(f, "ğŸ”§ Putå‘ç°æ–‡ä»¶å·²å­˜åœ¨ï¼Œä½¿ç”¨Updateæ›´æ–°: %s", normalizedRemote)
		err := existingObj.Update(ctx, in, src, options...)
		if err != nil {
			return nil, err
		}
		return existingObj, nil
	} else if err != fs.ErrorObjectNotFound {
		// å…¶ä»–é”™è¯¯ï¼Œè¿”å›é”™è¯¯
		fs.Debugf(f, "ğŸ”§ Putæ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§æ—¶å‡ºé”™: %v", err)
		return nil, fmt.Errorf("failed to check if file exists: %w", err)
	}
	// æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç»§ç»­åˆ›å»ºæ–°æ–‡ä»¶
	fs.Debugf(f, "ğŸ”§ Putç¡®è®¤æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶: %s", normalizedRemote)

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return nil, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯parentFileIDæ˜¯å¦çœŸçš„å­˜åœ¨
	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d æ˜¯å¦å­˜åœ¨", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		fs.Errorf(f, "âŒ éªŒè¯çˆ¶ç›®å½•IDå¤±è´¥: %v", err)
		// å°è¯•æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		fs.Debugf(f, "æ¸…ç†ç›®å½•ç¼“å­˜å¹¶é‡è¯•")
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else if !exists {
		fs.Errorf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•é‡å»ºç›®å½•ç¼“å­˜", parentFileID)
		// æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºç›®å½•ç¼“å­˜åæŸ¥æ‰¾å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºåè§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡å»ºåæ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// Use streaming optimization for cross-cloud transfers
	obj, err := f.streamingPut(ctx, in, src, parentFileID, fileName)

	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}

	return obj, err
}

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	// In a real implementation, you would fetch metadata if not already available.
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// In a real implementation, you would fetch metadata if not already available.
	return o.size
}

// Hash returns the MD5 checksum
func (o *Object) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t != fshash.MD5 {
		return "", fshash.ErrUnsupported
	}
	// In a real implementation, you would fetch metadata if not already available.
	return o.md5sum, nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// In a real implementation, you would fetch metadata if not already available.
	return o.id
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	// è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šæ£€æµ‹å¤§æ–‡ä»¶å¹¶å¯ç”¨å¤šçº¿ç¨‹ä¸‹è½½
	// æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç¦ç”¨å¹¶å‘ä¸‹è½½é€‰é¡¹ï¼Œé¿å…é‡å¤å¹¶å‘
	hasDisableOption := false
	for _, option := range options {
		if option.String() == "DisableConcurrentDownload" {
			hasDisableOption = true
			break
		}
	}

	if !hasDisableOption && o.size >= 10*1024*1024 { // 10MBé˜ˆå€¼
		return o.openWithConcurrency(ctx, options...)
	}

	var resp *http.Response

	// Use pacer for download with retry (use strict pacer for download URL requests)
	err := o.fs.strictPacer.Call(func() (bool, error) {
		// Get download URL (may need refresh)
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, nil, err)
		}

		// Create HTTP request
		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, fmt.Errorf("failed to create download request: %w", err)
		}

		// Handle range requests
		var start, end int64 = 0, -1
		for _, option := range options {
			switch x := option.(type) {
			case *fs.RangeOption:
				start, end = x.Start, x.End
			case *fs.SeekOption:
				start = x.Offset
			}
		}

		if start > 0 || end >= 0 {
			rangeHeader := fmt.Sprintf("bytes=%d-", start)
			if end >= 0 {
				rangeHeader = fmt.Sprintf("bytes=%d-%d", start, end)
			}
			req.Header.Set("Range", rangeHeader)
			fs.Debugf(o.fs, "è¯·æ±‚èŒƒå›´: %s", rangeHeader)
		}

		// Set user agent
		req.Header.Set("User-Agent", o.fs.opt.UserAgent)
		// Note: Don't set timeout here as it will be applied to the entire download

		// Make the request using rclone standard HTTP client
		client := fshttp.NewClient(ctx)
		resp, err = client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// Check status code
		switch resp.StatusCode {
		case http.StatusOK, http.StatusPartialContent:
			return false, nil // Success
		case http.StatusFound, http.StatusMovedPermanently, http.StatusTemporaryRedirect, http.StatusPermanentRedirect:
			// Let the HTTP client handle redirects automatically
			// This should not happen if the client is configured correctly
			resp.Body.Close()
			return false, fmt.Errorf("unexpected redirect response %d - client should handle this automatically", resp.StatusCode)
		case http.StatusRequestedRangeNotSatisfiable:
			resp.Body.Close()
			return false, fmt.Errorf("range not satisfiable")
		case http.StatusNotFound:
			resp.Body.Close()
			return false, fs.ErrorObjectNotFound
		default:
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("download failed with status %d: %s", resp.StatusCode, string(body)))
		}
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// openWithConcurrency ä½¿ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨æ‰“å¼€æ–‡ä»¶ï¼ˆç”¨äºè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// Start concurrent download
	fs.Debugf(o, "Starting concurrent download for: %s", o.Remote())

	// Use custom concurrent download implementation
	return o.openWithCustomConcurrency(ctx, options...)
}

// openNormal æ™®é€šçš„æ‰“å¼€æ–¹æ³•ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
func (o *Object) openNormal(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	var resp *http.Response

	// ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½
	err := o.fs.downloadPacer.Call(func() (bool, error) {
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: %w", err))
		}

		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, err
		}

		// å¤„ç†Rangeé€‰é¡¹
		fs.FixRangeOption(options, o.size)
		fs.OpenOptionAddHTTPHeaders(req.Header, options)

		// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
		client := fshttp.NewClient(ctx)
		resp, err = client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err))
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d, å“åº”: %s", resp.StatusCode, string(body)))
		}

		return false, nil
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// openWithCustomConcurrency ä½¿ç”¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½ï¼ˆå½“ç»Ÿä¸€ä¸‹è½½å™¨ä¸å¯ç”¨æ—¶ï¼‰
func (o *Object) openWithCustomConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Debugf(o, "Starting custom concurrent download: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "123_custom_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// ä½¿ç”¨ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é€»è¾‘
	err = o.downloadWithCustomConcurrency(ctx, tempFile, options...)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "âš ï¸ è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Debugf(o, "Custom concurrent download completed: %s", fs.SizeSuffix(o.size))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return &ConcurrentDownloadReader{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// downloadWithCustomConcurrency è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å®ç°
func (o *Object) downloadWithCustomConcurrency(ctx context.Context, tempFile *os.File, _ ...fs.OpenOption) error {
	// ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é…ç½®
	chunkSize := int64(50 * 1024 * 1024) // 50MBåˆ†ç‰‡
	maxConcurrency := 6                  // 6çº¿ç¨‹å¹¶å‘

	fileSize := o.size
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	if numChunks < int64(maxConcurrency) {
		maxConcurrency = int(numChunks)
	}

	fs.Infof(o, "ğŸ“Š 123ç½‘ç›˜è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// è·å–ä¸‹è½½URL
	downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
	if err != nil {
		return fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
	}

	// åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡
	var wg sync.WaitGroup
	errChan := make(chan error, maxConcurrency)
	semaphore := make(chan struct{}, maxConcurrency)

	for i := int64(0); i < numChunks; i++ {
		start := i * chunkSize
		end := start + chunkSize - 1
		if end >= fileSize {
			end = fileSize - 1
		}

		wg.Add(1)
		go func(chunkIndex, start, end int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// ä¸‹è½½åˆ†ç‰‡
			err := o.downloadChunk(ctx, downloadURL, tempFile, start, end, chunkIndex)
			if err != nil {
				errChan <- fmt.Errorf("åˆ†ç‰‡%dä¸‹è½½å¤±è´¥: %w", chunkIndex, err)
			}
		}(i, start, end)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	for err := range errChan {
		return err
	}

	return nil
}

// downloadChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡
func (o *Object) downloadChunk(ctx context.Context, url string, tempFile *os.File, start, end, chunkIndex int64) error {
	// åˆ›å»ºHTTPè¯·æ±‚
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}

	// è®¾ç½®Rangeå¤´
	req.Header.Set("Range", fmt.Sprintf("bytes=%d-%d", start, end))
	req.Header.Set("User-Agent", o.fs.opt.UserAgent)

	// æ‰§è¡Œè¯·æ±‚ï¼Œä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(ctx)
	resp, err := client.Do(req)
	if err != nil {
		return fmt.Errorf("æ‰§è¡Œä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode != http.StatusPartialContent && resp.StatusCode != http.StatusOK {
		return fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	// è¯»å–æ•°æ®
	expectedSize := end - start + 1
	buffer := make([]byte, expectedSize)
	totalRead := int64(0)

	for totalRead < expectedSize {
		n, err := resp.Body.Read(buffer[totalRead:])
		if n > 0 {
			totalRead += int64(n)
		}
		if err != nil {
			if err == io.EOF {
				break
			}
			return fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®å¤§å°
	if totalRead != expectedSize {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, totalRead)
	}

	// å†™å…¥åˆ°æ–‡ä»¶
	writtenBytes, err := tempFile.WriteAt(buffer[:totalRead], start)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯å†™å…¥çš„å­—èŠ‚æ•°
	if int64(writtenBytes) != totalRead {
		return fmt.Errorf("åˆ†ç‰‡å†™å…¥ä¸å®Œæ•´: æœŸæœ›å†™å…¥%dï¼Œå®é™…å†™å…¥%d", totalRead, writtenBytes)
	}

	fs.Debugf(o, " 123ç½‘ç›˜åˆ†ç‰‡%dä¸‹è½½å®Œæˆ: %s (bytes=%d-%d)",
		chunkIndex, fs.SizeSuffix(totalRead), start, end)

	return nil
}

// Update the object with new content.
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œå¤šçº¿ç¨‹ä¸Šä¼ 
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	fs.Debugf(o.fs, "è°ƒç”¨Update: %s", o.remote)

	startTime := time.Now()

	// Use dircache to find parent directory
	leaf, parentID, err := o.fs.findPathSafe(ctx, o.remote, false)
	if err != nil {
		return err
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(o.fs, "âš ï¸ æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedFileName
	}

	// ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡Œæ›´æ–°
	fs.Debugf(o.fs, "ğŸ”„ ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡ŒUpdateæ“ä½œ")
	newObj, err := o.fs.streamingPut(ctx, in, src, parentFileID, leaf)

	// è®°å½•ä¸Šä¼ å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(startTime)
	if err != nil {
		fs.Errorf(o.fs, "âŒ Updateæ“ä½œå¤±è´¥: %v", err)
		return err
	}

	// æ›´æ–°å½“å‰å¯¹è±¡çš„å…ƒæ•°æ®
	o.size = newObj.size
	o.md5sum = newObj.md5sum
	o.modTime = newObj.modTime
	o.id = newObj.id

	fs.Debugf(o.fs, "âœ… Updateæ“ä½œæˆåŠŸï¼Œè€—æ—¶: %v", duration)

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	fs.Debugf(o.fs, "ğŸ—‘ï¸ è°ƒç”¨Remove: %s", o.remote)

	// æ£€æŸ¥æ–‡ä»¶IDæ˜¯å¦æœ‰æ•ˆ
	if o.id == "" {
		fs.Debugf(o.fs, "âš ï¸ æ–‡ä»¶IDä¸ºç©ºï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶: %s", o.remote)
		// å¯¹äºpartialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬æ— æ³•åˆ é™¤ï¼Œä½†ä¹Ÿä¸åº”è¯¥æŠ¥é”™
		// å› ä¸ºè¿™äº›æ–‡ä»¶å¯èƒ½æ ¹æœ¬ä¸å­˜åœ¨äºæœåŠ¡å™¨ä¸Š
		return nil
	}

	// Convert file ID to int64
	fileID, err := parseFileID(o.id)
	if err != nil {
		fs.Debugf(o.fs, "âŒ è§£ææ–‡ä»¶IDå¤±è´¥ï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶: %s, é”™è¯¯: %v", o.remote, err)
		// å¯¹äºæ— æ³•è§£æçš„æ–‡ä»¶IDï¼Œæˆ‘ä»¬ä¹Ÿä¸æŠ¥é”™ï¼Œå› ä¸ºè¿™é€šå¸¸æ˜¯partialæ–‡ä»¶
		return nil
	}

	err = o.fs.deleteFile(ctx, fileID)
	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}
	return err
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// DirCacher interface implementation for dircache
// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "ğŸ” æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// Use pagination to search through all files, similar to pathToFileID
	next := "0"
	maxIterations := 1000 // Prevent infinite loops
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return "", false, fmt.Errorf("æŸ¥æ‰¾å¶èŠ‚ç‚¹ %s è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %d", leaf, maxIterations)
		}

		lastFileIDInt, err := strconv.Atoi(next)
		if err != nil {
			return "", false, fmt.Errorf("invalid next token: %s", next)
		}

		var response *ListResponse
		// Use list pacer for rate limitingï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
		err = f.listPacer.Call(func() (bool, error) {
			response, err = f.ListFile(ctx, int(parentFileID), 100, "", "", lastFileIDInt)
			if err != nil {
				return shouldRetry(ctx, nil, err)
			}
			return false, nil
		})
		if err != nil {
			return "", false, err
		}

		if response.Code != 0 {
			return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Search for the leaf in current page
		for _, file := range response.Data.FileList {
			// ğŸ”§ æ£€æŸ¥æ–‡ä»¶ååŒ¹é…ä¸”ä¸åœ¨å›æ”¶ç«™ä¸”æœªè¢«å®¡æ ¸é©³å›
			if file.Filename == leaf && file.Trashed == 0 && file.Status < 100 {
				foundID = strconv.FormatInt(file.FileID, 10)
				found = true
				// Cache the found item's path/ID mapping
				parentPath, ok := f.dirCache.GetInv(pathID)
				if ok {
					var itemPath string
					if parentPath == "" {
						itemPath = leaf
					} else {
						itemPath = parentPath + "/" + leaf
					}
					f.dirCache.Put(itemPath, foundID)
				}
				fs.Debugf(f, "âœ… FindLeafæ‰¾åˆ°é¡¹ç›®: %s -> ID=%s, Type=%d", leaf, foundID, file.Type)
				return foundID, found, nil
			}
		}

		// Check if there are more pages
		if len(response.Data.FileList) == 0 {
			break
		}

		nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
		if nextRaw == "-1" {
			break
		} else {
			next = nextRaw
		}
	}

	return "", false, nil
}

// findLeafWithForceRefresh å¼ºåˆ¶åˆ·æ–°ç¼“å­˜åæŸ¥æ‰¾å¶èŠ‚ç‚¹
// ç”¨äºè§£å†³ç¼“å­˜ä¸ä¸€è‡´å¯¼è‡´çš„ç›®å½•æŸ¥æ‰¾å¤±è´¥é—®é¢˜
func (f *Fs) findLeafWithForceRefresh(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "ğŸ”„ å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// Use pagination to search through all files with force refresh
	next := "0"
	maxIterations := 1000 // Prevent infinite loops
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return "", false, fmt.Errorf("å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å¶èŠ‚ç‚¹ %s è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %d", leaf, maxIterations)
		}

		lastFileIDInt, err := strconv.Atoi(next)
		if err != nil {
			return "", false, fmt.Errorf("invalid next token: %s", next)
		}

		var response *ListResponse
		// Use list pacer for rate limiting, force refresh by calling API directlyï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
		err = f.listPacer.Call(func() (bool, error) {
			response, err = f.ListFile(ctx, int(parentFileID), 100, "", "", lastFileIDInt)
			if err != nil {
				return shouldRetry(ctx, nil, err)
			}
			return false, nil
		})
		if err != nil {
			return "", false, fmt.Errorf("å¼ºåˆ¶åˆ·æ–°APIè°ƒç”¨å¤±è´¥: %w", err)
		}

		if response.Code != 0 {
			return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Search for the leaf in current page
		for _, file := range response.Data.FileList {
			// ğŸ”§ æ£€æŸ¥æ–‡ä»¶ååŒ¹é…ä¸”ä¸åœ¨å›æ”¶ç«™ä¸”æœªè¢«å®¡æ ¸é©³å›
			if file.Filename == leaf && file.Trashed == 0 && file.Status < 100 {
				foundID = strconv.FormatInt(file.FileID, 10)
				found = true

				// Cache the found item's path/ID mapping
				parentPath, ok := f.dirCache.GetInv(pathID)
				if ok {
					var itemPath string
					if parentPath == "" {
						itemPath = leaf
					} else {
						itemPath = parentPath + "/" + leaf
					}
					f.dirCache.Put(itemPath, foundID)
				}

				fs.Debugf(f, "âœ… å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°å¶èŠ‚ç‚¹: %s -> %s, Type=%d", leaf, foundID, file.Type)
				return foundID, found, nil
			}
		}

		// Check if there are more pages
		if len(response.Data.FileList) == 0 {
			break
		}

		nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
		if nextRaw == "-1" {
			break
		} else {
			next = nextRaw
		}
	}

	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æœªæ‰¾åˆ°å¶èŠ‚ç‚¹: %s", leaf)
	return "", false, nil
}

// CreateDir creates a directory with the given name in the parent folder pathID.
// Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	fs.Debugf(f, "åˆ›å»ºç›®å½•: pathID=%s, leaf=%s", pathID, leaf)

	// éªŒè¯å‚æ•°
	if leaf == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if pathID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®å½•å
	cleanedDirName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(f, "âš ï¸ ç›®å½•åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedDirName
	}

	// Create the directory using the existing createDirectory method
	dirID, err := f.createDirectory(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}

	return dirID, nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs          = (*Fs)(nil)
	_ fs.Object      = (*Object)(nil)
	_ fs.Mover       = (*Fs)(nil)
	_ fs.DirMover    = (*Fs)(nil)
	_ fs.Copier      = (*Fs)(nil)
	_ fs.Abouter     = (*Fs)(nil)
	_ fs.Purger      = (*Fs)(nil)
	_ fs.Shutdowner  = (*Fs)(nil)
	_ fs.ObjectInfo  = (*Object)(nil)
	_ fs.IDer        = (*Object)(nil)
	_ fs.SetModTimer = (*Object)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, _ configmap.Mapper) bool {
	if f.opt.Token == "" {
		fs.Debugf(f, "é…ç½®ä¸­æœªæ‰¾åˆ°ä»¤ç‰Œï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰")
		return false
	}

	var tokenData TokenJSON                                                // Use the new TokenJSON struct
	var err error                                                          // Declare err here
	if err = json.Unmarshal([]byte(f.opt.Token), &tokenData); err != nil { // Unmarshal unobscured token
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œå¤±è´¥ï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰: %v", err)
		return false
	}

	f.token = tokenData.AccessToken
	f.tokenExpiry, err = time.Parse(time.RFC3339, tokenData.Expiry) // Use tokenData.Expiry
	if err != nil {
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œè¿‡æœŸæ—¶é—´å¤±è´¥: %v", err)
		return false
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "é…ç½®ä¸­çš„ä»¤ç‰Œä¸å®Œæ•´")
		return false
	}

	fs.Debugf(f, "ä»é…ç½®æ–‡ä»¶åŠ è½½ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)
	return true
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(_ context.Context, m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - æä¾›çš„æ˜ å°„å™¨ä¸ºç©º")
		return
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	tokenData := TokenJSON{ // Use the new TokenJSON struct
		AccessToken: f.token,
		Expiry:      f.tokenExpiry.Format(time.RFC3339), // Use Expiry
	}
	tokenJSON, err := json.Marshal(tokenData)
	if err != nil {
		fs.Errorf(f, "âŒ ä¿å­˜ä»¤ç‰Œæ—¶åºåˆ—åŒ–å¤±è´¥: %v", err)
		return
	}

	m.Set("token", string(tokenJSON))
	// m.Set does not return an error, so we assume it succeeds.
	// Any errors related to config saving would be handled by the underlying config system.

	fs.Debugf(f, "ä½¿ç”¨åŸå§‹åç§°%qä¿å­˜ä»¤ç‰Œåˆ°é…ç½®æ–‡ä»¶", f.originalName)
}

// Removed unused function: setupTokenRenewer

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	f.tokenMu.Lock()

	// Check if another thread has successfully refreshed while we were waiting
	if !refreshTokenExpired && !forceRefresh && f.token != "" && time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		fs.Debugf(f, "è·å–é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡åˆ·æ–°")
		f.tokenMu.Unlock()
		return nil
	}
	defer f.tokenMu.Unlock()

	// Perform the actual token refresh
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token during refresh: %w", err)
	}

	f.token = accessToken
	f.tokenExpiry = expiredAt

	// Save the refreshed token to config
	f.saveToken(ctx, f.m)

	return nil
}

// GetAccessToken fetches a new access token from the 123Pan API.
func (f *Fs) GetAccessToken(ctx context.Context) error {
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token: %w", err)
	}
	f.token = accessToken
	f.tokenExpiry = expiredAt
	return nil
}

type ClientCredentials struct {
	ClientID     string `json:"clientID"`
	ClientSecret string `json:"clientSecret"`
}

type TokenJSON struct {
	AccessToken string `json:"access_token"`
	Expiry      string `json:"expiry"`
}

type AccessTokenData struct {
	AccessToken string `json:"accessToken"`
	ExpiredAt   string `json:"expiredAt"`
}

type Response struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     AccessTokenData
	XTraceID string `json:"x-traceID"`
}

func GetAccessToken(clientID, clientSecret string) (string, time.Time, error) {
	credentials := ClientCredentials{
		ClientID:     clientID,
		ClientSecret: clientSecret,
	}
	payload, err := json.Marshal(credentials)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to marshal payload: %v", err)
	}
	req, err := http.NewRequest("POST", "https://open-api.123pan.com/api/v1/access_token", bytes.NewBuffer(payload))
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to create request: %v", err)
	}
	req.Header.Set("Platform", "open_platform")
	req.Header.Set("Content-Type", "application/json")
	client := fshttp.NewClient(context.Background())
	resp, err := client.Do(req)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to send request: %v", err)
	}
	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return "", time.Time{}, fmt.Errorf("unexpected status code: %d", resp.StatusCode)
	}
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to read response body: %v", err)
	}
	var result Response
	if err = json.Unmarshal(body, &result); err != nil {
		return "", time.Time{}, fmt.Errorf("failed to unmarshal response: %v", err)
	}
	expiredAt, err := time.Parse(time.RFC3339, result.Data.ExpiredAt)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to parse expiration time: %v", err)
	}
	return result.Data.AccessToken, expiredAt, nil
}

// Name of the remote (e.g. "123")
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (e.g. "bucket" or "bucket/dir")
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("123 root '%s'", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	return f.features
}

// Hashes returns the supported hash sets.
func (f *Fs) Hashes() fshash.Set {
	return fshash.Set(fshash.MD5)
}

// About gets quota information
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	fs.Debugf(f, "è°ƒç”¨About")

	userInfo, err := f.getUserInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get user info: %w", err)
	}

	usage := &fs.Usage{}
	if userInfo.Data.SpacePermanent > 0 {
		usage.Total = fs.NewUsageValue(userInfo.Data.SpacePermanent)
	}
	if userInfo.Data.SpaceUsed > 0 {
		usage.Used = fs.NewUsageValue(userInfo.Data.SpaceUsed)
	}
	if userInfo.Data.SpacePermanent > 0 && userInfo.Data.SpaceUsed > 0 {
		free := userInfo.Data.SpacePermanent - userInfo.Data.SpaceUsed
		if free > 0 {
			usage.Free = fs.NewUsageValue(free)
		}
	}

	return usage, nil
}

// Purge deletes all the files in the directory
func (f *Fs) Purge(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨Purgeåˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List all files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return fmt.Errorf("æ¸…ç†ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		// ä½¿ç”¨ListFile APIæ¸…ç†ç›®å½•ï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return err
		}

		if response.Code != 0 {
			return fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// ğŸ”§ æ”¶é›†æ‰€æœ‰æœ‰æ•ˆæ–‡ä»¶ï¼šåŒæ—¶æ£€æŸ¥å›æ”¶ç«™å’Œå®¡æ ¸çŠ¶æ€
		for _, file := range response.Data.FileList {
			// å¿…é¡»è¿‡æ»¤å›æ”¶ç«™æ–‡ä»¶ (trashed=1) å’Œå®¡æ ¸é©³å›æ–‡ä»¶ (status>=100)
			if file.Trashed == 0 && file.Status < 100 {
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Delete all files
	for _, file := range allFiles {
		err := f.deleteFile(ctx, file.FileID)
		if err != nil {
			fs.Errorf(f, "âŒ åˆ é™¤æ–‡ä»¶å¤±è´¥ %s: %v", file.Filename, err)
			// Continue with other files
		}
	}

	return nil
}

// Shutdown the backend
func (f *Fs) Shutdown(ctx context.Context) error {
	fs.Debugf(f, "è°ƒç”¨å…³é—­")

	// Stop token renewer if it exists
	if f.tokenRenewer != nil {
		f.tokenRenewer.Stop()
		f.tokenRenewer = nil
	}

	return nil
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "è°ƒç”¨åˆ—è¡¨ï¼Œç›®å½•: %s", dir)

	// ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœdirä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
	if dir == "" && f.root != "" {
		// æ£€æŸ¥rootæ˜¯å¦æŒ‡å‘ä¸€ä¸ªæ–‡ä»¶
		rootObj, err := f.NewObject(ctx, "")
		if err == nil {
			// rootæŒ‡å‘ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨: %s", f.root)
			return fs.DirEntries{rootObj}, nil
		}
		// å¦‚æœNewObjectå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºrootæŒ‡å‘ç›®å½•
		if err == fs.ErrorNotAFile {
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘ç›®å½•ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s", f.root)
		} else {
			fs.Debugf(f, "rootè·¯å¾„æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s, é”™è¯¯: %v", f.root, err)
		}
	}

	// ä½¿ç”¨ç›®å½•ç¼“å­˜æŸ¥æ‰¾ç›®å½•ID
	fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•ID: %s", dir)
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•å¤±è´¥ %s: %v", dir, err)
		return nil, err
	}
	fs.Debugf(f, "æ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %s", dirID, dir)

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return nil, fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return nil, fmt.Errorf("åˆ—è¡¨ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		// ä½¿ç”¨ListFile APIåˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ï¼ˆlimit=100æ˜¯APIæœ€å¤§é™åˆ¶ï¼‰
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return nil, err
		}

		if response.Code != 0 {
			return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// ğŸ”§ å®Œå–„è¿‡æ»¤é€»è¾‘ï¼šåŒæ—¶æ£€æŸ¥å›æ”¶ç«™å’Œå®¡æ ¸çŠ¶æ€
		for _, file := range response.Data.FileList {
			// å¿…é¡»è¿‡æ»¤å›æ”¶ç«™æ–‡ä»¶ (trashed=1) å’Œå®¡æ ¸é©³å›æ–‡ä»¶ (status>=100)
			if file.Trashed == 0 && file.Status < 100 {
				fs.Debugf(f, "âœ… æ–‡ä»¶é€šè¿‡è¿‡æ»¤: %s (trashed=%d, status=%d)", file.Filename, file.Trashed, file.Status)
				allFiles = append(allFiles, file)
			} else {
				fs.Debugf(f, "ğŸ—‘ï¸ æ–‡ä»¶è¢«è¿‡æ»¤: %s (trashed=%d, status=%d)", file.Filename, file.Trashed, file.Status)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Convert to fs.DirEntries
	for _, file := range allFiles {
		// ä¿®å¤ï¼šæ¸…ç†APIè¿”å›çš„æ–‡ä»¶åï¼Œå»é™¤å‰åç©ºæ ¼å¹¶è¿›è¡ŒURLè§£ç 
		cleanedFilename := strings.TrimSpace(file.Filename)

		// URLè§£ç æ–‡ä»¶åï¼ˆå¤„ç†%3Aç­‰ç¼–ç ï¼‰
		if decodedName, err := url.QueryUnescape(cleanedFilename); err == nil {
			cleanedFilename = decodedName
		}
		if cleanedFilename != file.Filename {
			fs.Debugf(f, " æ¸…ç†æ–‡ä»¶å: [%s] -> [%s]", file.Filename, cleanedFilename)
		}

		remote := cleanedFilename
		if dir != "" {
			remote = strings.Trim(dir+"/"+cleanedFilename, "/")
		}

		// âœ… ç›´æ¥ä½¿ç”¨123ç½‘ç›˜APIçš„Typeå­—æ®µï¼Œæ— éœ€é¢å¤–åˆ¤æ–­
		isDir := (file.Type == 1)
		fs.Debugf(f, "âœ… 123ç½‘ç›˜List APIç±»å‹: '%s' Type=%d, isDir=%v", cleanedFilename, file.Type, isDir)

		if isDir { // Directory
			// Cache the directory ID for future use
			f.dirCache.Put(remote, strconv.FormatInt(file.FileID, 10))
			d := fs.NewDir(remote, time.Time{})
			entries = append(entries, d)
		} else { // File
			o := &Object{
				fs:          f,
				remote:      remote,
				hasMetaData: true,
				id:          strconv.FormatInt(file.FileID, 10),
				size:        file.Size,
				md5sum:      file.Etag,
				modTime:     time.Now(), // We'll parse this properly later
				isDir:       false,
			}
			entries = append(entries, o)
		}
	}

	return entries, nil
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ›å»ºç›®å½•: %s", dir)

	// Use dircache to create the directory
	_, err := f.dirCache.FindDir(ctx, dir, true)
	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}
	return err
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	fileID, err := strconv.ParseInt(dirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid directory ID: %s", dirID)
	}

	// Delete the directory
	err = f.deleteFile(ctx, fileID)
	if err != nil {
		return err
	}

	// Remove from cache
	f.dirCache.FlushDir(dir)

	// ç§»é™¤ç¼“å­˜æ¸…ç†

	return nil
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨ç§»åŠ¨ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Moveæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		return nil, fs.ErrorCantMove
	}

	// Use dircache to find destination directory
	dstName, dstDirID, err := f.findPathSafe(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	cleanedDstName, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "âš ï¸ ç§»åŠ¨æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		dstName = cleanedDstName
	}

	// è½¬æ¢ç›®æ ‡ç›®å½•ID
	dstParentID, err := strconv.ParseInt(dstDirID, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid destination directory ID: %s", dstDirID)
	}

	// ç”Ÿæˆå”¯ä¸€çš„ç›®æ ‡æ–‡ä»¶åï¼Œé¿å…å†²çª
	uniqueDstName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
	if err != nil {
		fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åç§°: %v", err)
		uniqueDstName = dstName
	}

	if uniqueDstName != dstName {
		fs.Logf(f, "âš ï¸ æ£€æµ‹åˆ°æ–‡ä»¶åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", dstName, uniqueDstName)
		dstName = uniqueDstName
	}

	// Convert IDs to int64
	fileID, err := strconv.ParseInt(srcObj.id, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid file ID: %s", srcObj.id)
	}

	// Get source directory ID for comparison
	_, srcDirID, err := f.dirCache.FindPath(ctx, srcObj.Remote(), false)
	if err != nil {
		return nil, fmt.Errorf("failed to find source directory: %w", err)
	}

	// è·å–æºæ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰
	srcBaseName := filepath.Base(srcObj.Remote())
	fs.Debugf(f, "æºæ–‡ä»¶åŸºç¡€åç§°: %s, ç›®æ ‡æ–‡ä»¶å: %s", srcBaseName, dstName)

	// Check if we're moving to the same directory
	if srcDirID == dstDirID {
		// Same directory - only rename if name changed
		if dstName != srcBaseName {
			fs.Debugf(f, "åŒç›®å½•ç§»åŠ¨ï¼Œä»…é‡å‘½å %s ä¸º %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				return nil, err
			}
		} else {
			fs.Debugf(f, "åŒç›®å½•åŒå - æ— éœ€æ“ä½œ")
		}
	} else {
		// ä¸åŒç›®å½• - å…ˆç§»åŠ¨ï¼Œç„¶åæ ¹æ®éœ€è¦é‡å‘½å
		fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶ä»ç›®å½• %s åˆ° %s", srcDirID, dstDirID)

		// åœ¨è·¨ç›®å½•ç§»åŠ¨æ—¶ï¼Œå…ˆæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶
		if dstName == srcBaseName {
			// å¦‚æœç›®æ ‡æ–‡ä»¶åä¸æºæ–‡ä»¶åç›¸åŒï¼Œæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡ä»¶
			exists, existingFileID, err := f.fileExistsInDirectory(ctx, dstParentID, dstName)
			if err != nil {
				fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err)
			} else if exists {
				fs.Debugf(f, "ç›®æ ‡ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ŒID: %d", existingFileID)
				if existingFileID == fileID {
					fs.Debugf(f, "æ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
					// æ–‡ä»¶å·²ç»åœ¨æ­£ç¡®ä½ç½®ï¼Œç›´æ¥è¿”å›æˆåŠŸ
					return &Object{
						fs:          f,
						remote:      remote,
						hasMetaData: srcObj.hasMetaData,
						id:          srcObj.id,
						size:        srcObj.size,
						md5sum:      srcObj.md5sum,
						modTime:     srcObj.modTime,
						isDir:       srcObj.isDir,
					}, nil
				} else {
					fs.Debugf(f, "ç›®æ ‡ç›®å½•å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°")
					uniqueName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
					if err != nil {
						fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
					} else {
						dstName = uniqueName
						fs.Logf(f, "âš ï¸ ä½¿ç”¨å”¯ä¸€æ–‡ä»¶åé¿å…å†²çª: %s", dstName)
					}
				}
			}
		}

		err = f.moveFile(ctx, fileID, dstParentID)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½• - ä¿®å¤å­—ç¬¦ä¸²åŒ¹é…é—®é¢˜
			if strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸­") ||
				strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
				fs.Debugf(f, "æ–‡ä»¶å¯èƒ½å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œæ£€æŸ¥çŠ¶æ€")
				exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, srcBaseName)
				if checkErr == nil && exists && existingFileID == fileID {
					fs.Debugf(f, "ç¡®è®¤æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œç»§ç»­é‡å‘½åæ­¥éª¤")
					// æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œè·³è¿‡ç§»åŠ¨æ­¥éª¤
				} else {
					// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
					if strings.HasSuffix(srcBaseName, ".partial") {
						fs.Debugf(f, "Partialæ–‡ä»¶ç§»åŠ¨å†²çªï¼Œå°è¯•å¤„ç†æ–‡ä»¶åå†²çª")
						return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
					}
					return nil, err
				}
			} else {
				return nil, err
			}
		}

		// If the name changed, rename the file
		if dstName != srcBaseName {
			fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åæ–‡ä»¶: %s -> %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				// æ£€æŸ¥é‡å‘½åå¤±è´¥çš„åŸå›  - å¢å¼ºé”™è¯¯åŒ¹é…
				if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
					strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
					fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œç›®å½•ä¸­å·²æœ‰é‡åæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡æ–‡ä»¶")
					exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, dstName)
					if checkErr == nil && exists && existingFileID == fileID {
						fs.Debugf(f, "æ–‡ä»¶å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæ“ä½œå®é™…å·²æˆåŠŸ")
						// æ–‡ä»¶å·²ç»æœ‰æ­£ç¡®çš„åç§°ï¼Œè®¤ä¸ºæ“ä½œæˆåŠŸ
					} else {
						// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
						if strings.HasSuffix(srcBaseName, ".partial") {
							fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œå°è¯•ç‰¹æ®Šå¤„ç†")
							return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
						}
						return nil, err
					}
				} else {
					return nil, err
				}
			}
		}
	}

	// ç§»é™¤ç¼“å­˜æ¸…ç†

	// Return the moved object
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: srcObj.hasMetaData,
		id:          srcObj.id,
		size:        srcObj.size,
		md5sum:      srcObj.md5sum,
		modTime:     srcObj.modTime,
		isDir:       srcObj.isDir,
	}, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	fs.Debugf(f, "è°ƒç”¨ç›®å½•ç§»åŠ¨ %s åˆ° %s", srcRemote, dstRemote)

	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(f, "æ— æ³•ç§»åŠ¨ç›®å½• - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return fs.ErrorCantDirMove
	}

	// Find source directory ID
	srcDirID, err := srcFs.dirCache.FindDir(ctx, srcRemote, false)
	if err != nil {
		return err
	}

	// Find destination parent directory
	dstName, dstParentID, err := f.dirCache.FindPath(ctx, dstRemote, true)
	if err != nil {
		return err
	}

	// Convert IDs to int64
	srcFileID, err := strconv.ParseInt(srcDirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid source directory ID: %s", srcDirID)
	}

	dstParentFileID, err := strconv.ParseInt(dstParentID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid destination parent ID: %s", dstParentID)
	}

	// è·å–æºç›®å½•çš„åŸºç¡€åç§°
	srcBaseName := filepath.Base(srcRemote)
	fs.Debugf(f, "æºç›®å½•åŸºç¡€åç§°: %s, ç›®æ ‡ç›®å½•å: %s", srcBaseName, dstName)
	fs.Debugf(f, "æºç›®å½•ID: %d, ç›®æ ‡çˆ¶ç›®å½•ID: %d", srcFileID, dstParentFileID)

	// æ£€æŸ¥æ˜¯å¦å°è¯•ç§»åŠ¨åˆ°åŒä¸€ä¸ªçˆ¶ç›®å½•
	srcParentID, err := f.getParentID(ctx, srcFileID)
	if err != nil {
		fs.Debugf(f, "è·å–æºç›®å½•çˆ¶IDå¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "æºç›®å½•å½“å‰çˆ¶ID: %d", srcParentID)
		if srcParentID == dstParentFileID && dstName == srcBaseName {
			fs.Debugf(f, "ç›®å½•å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
			return nil
		}
	}

	// Move the directory
	err = f.moveFile(ctx, srcFileID, dstParentFileID)
	if err != nil {
		return err
	}

	// If the name changed, rename the directory
	if dstName != srcBaseName {
		fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åç›®å½•: %s -> %s", srcBaseName, dstName)
		err = f.renameFile(ctx, srcFileID, dstName)
		if err != nil {
			return err
		}
	}

	// Update cache
	srcFs.dirCache.FlushDir(srcRemote)
	f.dirCache.FlushDir(dstRemote)

	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨å¤åˆ¶ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Copyæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•å¤åˆ¶ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantCopy
	}

	// Use dircache to find destination directory
	dstName, _, err := f.findPathSafe(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	_, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "âš ï¸ å¤åˆ¶æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		// Note: cleaned name is not used in current implementation
	}

	// 123Pan doesn't have a direct copy API, use download+upload
	fs.Debugf(f, "123ç½‘ç›˜ä¸æ”¯æŒæœåŠ¡å™¨ç«¯å¤åˆ¶ï¼Œä½¿ç”¨ä¸‹è½½+ä¸Šä¼ æ–¹å¼")
	return f.copyViaDownloadUpload(ctx, srcObj, remote)
}

// copyViaDownloadUpload copies a file by downloading and re-uploading
func (f *Fs) copyViaDownloadUpload(ctx context.Context, srcObj *Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "é€šè¿‡ä¸‹è½½+ä¸Šä¼ å¤åˆ¶ %s åˆ° %s", srcObj.remote, remote)

	// Open source file for reading
	src, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file: %w", err)
	}
	defer src.Close()

	// Create a new object info for the destination
	dstInfo := &ObjectInfo{
		fs:      f,
		remote:  remote,
		size:    srcObj.size,
		md5sum:  srcObj.md5sum,
		modTime: srcObj.modTime,
	}

	// Upload to destination
	return f.Put(ctx, src, dstInfo)
}

// ObjectInfo implements fs.ObjectInfo for copy operations
type ObjectInfo struct {
	fs      *Fs
	remote  string
	size    int64
	md5sum  string
	modTime time.Time
}

func (oi *ObjectInfo) Fs() fs.Info                           { return oi.fs }
func (oi *ObjectInfo) Remote() string                        { return oi.remote }
func (oi *ObjectInfo) Size() int64                           { return oi.size }
func (oi *ObjectInfo) ModTime(ctx context.Context) time.Time { return oi.modTime }
func (oi *ObjectInfo) Storable() bool                        { return true }
func (oi *ObjectInfo) String() string                        { return oi.remote }
func (oi *ObjectInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t == fshash.MD5 {
		return oi.md5sum, nil
	}
	return "", fshash.ErrUnsupported
}

// Removed unused functions: getHTTPClient, getAdaptiveTimeout, detectNetworkSpeed, getOptimalConcurrency, getOptimalChunkSize

// measureNetworkLatency removed - use reasonable default latency
// This function is no longer needed as we simplified network detection
