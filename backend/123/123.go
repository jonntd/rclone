package _123

import (
	"bytes"
	"context"
	"crypto/md5"
	"encoding/json"
	"fmt"
	"hash"
	"io"
	"mime/multipart"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"slices"
	"strconv"
	"strings"
	"sync"
	"time"
	"unicode/utf8"

	"github.com/rclone/rclone/lib/atexit"
	"github.com/rclone/rclone/lib/dircache"
	"github.com/rclone/rclone/lib/encoder"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/rest"
	"golang.org/x/oauth2"

	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	"github.com/rclone/rclone/fs/fshttp"
	fshash "github.com/rclone/rclone/fs/hash"
)

const (
	openAPIRootURL     = "https://open-api.123pan.com"
	defaultUserAgent   = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
	tokenRefreshWindow = 10 * time.Minute

	// å®˜æ–¹QPSé™åˆ¶æ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
	// é«˜é¢‘ç‡API (15-20 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	listV2APIMinSleep = 200 * time.Millisecond // ~5 QPS ç”¨äº api/v2/file/list (å®˜æ–¹15 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	fileMoveMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/move (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileInfosMinSleep   = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/infos (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	userInfoMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/user/info (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	accessTokenMinSleep = 300 * time.Millisecond // ~3.3 QPS ç”¨äº api/v1/access_token (å®˜æ–¹8 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä½é¢‘ç‡API (5 QPS) - åŸºäºä¸Šä¼ é€Ÿåº¦åˆ†æå¤§å¹…ä¼˜åŒ–æ€§èƒ½
	// æ³¨æ„ï¼šuploadCreateMinSleep, mkdirMinSleep, fileTrashMinSleep å·²åˆ é™¤ï¼ˆæœªä½¿ç”¨ï¼‰

	downloadInfoMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº api/v1/file/download_info (ä¿æŒä¸å˜)

	// æœ€ä½é¢‘ç‡API (1 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	// ç‰¹æ®ŠAPI (ä¿å®ˆä¼°è®¡) - åŸºäº48ç§’/åˆ†ç‰‡é—®é¢˜å¤§å¹…ä¼˜åŒ–åˆ†ç‰‡ä¸Šä¼ æ€§èƒ½
	uploadV2SliceMinSleep = 50 * time.Millisecond // ~20.0 QPS ç”¨äº upload/v2/file/slice (æ¿€è¿›æå‡ï¼Œè§£å†³ä¸Šä¼ æ…¢é—®é¢˜)

	maxSleep      = 30 * time.Second // 429é€€é¿çš„æœ€å¤§ç¡çœ æ—¶é—´
	decayConstant = 2

	// æ–‡ä»¶ä¸Šä¼ ç›¸å…³å¸¸é‡
	singleStepUploadLimit = 1024 * 1024 * 1024 // 1GB - å•æ­¥ä¸Šä¼ APIçš„æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆä¿®å¤ï¼šåŸæ¥é”™è¯¯è®¾ç½®ä¸º50MBï¼‰
	maxMemoryBufferSize   = 1024 * 1024 * 1024 // 1GB - å†…å­˜ç¼“å†²çš„æœ€å¤§å¤§å°ï¼ˆä»512MBæå‡ï¼‰
	maxFileNameBytes      = 255                // æ–‡ä»¶åçš„æœ€å¤§å­—èŠ‚é•¿åº¦ï¼ˆUTF-8ç¼–ç ï¼‰

	// æ–‡ä»¶å†²çªå¤„ç†ç­–ç•¥å¸¸é‡å·²ç§»é™¤ï¼Œå½“å‰ä½¿ç”¨APIé»˜è®¤è¡Œä¸º

	// ä¸Šä¼ ç›¸å…³å¸¸é‡ - ä¼˜åŒ–å¤§æ–‡ä»¶ä¼ è¾“æ€§èƒ½
	defaultChunkSize    = 100 * fs.Mebi // å¢åŠ é»˜è®¤åˆ†ç‰‡å¤§å°åˆ°100MB
	minChunkSize        = 50 * fs.Mebi  // å¢åŠ æœ€å°åˆ†ç‰‡å¤§å°åˆ°50MB
	maxChunkSize        = 500 * fs.Mebi // è®¾ç½®æœ€å¤§åˆ†ç‰‡å¤§å°ä¸º500MB
	defaultUploadCutoff = 100 * fs.Mebi // é™ä½åˆ†ç‰‡ä¸Šä¼ é˜ˆå€¼
	maxUploadParts      = 10000

	// 123ç½‘ç›˜ç‰¹å®šå¸¸é‡ (ä»commonåŒ…è¿ç§»)
	// ç§»é™¤æœªä½¿ç”¨çš„default123QPSå¸¸é‡

	// ç§»é™¤æœªä½¿ç”¨çš„è·¨äº‘ä¼ è¾“å¸¸é‡

	// è¿æ¥å’Œè¶…æ—¶è®¾ç½® - é’ˆå¯¹å¤§æ–‡ä»¶ä¼ è¾“ä¼˜åŒ–çš„å‚æ•°
	// ç§»é™¤æœªä½¿ç”¨çš„è¶…æ—¶å¸¸é‡

	// æ³¨æ„ï¼šæ–‡ä»¶å¤§å°åˆ¤æ–­å¸¸é‡å·²æœ¬åœ°å®šä¹‰

	// æ–‡ä»¶åéªŒè¯ç›¸å…³å¸¸é‡
	maxFileNameLength = 256          // 123ç½‘ç›˜æ–‡ä»¶åæœ€å¤§é•¿åº¦ï¼ˆåŒ…æ‹¬æ‰©å±•åï¼‰
	invalidChars      = `"\/:*?|><\` // 123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
	replacementChar   = "_"          // ç”¨äºæ›¿æ¢éæ³•å­—ç¬¦çš„å®‰å…¨å­—ç¬¦

	// ç§»é™¤ç¼“å­˜ç›¸å…³å¸¸é‡ï¼Œä½¿ç”¨rcloneæ ‡å‡†

	// é…ç½®éªŒè¯ç›¸å…³å¸¸é‡
	MaxListChunk          = 10000 // æœ€å¤§åˆ—è¡¨å—å¤§å°
	MaxUploadPartsLimit   = 10000 // æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°é™åˆ¶
	MaxConcurrentLimit    = 100   // æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	DefaultListChunk      = 1000  // é»˜è®¤åˆ—è¡¨å—å¤§å°
	DefaultMaxUploadParts = 1000  // é»˜è®¤æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°

	// ç½‘ç»œè´¨é‡è¯„ä¼°å¸¸é‡
	LatencyThreshold          = 50              // å»¶è¿Ÿé˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
	LatencyScoreRange         = 1000            // å»¶è¿Ÿåˆ†æ•°è®¡ç®—èŒƒå›´
	QualityAdjustmentInterval = 5 * time.Minute // è´¨é‡è°ƒæ•´é—´éš”

	// ç¼“å­˜ç®¡ç†å¸¸é‡
	PreloadQueueCapacity    = 1000 // é¢„åŠ è½½é˜Ÿåˆ—å®¹é‡
	MaxHotFiles             = 100  // æœ€å¤§çƒ­ç‚¹æ–‡ä»¶æ•°
	HotFileCleanupBatchSize = 20   // çƒ­ç‚¹æ–‡ä»¶æ¸…ç†æ‰¹æ¬¡å¤§å°

	// ç½‘ç»œæ£€æµ‹å¸¸é‡
	NetworkTestSize = 512 * 1024 // ç½‘ç»œæµ‹è¯•æ–‡ä»¶å¤§å°ï¼ˆ512KBï¼‰
)

// Options å®šä¹‰æ­¤åç«¯çš„é…ç½®é€‰é¡¹
// ç¬¦åˆrcloneæ ‡å‡†è®¾è®¡åŸåˆ™çš„é…ç½®ç»“æ„
type Options struct {
	// 123ç½‘ç›˜ç‰¹å®šçš„è®¤è¯é…ç½®
	ClientID     string `config:"client_id"`
	ClientSecret string `config:"client_secret"`
	Token        string `config:"token"`
	UserAgent    string `config:"user_agent"`
	RootFolderID string `config:"root_folder_id"`

	// 123ç½‘ç›˜ç‰¹æœ‰çš„é…ç½®é€‰é¡¹
	MaxUploadParts int `config:"max_upload_parts"`

	// 123ç½‘ç›˜ç‰¹å®šçš„QPSæ§åˆ¶é€‰é¡¹
	UploadPacerMinSleep   fs.Duration `config:"upload_pacer_min_sleep"`
	DownloadPacerMinSleep fs.Duration `config:"download_pacer_min_sleep"`
	StrictPacerMinSleep   fs.Duration `config:"strict_pacer_min_sleep"`

	// ç¼–ç é…ç½®
	Enc encoder.MultiEncoder `config:"encoding"` // æ–‡ä»¶åç¼–ç è®¾ç½®

	// ç§»é™¤ç¼“å­˜ä¼˜åŒ–é…ç½®ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç¼“å­˜
}

// Fs è¡¨ç¤ºè¿œç¨‹123ç½‘ç›˜é©±åŠ¨å™¨å®ä¾‹
type Fs struct {
	name         string       // æ­¤è¿œç¨‹å®ä¾‹çš„åç§°
	originalName string       // æœªä¿®æ”¹çš„åŸå§‹é…ç½®åç§°
	root         string       // å½“å‰æ“ä½œçš„æ ¹è·¯å¾„
	opt          Options      // è§£æåçš„é…ç½®é€‰é¡¹
	features     *fs.Features // å¯é€‰åŠŸèƒ½ç‰¹æ€§

	// APIå®¢æˆ·ç«¯å’Œèº«ä»½éªŒè¯ç›¸å…³
	rst          *rest.Client     // rcloneæ ‡å‡†restå®¢æˆ·ç«¯
	token        string           // ç¼“å­˜çš„è®¿é—®ä»¤ç‰Œ
	tokenExpiry  time.Time        // ç¼“å­˜è®¿é—®ä»¤ç‰Œçš„è¿‡æœŸæ—¶é—´
	tokenMu      sync.Mutex       // ä¿æŠ¤tokenå’ŒtokenExpiryçš„äº’æ–¥é”
	tokenRenewer *oauthutil.Renew // ä»¤ç‰Œè‡ªåŠ¨æ›´æ–°å™¨

	// ç§»é™¤æœªä½¿ç”¨çš„cacheMuå­—æ®µ

	// é…ç½®å’ŒçŠ¶æ€ä¿¡æ¯
	m            configmap.Mapper // é…ç½®æ˜ å°„å™¨
	rootFolderID string           // æ ¹æ–‡ä»¶å¤¹çš„ID

	// æ€§èƒ½å’Œé€Ÿç‡é™åˆ¶ - é’ˆå¯¹ä¸åŒAPIé™åˆ¶çš„å¤šä¸ªè°ƒé€Ÿå™¨
	listPacer     *fs.Pacer // æ–‡ä»¶åˆ—è¡¨APIçš„è°ƒé€Ÿå™¨ï¼ˆçº¦2 QPSï¼‰
	strictPacer   *fs.Pacer // ä¸¥æ ¼APIï¼ˆå¦‚user/info, move, deleteï¼‰çš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	uploadPacer   *fs.Pacer // ä¸Šä¼ APIçš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	downloadPacer *fs.Pacer // ä¸‹è½½APIçš„è°ƒé€Ÿå™¨ï¼ˆ2 QPSï¼‰

	// ç›®å½•ç¼“å­˜
	dirCache *dircache.DirCache

	// ç§»é™¤è‡ªå®šä¹‰ç¼“å­˜ï¼Œä½¿ç”¨rcloneæ ‡å‡†dircache

	// æ€§èƒ½ä¼˜åŒ–ç›¸å…³ - ä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶æ›¿ä»£è¿‡åº¦å¼€å‘çš„ç®¡ç†å™¨

	// Unified components removed - using rclone standard implementations

	// ç§»é™¤å¤æ‚çš„ç¼“å­˜é…ç½®å’Œç»Ÿä¸€åç«¯é…ç½®

	// ç§»é™¤ä¸Šä¼ åŸŸåç¼“å­˜ï¼Œä½¿ç”¨rcloneæ ‡å‡†
}

// ç§»é™¤å¤æ‚çš„ç¼“å­˜é…ç½®ç»“æ„ä½“ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶

// ç§»é™¤PathToIDCacheEntryï¼Œä½¿ç”¨rcloneæ ‡å‡†dircache

// ProgressReadCloser åŒ…è£…ReadCloserä»¥æä¾›è¿›åº¦è·Ÿè¸ªå’Œèµ„æºç®¡ç†
type ProgressReadCloser struct {
	io.ReadCloser
	fs              *Fs
	totalSize       int64
	transferredSize int64
	startTime       time.Time // ä¸‹è½½å¼€å§‹æ—¶é—´
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (prc *ProgressReadCloser) Read(p []byte) (n int, err error) {
	n, err = prc.ReadCloser.Read(p)
	if n > 0 {
		prc.transferredSize += int64(n)
		// rcloneå†…ç½®çš„accountingä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦è·Ÿè¸ª
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£ï¼ŒåŒæ—¶æ¸…ç†èµ„æº
func (prc *ProgressReadCloser) Close() error {
	// å…³é—­åº•å±‚ReadCloser
	return prc.ReadCloser.Close()
}

// Object æè¿°123ç½‘ç›˜ä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å¯¹è±¡
type Object struct {
	fs          *Fs       // çˆ¶æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	remote      string    // è¿œç¨‹è·¯å¾„
	hasMetaData bool      // æ˜¯å¦å·²åŠ è½½å…ƒæ•°æ®
	id          string    // æ–‡ä»¶/æ–‡ä»¶å¤¹ID
	size        int64     // æ–‡ä»¶å¤§å°
	md5sum      string    // MD5å“ˆå¸Œå€¼
	modTime     time.Time // ä¿®æ”¹æ—¶é—´
	isDir       bool      // æ˜¯å¦ä¸ºç›®å½•
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file     *os.File
	tempPath string
}

// Read å®ç°io.Readeræ¥å£
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	var err error
	if r.file != nil {
		err = r.file.Close()
		r.file = nil
	}
	if r.tempPath != "" {
		if removeErr := os.Remove(r.tempPath); removeErr != nil {
			fs.Debugf(nil, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", r.tempPath, removeErr)
		} else {
			fs.Debugf(nil, "å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: %s", r.tempPath)
		}
		r.tempPath = ""
	}
	return err
}

// æ–‡ä»¶åéªŒè¯ç›¸å…³çš„å…¨å±€å˜é‡
var (
	// invalidCharsRegex ç”¨äºåŒ¹é…123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
	invalidCharsRegex = regexp.MustCompile(`["\/:*?|><\\]`)
)

func validateFileName(name string) error {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„è·¯å¾„æ£€æŸ¥
	if strings.Contains(name, "/") || strings.Contains(name, "\\") {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½åŒ…å«è·¯å¾„åˆ†éš”ç¬¦")
	}

	// åŸºç¡€æ£€æŸ¥ï¼šç©ºå€¼å’Œç©ºæ ¼
	if strings.TrimSpace(name) == "" {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½ä¸ºç©ºæˆ–å…¨éƒ¨æ˜¯ç©ºæ ¼")
	}

	// UTF-8æœ‰æ•ˆæ€§æ£€æŸ¥
	if !utf8.ValidString(name) {
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«æ— æ•ˆçš„UTF-8å­—ç¬¦")
	}

	// 123ç½‘ç›˜ç‰¹å®šçš„é•¿åº¦æ£€æŸ¥ï¼ˆå­—ç¬¦æ•°ï¼‰
	runeCount := utf8.RuneCountInString(name)
	if runeCount > maxFileNameLength {
		return fmt.Errorf("æ–‡ä»¶åé•¿åº¦è¶…è¿‡é™åˆ¶ï¼šå½“å‰%dä¸ªå­—ç¬¦ï¼Œæœ€å¤§å…è®¸%dä¸ªå­—ç¬¦",
			runeCount, maxFileNameLength)
	}

	// å­—èŠ‚é•¿åº¦æ£€æŸ¥
	byteLength := len([]byte(name))
	if byteLength > maxFileNameBytes {
		return fmt.Errorf("æ–‡ä»¶åå­—èŠ‚é•¿åº¦è¶…è¿‡%d: %d", maxFileNameBytes, byteLength)
	}

	// ç¦ç”¨å­—ç¬¦æ£€æŸ¥
	if invalidCharsRegex.MatchString(name) {
		invalidFound := invalidCharsRegex.FindAllString(name, -1)
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«ä¸å…è®¸çš„å­—ç¬¦ï¼š%vï¼Œ123ç½‘ç›˜ä¸å…è®¸ä½¿ç”¨ä»¥ä¸‹å­—ç¬¦ï¼š%s",
			invalidFound, invalidChars)
	}

	return nil
}

// cleanFileName æ¸…ç†æ–‡ä»¶åï¼Œå°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºå®‰å…¨å­—ç¬¦
// åŒæ—¶ç¡®ä¿æ–‡ä»¶åé•¿åº¦ä¸è¶…è¿‡é™åˆ¶
func cleanFileName(name string) string {
	if name == "" {
		return "æœªå‘½åæ–‡ä»¶"
	}

	// æ›¿æ¢éæ³•å­—ç¬¦
	cleaned := invalidCharsRegex.ReplaceAllString(name, replacementChar)

	// å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†
	if utf8.RuneCountInString(cleaned) > maxFileNameLength {
		// ä¿ç•™æ–‡ä»¶æ‰©å±•å
		ext := filepath.Ext(cleaned)
		nameWithoutExt := strings.TrimSuffix(cleaned, ext)

		// è®¡ç®—å¯ç”¨çš„æ–‡ä»¶åé•¿åº¦ï¼ˆæ€»é•¿åº¦ - æ‰©å±•åé•¿åº¦ï¼‰
		maxNameLength := maxFileNameLength - utf8.RuneCountInString(ext)
		if maxNameLength < 1 {
			// å¦‚æœæ‰©å±•åå¤ªé•¿ï¼Œåªä¿ç•™éƒ¨åˆ†æ‰©å±•å
			maxNameLength = maxFileNameLength - 10 // ä¿ç•™è‡³å°‘10ä¸ªå­—ç¬¦ç»™æ–‡ä»¶å
			maxNameLength = max(maxNameLength, 1)
			extRunes := []rune(ext)
			if len(extRunes) > 10 {
				ext = string(extRunes[:10])
			}
		}

		// æˆªæ–­æ–‡ä»¶åä¸»ä½“éƒ¨åˆ†
		nameRunes := []rune(nameWithoutExt)
		if len(nameRunes) > maxNameLength {
			nameWithoutExt = string(nameRunes[:maxNameLength])
		}

		cleaned = nameWithoutExt + ext
	}

	return cleaned
}

// validateAndCleanFileName éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶åçš„ä¾¿æ·å‡½æ•°
// å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
func validateAndCleanFileName(name string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(name); err == nil {
		return name, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(name)

	// è¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
	warning = fmt.Errorf("æ–‡ä»¶åå·²è‡ªåŠ¨æ¸…ç†ï¼šåŸå§‹åç§° '%s' -> æ¸…ç†ååç§° '%s'", name, cleanedName)
	return cleanedName, warning
}

// generateUniqueFileName ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª
func (f *Fs) generateUniqueFileName(ctx context.Context, parentFileID int64, baseName string) (string, error) {
	// é¦–å…ˆæ£€æŸ¥åŸå§‹æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
	exists, err := f.fileExists(ctx, parentFileID, baseName)
	if err != nil {
		return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
	}

	if !exists {
		return baseName, nil
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
	ext := filepath.Ext(baseName)
	nameWithoutExt := strings.TrimSuffix(baseName, ext)

	for i := 1; i <= 999; i++ {
		newName := fmt.Sprintf("%s (%d)%s", nameWithoutExt, i, ext)

		// éªŒè¯æ–°æ–‡ä»¶åé•¿åº¦
		if len([]byte(newName)) > maxFileNameBytes {
			// å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­åŸºç¡€åç§°
			suffixBytes := make([]byte, 0, 20) // é¢„åˆ†é…è¶³å¤Ÿçš„å®¹é‡
			suffixBytes = fmt.Appendf(suffixBytes, " (%d)%s", i, ext)
			maxBaseLength := maxFileNameBytes - len(suffixBytes)
			if maxBaseLength > 0 {
				truncatedBase := string([]byte(nameWithoutExt)[:maxBaseLength])
				newName = fmt.Sprintf("%s (%d)%s", truncatedBase, i, ext)
			} else {
				// å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œä½¿ç”¨ç®€åŒ–åç§°
				newName = fmt.Sprintf("file_%d%s", i, ext)
			}
		}

		exists, err := f.fileExists(ctx, parentFileID, newName)
		if err != nil {
			return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
		}

		if !exists {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
			return newName, nil
		}
	}

	// å¦‚æœå°è¯•äº†999æ¬¡éƒ½æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€åç§°ï¼Œä½¿ç”¨æ—¶é—´æˆ³
	timestamp := time.Now().Unix()
	newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, timestamp, ext)
	fs.Debugf(f, "ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
	return newName, nil
}

// fileExists æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExists(ctx context.Context, parentFileID int64, fileName string) (bool, error) {
	// ä½¿ç”¨ListFile APIæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", 0)
	if err != nil {
		return false, err
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			return true, nil
		}
	}

	return false, nil
}

// verifyParentFileID éªŒè¯çˆ¶ç›®å½•IDæ˜¯å¦å­˜åœ¨ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç§»é™¤ç¼“å­˜
func (f *Fs) verifyParentFileID(ctx context.Context, parentFileID int64) (bool, error) {
	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID: %d", parentFileID)

	// ç›´æ¥æ‰§è¡ŒAPIéªŒè¯ï¼Œä½¿ç”¨ListFile API
	response, err := f.ListFile(ctx, int(parentFileID), 1, "", "", 0)
	if err != nil {
		fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %v", parentFileID, err)
		return false, err
	}

	isValid := response.Code == 0
	if !isValid {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼ŒAPIè¿”å›: code=%d, message=%s", parentFileID, response.Code, response.Message)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	return isValid, nil
}

// getCorrectParentFileID è·å–ä¸Šä¼ APIè®¤å¯çš„æ­£ç¡®çˆ¶ç›®å½•ID
// ç»Ÿä¸€ç‰ˆæœ¬ï¼šä¸ºå•æ­¥ä¸Šä¼ å’Œåˆ†ç‰‡ä¸Šä¼ æä¾›ä¸€è‡´çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
func (f *Fs) getCorrectParentFileID(ctx context.Context, cachedParentID int64) (int64, error) {
	fs.Debugf(f, " ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥ï¼Œç¼“å­˜ID: %d", cachedParentID)

	// ç­–ç•¥1ï¼šé‡æ–°è·å–ç›®å½•ç»“æ„
	fs.Infof(f, "ğŸ”„ é‡æ–°è·å–ç›®å½•ç»“æ„")

	// åªé‡ç½®dirCacheï¼Œä¸å®Œå…¨æ¸…ç©ºï¼Œä¿æŒå…¶ä»–æœ‰æ•ˆç¼“å­˜
	if f.dirCache != nil {
		fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", cachedParentID)
		f.dirCache.ResetRoot()
	}

	// ç­–ç•¥2ï¼šå°è¯•é‡æ–°éªŒè¯åŸå§‹IDï¼ˆæ¸…ç†ç¼“å­˜åå¯èƒ½æ¢å¤ï¼‰
	fs.Debugf(f, "ğŸ”„ é‡æ–°éªŒè¯åŸå§‹ç›®å½•ID: %d", cachedParentID)
	exists, err := f.verifyParentFileID(ctx, cachedParentID)
	if err == nil && exists {
		fs.Infof(f, "âœ… åŸå§‹ç›®å½•ID %d åœ¨æ¸…ç†ç¼“å­˜åéªŒè¯æˆåŠŸ", cachedParentID)
		return cachedParentID, nil
	}

	// ç­–ç•¥3ï¼šå°è¯•éªŒè¯æ ¹ç›®å½•æ˜¯å¦å¯ç”¨
	// æ ¹æ®å®˜æ–¹APIç¤ºä¾‹ï¼Œå¾ˆå¤šæƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨æ ¹ç›®å½• (parentFileID: 0)
	fs.Debugf(f, "ğŸ§ª å°è¯•éªŒè¯æ ¹ç›®å½• (parentFileID: 0) æ˜¯å¦å¯ç”¨")
	rootExists, err := f.verifyParentFileID(ctx, 0)
	if err == nil && rootExists {
		fs.Infof(f, "âœ… æ ¹ç›®å½•éªŒè¯æˆåŠŸï¼Œä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºå›é€€æ–¹æ¡ˆ")
		return 0, nil
	}

	// ç­–ç•¥4ï¼šå¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œä»ç„¶å›é€€åˆ°æ ¹ç›®å½•ï¼ˆå¼ºåˆ¶ç­–ç•¥ï¼‰
	// è¿™æ˜¯å› ä¸ºæ ¹ç›®å½•(0)åœ¨123ç½‘ç›˜APIä¸­æ€»æ˜¯å­˜åœ¨çš„
	fs.Infof(f, "âš ï¸ æ‰€æœ‰éªŒè¯ç­–ç•¥å¤±è´¥ï¼Œå¼ºåˆ¶ä½¿ç”¨æ ¹ç›®å½• (parentFileID: 0)")
	return 0, nil
}

// ç¼“å­˜ç›¸å…³å‡½æ•°å·²ç§»é™¤ï¼Œä½¿ç”¨rcloneæ ‡å‡†dircache

// ç§»é™¤æœªä½¿ç”¨çš„TaskIDç›¸å…³å‡½æ•°

// calculatePollingStrategy è®¡ç®—æ™ºèƒ½è½®è¯¢ç­–ç•¥
// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è½®è¯¢å‚æ•°å’Œè¿ç»­å¤±è´¥é˜ˆå€¼
func (f *Fs) calculatePollingStrategy(fileSize int64) (maxRetries int, baseInterval time.Duration, maxConsecutiveFailures int) {
	// åŸºç¡€è½®è¯¢é—´éš”ï¼ˆæŒ‰å®˜æ–¹æ–‡æ¡£è¦æ±‚ï¼‰
	baseInterval = 1 * time.Second

	// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´æœ€å¤§é‡è¯•æ¬¡æ•°å’Œè¿ç»­å¤±è´¥é˜ˆå€¼
	switch {
	case fileSize < 100*1024*1024: // å°äº100MB
		maxRetries = 180           // 3åˆ†é’Ÿ
		maxConsecutiveFailures = 8 // å°æ–‡ä»¶å®¹é”™æ€§ç¨ä½
	case fileSize < 500*1024*1024: // å°äº500MB
		maxRetries = 300            // 5åˆ†é’Ÿ
		maxConsecutiveFailures = 12 // ä¸­ç­‰æ–‡ä»¶é€‚ä¸­å®¹é”™
	case fileSize < 1*1024*1024*1024: // å°äº1GB
		maxRetries = 600            // 10åˆ†é’Ÿ
		maxConsecutiveFailures = 15 // å¤§æ–‡ä»¶æé«˜å®¹é”™æ€§
	case fileSize < 5*1024*1024*1024: // å°äº5GB
		maxRetries = 900            // 15åˆ†é’Ÿ
		maxConsecutiveFailures = 20 // è¶…å¤§æ–‡ä»¶é«˜å®¹é”™æ€§
	default: // 5GBä»¥ä¸Š
		maxRetries = 1200           // 20åˆ†é’Ÿ
		maxConsecutiveFailures = 25 // å·¨å¤§æ–‡ä»¶æœ€é«˜å®¹é”™æ€§
	}

	fs.Debugf(f, "æ–‡ä»¶å¤§å°: %s, è½®è¯¢ç­–ç•¥: æœ€å¤§%dæ¬¡, é—´éš”%v, è¿ç»­å¤±è´¥é˜ˆå€¼%d",
		fs.SizeSuffix(fileSize), maxRetries, baseInterval, maxConsecutiveFailures)

	return maxRetries, baseInterval, maxConsecutiveFailures
}

// calculateDynamicInterval è®¡ç®—åŠ¨æ€è½®è¯¢é—´éš”
// ä¼˜åŒ–ï¼šæ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°ã€å°è¯•æ¬¡æ•°å’Œé”™è¯¯ç±»å‹æ™ºèƒ½è°ƒæ•´é—´éš”
func (f *Fs) calculateDynamicInterval(baseInterval time.Duration, consecutiveFailures, attempt int, lastErr error) time.Duration {
	// åŸºç¡€é—´éš”
	interval := baseInterval

	// ç®€åŒ–ï¼šæ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é€€é¿ç­–ç•¥
	isNetworkErr := lastErr != nil && (strings.Contains(lastErr.Error(), "connection") ||
		strings.Contains(lastErr.Error(), "timeout") ||
		strings.Contains(lastErr.Error(), "network"))

	// æ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°å¢åŠ å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
	if consecutiveFailures > 0 {
		backoffMultiplier := 1 << uint(consecutiveFailures) // 2^consecutiveFailures

		// ç®€åŒ–ï¼šç½‘ç»œé”™è¯¯ä½¿ç”¨æ›´æ¸©å’Œçš„é€€é¿ç­–ç•¥
		if isNetworkErr {
			if backoffMultiplier > 4 {
				backoffMultiplier = 4 // ç½‘ç»œé”™è¯¯æœ€å¤§4å€å»¶è¿Ÿ
			}
		} else {
			if backoffMultiplier > 8 {
				backoffMultiplier = 8 // å…¶ä»–é”™è¯¯æœ€å¤§8å€å»¶è¿Ÿ
			}
		}

		interval = time.Duration(backoffMultiplier) * baseInterval
	}

	// ç®€åŒ–ï¼šé•¿æ—¶é—´è½®è¯¢æ—¶çš„æ™ºèƒ½é—´éš”è°ƒæ•´
	if attempt > 60 { // 1åˆ†é’Ÿå
		if isNetworkErr {
			interval = interval * 3 // ç½‘ç»œé”™è¯¯å¢åŠ æ›´å¤šå»¶è¿Ÿ
		} else {
			interval = interval * 2 // å…¶ä»–é”™è¯¯é€‚åº¦å¢åŠ 
		}
	}
	if attempt > 300 { // 5åˆ†é’Ÿå
		interval = interval * 2 // è¿›ä¸€æ­¥å¢åŠ é—´éš”
	}

	// æœ€å¤§é—´éš”é™åˆ¶ä¸º30ç§’
	if interval > 30*time.Second {
		interval = 30 * time.Second
	}

	return interval
}

// å·²ç§»é™¤ï¼šisNetworkError å‡½æ•°å·²æœ¬åœ°å®ç°
// ä½¿ç”¨ç»Ÿä¸€çš„ç½‘ç»œé”™è¯¯æ£€æµ‹æœºåˆ¶ï¼Œæé«˜å‡†ç¡®æ€§å’Œä¸€è‡´æ€§

// isRetryableError åˆ¤æ–­å“åº”é”™è¯¯ä»£ç æ˜¯å¦å¯é‡è¯•
// å¢å¼ºï¼šåŸºäº123ç½‘ç›˜APIæ–‡æ¡£çš„é”™è¯¯ä»£ç åˆ†ç±»
func (f *Fs) isRetryableError(code int) bool {
	retryableCodes := []int{
		20101, // æœåŠ¡å™¨ç¹å¿™
		20103, // ä¸´æ—¶é”™è¯¯
		20104, // ç³»ç»Ÿç»´æŠ¤
		20105, // æœåŠ¡æš‚ä¸å¯ç”¨
		500,   // å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
		502,   // ç½‘å…³é”™è¯¯
		503,   // æœåŠ¡ä¸å¯ç”¨
		504,   // ç½‘å…³è¶…æ—¶
	}

	return slices.Contains(retryableCodes, code)
}

// ç§»é™¤verifyChunkIntegrityå‡½æ•°ï¼Œæœªä½¿ç”¨

// ç§»é™¤resumeUploadWithIntegrityCheckå‡½æ•°ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶

// getParentID è·å–æ–‡ä»¶æˆ–ç›®å½•çš„çˆ¶ç›®å½•ID
func (f *Fs) getParentID(ctx context.Context, fileID int64) (int64, error) {
	fs.Debugf(f, "å°è¯•è·å–æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID", fileID)

	// ç›´æ¥é€šè¿‡APIæŸ¥è¯¢ï¼Œå› ä¸ºç¼“å­˜æœç´¢åŠŸèƒ½æœªå®ç°

	// å¦‚æœç¼“å­˜ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡APIæŸ¥è¯¢
	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…APIè·å–æ–‡ä»¶ä¿¡æ¯
	var response struct {
		Code int `json:"code"`
		Data struct {
			ParentFileID int64 `json:"parentFileId"`
		} `json:"data"`
		Message string `json:"message"`
	}

	url := fmt.Sprintf("/api/v1/file/info?fileId=%d", fileID)
	err := f.makeAPICallWithRest(ctx, url, "GET", nil, &response)
	if err != nil {
		return 0, fmt.Errorf("è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return 0, fmt.Errorf("APIè¿”å›é”™è¯¯: %s", response.Message)
	}

	fs.Debugf(f, "é€šè¿‡APIè·å–åˆ°æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID: %d", fileID, response.Data.ParentFileID)
	return response.Data.ParentFileID, nil
}

// fileExistsInDirectory æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExistsInDirectory(ctx context.Context, parentID int64, fileName string) (bool, int64, error) {
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• %d ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶: %s", parentID, fileName)

	response, err := f.ListFile(ctx, int(parentID), 100, "", "", 0)
	if err != nil {
		return false, 0, err
	}

	if response.Code != 0 {
		return false, 0, fmt.Errorf("ListFile APIé”™è¯¯: code=%d, message=%s", response.Code, response.Message)
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			fs.Debugf(f, "æ‰¾åˆ°æ–‡ä»¶ %sï¼ŒID: %d", fileName, file.FileID)
			return true, int64(file.FileID), nil
		}
	}

	fs.Debugf(f, "ç›®å½• %d ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: %s", parentID, fileName)
	return false, 0, nil
}

// normalizePath è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†123ç½‘ç›˜è·¯å¾„çš„ç‰¹æ®Šè¦æ±‚
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†è·¯å¾„å¤„ç†åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè¦æ±‚
// dircacheè¦æ±‚ï¼šè·¯å¾„ä¸åº”è¯¥ä»¥/å¼€å¤´æˆ–ç»“å°¾
func normalizePath(path string) string {
	if path == "" {
		return ""
	}

	// ä½¿ç”¨filepath.Cleanè¿›è¡ŒåŸºç¡€è·¯å¾„æ¸…ç†
	path = filepath.Clean(path)

	// å¤„ç†filepath.Cleançš„ç‰¹æ®Šè¿”å›å€¼
	if path == "." {
		return ""
	}

	// ç§»é™¤å¼€å¤´çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimPrefix(path, "/")
	path = strings.TrimPrefix(path, "\\") // å¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦

	// ç§»é™¤ç»“å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimSuffix(path, "/")
	path = strings.TrimSuffix(path, "\\")

	// å°†åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆæ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦ï¼‰
	path = strings.ReplaceAll(path, "\\", "/")

	// æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç»“æœä¸ä»¥/å¼€å¤´æˆ–ç»“å°¾
	path = strings.Trim(path, "/")

	return path
}

// isRemoteSource æ£€æŸ¥æºå¯¹è±¡æ˜¯å¦æ¥è‡ªè¿œç¨‹äº‘ç›˜ï¼ˆéæœ¬åœ°æ–‡ä»¶ï¼‰
func (f *Fs) isRemoteSource(src fs.ObjectInfo) bool {
	// æ£€æŸ¥æºå¯¹è±¡çš„ç±»å‹ï¼Œå¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œåˆ™è®¤ä¸ºæ˜¯è¿œç¨‹æº
	// è¿™å¯ä»¥é€šè¿‡æ£€æŸ¥æºå¯¹è±¡çš„Fs()æ–¹æ³•è¿”å›çš„ç±»å‹æ¥åˆ¤æ–­
	srcFs := src.Fs()
	if srcFs == nil {
		fs.Debugf(f, " isRemoteSource: srcFsä¸ºnilï¼Œè¿”å›false")
		return false
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	fsType := srcFs.Name()

	// ä¿®å¤ï¼šæ­£ç¡®è¯†åˆ«æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	// æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„åç§°å¯èƒ½æ˜¯ "local" æˆ– "local{suffix}"
	isLocal := fsType == "local" || strings.HasPrefix(fsType, "local{")
	isRemote := !isLocal && fsType != ""

	fs.Debugf(f, " isRemoteSourceæ£€æµ‹: fsType='%s', isLocal=%v, isRemote=%v", fsType, isLocal, isRemote)

	// å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œç›´æ¥è¿”å›false
	if isLocal {
		fs.Debugf(f, " æ˜ç¡®è¯†åˆ«ä¸ºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ: %s", fsType)
		return false
	}

	// ç‰¹åˆ«æ£€æµ‹115ç½‘ç›˜å’Œå…¶ä»–äº‘ç›˜
	if strings.Contains(fsType, "115") || strings.Contains(fsType, "pan") {
		fs.Debugf(f, " æ˜ç¡®è¯†åˆ«ä¸ºäº‘ç›˜æº: %s", fsType)
		return true
	}

	return isRemote
}

// init å‡½æ•°ç”¨äºå‘Fsæ³¨å†Œ123ç½‘ç›˜åç«¯
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "123",
		Description: "123ç½‘ç›˜é©±åŠ¨å™¨",
		NewFs:       newFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name:     "client_id",
			Help:     "123ç½‘ç›˜APIå®¢æˆ·ç«¯IDã€‚",
			Required: true,
		}, {
			Name:      "client_secret",
			Help:      "123ç½‘ç›˜APIå®¢æˆ·ç«¯å¯†é’¥ã€‚",
			Required:  true,
			Sensitive: true,
		}, {
			Name:      "token",
			Help:      "OAuthè®¿é—®ä»¤ç‰Œï¼ˆJSONæ ¼å¼ï¼‰ã€‚é€šå¸¸ç•™ç©ºã€‚",
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Help:     "APIè¯·æ±‚çš„User-Agentå¤´ã€‚",
			Default:  defaultUserAgent,
			Advanced: true,
		}, {
			Name:     "root_folder_id",
			Help:     "æ ¹æ–‡ä»¶å¤¹çš„IDã€‚ç•™ç©ºè¡¨ç¤ºæ ¹ç›®å½•ã€‚",
			Default:  "0",
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ ä¸­çš„æœ€å¤§åˆ†ç‰‡æ•°ã€‚",
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name:     "upload_pacer_min_sleep",
			Help:     "ä¸Šä¼ APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚è®¾ç½®æ›´é«˜çš„å€¼å¯ä»¥é¿å…429é”™è¯¯ã€‚",
			Default:  uploadV2SliceMinSleep,
			Advanced: true,
		}, {
			Name:     "download_pacer_min_sleep",
			Help:     "ä¸‹è½½APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  downloadInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "strict_pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆmove, deleteç­‰ï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  fileMoveMinSleep,
			Advanced: true,
		}, {
			Name:     config.ConfigEncoding,
			Help:     config.ConfigEncodingHelp,
			Advanced: true,
			// é»˜è®¤åŒ…å«Slashå’ŒInvalidUtf8ç¼–ç ï¼Œé€‚é…ä¸­æ–‡æ–‡ä»¶åå’Œç‰¹æ®Šå­—ç¬¦
			Default: (encoder.EncodeCtl |
				encoder.EncodeLeftSpace |
				encoder.EncodeRightSpace |
				encoder.EncodeSlash | // æ–°å¢ï¼šé»˜è®¤ç¼–ç æ–œæ 
				encoder.EncodeInvalidUtf8), // æ–°å¢ï¼šç¼–ç æ— æ•ˆUTF-8å­—ç¬¦
		}},
	})
}

var commandHelp = []fs.CommandHelp{{
	Name:  "media-sync",
	Short: "åŒæ­¥åª’ä½“åº“å¹¶åˆ›å»ºä¼˜åŒ–çš„.strmæ–‡ä»¶",
	Long: `å°†123ç½‘ç›˜ä¸­çš„è§†é¢‘æ–‡ä»¶åŒæ­¥åˆ°æœ¬åœ°ç›®å½•ï¼Œåˆ›å»ºå¯¹åº”çš„.strmæ–‡ä»¶ã€‚
.strmæ–‡ä»¶å°†åŒ…å«ä¼˜åŒ–çš„fileIdæ ¼å¼ï¼Œæ”¯æŒç›´æ¥æ’­æ”¾å’Œåª’ä½“åº“ç®¡ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend media-sync 123:Movies /local/media/movies
rclone backend media-sync 123:Videos /local/media/videos -o min-size=200M -o strm-format=true

æ”¯æŒçš„è§†é¢‘æ ¼å¼: mp4, mkv, avi, mov, wmv, flv, webm, m4v, 3gp, ts, m2ts
.strmæ–‡ä»¶å†…å®¹æ ¼å¼: 123://fileId (å¯é€šè¿‡strm-formaté€‰é¡¹è°ƒæ•´)`,
	Opts: map[string]string{
		"min-size":    "æœ€å°æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼Œå°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†è¢«å¿½ç•¥ (é»˜è®¤: 100M)",
		"strm-format": ".strmæ–‡ä»¶å†…å®¹æ ¼å¼: true(ä¼˜åŒ–æ ¼å¼)/false(è·¯å¾„æ ¼å¼) (é»˜è®¤: trueï¼Œå…¼å®¹: fileid/path)",
		"include":     "åŒ…å«çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš” (é»˜è®¤: mp4,mkv,avi,mov,wmv,flv,webm,m4v,3gp,ts,m2ts)",
		"exclude":     "æ’é™¤çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš”",
		"update-mode": "æ›´æ–°æ¨¡å¼: full/incremental (é»˜è®¤: full)",
		"sync-delete": "åŒæ­¥åˆ é™¤: false(ä»…åˆ›å»º.strmæ–‡ä»¶)/true(åˆ é™¤å­¤ç«‹æ–‡ä»¶å’Œç©ºç›®å½•) (é»˜è®¤: trueï¼Œç±»ä¼¼rclone sync)",
		"dry-run":     "é¢„è§ˆæ¨¡å¼ï¼Œæ˜¾ç¤ºå°†è¦åˆ›å»ºçš„æ–‡ä»¶ä½†ä¸å®é™…åˆ›å»º (true/false)",
		"target-path": "ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœä¸åœ¨å‚æ•°ä¸­æŒ‡å®šåˆ™å¿…é¡»é€šè¿‡æ­¤é€‰é¡¹æä¾›",
	},
}, {
	Name:  "get-download-url",
	Short: "é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL",
	Long: `é€šè¿‡123ç½‘ç›˜çš„fileIdæˆ–.strmæ–‡ä»¶å†…å®¹è·å–å®é™…çš„ä¸‹è½½URLã€‚
æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œç‰¹åˆ«é€‚ç”¨äºåª’ä½“æœåŠ¡å™¨å’Œ.strmæ–‡ä»¶å¤„ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend get-download-url 123: "123://17995550"
rclone backend get-download-url 123: "17995550"
rclone backend get-download-url 123: "/path/to/file.mp4"
rclone backend get-download-url 123: "123://17995550" -o user-agent="Custom-UA"

è¾“å…¥æ ¼å¼æ”¯æŒ:
- 123://fileId æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
- çº¯fileId
- æ–‡ä»¶è·¯å¾„ (è‡ªåŠ¨è§£æä¸ºfileId)`,
	Opts: map[string]string{
		"user-agent": "è‡ªå®šä¹‰User-Agentå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰",
	},
},
}

// Command æ‰§è¡Œåç«¯ç‰¹å®šçš„å‘½ä»¤ã€‚
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {

	case "media-sync":
		// ğŸ¬ æ–°å¢ï¼šåª’ä½“åº“åŒæ­¥åŠŸèƒ½
		return f.mediaSyncCommand(ctx, arg, opt)

	case "get-download-url":
		// ğŸ”— æ–°å¢ï¼šé€šè¿‡fileIdè·å–ä¸‹è½½URL
		return f.getDownloadURLCommand(ctx, arg, opt)

	default:
		return nil, fs.ErrorCommandNotFound
	}
}

type ListRequest struct {
	ParentFileID int    `json:"parentFileId"`
	Limit        int    `json:"limit"`
	SearchData   string `json:"searchData,omitempty"`
	SearchMode   string `json:"searchMode,omitempty"`
	LastFileID   int    `json:"lastFileID,omitempty"`
}

type ListResponse struct {
	Code    int                   `json:"code"`
	Message string                `json:"message"`
	Data    GetFileListRespDataV2 `json:"data"` // æ›´æ”¹ä¸ºç‰¹å®šç±»å‹
}

// GetFileListRespDataV2 è¡¨ç¤ºæ–‡ä»¶åˆ—è¡¨å“åº”çš„æ•°æ®ç»“æ„
type GetFileListRespDataV2 struct {
	// -1ä»£è¡¨æœ€åä¸€é¡µï¼ˆæ— éœ€å†ç¿»é¡µæŸ¥è¯¢ï¼‰, å…¶ä»–ä»£è¡¨ä¸‹ä¸€é¡µå¼€å§‹çš„æ–‡ä»¶idï¼Œæºå¸¦åˆ°è¯·æ±‚å‚æ•°ä¸­
	LastFileId int64 `json:"lastFileId"`
	// æ–‡ä»¶åˆ—è¡¨
	FileList []FileListInfoRespDataV2 `json:"fileList"`
}

// FileListInfoRespDataV2 è¡¨ç¤ºæ¥è‡ªAPIå“åº”çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹é¡¹ç›®
type FileListInfoRespDataV2 struct {
	// æ–‡ä»¶ID
	FileID int64 `json:"fileID"`
	// æ–‡ä»¶å
	Filename string `json:"filename"`
	// 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	Type int `json:"type"`
	// æ–‡ä»¶å¤§å°
	Size int64 `json:"size"`
	// md5
	Etag string `json:"etag"`
	// æ–‡ä»¶å®¡æ ¸çŠ¶æ€, å¤§äº 100 ä¸ºå®¡æ ¸é©³å›æ–‡ä»¶
	Status int `json:"status"`
	// ç›®å½•ID
	ParentFileID int64 `json:"parentFileID"`
	// æ–‡ä»¶åˆ†ç±», 0-æœªçŸ¥ 1-éŸ³é¢‘ 2-è§†é¢‘ 3-å›¾ç‰‡
	Category int `json:"category"`
}

func (f *Fs) ListFile(ctx context.Context, parentFileID, limit int, searchData, searchMode string, lastFileID int) (*ListResponse, error) {
	fs.Debugf(f, "è°ƒç”¨ListFileï¼Œå‚æ•°ï¼šparentFileID=%d, limit=%d, lastFileID=%d", parentFileID, limit, lastFileID)

	// ç§»é™¤ç¼“å­˜æŸ¥è¯¢ï¼Œç›´æ¥è°ƒç”¨API

	// æ„é€ æŸ¥è¯¢å‚æ•°
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", limit))

	if searchData != "" {
		params.Add("searchData", searchData)
	}
	if searchMode != "" {
		params.Add("searchMode", searchMode)
	}
	if lastFileID != 0 {
		params.Add("lastFileID", fmt.Sprintf("%d", lastFileID))
	}

	// æ„é€ å¸¦æŸ¥è¯¢å‚æ•°çš„ç«¯ç‚¹
	endpoint := "/api/v2/file/list?" + params.Encode()
	fs.Debugf(f, "APIç«¯ç‚¹: %s%s", openAPIRootURL, endpoint)

	// ç›´æ¥ä½¿ç”¨v2 APIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var result ListResponse
	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &result)
	if err != nil {
		fs.Debugf(f, "ListFile APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "ListFile APIå“åº”: code=%d, message=%s, fileCount=%d", result.Code, result.Message, len(result.Data.FileList))

	// ç§»é™¤ç¼“å­˜ä¿å­˜ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

	return &result, nil
}

func (f *Fs) pathToFileID(ctx context.Context, filePath string) (string, error) {
	fs.Debugf(f, " pathToFileIDå¼€å§‹: filePath=%s", filePath)

	// æ ¹ç›®å½•
	if filePath == "/" {
		return "0", nil
	}
	if len(filePath) > 1 && strings.HasSuffix(filePath, "/") {
		filePath = filePath[:len(filePath)-1]
	}

	// ç§»é™¤è·¯å¾„æ˜ å°„ç¼“å­˜æŸ¥è¯¢ï¼Œç›´æ¥è§£æè·¯å¾„
	currentID := "0"
	parts := strings.Split(filePath, "/")
	if len(parts) > 0 && parts[0] == "" { // å¦‚æœè·¯å¾„ä»¥/å¼€å¤´ï¼Œç§»é™¤ç©ºå­—ç¬¦ä¸²
		parts = parts[1:]
	}
	for _, part := range parts {
		if part == "" {
			continue
		}
		findPart := false
		next := "0"           // æ ¹æ®APIä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºnext
		maxIterations := 1000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š1000æ¬¡è¿­ä»£
		iteration := 0
		for {
			iteration++
			if iteration > maxIterations {
				return "", fmt.Errorf("æŸ¥æ‰¾è·¯å¾„ %s è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", part, maxIterations)
			}
			parentFileID, err := strconv.Atoi(currentID)
			if err != nil {
				return "", fmt.Errorf("invalid parentFileId: %s", currentID)
			}

			lastFileIDInt, err := strconv.Atoi(next)
			if err != nil {
				return "", fmt.Errorf("invalid next token: %s", next)
			}

			var response *ListResponse
			// ä½¿ç”¨åˆ—è¡¨è°ƒé€Ÿå™¨è¿›è¡Œé€Ÿç‡é™åˆ¶
			err = f.listPacer.Call(func() (bool, error) {
				response, err = f.ListFile(ctx, parentFileID, 100, "", "", lastFileIDInt)
				if err != nil {
					return shouldRetry(ctx, nil, err)
				}
				return false, nil
			})
			if err != nil {
				return "", fmt.Errorf("list file failed: %w", err)
			}
			if response.Code != 200 && response.Code != 0 {
				return "", fmt.Errorf("API returned error code %d: %s", response.Code, response.Message)
			}
			infoList := response.Data.FileList
			if len(infoList) == 0 {
				break
			}
			for _, item := range infoList {
				if item.Filename == part {
					currentID = strconv.FormatInt(item.FileID, 10)
					findPart = true

					// è®°å½•æ‰¾åˆ°çš„é¡¹ç›®ç±»å‹ä¿¡æ¯ï¼Œç”¨äºåç»­ç¼“å­˜
					isDir := (item.Type == 1) // Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
					fs.Debugf(f, "pathToFileIDæ‰¾åˆ°é¡¹ç›®: %s -> ID=%s, Type=%d, isDir=%v", part, currentID, item.Type, isDir)

					// ç§»é™¤è·¯å¾„ç±»å‹ç¼“å­˜ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

					break
				}
			}
			if findPart {
				break
			}

			nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
			if nextRaw == "-1" {
				break
			} else {
				next = nextRaw
				// æ— éœ€æ‰‹åŠ¨ç¡çœ  - è°ƒé€Ÿå™¨å¤„ç†é€Ÿç‡é™åˆ¶
			}
		}
		if !findPart {
			return "", fs.ErrorObjectNotFound
		}
	}

	if currentID == "0" && filePath != "/" {
		return "", fs.ErrorObjectNotFound
	}

	// ç¼“å­˜è·¯å¾„æ˜ å°„ç»“æœå·²åœ¨è·¯å¾„è§£æå¾ªç¯ä¸­æ­£ç¡®å¤„ç†

	fs.Debugf(f, " pathToFileIDç»“æœ: filePath=%s -> currentID=%s", filePath, currentID)
	return currentID, nil
}

type FileDetail struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

type FileInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		List []FileDetail `json:"list"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

type FileDetailResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID       int64  `json:"fileID"`
		Filename     string `json:"filename"`
		Type         int    `json:"type"`
		Size         int64  `json:"size"`
		ETag         string `json:"etag"`
		Status       int    `json:"status"`
		ParentFileID int64  `json:"parentFileID"`
		CreateAt     string `json:"createAt"`
		Trashed      int    `json:"trashed"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

// FileInfo è¡¨ç¤º'list'æ•°ç»„ä¸­å•ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
type FileInfo struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

// FileInfosResponse æ˜¯APIå“åº”çš„é¡¶çº§ç»“æ„

// å®šä¹‰Dataç»“æ„ä½“ï¼Œæ˜ å°„dataå­—æ®µçš„å†…å®¹
type Data struct {
	FileList []FileInfo `json:"fileList"`
}

// å®šä¹‰FileInfosResponseç»“æ„ä½“ï¼Œæ˜ å°„æ•´ä¸ªå“åº”JSON
type FileInfosResponse struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     Data   `json:"data"`
	XTraceID string `json:"x-traceID"`
}

// å®šä¹‰FileInfoRequestç»“æ„ä½“ï¼Œç”¨äºå‘é€è¯·æ±‚çš„payload
type FileInfoRequest struct {
	FileIDs []int64 `json:"fileIDs"`
}

type DownloadInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		DownloadURL string `json:"downloadUrl"` // ç›´é“¾åœ°å€
		ExpireTime  string `json:"expireTime"`  // è¿‡æœŸæ—¶é—´
	} `json:"data"`
}

type UploadCreateResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID      int64  `json:"fileID"`
		PreuploadID string `json:"preuploadID"`
		Reuse       bool   `json:"reuse"`
		SliceSize   int64  `json:"sliceSize"`
	} `json:"data"`
}

// ç§»é™¤UploadProgresså¤æ‚çŠ¶æ€ç®¡ç†ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶

type UserInfoResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		UID            int64  `json:"uid"`
		Username       string `json:"username"`
		DisplayName    string `json:"displayName"`
		HeadImage      string `json:"headImage"`
		Passport       string `json:"passport"`
		Mail           string `json:"mail"`
		SpaceUsed      int64  `json:"spaceUsed"`
		SpacePermanent int64  `json:"spacePermanent"`
		SpaceTemp      int64  `json:"spaceTemp"`
		SpaceTempExpr  int64  `json:"spaceTempExpr"` // ä¿®å¤ï¼šAPIè¿”å›æ•°å­—è€Œéå­—ç¬¦ä¸²
		Vip            bool   `json:"vip"`
		DirectTraffic  int64  `json:"directTraffic"`
		IsHideUID      bool   `json:"isHideUID"`
	} `json:"data"`
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, userAgent string) (string, error) {
	// Ensure token is valid before making API calls
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return "", err
	}

	// è®°å½•ä½¿ç”¨çš„ User-Agent
	if userAgent != "" {
		fs.Debugf(f, "ğŸŒ 123ç½‘ç›˜ä½¿ç”¨è‡ªå®šä¹‰User-Agent: %s", userAgent)
	} else {
		fs.Debugf(f, "ğŸŒ 123ç½‘ç›˜ä½¿ç”¨é»˜è®¤User-Agent")
	}

	if filePath == "" {
		filePath = f.root
	}

	fileID, err := f.pathToFileID(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to get file ID for path %q: %w", filePath, err)
	}

	// æ³¨æ„ï¼š123ç½‘ç›˜å½“å‰çš„getDownloadURLæ–¹æ³•ä¸ç›´æ¥æ”¯æŒè‡ªå®šä¹‰UA
	// ä½†æˆ‘ä»¬è®°å½•äº†UAå‚æ•°ï¼Œä¸ºå°†æ¥çš„å®ç°åšå‡†å¤‡
	fs.Debugf(f, "ğŸ”„ 123ç½‘ç›˜é€šè¿‡è·¯å¾„è·å–ä¸‹è½½URL: è·¯å¾„=%s, fileId=%s", filePath, fileID)

	// Use the standard getDownloadURL method
	return f.getDownloadURL(ctx, fileID)
}

// getDownloadURLCommand é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL
func (f *Fs) getDownloadURLCommand(ctx context.Context, args []string, opt map[string]string) (any, error) {
	if len(args) < 1 {
		return nil, fmt.Errorf("éœ€è¦æä¾›fileIdã€123://fileIdæ ¼å¼æˆ–æ–‡ä»¶è·¯å¾„")
	}

	input := args[0]
	fs.Debugf(f, "ğŸ”— å¤„ç†ä¸‹è½½URLè¯·æ±‚: %s", input)

	var fileID string
	var err error

	// è§£æè¾“å…¥æ ¼å¼
	if fileID, found := strings.CutPrefix(input, "123://"); found {
		// 123://fileId æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
		fs.Debugf(f, "âœ… è§£æ.strmæ ¼å¼: fileId=%s", fileID)
	} else if strings.HasPrefix(input, "/") {
		// æ–‡ä»¶è·¯å¾„æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºfileId
		fs.Debugf(f, "ğŸ” è§£ææ–‡ä»¶è·¯å¾„: %s", input)
		fileID, err = f.pathToFileID(ctx, input)
		if err != nil {
			return nil, fmt.Errorf("è·¯å¾„è½¬æ¢fileIdå¤±è´¥ %q: %w", input, err)
		}
		fs.Debugf(f, "âœ… è·¯å¾„è½¬æ¢æˆåŠŸ: %s -> %s", input, fileID)
	} else {
		// å‡è®¾æ˜¯çº¯fileId
		fileID = input
		fs.Debugf(f, "âœ… ä½¿ç”¨çº¯fileId: %s", fileID)
	}

	// éªŒè¯fileIdæ ¼å¼
	if fileID == "" {
		return nil, fmt.Errorf("æ— æ•ˆçš„fileId: %s", input)
	}

	// è·å–ä¸‹è½½URL
	userAgent := opt["user-agent"]
	if userAgent != "" {
		// å¦‚æœæä¾›äº†è‡ªå®šä¹‰UAï¼Œä½¿ç”¨getDownloadURLByUAæ–¹æ³•
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URL: fileId=%s, UA=%s", fileID, userAgent)

		// éœ€è¦å…ˆå°†fileIDè½¬æ¢å›è·¯å¾„ï¼Œå› ä¸ºgetDownloadURLByUAéœ€è¦è·¯å¾„å‚æ•°
		var filePath string
		if strings.HasPrefix(input, "/") {
			filePath = input // å¦‚æœåŸå§‹è¾“å…¥æ˜¯è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
		} else {
			// å¦‚æœåŸå§‹è¾“å…¥æ˜¯fileIDï¼Œæˆ‘ä»¬æ— æ³•è½»æ˜“è½¬æ¢å›è·¯å¾„ï¼Œæ‰€ä»¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•
			fs.Debugf(f, "âš ï¸ è‡ªå®šä¹‰UAä»…æ”¯æŒè·¯å¾„è¾“å…¥ï¼ŒfileIDè¾“å…¥å°†ä½¿ç”¨æ ‡å‡†æ–¹æ³•")
			downloadURL, err := f.getDownloadURL(ctx, fileID)
			if err != nil {
				return nil, fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
			}
			fs.Infof(f, "âœ… æˆåŠŸè·å–123ç½‘ç›˜ä¸‹è½½URL: fileId=%s (æ ‡å‡†æ–¹æ³•)", fileID)
			return downloadURL, nil
		}

		downloadURL, err := f.getDownloadURLByUA(ctx, filePath, userAgent)
		if err != nil {
			return nil, fmt.Errorf("ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URLå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "Got download URL with custom UA: fileId=%s", fileID)
		return downloadURL, nil
	} else {
		downloadURL, err := f.getDownloadURL(ctx, fileID)
		if err != nil {
			return nil, fmt.Errorf("failed to get download URL: %w", err)
		}
		fs.Debugf(f, "Got download URL: fileId=%s", fileID)
		return downloadURL, nil
	}
}

// Remove redundant validation functions - use standard validation instead

// validateDuration éªŒè¯æ—¶é—´é…ç½®ä¸ä¸ºè´Ÿæ•°
func validateDuration(fieldName string, value time.Duration) error {
	if value < 0 {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºè´Ÿæ•°", fieldName)
	}
	return nil
}

// ç§»é™¤æœªä½¿ç”¨çš„validateSizeå‡½æ•°

// validateOptions éªŒè¯é…ç½®é€‰é¡¹çš„æœ‰æ•ˆæ€§
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»“æ„åŒ–éªŒè¯æ–¹æ³•ï¼Œå‡å°‘é‡å¤ä»£ç 
func validateOptions(opt *Options) error {
	var errors []string

	// Validate required authentication info
	if opt.ClientID == "" {
		errors = append(errors, "client_id cannot be empty")
	}
	if opt.ClientSecret == "" {
		errors = append(errors, "client_secret cannot be empty")
	}

	// Validate numeric ranges
	if opt.MaxUploadParts <= 0 || opt.MaxUploadParts > MaxUploadPartsLimit {
		errors = append(errors, fmt.Sprintf("max_upload_parts must be between 1-%d", MaxUploadPartsLimit))
	}

	// éªŒè¯123ç½‘ç›˜ç‰¹å®šçš„æ—¶é—´é…ç½®
	if err := validateDuration("upload_pacer_min_sleep", time.Duration(opt.UploadPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("download_pacer_min_sleep", time.Duration(opt.DownloadPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("strict_pacer_min_sleep", time.Duration(opt.StrictPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯UserAgenté•¿åº¦
	if opt.UserAgent != "" && len(opt.UserAgent) > 200 {
		errors = append(errors, "user_agent é•¿åº¦ä¸èƒ½è¶…è¿‡ 200 å­—ç¬¦")
	}

	// éªŒè¯RootFolderIDæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
	if opt.RootFolderID != "" {
		if _, err := parseFileID(opt.RootFolderID); err != nil {
			errors = append(errors, "root_folder_id å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•°å­—")
		}
	}

	// è¿”å›èšåˆçš„é”™è¯¯ä¿¡æ¯
	if len(errors) > 0 {
		return fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %s", strings.Join(errors, "; "))
	}

	return nil
}

// createPacer åˆ›å»ºpacerçš„å·¥å‚å‡½æ•°ï¼Œå‡å°‘é‡å¤é…ç½®ä»£ç 
func createPacer(ctx context.Context, minSleep, maxSleep time.Duration, decayConstant float64) *fs.Pacer {
	return fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(minSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))
}

// normalizeOptions æ ‡å‡†åŒ–å’Œä¿®æ­£é…ç½®é€‰é¡¹
func normalizeOptions(opt *Options) {
	// è®¾ç½®123ç½‘ç›˜ç‰¹æœ‰é…ç½®çš„é»˜è®¤å€¼
	if opt.MaxUploadParts <= 0 {
		opt.MaxUploadParts = DefaultMaxUploadParts
	}

	// è®¾ç½®123ç½‘ç›˜ç‰¹å®šçš„paceré»˜è®¤å€¼
	if time.Duration(opt.UploadPacerMinSleep) <= 0 {
		opt.UploadPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.DownloadPacerMinSleep) <= 0 {
		opt.DownloadPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.StrictPacerMinSleep) <= 0 {
		opt.StrictPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}

	// è®¾ç½®é»˜è®¤UserAgent
	if opt.UserAgent == "" {
		opt.UserAgent = "rclone/" + fs.Version
	}
}

// ç§»é™¤æœªä½¿ç”¨çš„æ ¡éªŒå’Œç›¸å…³å‡½æ•°

// ç§»é™¤æœªä½¿ç”¨çš„fastValidateCacheEntryå‡½æ•°

// shouldRetry æ ¹æ®å“åº”å’Œé”™è¯¯ç¡®å®šæ˜¯å¦é‡è¯•APIè°ƒç”¨ï¼ˆæœ¬åœ°å®ç°ï¼Œæ›¿ä»£commonåŒ…ï¼‰
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„é‡è¯•åˆ¤æ–­
	return fserrors.ShouldRetry(err), err
}

// Remove redundant error handler functions - use rclone standard shouldRetry

// ç§»é™¤getUnifiedConfigå‡½æ•°ï¼Œä¸å†ä½¿ç”¨ç»Ÿä¸€é…ç½®

// æœ¬åœ°å·¥å…·å‡½æ•°ï¼Œæ›¿ä»£commonåŒ…çš„åŠŸèƒ½

// isLargeFileUpload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¤§æ–‡ä»¶ä¸Šä¼ æ¨¡å¼ - ç®€åŒ–ç‰ˆæœ¬
func (f *Fs) isLargeFileUpload(fileSize int64) bool {
	// ä½¿ç”¨ç®€å•çš„é˜ˆå€¼åˆ¤æ–­ï¼Œå¤§äº100MBä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ 
	return fileSize > 100*1024*1024
}

// getOptimalChunkCount æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜åˆ†ç‰‡æ•°é‡ - ç®€åŒ–ç‰ˆæœ¬
func (f *Fs) getOptimalChunkCount(fileSize int64) int {
	// ç®€åŒ–è®¡ç®—ï¼Œæ¯ä¸ªåˆ†ç‰‡10MB
	chunkSize := int64(10 * 1024 * 1024)
	count := int(fileSize / chunkSize)
	if fileSize%chunkSize != 0 {
		count++
	}
	if count > f.opt.MaxUploadParts {
		count = f.opt.MaxUploadParts
	}
	return count
}

// getAdjustedChunkSize æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´åˆ†ç‰‡å¤§å° - ç®€åŒ–ç‰ˆæœ¬
func (f *Fs) getAdjustedChunkSize(fileSize int64) fs.SizeSuffix {
	// ä½¿ç”¨å›ºå®šçš„åˆ†ç‰‡å¤§å°
	return fs.SizeSuffix(10 * 1024 * 1024) // 10MB
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡ŒAPIè°ƒç”¨ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶
// è¿™æ˜¯æ¨èçš„APIè°ƒç”¨æ–¹æ³•ï¼Œæ›¿ä»£ç›´æ¥ä½¿ç”¨HTTPå®¢æˆ·ç«¯
func (f *Fs) makeAPICallWithRest(ctx context.Context, endpoint string, method string, reqBody any, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API: %s %s", method, endpoint)

	// å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿restå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
	if f.rst == nil {
		fs.Errorf(f, "âš ï¸  restå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆ›å»º")
		// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºrestå®¢æˆ·ç«¯
		client := fshttp.NewClient(context.Background())
		f.rst = rest.NewClient(client).SetRoot(openAPIRootURL)
		fs.Debugf(f, " restå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºæˆåŠŸ")
	}

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - æ ¹æ®å®˜æ–¹æ–‡æ¡£åªæœ‰ä¸¤ä¸ªAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		// åªæœ‰åˆ†ç‰‡ä¸Šä¼ å’Œå•æ­¥ä¸Šä¼ ä½¿ç”¨ä¸Šä¼ åŸŸåï¼ˆå®˜æ–¹æ–‡æ¡£æ˜ç¡®è¯´æ˜ï¼‰
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		// å…¶ä»–APIï¼ˆåŒ…æ‹¬createã€upload_completeï¼‰ä½¿ç”¨æ ‡å‡†APIåŸŸå
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.CallJSON(ctx, &opts, reqBody, respBody)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		return shouldRetry(ctx, resp, err)
	})

	if err != nil {
		return fmt.Errorf("APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// makeAPICallWithRestMultipartToDomain ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨åˆ°æŒ‡å®šåŸŸå
func (f *Fs) makeAPICallWithRestMultipartToDomain(ctx context.Context, domain string, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart APIåˆ°åŸŸå: %s %s %s", method, domain, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ä½¿ç”¨è°ƒé€Ÿå™¨è¿›è¡ŒAPIè°ƒç”¨
	return pacer.Call(func() (bool, error) {
		opts := rest.Opts{
			Method:  method,
			RootURL: domain,
			Path:    endpoint,
			Body:    body,
			ExtraHeaders: map[string]string{
				"Content-Type": contentType,
			},
		}

		// æ·»åŠ å¿…è¦çš„è®¤è¯å¤´
		opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
		opts.ExtraHeaders["Platform"] = "open_platform"
		opts.ExtraHeaders["User-Agent"] = f.opt.UserAgent

		var resp *http.Response
		var err error
		resp, err = f.rst.Call(ctx, &opts)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
			if fserrors.ShouldRetry(err) {
				fs.Debugf(f, "APIè°ƒç”¨å¤±è´¥ï¼Œå°†é‡è¯•: %v", err)
				return true, err
			}
			return false, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
		}
		defer resp.Body.Close()

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode >= 400 {
			body, _ := io.ReadAll(resp.Body)

			// ç‰¹æ®Šå¤„ç†401é”™è¯¯ - tokenè¿‡æœŸ
			if resp.StatusCode == http.StatusUnauthorized {
				fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
				// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
				refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
				if refreshErr != nil {
					fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
					return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
				}
				// æ›´æ–°Authorizationå¤´
				opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
				fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
				return true, nil // é‡è¯•
			}

			return false, fmt.Errorf("APIè¿”å›é”™è¯¯çŠ¶æ€ %d: %s", resp.StatusCode, string(body))
		}

		// è§£æå“åº”JSON
		if respBody != nil {
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”JSONå¤±è´¥: %w", err)
			}
		}

		return false, nil
	})
}

// makeAPICallWithRestMultipart ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨
func (f *Fs) makeAPICallWithRestMultipart(ctx context.Context, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart API: %s %s", method, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - ç‰¹å®šAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		Body:    body,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
			"Content-Type":  contentType,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.Call(ctx, &opts)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// è§£æå“åº”
		if respBody != nil {
			defer resp.Body.Close()
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”å¤±è´¥: %w", err)
			}
		}

		return false, nil
	})

	if err != nil {
		return fmt.Errorf("multipart APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// getPacerForEndpoint æ ¹æ®APIç«¯ç‚¹è¿”å›é€‚å½“çš„è°ƒé€Ÿå™¨
// åŸºäºå®˜æ–¹APIé™æµæ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
// æ›´æ–°æ—¶é—´ï¼š2025-07-04ï¼Œä½¿ç”¨æœ€æ–°å®˜æ–¹QPSé™åˆ¶è¡¨
func (f *Fs) getPacerForEndpoint(endpoint string) *fs.Pacer {
	switch {
	// é«˜é¢‘ç‡API (15-20 QPS)
	case strings.Contains(endpoint, "/api/v2/file/list"):
		return f.listPacer // ~14 QPS (å®˜æ–¹15 QPS)
	case strings.Contains(endpoint, "/upload/v2/file/get_upload_url"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"):
		return f.downloadPacer // ~16 QPS (å®˜æ–¹20 QPS)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS)
	case strings.Contains(endpoint, "/api/v1/file/move"),
		strings.Contains(endpoint, "/api/v1/file/name"),
		strings.Contains(endpoint, "/api/v1/file/infos"),
		strings.Contains(endpoint, "/api/v1/user/info"):
		return f.listPacer // ~8 QPS (å®˜æ–¹10 QPS)
	case strings.Contains(endpoint, "/api/v1/access_token"):
		return f.downloadPacer // ~6 QPS (å®˜æ–¹8 QPS)

	// ä½é¢‘ç‡API (5 QPS)
	case strings.Contains(endpoint, "/upload/v1/file/create"),
		strings.Contains(endpoint, "/upload/v2/file/create"),
		strings.Contains(endpoint, "/upload/v1/file/mkdir"),
		strings.Contains(endpoint, "/api/v1/file/trash"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"),
		strings.Contains(endpoint, "/api/v1/file/download_info"):
		return f.strictPacer // ~4 QPS (å®˜æ–¹5 QPS)

	// æœ€ä½é¢‘ç‡API (1 QPS)
	case strings.Contains(endpoint, "/api/v1/file/delete"),
		strings.Contains(endpoint, "/api/v1/file/list"),
		strings.Contains(endpoint, "/api/v1/video/transcode/list"):
		return f.strictPacer // ~0.8 QPS (å®˜æ–¹1 QPS) - ä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶

	// v2åˆ†ç‰‡ä¸Šä¼ API (ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨ä¸“ç”¨çš„uploadPacer)
	case strings.Contains(endpoint, "/upload/v2/file/slice"):
		return f.uploadPacer // ~5 QPS (åŸºäºå®˜æ–¹æ–‡æ¡£å’Œå®é™…æµ‹è¯•)

	// ä¸Šä¼ åŸŸåAPI (ç‰¹æ®Šå¤„ç†)
	case strings.Contains(endpoint, "/upload/v2/file/domain"):
		return f.strictPacer // ä¿å®ˆå¤„ç†ï¼Œé¿å…é¢‘ç¹è°ƒç”¨

	default:
		// ä¸ºäº†å®‰å…¨èµ·è§ï¼ŒæœªçŸ¥ç«¯ç‚¹é»˜è®¤ä½¿ç”¨æœ€ä¸¥æ ¼çš„è°ƒé€Ÿå™¨
		fs.Debugf(f, "âš ï¸  æœªçŸ¥APIç«¯ç‚¹ï¼Œä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶: %s", endpoint)
		return f.strictPacer // æœ€ä¸¥æ ¼çš„é™åˆ¶
	}
}

// ErrorContext é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
type ErrorContext struct {
	Operation string            // æ“ä½œç±»å‹
	Method    string            // HTTPæ–¹æ³•
	Endpoint  string            // APIç«¯ç‚¹
	FileID    string            // æ–‡ä»¶IDï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	FileName  string            // æ–‡ä»¶åï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	Extra     map[string]string // é¢å¤–ä¿¡æ¯
}

// Remove redundant error wrapper - use standard fmt.Errorf

// ç§»é™¤æœªä½¿ç”¨çš„ResumeInfoæ–­ç‚¹ç»­ä¼ ç»“æ„ä½“ï¼Œç®€åŒ–ä»£ç 

// ç§»é™¤saveUploadProgressã€loadUploadProgressã€removeUploadProgresså‡½æ•°
// ä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶ï¼Œæ— éœ€å¤æ‚çš„çŠ¶æ€ç®¡ç†

// FileIntegrityVerifier æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å™¨
// åŠŸèƒ½å¢å¼ºï¼šå®ç°æ–‡ä»¶å®Œæ•´æ€§éªŒè¯æœºåˆ¶
type FileIntegrityVerifier struct {
	fs *Fs
}

// NewFileIntegrityVerifier åˆ›å»ºæ–‡ä»¶å®Œæ•´æ€§éªŒè¯å™¨
func NewFileIntegrityVerifier(fs *Fs) *FileIntegrityVerifier {
	return &FileIntegrityVerifier{fs: fs}
}

// VerifyFileIntegrity éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
// åŠŸèƒ½å¢å¼ºï¼šæ”¯æŒå¤šç§å“ˆå¸Œç®—æ³•çš„æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
func (fiv *FileIntegrityVerifier) VerifyFileIntegrity(ctx context.Context, filePath string, expectedMD5, expectedSHA1 string, fileSize int64) (*IntegrityResult, error) {
	result := &IntegrityResult{
		FilePath:     filePath,
		FileSize:     fileSize,
		ExpectedMD5:  expectedMD5,
		ExpectedSHA1: expectedSHA1,
		StartTime:    time.Now(),
	}

	// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	fileInfo, err := os.Stat(filePath)
	if err != nil {
		result.Error = fmt.Sprintf("æ–‡ä»¶ä¸å­˜åœ¨: %v", err)
		return result, err
	}

	// éªŒè¯æ–‡ä»¶å¤§å°
	actualSize := fileInfo.Size()
	if actualSize != fileSize {
		result.Error = fmt.Sprintf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›=%d, å®é™…=%d", fileSize, actualSize)
		result.SizeMatch = false
		return result, fmt.Errorf("æ–‡ä»¶å¤§å°éªŒè¯å¤±è´¥")
	}
	result.SizeMatch = true

	// æ‰“å¼€æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—
	file, err := os.Open(filePath)
	if err != nil {
		result.Error = fmt.Sprintf("æ— æ³•æ‰“å¼€æ–‡ä»¶: %v", err)
		return result, err
	}
	defer file.Close()

	// Use standard MD5 calculation
	hasher := md5.New()
	if _, err := io.Copy(hasher, file); err != nil {
		result.Error = fmt.Sprintf("è®¡ç®—MD5å¤±è´¥: %v", err)
		return result, err
	}

	// Get calculation result
	result.ActualMD5 = fmt.Sprintf("%x", hasher.Sum(nil))
	result.ActualSHA1 = "" // æš‚æ—¶ä¸è®¡ç®—SHA1ï¼Œå‡å°‘è®¡ç®—å¼€é”€
	result.EndTime = time.Now()
	result.Duration = result.EndTime.Sub(result.StartTime)

	// éªŒè¯MD5
	if expectedMD5 != "" {
		result.MD5Match = strings.EqualFold(result.ActualMD5, expectedMD5)
		if !result.MD5Match {
			result.Error = fmt.Sprintf("MD5ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedMD5, result.ActualMD5)
		}
	}

	// éªŒè¯SHA1
	if expectedSHA1 != "" {
		result.SHA1Match = strings.EqualFold(result.ActualSHA1, expectedSHA1)
		if !result.SHA1Match {
			if result.Error != "" {
				result.Error += "; "
			}
			result.Error += fmt.Sprintf("SHA1ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedSHA1, result.ActualSHA1)
		}
	}

	// åˆ¤æ–­æ•´ä½“éªŒè¯ç»“æœ
	result.IsValid = result.SizeMatch &&
		(expectedMD5 == "" || result.MD5Match) &&
		(expectedSHA1 == "" || result.SHA1Match)

	if result.IsValid {
		fs.Debugf(fiv.fs, " æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡: %s (è€—æ—¶: %v)", filePath, result.Duration)
	} else {
		fs.Debugf(fiv.fs, "âŒ æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å¤±è´¥: %s - %s", filePath, result.Error)
	}

	return result, nil
}

// IntegrityResult å®Œæ•´æ€§éªŒè¯ç»“æœ
type IntegrityResult struct {
	FilePath     string        `json:"file_path"`
	FileSize     int64         `json:"file_size"`
	ExpectedMD5  string        `json:"expected_md5"`
	ExpectedSHA1 string        `json:"expected_sha1"`
	ActualMD5    string        `json:"actual_md5"`
	ActualSHA1   string        `json:"actual_sha1"`
	SizeMatch    bool          `json:"size_match"`
	MD5Match     bool          `json:"md5_match"`
	SHA1Match    bool          `json:"sha1_match"`
	IsValid      bool          `json:"is_valid"`
	StartTime    time.Time     `json:"start_time"`
	EndTime      time.Time     `json:"end_time"`
	Duration     time.Duration `json:"duration"`
	Error        string        `json:"error,omitempty"`
}

// ç§»é™¤æœªä½¿ç”¨çš„AccessInfoå’ŒEnhancedDownloadURLEntryç»“æ„ä½“

// parseFileID é€šç”¨çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
func parseFileID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseFileIDWithContext å¸¦ä¸Šä¸‹æ–‡çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
func parseFileIDWithContext(idStr, context string) (int64, error) {
	id, err := parseFileID(idStr)
	if err != nil {
		return 0, fmt.Errorf("%s: %w", context, err)
	}
	return id, nil
}

// parseParentID è§£æçˆ¶ç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºçˆ¶ç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseParentID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„çˆ¶ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseDirID è§£æç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseDirID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// getFileInfo æ ¹æ®IDè·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
func (f *Fs) getFileInfo(ctx context.Context, fileID string) (*FileListInfoRespDataV2, error) {
	fs.Debugf(f, " getFileInfoå¼€å§‹: fileID=%s", fileID)

	// éªŒè¯æ–‡ä»¶ID
	_, err := parseFileIDWithContext(fileID, "è·å–æ–‡ä»¶ä¿¡æ¯")
	if err != nil {
		fs.Debugf(f, " getFileInfoæ–‡ä»¶IDéªŒè¯å¤±è´¥: %v", err)
		return nil, err
	}

	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…API - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response FileDetailResponse
	endpoint := fmt.Sprintf("/api/v1/file/detail?fileID=%s", fileID)

	fs.Debugf(f, " getFileInfoè°ƒç”¨API: %s", endpoint)
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		fs.Debugf(f, " getFileInfo APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, " getFileInfo APIå“åº”: code=%d, message=%s", response.Code, response.Message)
	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ‡å‡†æ ¼å¼
	fileInfo := &FileListInfoRespDataV2{
		FileID:       response.Data.FileID,
		Filename:     response.Data.Filename,
		Type:         response.Data.Type,
		Size:         response.Data.Size,
		Etag:         response.Data.ETag,
		Status:       response.Data.Status,
		ParentFileID: response.Data.ParentFileID,
		Category:     0, // è¯¦æƒ…å“åº”ä¸­æœªæä¾›
	}

	fs.Debugf(f, " getFileInfoæˆåŠŸ: fileID=%s, filename=%s, type=%d, size=%d", fileID, fileInfo.Filename, fileInfo.Type, fileInfo.Size)
	return fileInfo, nil
}

// getDownloadURL è·å–æ–‡ä»¶çš„ä¸‹è½½URL
func (f *Fs) getDownloadURL(ctx context.Context, fileID string) (string, error) {
	// ä¿®å¤URLé¢‘ç¹è·å–ï¼šå‡å°‘APIè°ƒç”¨é¢‘ç‡
	fs.Debugf(f, "123ç½‘ç›˜è·å–ä¸‹è½½URL: fileID=%s", fileID)

	var response DownloadInfoResponse
	endpoint := fmt.Sprintf("/api/v1/file/download_info?fileID=%s", fileID)

	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, " 123ç½‘ç›˜ä¸‹è½½URLè·å–æˆåŠŸ: fileID=%s", fileID)

	return response.Data.DownloadURL, nil
}

// createDirectory åˆ›å»ºæ–°ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™è¿”å›ç°æœ‰ç›®å½•çš„ID
func (f *Fs) createDirectory(ctx context.Context, parentID, name string) (string, error) {
	// éªŒè¯å‚æ•°
	if name == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if parentID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯ç›®å½•å
	if err := validateFileName(name); err != nil {
		return "", fmt.Errorf("ç›®å½•åéªŒè¯å¤±è´¥: %w", err)
	}

	// å°†çˆ¶ç›®å½•IDè½¬æ¢ä¸ºint64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return "", err
	}

	// é¦–å…ˆæ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„APIè°ƒç”¨
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• '%s' æ˜¯å¦å·²å­˜åœ¨äºçˆ¶ç›®å½• %s", name, parentID)
	existingID, found, err := f.FindLeaf(ctx, parentID, name)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç°æœ‰ç›®å½•æ—¶å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
	} else if found {
		fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼ŒID: %s", name, existingID)
		return existingID, nil
	}

	// å¦‚æœå¸¸è§„æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼ˆé˜²æ­¢ç¼“å­˜é—®é¢˜ï¼‰
	if !found && err == nil {
		fs.Debugf(f, "å¸¸è§„æŸ¥æ‰¾æœªæ‰¾åˆ°ç›®å½• '%s'ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾", name)
		existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
		if err != nil {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
		} else if found {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°ç›®å½• '%s'ï¼ŒID: %s", name, existingID)
			return existingID, nil
		}
	}

	// ç›®å½•ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»º
	fs.Debugf(f, "ç›®å½• '%s' ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º", name)
	reqBody := map[string]any{
		"parentID": strconv.FormatInt(parentFileID, 10),
		"name":     name,
	}

	// è¿›è¡ŒAPIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    any    `json:"data"` // ä½¿ç”¨anyå› ä¸ºä¸ç¡®å®šå…·ä½“ç»“æ„
	}

	err = f.makeAPICallWithRest(ctx, "/upload/v1/file/mkdir", "POST", reqBody, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯
		if response.Code == 1 && strings.Contains(response.Message, "å·²ç»æœ‰åŒåæ–‡ä»¶å¤¹") {
			fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼Œå¼ºåˆ¶æ¸…ç†ç¼“å­˜åæŸ¥æ‰¾ç°æœ‰ç›®å½•ID", name)

			// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨åŒæ­¥
			time.Sleep(100 * time.Millisecond)

			// æŸ¥æ‰¾ç°æœ‰ç›®å½•çš„ID - ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼
			existingID, found, err := f.findLeafWithForceRefresh(ctx, parentID, name)
			if err != nil {
				return "", fmt.Errorf("æŸ¥æ‰¾ç°æœ‰ç›®å½•å¤±è´¥: %w", err)
			}
			if !found {
				// å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼ˆå¯èƒ½æ˜¯APIå»¶è¿Ÿï¼‰
				fs.Debugf(f, "ç¬¬ä¸€æ¬¡æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•é‡è¯•æŸ¥æ‰¾ç›®å½• '%s'", name)
				for retry := 0; retry < 3; retry++ {
					time.Sleep(time.Duration(retry+1) * 200 * time.Millisecond)
					existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
					if err == nil && found {
						break
					}
					fs.Debugf(f, "é‡è¯•ç¬¬%dæ¬¡æŸ¥æ‰¾ç›®å½• '%s': found=%v, err=%v", retry+1, name, found, err)
				}

				if !found {
					return "", fmt.Errorf("ç›®å½•å·²å­˜åœ¨ä½†æ— æ³•æ‰¾åˆ°å…¶IDï¼Œå¯èƒ½æ˜¯APIåŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜")
				}
			}

			fs.Debugf(f, "æ‰¾åˆ°ç°æœ‰ç›®å½•ID: %s", existingID)
			return existingID, nil
		}
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// åˆ›å»ºæˆåŠŸï¼Œä½†APIå¯èƒ½ä¸è¿”å›ç›®å½•IDï¼Œéœ€è¦é€šè¿‡æŸ¥æ‰¾è·å–
	fs.Debugf(f, "ç›®å½• '%s' åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹æŸ¥æ‰¾æ–°ç›®å½•ID", name)

	// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨ç«¯åŒæ­¥å®Œæˆ
	time.Sleep(200 * time.Millisecond)

	// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•çš„IDï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
	var newDirID string
	var foundNew bool
	var errNew error

	// å°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼Œå¤„ç†æœåŠ¡å™¨ç«¯åŒæ­¥å»¶è¿Ÿ
	maxRetries := 5
	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// é€’å¢ç­‰å¾…æ—¶é—´ï¼š200ms, 400ms, 600ms, 800ms
			waitTime := time.Duration(attempt*200) * time.Millisecond
			fs.Debugf(f, "ç¬¬%dæ¬¡é‡è¯•æŸ¥æ‰¾æ–°ç›®å½• '%s'ï¼Œç­‰å¾… %v", attempt+1, name, waitTime)
			time.Sleep(waitTime)
		}

		// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼Œè·³è¿‡ç¼“å­˜ç›´æ¥ä»APIè·å–æœ€æ–°æ•°æ®
		newDirID, foundNew, errNew = f.findLeafWithForceRefresh(ctx, parentID, name)
		if errNew != nil {
			fs.Debugf(f, "ç¬¬%dæ¬¡æŸ¥æ‰¾æ–°ç›®å½•å¤±è´¥: %v", attempt+1, errNew)
			continue
		}

		if foundNew {
			fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æˆåŠŸæ‰¾åˆ°æ–°åˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", attempt+1, name, newDirID)
			break
		}

		fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æœªæ‰¾åˆ°æ–°ç›®å½• '%s'", attempt+1, name)
	}

	// æœ€ç»ˆæ£€æŸ¥ç»“æœ
	if errNew != nil {
		return "", fmt.Errorf("æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•å¤±è´¥ï¼Œå·²é‡è¯•%dæ¬¡: %w", maxRetries, errNew)
	}
	if !foundNew {
		return "", fmt.Errorf("æ–°åˆ›å»ºçš„ç›®å½•æœªæ‰¾åˆ°ï¼Œå·²é‡è¯•%dæ¬¡ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨åŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜", maxRetries)
	}

	fs.Debugf(f, "æˆåŠŸåˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", name, newDirID)
	return newDirID, nil
}

// createUpload åˆ›å»ºä¸Šä¼ ä¼šè¯
func (f *Fs) createUpload(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " åˆ†ç‰‡ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		// ç»Ÿä¸€å¤„ç†ç­–ç•¥ï¼šå¦‚æœé¢„éªŒè¯å¤±è´¥ï¼Œå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID
		fs.Errorf(f, "âš ï¸ åˆ†ç‰‡ä¸Šä¼ : é¢„éªŒè¯å‘ç°çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID", parentFileID)

		// ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
		realParentFileID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}

		if realParentFileID != parentFileID {
			fs.Infof(f, "ğŸ”„ åˆ†ç‰‡ä¸Šä¼ : å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œä½¿ç”¨æ­£ç¡®IDç»§ç»­ä¸Šä¼ ", realParentFileID, parentFileID)
			parentFileID = realParentFileID
		} else {
			fs.Debugf(f, "âš ï¸ åˆ†ç‰‡ä¸Šä¼ : ä½¿ç”¨æ¸…ç†ç¼“å­˜åçš„çˆ¶ç›®å½•ID: %d ç»§ç»­å°è¯•", parentFileID)
		}
	} else {
		fs.Debugf(f, " åˆ†ç‰‡ä¸Šä¼ : çˆ¶ç›®å½•ID %d é¢„éªŒè¯é€šè¿‡", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹APIæ–‡æ¡£çš„æ­£ç¡®å‚æ•°å parentFileID (å¤§å†™D)
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸º123 APIåŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API - createä½¿ç”¨æ ‡å‡†APIåŸŸå
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		return nil, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯é€»è¾‘ï¼šæ£€æµ‹parentFileIDä¸å­˜åœ¨é”™è¯¯ï¼Œä½¿ç”¨ç»Ÿä¸€ä¿®å¤ç­–ç•¥
	if response.Code == 1 && strings.Contains(response.Message, "parentFileIDä¸å­˜åœ¨") {
		fs.Errorf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œæ¸…ç†ç¼“å­˜", parentFileID)

		// ä½¿ç”¨ä¸å•æ­¥ä¸Šä¼ å®Œå…¨ä¸€è‡´çš„ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
		fs.Infof(f, "ğŸ”„ ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥")
		correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}

		// å¦‚æœè·å¾—äº†ä¸åŒçš„çˆ¶ç›®å½•IDï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯
		if correctParentID != parentFileID {
			fs.Infof(f, "ğŸ”„ å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯", correctParentID, parentFileID)

			// ä½¿ç”¨æ­£ç¡®çš„ç›®å½•IDé‡æ–°æ„å»ºè¯·æ±‚
			reqBody["parentFileID"] = correctParentID

			// é‡æ–°è°ƒç”¨API
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("ä½¿ç”¨æ­£ç¡®ç›®å½•IDé‡è¯•APIè°ƒç”¨å¤±è´¥: %w", err)
			}
		} else {
			// å³ä½¿æ˜¯ç›¸åŒçš„IDï¼Œä¹Ÿè¦é‡æ–°å°è¯•ï¼Œå› ä¸ºç¼“å­˜å·²ç»æ¸…ç†
			fs.Infof(f, "ğŸ”„ ä½¿ç”¨æ¸…ç†ç¼“å­˜åçš„çˆ¶ç›®å½•ID: %dï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯", correctParentID)
			reqBody["parentFileID"] = correctParentID
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("æ¸…ç†ç¼“å­˜åé‡è¯•APIè°ƒç”¨å¤±è´¥: %w", err)
			}
		}
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, "åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	return &response, nil
}

// createUploadV2 ä¸“é—¨ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åˆ›å»ºä¼šè¯ï¼Œå¼ºåˆ¶ä½¿ç”¨v2 API
func (f *Fs) createUploadV2(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// éªŒè¯æ–‡ä»¶åç¬¦åˆAPIè¦æ±‚
	if err := validateFileName(filename); err != nil {
		return nil, fmt.Errorf("æ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}

	// é¦–å…ˆéªŒè¯parentFileIDæ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "createUploadV2: éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		return nil, fmt.Errorf("çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹APIæ–‡æ¡£çš„æ­£ç¡®å‚æ•°å parentFileID (å¤§å†™D)
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// å¼ºåˆ¶ä½¿ç”¨v2 APIï¼Œä¸ä½¿ç”¨ç‰ˆæœ¬å›é€€ - ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•
	bodyBytes, _ := json.Marshal(reqBody)
	fs.Debugf(f, "è°ƒç”¨v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯: %s", string(bodyBytes))
	// æ ¹æ®å®˜æ–¹æ–‡æ¡£ä½¿ç”¨v2ç«¯ç‚¹å’Œæ ‡å‡†APIåŸŸå
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		fs.Errorf(f, "v2 APIè°ƒç”¨å¤±è´¥: %v", err)

		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if strings.Contains(err.Error(), "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸  çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå¯èƒ½ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦æ¸…ç†ç¼“å­˜é‡è¯•", parentFileID)
			// é‡ç½®ç›®å½•ç¼“å­˜
			if f.dirCache != nil {
				fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", parentFileID)
				f.dirCache.ResetRoot()
			}
			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: %w", err)
		}

		return nil, fmt.Errorf("v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// è¯¦ç»†è®°å½•APIå“åº”
	fs.Debugf(f, "v2 APIå“åº”: Code=%d, Message='%s', FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Code, response.Message, response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	if response.Code != 0 {
		fs.Errorf(f, "v2 APIè¿”å›é”™è¯¯: Code=%d, Message='%s'", response.Code, response.Message)
		return nil, fmt.Errorf("v2 API error %d: %s", response.Code, response.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸ºç§’ä¼ 
	if response.Data.Reuse {
		fs.Debugf(f, "ğŸš€ v2 APIç§’ä¼ æˆåŠŸ: FileID=%d, æ–‡ä»¶å·²å­˜åœ¨äºæœåŠ¡å™¨", response.Data.FileID)
		return &response, nil
	}

	// éç§’ä¼ æƒ…å†µï¼ŒéªŒè¯PreuploadIDä¸ä¸ºç©º
	if response.Data.PreuploadID == "" {
		fs.Errorf(f, "âŒ v2 APIå“åº”å¼‚å¸¸: éç§’ä¼ æƒ…å†µä¸‹PreuploadIDä¸ºç©ºï¼Œå®Œæ•´å“åº”: %+v", response)
		return nil, fmt.Errorf("v2 APIè¿”å›çš„é¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, " v2åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.SliceSize)

	return &response, nil
}

// uploadFile ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹
func (f *Fs) uploadFile(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64) error {
	return f.uploadFileWithPath(ctx, in, createResp, size, "")
}

// uploadFileWithPath ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileWithPath(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64, filePath string) error {
	// ç¡®å®šå—å¤§å°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		// ä½¿ç”¨rcloneå…¨å±€é…ç½®çš„å¤šçº¿ç¨‹åˆ†ç‰‡å¤§å°
		chunkSize = int64(fs.GetConfig(ctx).MultiThreadChunkSize)
	}

	// ç¡®ä¿å—å¤§å°åœ¨é™åˆ¶èŒƒå›´å†…
	if chunkSize < int64(minChunkSize) {
		chunkSize = int64(minChunkSize)
	}
	if chunkSize > int64(maxChunkSize) {
		chunkSize = int64(maxChunkSize)
	}

	// å¯¹äºå°æ–‡ä»¶ï¼Œå…¨éƒ¨è¯»å…¥å†…å­˜ï¼ˆæ— éœ€æ–­ç‚¹ç»­ä¼ ï¼‰
	if size <= chunkSize {
		data, err := io.ReadAll(in)
		if err != nil {
			return fmt.Errorf("failed to read file data: %w", err)
		}

		if int64(len(data)) != size {
			return fmt.Errorf("size mismatch: expected %d, got %d", size, len(data))
		}

		// å•éƒ¨åˆ†ä¸Šä¼ 
		return f.uploadSinglePart(ctx, createResp.Data.PreuploadID, data)
	}

	// å¤§æ–‡ä»¶çš„å¤šéƒ¨åˆ†ä¸Šä¼ ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
	return f.uploadMultiPart(ctx, in, createResp.Data.PreuploadID, size, chunkSize)
}

// uploadSinglePart å•éƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadSinglePart(ctx context.Context, preuploadID string, data []byte) error {
	// è®¡ç®—åˆ†ç‰‡MD5
	chunkHash := fmt.Sprintf("%x", md5.Sum(data))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err := f.uploadPartWithMultipart(ctx, preuploadID, 1, chunkHash, data)
	if err != nil {
		return fmt.Errorf("failed to upload part: %w", err)
	}

	// å®Œæˆä¸Šä¼  - ä½¿ç”¨å¸¦æ–‡ä»¶å¤§å°çš„ç‰ˆæœ¬ä»¥æ”¯æŒå¢å¼ºéªŒè¯
	_, err = f.completeUploadWithResultAndSize(ctx, preuploadID, int64(len(data)))
	return err
}

// uploadMultiPart ç®€åŒ–çš„å¤šéƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
func (f *Fs) uploadMultiPart(ctx context.Context, in io.Reader, preuploadID string, size, chunkSize int64) error {
	// é˜²æ­¢é™¤é›¶é”™è¯¯
	if chunkSize <= 0 {
		return fmt.Errorf("æ— æ•ˆçš„åˆ†ç‰‡å¤§å°: %d", chunkSize)
	}

	uploadNums := (size + chunkSize - 1) / chunkSize

	// é™åˆ¶åˆ†ç‰‡æ•°é‡
	if uploadNums > int64(f.opt.MaxUploadParts) {
		return fmt.Errorf("file too large: would require %d parts, max allowed is %d", uploadNums, f.opt.MaxUploadParts)
	}

	fs.Debugf(f, "å¼€å§‹åˆ†ç‰‡ä¸Šä¼ : %dä¸ªåˆ†ç‰‡ï¼Œæ¯ç‰‡%s", uploadNums, fs.SizeSuffix(chunkSize))

	buffer := make([]byte, chunkSize)

	// ç®€åŒ–çš„åˆ†ç‰‡ä¸Šä¼ å¾ªç¯ï¼Œæ— æ–­ç‚¹ç»­ä¼ 
	for partIndex := int64(0); partIndex < uploadNums; partIndex++ {
		partNumber := partIndex + 1

		// è¯»å–å—æ•°æ®
		var partData []byte
		if partIndex == uploadNums-1 {
			// æœ€åä¸€åˆ†ç‰‡ - è¯»å–å‰©ä½™æ•°æ®
			remaining := size - partIndex*chunkSize
			partData = make([]byte, remaining)
			_, err := io.ReadFull(in, partData)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
		} else {
			// å¸¸è§„åˆ†ç‰‡
			_, err := io.ReadFull(in, buffer)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
			partData = buffer
		}

		// è®¡ç®—åˆ†ç‰‡MD5
		chunkHash := fmt.Sprintf("%x", md5.Sum(partData))

		// ä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶ä¸Šä¼ åˆ†ç‰‡
		err := f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkHash, partData)
		if err != nil {
			return fmt.Errorf("failed to upload part %d: %w", partNumber, err)
		}

		fs.Debugf(f, "å·²ä¸Šä¼ åˆ†ç‰‡ %d/%d", partNumber, uploadNums)
	}

	// å®Œæˆä¸Šä¼ 
	_, err := f.completeUploadWithResultAndSize(ctx, preuploadID, size)
	if err != nil {
		return err
	}

	return nil
}

// uploadPartWithMultipart ä½¿ç”¨æ­£ç¡®çš„multipart/form-dataæ ¼å¼ä¸Šä¼ åˆ†ç‰‡
// ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€é‡è¯•ç®¡ç†å™¨ï¼Œé¿å…å¤šå±‚é‡è¯•åµŒå¥—
func (f *Fs) uploadPartWithMultipart(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	if len(data) == 0 {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(data))

	// ç®€åŒ–é‡è¯•é€»è¾‘ - ä½¿ç”¨rcloneæ ‡å‡†çš„fs.Paceré‡è¯•æœºåˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		err := f.uploadPartDirectly(ctx, preuploadID, partNumber, chunkHash, data)
		return fserrors.ShouldRetry(err), err
	})
}

// uploadPartDirectly ç›´æ¥ä¸Šä¼ åˆ†ç‰‡ï¼Œä¸åŒ…å«é‡è¯•é€»è¾‘
// å…³é”®ä¿®å¤ï¼šå°†é‡è¯•é€»è¾‘ä»ä¸šåŠ¡é€»è¾‘ä¸­åˆ†ç¦»
func (f *Fs) uploadPartDirectly(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	// ä¿®å¤ï¼šç§»é™¤ uploadPacer.Callï¼Œç›´æ¥æ‰§è¡Œï¼Œé‡è¯•ç”±ç»Ÿä¸€ç®¡ç†å™¨å¤„ç†
	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
	}

	// åˆ›å»ºmultipartè¡¨å•æ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	writer.WriteField("preuploadID", preuploadID)
	writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
	writer.WriteField("sliceMD5", chunkHash)

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
	}

	_, err = part.Write(data)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	err = writer.Close()
	if err != nil {
		return fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// ä¿®å¤å¡ä½é—®é¢˜ï¼šä¸ºåˆ†ç‰‡ä¸Šä¼ æ·»åŠ è¶…æ—¶æ§åˆ¶
	uploadCtx, cancel := context.WithTimeout(ctx, 5*time.Minute) // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š5åˆ†é’Ÿ
	defer cancel()

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè°ƒç”¨åˆ†ç‰‡ä¸Šä¼ API
	err = f.makeAPICallWithRestMultipartToDomain(uploadCtx, uploadDomain, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d ä¸Šä¼ æˆåŠŸ", partNumber)
	return nil
}

// UploadCompleteResult ä¸Šä¼ å®Œæˆç»“æœ
type UploadCompleteResult struct {
	FileID int64  `json:"fileID"`
	Etag   string `json:"etag"`
}

// ç§»é™¤æœªä½¿ç”¨çš„ChunkAnalysisç»“æ„ä½“

// completeUploadWithResultAndSize å®Œæˆå¤šéƒ¨åˆ†ä¸Šä¼ å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒæ™ºèƒ½è½®è¯¢ç­–ç•¥
// å¢å¼ºï¼šå®ç°åŠ¨æ€è½®è¯¢é—´éš”å’Œæ™ºèƒ½é”™è¯¯å¤„ç†
func (f *Fs) completeUploadWithResultAndSize(ctx context.Context, preuploadID string, fileSize int64) (*UploadCompleteResult, error) {
	fs.Debugf(f, " ä½¿ç”¨å¢å¼ºçš„ä¸Šä¼ éªŒè¯é€»è¾‘ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))

	reqBody := map[string]any{
		"preuploadID": preuploadID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			Completed bool   `json:"completed"`
			FileID    int64  `json:"fileID"`
			Etag      string `json:"etag"`
		} `json:"data"`
	}

	// ä¼˜åŒ–ï¼šæ™ºèƒ½è½®è¯¢ç­–ç•¥ï¼Œæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´å‚æ•°å’Œé˜ˆå€¼
	maxRetries, baseInterval, maxConsecutiveFailures := f.calculatePollingStrategy(fileSize)
	fs.Debugf(f, "æ™ºèƒ½è½®è¯¢ç­–ç•¥: æœ€å¤§é‡è¯•%dæ¬¡, åŸºç¡€é—´éš”%v, è¿ç»­å¤±è´¥é˜ˆå€¼%d",
		maxRetries, baseInterval, maxConsecutiveFailures)

	// ä¼˜åŒ–ï¼šæ™ºèƒ½è½®è¯¢é€»è¾‘ï¼Œæ”¯æŒåŠ¨æ€é—´éš”å’Œé”™è¯¯åˆ†ç±»
	consecutiveFailures := 0
	var lastError error // è®°å½•æœ€åä¸€æ¬¡é”™è¯¯ï¼Œç”¨äºåŠ¨æ€é—´éš”è®¡ç®—

	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// ä¼˜åŒ–ï¼šåŠ¨æ€è½®è¯¢é—´éš”ï¼Œæ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°å’Œé”™è¯¯ç±»å‹è°ƒæ•´
			interval := f.calculateDynamicInterval(baseInterval, consecutiveFailures, attempt, lastError)
			fs.Debugf(f, "è½®è¯¢é—´éš”: %v (è¿ç»­å¤±è´¥: %d, é”™è¯¯ç±»å‹: %T)", interval, consecutiveFailures, lastError)

			select {
			case <-ctx.Done():
				return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯è¢«å–æ¶ˆ: %w", ctx.Err())
			case <-time.After(interval):
				// ç»§ç»­è½®è¯¢
			}
		}

		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			lastError = err
			consecutiveFailures++
			fs.Debugf(f, "Upload complete API call failed (attempt %d/%d, consecutive failures: %d/%d): %v",
				attempt+1, maxRetries, consecutiveFailures, maxConsecutiveFailures, err)

			// Use standard retry logic
			if retry, _ := shouldRetry(ctx, nil, err); retry {
				fs.Debugf(f, "Standard retry logic suggests retry")
				select {
				case <-ctx.Done():
					return nil, fmt.Errorf("upload verification cancelled: %w", ctx.Err())
				case <-time.After(time.Duration(attempt) * time.Second):
				}
				continue
			}

			// ç®€åŒ–ï¼šä½¿ç”¨åŠ¨æ€è¿ç»­å¤±è´¥é˜ˆå€¼ï¼Œç½‘ç»œé”™è¯¯ç‰¹æ®Šå¤„ç†
			errStr := err.Error()
			if (strings.Contains(errStr, "connection") || strings.Contains(errStr, "timeout") ||
				strings.Contains(errStr, "network")) && consecutiveFailures < maxConsecutiveFailures/2 {
				fs.Debugf(f, "ç½‘ç»œé”™è¯¯ï¼Œç»§ç»­é‡è¯• (è¿ç»­å¤±è´¥: %d/%d)", consecutiveFailures, maxConsecutiveFailures/2)
				continue // ç½‘ç»œé”™è¯¯çš„å®¹é”™æ€§æ›´é«˜
			}
			if consecutiveFailures >= maxConsecutiveFailures {
				return nil, fmt.Errorf("è¿ç»­APIè°ƒç”¨å¤±è´¥æ¬¡æ•°è¿‡å¤š (%d/%d): %w",
					consecutiveFailures, maxConsecutiveFailures, err)
			}
			continue
		}

		// é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
		consecutiveFailures = 0
		progressTime := time.Now()

		if response.Code == 0 && response.Data.Completed {
			totalElapsed := time.Since(progressTime)
			fs.Infof(f, "âœ… ä¸Šä¼ éªŒè¯æˆåŠŸå®Œæˆ (æ€»è€—æ—¶: %v, å°è¯•æ¬¡æ•°: %d/%d)",
				totalElapsed, attempt+1, maxRetries)
			return &UploadCompleteResult{
				FileID: response.Data.FileID,
				Etag:   response.Data.Etag,
			}, nil
		}

		if response.Code == 0 && !response.Data.Completed {
			// ä¼˜åŒ–ï¼šcompleted=falseçŠ¶æ€çš„æ™ºèƒ½å¤„ç†å’Œè¯¦ç»†æ—¥å¿—
			elapsed := time.Since(progressTime)
			progress := float64(attempt+1) / float64(maxRetries) * 100

			fs.Debugf(f, "ğŸ“‹ æœåŠ¡å™¨è¿”å›completed=falseï¼Œç»§ç»­è½®è¯¢ (è¿›åº¦: %.1f%%, %d/%d, å·²ç­‰å¾…: %v)",
				progress, attempt+1, maxRetries, elapsed)

			// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è­¦å‘Šé˜ˆå€¼
			warningThreshold := 5 * time.Minute
			if fileSize > 1*1024*1024*1024 { // å¤§äº1GB
				warningThreshold = 15 * time.Minute
			}

			if elapsed > warningThreshold {
				fs.Logf(f, "âš ï¸ éªŒè¯ç­‰å¾…æ—¶é—´è¿‡é•¿(%v)ï¼Œæ–‡ä»¶å¤§å°: %sï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜",
					elapsed, fs.SizeSuffix(fileSize))
			}
			continue
		}

		// å¢å¼ºï¼šé”™è¯¯ä»£ç åˆ†ç±»å¤„ç†
		if f.isRetryableError(response.Code) {
			fs.Debugf(f, "âš ï¸ é‡åˆ°å¯é‡è¯•é”™è¯¯ %d: %sï¼Œç»§ç»­è½®è¯¢", response.Code, response.Message)
			continue
		}

		// å…¶ä»–é”™è¯¯ï¼Œç›´æ¥è¿”å›
		return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯å¤±è´¥: API error %d: %s", response.Code, response.Message)
	}

	return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯è¶…æ—¶ï¼Œå·²è½®è¯¢%dæ¬¡", maxRetries)
}

// uploadPartStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ åˆ†ç‰‡ï¼Œé¿å…å¤§å†…å­˜å ç”¨
func (f *Fs) uploadPartStream(ctx context.Context, uploadURL string, reader io.Reader, size int64) error {
	req, err := http.NewRequestWithContext(ctx, "PUT", uploadURL, reader)
	if err != nil {
		return fmt.Errorf("failed to create stream upload request: %w", err)
	}

	req.ContentLength = size
	req.Header.Set("Content-Type", "application/octet-stream")

	// ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶æœºåˆ¶
	adaptiveTimeout := f.getAdaptiveTimeout(size, "chunked_upload")
	var cancel context.CancelFunc
	ctx, cancel = context.WithTimeout(ctx, adaptiveTimeout)
	defer cancel()
	req = req.WithContext(ctx)

	// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(ctx)
	resp, err := client.Do(req)
	if err != nil {
		return fmt.Errorf("stream upload request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusCreated {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("stream upload failed with status %d: %s", resp.StatusCode, string(body))
	}

	return nil
}

// streamingPut ç»Ÿä¸€çš„æµå¼ä¸Šä¼ å…¥å£ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“ä¼˜åŒ–
func (f *Fs) streamingPut(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è¿›å…¥streamingPutå‡½æ•°ï¼Œæ–‡ä»¶: %s", fileName)

	// é€šç”¨è·¨äº‘ä¼ è¾“æ£€æµ‹ï¼šæ£€æµ‹æ˜¯å¦æ¥è‡ªå…¶ä»–äº‘å­˜å‚¨æœåŠ¡
	srcFsString := src.Fs().String()
	currentFsString := f.String()

	// æ£€æµ‹è·¨äº‘ä¼ è¾“ï¼šæºå’Œç›®æ ‡æ˜¯ä¸åŒçš„æ–‡ä»¶ç³»ç»Ÿ
	if srcFsString != currentFsString {
		// è¿›ä¸€æ­¥æ£€æµ‹æ˜¯å¦ä¸ºäº‘å­˜å‚¨åˆ°äº‘å­˜å‚¨çš„ä¼ è¾“ï¼ˆæ’é™¤æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰
		if !strings.Contains(srcFsString, "local") && !strings.Contains(currentFsString, "local") {
			fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s â†’ %sï¼Œå¯ç”¨ä¼˜åŒ–ä¼ è¾“æ¨¡å¼", srcFsString, currentFsString)
			return f.handleCrossCloudTransfer(ctx, in, src, parentFileID, fileName)
		}
	}

	// è·¨äº‘ä¼ è¾“æ£€æµ‹å’Œå¤„ç†
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s â†’ 123ç½‘ç›˜ (å¤§å°: %s)", src.Fs().Name(), fileName, fs.SizeSuffix(src.Size()))
		fs.Infof(f, "ğŸ“¤ ç›´æ¥ä½¿ç”¨æºäº‘ç›˜æ•°æ®æµè¿›è¡Œ123ç½‘ç›˜ä¸Šä¼ ...")
		// å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç›´æ¥ä¼ è¾“è€Œä¸æ˜¯æœ¬åœ°åŒ–ç¼“å­˜ï¼Œé¿å…äºŒæ¬¡ä¸‹è½½
		return f.handleCrossCloudTransfer(ctx, in, src, parentFileID, fileName)
	}

	// æœ¬åœ°æ–‡ä»¶ä½¿ç”¨æ ‡å‡†æµå¼ä¸Šä¼ 
	fs.Debugf(f, "æœ¬åœ°æ–‡ä»¶ä½¿ç”¨æ ‡å‡†æµå¼ä¸Šä¼ ")
	return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
}

// handleCrossCloudTransfer ç»Ÿä¸€çš„è·¨äº‘ä¼ è¾“å¤„ç†å‡½æ•°
// ä¿®å¤ï¼šå‚è€ƒ115ç½‘ç›˜å®ç°å†…éƒ¨ä¸¤æ­¥ä¼ è¾“ï¼Œå½»åº•è§£å†³è·¨äº‘ä¼ è¾“é—®é¢˜
func (f *Fs) handleCrossCloudTransfer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "Starting internal two-step transfer: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// Use internal two-step transfer to avoid cross-cloud transfer issues
	fs.Debugf(f, "Step 1: Download to local, Step 2: Upload to 123 from local")

	return f.internalTwoStepTransfer(ctx, in, src, parentFileID, fileName)
}

// ç§»é™¤getCachedUploadDomainå‡½æ•°ï¼Œç›´æ¥ä½¿ç”¨getUploadDomain

// uploadFromDownloadedData ä»å·²ä¸‹è½½çš„æ•°æ®ç›´æ¥ä¸Šä¼ ï¼Œé¿å…é€’å½’è°ƒç”¨
func (f *Fs) uploadFromDownloadedData(ctx context.Context, reader io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ“¤ ä»å·²ä¸‹è½½æ•°æ®ç›´æ¥ä¸Šä¼ : %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: è®¡ç®—MD5å“ˆå¸Œï¼ˆ123ç½‘ç›˜APIå¿…éœ€ï¼‰
	fs.Infof(f, "ğŸ” è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ...")

	// è¯»å–æ‰€æœ‰æ•°æ®å¹¶è®¡ç®—MD5
	data, err := io.ReadAll(reader)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–å·²ä¸‹è½½æ•°æ®å¤±è´¥: %w", err)
	}

	if int64(len(data)) != fileSize {
		return nil, fmt.Errorf("æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(int64(len(data))))
	}

	// Calculate MD5 hash
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

	// æ­¥éª¤2: å°è¯•ç§’ä¼ 
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		fs.Debugf(f, "ç§’ä¼ æ£€æŸ¥å¤±è´¥: %v", err)
	} else if isInstant {
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼èŠ‚çœä¸Šä¼ æ—¶é—´")
		return f.createObjectFromUpload(fileName, fileSize, md5Hash, createResp.Data.FileID, src.ModTime(ctx)), nil
	}

	// æ­¥éª¤3: ç§’ä¼ å¤±è´¥ï¼Œåˆ›å»ºä¸Šä¼ ä¼šè¯å¹¶å®é™…ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ : %s", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆç°åœ¨æœ‰äº†MD5ï¼‰
	createResp, err = f.createUploadV2(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// å‡†å¤‡ä¸Šä¼ è¿›åº¦ä¿¡æ¯
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		defaultNetworkSpeed := int64(20 * 1024 * 1024) // 20MB/s
		chunkSize = f.getOptimalChunkSize(fileSize, defaultNetworkSpeed)
	}
	// ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨å•æ­¥ä¸Šä¼ ï¼Œæ— éœ€è¿›åº¦è·Ÿè¸ª

	// ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ 
	uploadStartTime := time.Now()
	// ä½¿ç”¨å•æ­¥ä¸Šä¼ ï¼Œç®€åŒ–å¤„ç†
	result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(fileSize) / uploadDuration.Seconds() / (1024 * 1024) // MB/s

	fs.Infof(f, "Cross-cloud transfer completed: %s, upload: %v, speed: %.2f MB/s",
		fs.SizeSuffix(fileSize), uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// internalTwoStepTransfer å†…éƒ¨ä¸¤æ­¥ä¼ è¾“ï¼šä¸‹è½½åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åä¸Šä¼ åˆ°123ç½‘ç›˜
// è¿™æ˜¯å¤„ç†è·¨äº‘ä¼ è¾“æœ€å¯é çš„æ–¹æ³•ï¼Œé¿å…äº†æµå¼ä¼ è¾“çš„å¤æ‚æ€§
func (f *Fs) internalTwoStepTransfer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ”„ å¼€å§‹å†…éƒ¨ä¸¤æ­¥ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// Step 1: ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
	fs.Debugf(f, "Step 1: Download from source to temporary file...")

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶
	tempFile, err := os.CreateTemp("", "rclone-123-transfer-*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// è·å–æºæ–‡ä»¶çš„Reader
	var srcReader io.ReadCloser
	if in != nil {
		// å¦‚æœå·²ç»æœ‰Readerï¼Œç›´æ¥ä½¿ç”¨
		srcReader = io.NopCloser(in)
	} else {
		// å¦åˆ™æ‰“å¼€æºæ–‡ä»¶
		if srcObj, ok := src.(fs.Object); ok {
			srcReader, err = srcObj.Open(ctx)
			if err != nil {
				return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
			}
		} else {
			return nil, fmt.Errorf("æºå¯¹è±¡ä¸æ”¯æŒOpenæ“ä½œ")
		}
	}
	defer srcReader.Close()

	// ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, srcReader)
	if err != nil {
		return nil, fmt.Errorf("ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
	}

	// è®¡ç®—MD5
	md5sum := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "Step 1 completed: downloaded %s, MD5: %s", fs.SizeSuffix(written), md5sum)

	// Step 2: ä»ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ åˆ°123ç½‘ç›˜
	fs.Debugf(f, "Step 2: Upload from local to 123...")

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹
	_, err = tempFile.Seek(0, 0)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	// åˆ›å»ºæœ¬åœ°æ–‡ä»¶ä¿¡æ¯å¯¹è±¡
	localSrc := &localFileInfo{
		name:    fileName,
		size:    written,
		modTime: src.ModTime(ctx),
	}

	// ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å‡½æ•°ï¼Œæ­¤æ—¶æºå·²ç»æ˜¯æœ¬åœ°æ•°æ®
	return f.putWithKnownMD5(ctx, tempFile, localSrc, parentFileID, fileName, md5sum)
}

// localFileInfo æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ç»“æ„ä½“ï¼Œç”¨äºå†…éƒ¨ä¸¤æ­¥ä¼ è¾“
type localFileInfo struct {
	name    string
	size    int64
	modTime time.Time
}

func (l *localFileInfo) String() string {
	return l.name
}

func (l *localFileInfo) Remote() string {
	return l.name
}

func (l *localFileInfo) ModTime(ctx context.Context) time.Time {
	return l.modTime
}

func (l *localFileInfo) Size() int64 {
	return l.size
}

func (l *localFileInfo) Fs() fs.Info {
	return nil // æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ä¸éœ€è¦Fs
}

func (l *localFileInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	return "", fmt.Errorf("hash not supported for local file info") // æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ä¸æ”¯æŒHash
}

func (l *localFileInfo) Storable() bool {
	return true
}

// æ³¨æ„ï¼šcalculateOptimalChunkSizeå‡½æ•°å·²åˆ é™¤
// åˆ†ç‰‡å¤§å°ç°åœ¨ç»Ÿä¸€ä½¿ç”¨defaultChunkSizeæˆ–APIè¿”å›çš„SliceSize

// uploadWithKnownMD5FromRetry åœ¨é‡è¯•æƒ…å†µä¸‹ä½¿ç”¨å·²çŸ¥MD5ç›´æ¥ä¸Šä¼ 
// ä¿®å¤é‡å¤ä¸‹è½½ï¼šé¿å…é‡æ–°ä¸‹è½½ï¼Œç›´æ¥å°è¯•ä¸Šä¼ 
func (f *Fs) uploadWithKnownMD5FromRetry(ctx context.Context, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	fs.Infof(f, "ğŸš€ é‡è¯•ä¸Šä¼ ï¼Œä½¿ç”¨å·²çŸ¥MD5: %s", md5Hash)

	// å°è¯•ç§’ä¼ 
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		fs.Debugf(f, "é‡è¯•æ—¶åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %v", err)
		// å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´ä¸‹è½½æµç¨‹
		fs.Infof(f, "âš ï¸ é‡è¯•ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´ä¸‹è½½æµç¨‹")
		return f.handleCrossCloudTransfer(ctx, nil, src, parentFileID, fileName)
	}

	// æ£€æŸ¥æ˜¯å¦ç§’ä¼ æˆåŠŸ
	if createResp.Data.Reuse {
		fs.Infof(f, "âœ… é‡è¯•æ—¶ç§’ä¼ æˆåŠŸ: %s", fileName)
		return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
	}

	// å¦‚æœä¸èƒ½ç§’ä¼ ï¼Œè¯´æ˜éœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶ï¼Œå›é€€åˆ°å®Œæ•´æµç¨‹
	fs.Infof(f, "âš ï¸ é‡è¯•æ—¶æ— æ³•ç§’ä¼ ï¼Œéœ€è¦é‡æ–°ä¸‹è½½æ–‡ä»¶è¿›è¡Œä¸Šä¼ ")
	return f.handleCrossCloudTransfer(ctx, nil, src, parentFileID, fileName)
}

// UnifiedUploadParams ç»Ÿä¸€ä¸Šä¼ å‚æ•°ç»“æ„
type UnifiedUploadParams struct {
	ctx              context.Context
	reader           io.Reader
	md5Hash          string
	src              fs.ObjectInfo
	parentFileID     int64
	fileName         string
	downloadDuration time.Duration
	md5Duration      time.Duration
	startTime        time.Time
	isFromTempFile   bool
}

// unifiedUploadWithMD5 ç»Ÿä¸€çš„MD5ä¸Šä¼ å¤„ç†å‡½æ•°
// åˆå¹¶äº†uploadFromTempFileå’ŒcontinueWithKnownMD5çš„é€»è¾‘
func (f *Fs) unifiedUploadWithMD5(ctx context.Context, params UnifiedUploadParams) (*Object, error) {
	fileSize := params.src.Size()

	// æ­¥éª¤1: æ£€æŸ¥ç§’ä¼ 
	createResp, isInstant, err := f.checkInstantUpload(ctx, params.parentFileID, params.fileName, params.md5Hash, fileSize)
	if err != nil {
		return nil, err
	}

	if isInstant {
		totalDuration := time.Since(params.startTime)
		fs.Debugf(f, "Instant upload successful! Total time: %v (download: %v + MD5: %v)",
			totalDuration, params.downloadDuration, params.md5Duration)
		return f.createObjectFromUpload(params.fileName, fileSize, params.md5Hash, createResp.Data.FileID, params.src.ModTime(ctx)), nil
	}

	// æ­¥éª¤2: å®é™…ä¸Šä¼ 
	fs.Debugf(f, "Starting upload to 123...")
	uploadStartTime := time.Now()

	// Reset file pointer if it's a temp file
	if params.isFromTempFile {
		if seeker, ok := params.reader.(io.Seeker); ok {
			_, err = seeker.Seek(0, io.SeekStart)
			if err != nil {
				return nil, fmt.Errorf("failed to reset file pointer: %w", err)
			}
		}
	}

	err = f.uploadFile(ctx, params.reader, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("upload failed: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(params.startTime)

	// ç§»é™¤MD5ç¼“å­˜ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

	fs.Debugf(f, "Upload completed! Total time: %v (download: %v + MD5: %v + upload: %v)",
		totalDuration, params.downloadDuration, params.md5Duration, uploadDuration)

	return f.createObject(params.fileName, createResp.Data.FileID, fileSize, params.md5Hash, time.Now(), false), nil
}

// ç§»é™¤CrossCloudMD5Cacheï¼Œä½¿ç”¨rcloneæ ‡å‡†ç¼“å­˜æœºåˆ¶

// ç§»é™¤getCachedMD5å’ŒcacheMD5å‡½æ•°ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰MD5ç¼“å­˜

// putWithKnownMD5 å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ 
// è¿™æ˜¯æœ€é«˜æ•ˆçš„è·¯å¾„ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨123ç½‘ç›˜çš„ç§’ä¼ åŠŸèƒ½
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) putWithKnownMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	return f.putWithKnownMD5WithOptions(ctx, in, src, parentFileID, fileName, md5Hash, false)
}

// putWithKnownMD5WithOptions å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ ï¼Œæ”¯æŒè·³è¿‡å•æ­¥ä¸Šä¼ é€‰é¡¹
func (f *Fs) putWithKnownMD5WithOptions(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string, skipSingleStep bool) (*Object, error) {
	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆå°è¯•å•æ­¥ä¸Šä¼ APIï¼ˆé™¤éæ˜ç¡®è·³è¿‡ï¼‰
	fileSize := src.Size()
	if !skipSingleStep && fileSize < singleStepUploadLimit && fileSize > 0 && fileSize <= maxMemoryBufferSize {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", fileSize, singleStepUploadLimit)

		// è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
		data, err := io.ReadAll(in)
		if err != nil {
			fs.Debugf(f, "è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
		} else if int64(len(data)) == fileSize {
			// å°è¯•å•æ­¥ä¸Šä¼ 
			result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
			if err != nil {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
				// é‡æ–°åˆ›å»ºreaderç”¨äºä¼ ç»Ÿä¸Šä¼ 
				in = bytes.NewReader(data)
			} else {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
				return result, nil
			}
		}
	}

	// ä¼ ç»Ÿä¸Šä¼ æµç¨‹ï¼šä½¿ç”¨å·²çŸ¥çš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ç›¸åŒMD5çš„æ–‡ä»¶
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ /å»é‡ï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™æ˜¯æœ€ç†æƒ³çš„æƒ…å†µï¼šæ— éœ€å®é™…ä¼ è¾“æ•°æ®
	if createResp.Data.Reuse {
		fs.Debugf(f, "æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œéœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®
	fs.Debugf(f, "ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®")
	err = f.uploadFile(ctx, in, createResp, src.Size())
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	// æ³¨æ„ï¼šä½¿ç”¨å®é™…çš„æ–‡ä»¶åè€Œä¸æ˜¯src.Remote()ï¼Œå› ä¸ºsrc.Remote()å¯èƒ½åŒ…å«.partialåç¼€
	// è€Œå®é™…ä¸Šä¼ åˆ°123ç½‘ç›˜çš„æ–‡ä»¶åæ˜¯fileNameï¼ˆå·²å»é™¤.partialåç¼€ï¼‰
	return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
}

// putSmallFileWithMD5 å¤„ç†å°æ–‡ä»¶ï¼ˆâ‰¤10MBï¼‰çš„ä¸Šä¼ 
// é€šè¿‡ç¼“å­˜åˆ°å†…å­˜å…ˆè®¡ç®—MD5ï¼Œæœ€å¤§åŒ–åˆ©ç”¨ç§’ä¼ åŠŸèƒ½ï¼ŒåŒæ—¶æ§åˆ¶å†…å­˜ä½¿ç”¨
func (f *Fs) putSmallFileWithMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ç¼“å­˜å°æ–‡ä»¶ï¼ˆ%då­—èŠ‚ï¼‰åˆ°å†…å­˜ä»¥è®¡ç®—MD5ï¼Œå°è¯•ç§’ä¼ ", fileSize)

	// éªŒè¯æ–‡ä»¶å¤§å°çš„åˆç†æ€§
	if fileSize < 0 {
		return nil, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶å¤§å°: %d", fileSize)
	}
	if fileSize > 50*1024*1024 { // 50MBä¸Šé™
		fs.Debugf(f, "æ–‡ä»¶è¿‡å¤§ï¼ˆ%då­—èŠ‚ï¼‰ï¼Œä¸é€‚åˆå†…å­˜ç¼“å­˜ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ", fileSize)
		return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
	}

	// ç®€åŒ–å†…å­˜ç®¡ç† - ä½¿ç”¨Goæ ‡å‡†åº“ï¼Œç§»é™¤è¿‡åº¦å¤æ‚çš„å†…å­˜è·Ÿè¸ª

	// ä½¿ç”¨é™åˆ¶è¯»å–å™¨é˜²æ­¢è¯»å–è¶…è¿‡é¢„æœŸå¤§å°çš„æ•°æ®
	// è¿™å¯ä»¥é˜²æ­¢æ¶æ„æˆ–æŸåçš„æ–‡ä»¶å¯¼è‡´å†…å­˜è€—å°½
	var limitedReader io.Reader
	if fileSize > 0 {
		// å…è®¸è¯»å–æ¯”é¢„æœŸç¨å¤šä¸€ç‚¹çš„æ•°æ®æ¥æ£€æµ‹å¤§å°ä¸åŒ¹é…
		limitedReader = io.LimitReader(in, fileSize+1024) // é¢å¤–1KBç”¨äºæ£€æµ‹
	} else {
		// å¯¹äºæœªçŸ¥å¤§å°çš„æ–‡ä»¶ï¼Œè®¾ç½®åˆç†çš„ä¸Šé™
		limitedReader = io.LimitReader(in, 50*1024*1024) // 50MBä¸Šé™
	}

	// å°†æ–‡ä»¶è¯»å–åˆ°å†…å­˜ï¼Œä½¿ç”¨é™åˆ¶è¯»å–å™¨ä¿æŠ¤
	data, err := io.ReadAll(limitedReader)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–å°æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// ç«‹å³æ£€æŸ¥è¯»å–çš„æ•°æ®å¤§å°ï¼Œé˜²æ­¢å†…å­˜æµªè´¹
	actualSize := int64(len(data))
	if fileSize > 0 {
		if actualSize > fileSize {
			return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡é¢„æœŸ: å®é™… %d å­—èŠ‚ > é¢„æœŸ %d å­—èŠ‚ï¼Œå¯èƒ½æ–‡ä»¶å·²æŸå", actualSize, fileSize)
		}
		if actualSize != fileSize {
			fs.Debugf(f, "æ–‡ä»¶å¤§å°ä¸é¢„æœŸä¸ç¬¦: å®é™… %d å­—èŠ‚ï¼Œé¢„æœŸ %d å­—èŠ‚", actualSize, fileSize)
		}
	}

	// Calculate MD5 hash
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

	// ä½¿ç”¨è®¡ç®—å‡ºçš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ 
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ æˆåŠŸï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬é¿å…äº†å®é™…çš„æ•°æ®ä¼ è¾“
	if createResp.Data.Reuse {
		fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return f.createObject(src.Remote(), createResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®
	// ä½¿ç”¨bytes.NewReaderä»å†…å­˜ä¸­çš„æ•°æ®åˆ›å»ºreader
	fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®")
	err = f.uploadFile(ctx, bytes.NewReader(data), createResp, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return f.createObject(src.Remote(), createResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
}

// streamingPutWithBuffer ä½¿ç”¨ç¼“å†²ç­–ç•¥å¤„ç†è·¨äº‘ç›˜ä¼ è¾“
// è§£å†³"æºæ–‡ä»¶ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é—®é¢˜
func (f *Fs) streamingPutWithBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ä½¿ç”¨ç¼“å†²ç­–ç•¥è¿›è¡Œè·¨äº‘ç›˜ä¼ è¾“")

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
	fileSize := src.Size()

	// å¦‚æœæ–‡ä»¶å°äº100MBï¼Œç›´æ¥è¯»å…¥å†…å­˜
	if fileSize <= 100*1024*1024 {
		fs.Debugf(f, "å°æ–‡ä»¶(%s)ï¼Œä½¿ç”¨å†…å­˜ç¼“å†²", fs.SizeSuffix(fileSize))
		return f.streamingPutWithMemoryBuffer(ctx, in, src, parentFileID, fileName)
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²", fs.SizeSuffix(fileSize))
	return f.streamingPutWithTempFile(ctx, in, src, parentFileID, fileName)
}

// streamingPutWithMemoryBuffer ä½¿ç”¨å†…å­˜ç¼“å†²å¤„ç†å°æ–‡ä»¶
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) streamingPutWithMemoryBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// Calculate MD5
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

	fs.Debugf(f, "å†…å­˜ç¼“å†²å®Œæˆï¼Œæ–‡ä»¶å¤§å°: %d, MD5: %s", len(data), md5Hash)

	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
	if len(data) < singleStepUploadLimit {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", len(data), singleStepUploadLimit)

		result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
		if err != nil {
			fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ æ–¹å¼: %v", err)
			// å¦‚æœå•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿçš„åˆ†ç‰‡ä¸Šä¼ æ–¹å¼
			// ä½¿ç”¨skipSingleStepæ ‡å¿—é˜²æ­¢æ— é™é€’å½’
			return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
		}

		fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
		return result, nil
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼
	fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes >= %dï¼Œä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼", len(data), singleStepUploadLimit)
	return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
}

// streamingPutWithTempFile ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²å¤„ç†å¤§æ–‡ä»¶
func (f *Fs) streamingPutWithTempFile(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¯¹äºè¶…å¤§æ–‡ä»¶ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥
	fileSize := src.Size()
	if fileSize > singleStepUploadLimit { // å¤§äºå•æ­¥ä¸Šä¼ é™åˆ¶çš„æ–‡ä»¶ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“
		fs.Debugf(f, "è¶…å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“ç­–ç•¥ï¼ˆæ— æ–­ç‚¹ç»­ä¼ ï¼‰", fs.SizeSuffix(fileSize))
		return f.streamingPutWithChunks(ctx, in, src, parentFileID, fileName)
	}

	// å¯¹äºè¾ƒå°çš„å¤§æ–‡ä»¶ï¼Œç»§ç»­ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²ç­–ç•¥", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - ä½¿ç”¨å®‰å…¨çš„èµ„æºç®¡ç†
	tempFile, err := os.CreateTemp("", "rclone-123pan-*")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		// å®‰å…¨çš„èµ„æºæ¸…ç†ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æ‰§è¡Œ
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// è¾¹è¯»è¾¹å†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆï¼Œå†™å…¥: %d å­—èŠ‚, MD5: %s", written, md5Hash)

	// é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å·²çŸ¥MD5ä¸Šä¼ 
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// ç§»é™¤å†—ä½™çš„streamingPutWithTempFileForcedæ–¹æ³•ï¼Œç®€åŒ–ä¸Šä¼ é€»è¾‘

// streamingPutWithChunks ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“å¤„ç†è¶…å¤§æ–‡ä»¶
// ç®€åŒ–ç‰ˆæœ¬ï¼šæ— æ–­ç‚¹ç»­ä¼ åŠŸèƒ½ï¼Œèµ„æºå ç”¨æœ€å°‘ï¼ˆåªéœ€è¦å•ä¸ªåˆ†ç‰‡çš„ä¸´æ—¶å­˜å‚¨ï¼‰
// è¿™æ˜¯è¶…å¤§æ–‡ä»¶ä¼ è¾“çš„ç­–ç•¥ï¼šèŠ‚çœèµ„æºçš„åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) streamingPutWithChunks(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// ä¸ºå¤§æ–‡ä»¶æ·»åŠ ç”¨æˆ·å‹å¥½çš„åˆå§‹åŒ–æç¤º
	fileSize := src.Size()
	if fileSize > 1024*1024*1024 { // å¤§äº1GBçš„æ–‡ä»¶
		fs.Infof(f, "ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤§æ–‡ä»¶ä¸Šä¼  (%s) - å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°ï¼Œè¯·ç¨å€™...", fs.SizeSuffix(fileSize))
	}

	// éªŒè¯æºå¯¹è±¡
	srcObj, ok := src.(fs.Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFile(ctx, in, src, parentFileID, fileName)
	}

	fileSize = src.Size()

	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆå…ˆåˆ›å»ºä¼šè¯è·å–APIè¦æ±‚çš„SliceSizeï¼‰
	createResp, err := f.createChunkedUploadSession(ctx, parentFileID, fileName, fileSize)
	if err != nil {
		return nil, err
	}

	// å…³é”®ä¿®å¤ï¼šä½¿ç”¨APIè¿”å›çš„SliceSizeï¼Œè€Œä¸æ˜¯åŠ¨æ€è®¡ç®—çš„åˆ†ç‰‡å¤§å°
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize <= 0 {
		// å¦‚æœAPIæ²¡æœ‰è¿”å›æœ‰æ•ˆçš„SliceSizeï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€
		fs.Debugf(f, "âš ï¸ APIæœªè¿”å›æœ‰æ•ˆSliceSize(%d)ï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€", apiSliceSize)
		apiSliceSize = 64 * 1024 * 1024 // 64MBå›é€€å¤§å°
	}

	fs.Debugf(f, "å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“ï¼šæ–‡ä»¶å¤§å° %sï¼ŒAPIè¦æ±‚åˆ†ç‰‡å¤§å° %s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(apiSliceSize))

	// å¦‚æœæ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	if createResp.Data.Reuse {
		fs.Debugf(f, "æ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFile(ctx, in, src, parentFileID, fileName)
	}

	// å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“ï¼Œä½¿ç”¨APIè¦æ±‚çš„åˆ†ç‰‡å¤§å°
	return f.uploadFileInChunks(ctx, srcObj, createResp, fileSize, apiSliceSize, fileName)
}

// ChunkedUploadParams removed - was unused

// createChunkedUploadSession åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯
func (f *Fs) createChunkedUploadSession(ctx context.Context, parentFileID int64, fileName string, fileSize int64) (*UploadCreateResp, error) {
	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆä½¿ç”¨ä¸´æ—¶MD5ï¼Œåç»­ä¼šæ›´æ–°ï¼‰
	tempMD5 := fmt.Sprintf("%032x", time.Now().UnixNano())
	createResp, err := f.createUpload(ctx, parentFileID, fileName, tempMD5, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	return createResp, nil
}

// uploadFileInChunks ç®€åŒ–çš„åˆ†ç‰‡æµå¼ä¸Šä¼ ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶
func (f *Fs) uploadFileInChunks(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	fs.Debugf(f, "å¼€å§‹åˆ†ç‰‡æµå¼ä¸Šä¼ : %dä¸ªåˆ†ç‰‡", totalChunks)

	// ä½¿ç”¨å•çº¿ç¨‹ä¸Šä¼ ï¼Œç®€åŒ–é€»è¾‘
	return f.uploadChunksSingleThreaded(ctx, srcObj, createResp, fileSize, chunkSize, fileName)
}

// ConcurrencyParams å¹¶å‘å‚æ•°
type ConcurrencyParams struct {
	networkSpeed int64
	optimal      int
	actual       int
}

// ç§»é™¤prepareUploadProgresså‡½æ•°ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶

// ç§»é™¤validateUploadedChunkså‡½æ•°ï¼Œä½¿ç”¨rcloneæ ‡å‡†é‡è¯•æœºåˆ¶

// calculateConcurrencyParams è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
func (f *Fs) calculateConcurrencyParams(fileSize int64) *ConcurrencyParams {
	networkSpeed := f.detectNetworkSpeed(context.Background())
	optimalConcurrency := f.getOptimalConcurrency(fileSize, networkSpeed)

	// æ£€æŸ¥ç”¨æˆ·è®¾ç½®çš„æœ€å¤§å¹¶å‘æ•°é™åˆ¶ï¼Œä½¿ç”¨rcloneå…¨å±€é…ç½®
	maxConcurrency := optimalConcurrency
	globalTransfers := fs.GetConfig(context.Background()).Transfers
	if globalTransfers > 0 && globalTransfers < maxConcurrency {
		maxConcurrency = globalTransfers
	}

	return &ConcurrencyParams{
		networkSpeed: networkSpeed,
		optimal:      optimalConcurrency,
		actual:       maxConcurrency,
	}
}

// uploadChunksSingleThreaded ç®€åŒ–çš„å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) uploadChunksSingleThreaded(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	overallHasher := md5.New()

	fs.Debugf(f, "ä½¿ç”¨å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")

	// ç®€åŒ–çš„åˆ†ç‰‡ä¸Šä¼ å¾ªç¯ï¼Œæ— æ–­ç‚¹ç»­ä¼ 
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1

		// ä¸Šä¼ åˆ†ç‰‡
		if err := f.uploadSingleChunkWithStream(ctx, srcObj, overallHasher, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks); err != nil {
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		fs.Debugf(f, " åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ å¹¶è¿”å›ç»“æœ
	return f.finalizeChunkedUpload(ctx, createResp, overallHasher, fileSize, fileName, preuploadID)
}

// processSkippedChunkForMD5 å¤„ç†è·³è¿‡çš„åˆ†ç‰‡ä»¥è®¡ç®—MD5
func (f *Fs) processSkippedChunkForMD5(ctx context.Context, srcObj fs.Object, hasher hash.Hash, chunkIndex, chunkSize, fileSize int64, partNumber int64) error {
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	// è¯»å–åˆ†ç‰‡æ•°æ®ç”¨äºMD5è®¡ç®—
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	_, err = io.CopyN(hasher, chunkReader, actualChunkSize)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}

	return nil
}

// finalizeChunkedUpload å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¹¶è¿”å›ç»“æœ
func (f *Fs) finalizeChunkedUpload(ctx context.Context, _ *UploadCreateResp, hasher hash.Hash, fileSize int64, fileName, preuploadID string) (*Object, error) {
	// è®¡ç®—æœ€ç»ˆMD5
	finalMD5 := fmt.Sprintf("%x", hasher.Sum(nil))

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "åˆ†ç‰‡æµå¼ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return f.createObject(fileName, result.FileID, fileSize, finalMD5, time.Now(), false), nil
}

// attemptStreamingUpload å°è¯•æµå¼ä¸Šä¼ ä»¥é¿å…åŒå€æµé‡æ¶ˆè€—
// è¿™ä¸ªæ–¹æ³•å®ç°çœŸæ­£çš„è¾¹ä¸‹è¾¹ä¸Šä¼ è¾“ï¼Œæœ€å°åŒ–æµé‡ä½¿ç”¨
func (f *Fs) attemptStreamingUpload(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "å°è¯•æµå¼ä¸Šä¼ ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶å’ŒåŒå€æµé‡")

	// ä¼˜åŒ–çš„MD5è·å–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å·²çŸ¥å“ˆå¸Œï¼Œé¿å…é‡å¤è®¡ç®—
	var md5Hash string
	if hash, err := srcObj.Hash(ctx, fshash.MD5); err == nil && hash != "" {
		md5Hash = hash
		fs.Debugf(f, "ä½¿ç”¨æºå¯¹è±¡å·²çŸ¥MD5: %s", md5Hash)
	} else {
		// å¦‚æœæ²¡æœ‰å·²çŸ¥MD5ï¼Œè®¡ç®—MD5ï¼ˆåªè¯»å–ä¸€æ¬¡ï¼‰
		md5Reader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è®¡ç®—MD5: %w", err)
		}
		defer md5Reader.Close()

		// ä½¿ç”¨æ ‡å‡†MD5è®¡ç®—
		hasher := md5.New()
		_, err = io.Copy(hasher, md5Reader)
		if err != nil {
			return nil, err
		}
		md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
	}

	// ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å·²çŸ¥MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæµå¼ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// å¦‚æœè§¦å‘ç§’ä¼ ï¼Œç›´æ¥è¿”å›æˆåŠŸ
	if createResp.Data.Reuse {
		fs.Debugf(f, "æµå¼ä¼ è¾“è§¦å‘ç§’ä¼ ï¼ŒMD5: %s", md5Hash)
		return f.createObject(fileName, createResp.Data.FileID, srcObj.Size(), md5Hash, time.Now(), false), nil
	}

	// ç¬¬ä¸‰æ­¥ï¼šé‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œå®é™…ä¸Šä¼ 
	uploadReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•é‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸Šä¼ : %w", err)
	}
	defer uploadReader.Close()

	// æ‰§è¡Œå®é™…ä¸Šä¼ 
	err = f.uploadFile(ctx, uploadReader, createResp, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("æµå¼ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "æµå¼ä¸Šä¼ æˆåŠŸå®Œæˆï¼Œæ–‡ä»¶ID: %d", createResp.Data.FileID)

	return f.createObject(fileName, createResp.Data.FileID, srcObj.Size(), md5Hash, time.Now(), false), nil
}

// uploadSingleChunkWithStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
// è¿™æ˜¯åˆ†ç‰‡æµå¼ä¼ è¾“çš„æ ¸å¿ƒï¼šè¾¹ä¸‹è½½è¾¹ä¸Šä¼ ï¼Œåªéœ€è¦åˆ†ç‰‡å¤§å°çš„ä¸´æ—¶å­˜å‚¨
func (f *Fs) uploadSingleChunkWithStream(ctx context.Context, srcObj fs.Object, overallHasher io.Writer, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) error {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¼€å§‹æµå¼ä¸Šä¼ åˆ†ç‰‡ %d/%d (åç§»: %d, å¤§å°: %d)",
		partNumber, totalChunks, chunkStart, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡èŒƒå›´çš„æºæ–‡ä»¶æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æºæ–‡ä»¶æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæ­¤åˆ†ç‰‡ï¼ˆåªéœ€è¦åˆ†ç‰‡å¤§å°çš„å­˜å‚¨ï¼‰
	tempFile, err := os.CreateTemp("", fmt.Sprintf("rclone-123pan-chunk-%d-*", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// åˆ›å»ºåˆ†ç‰‡MD5è®¡ç®—å™¨
	chunkHasher := md5.New()

	// è¾¹è¯»è¾¹å†™ï¼šåŒæ—¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ã€åˆ†ç‰‡MD5å’Œæ•´ä½“MD5
	multiWriter := io.MultiWriter(tempFile, chunkHasher, overallHasher)

	// æµå¼ä¼ è¾“åˆ†ç‰‡æ•°æ®
	written, err := io.Copy(multiWriter, chunkReader)
	if err != nil {
		return fmt.Errorf("æµå¼ä¼ è¾“åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
	}

	if written != actualChunkSize {
		return fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, written)
	}

	// é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶åˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return fmt.Errorf("é‡æ–°å®šä½åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// æ³¨æ„ï¼šæ–°çš„uploadPartWithMultipartæ–¹æ³•å†…éƒ¨ä¼šè·å–ä¸Šä¼ åŸŸåï¼Œè¿™é‡Œä¸å†éœ€è¦

	// è¯»å–ä¸´æ—¶æ–‡ä»¶æ•°æ®ç”¨äºä¸Šä¼ 
	chunkData, err := io.ReadAll(tempFile)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	chunkMD5 := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err = f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkMD5, chunkData)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}
	fs.Debugf(f, "åˆ†ç‰‡ %d æµå¼ä¸Šä¼ æˆåŠŸï¼Œå¤§å°: %d, MD5: %s", partNumber, written, chunkMD5)

	return nil
}

// ç§»é™¤uploadChunksWithConcurrencyå‡½æ•°ï¼Œè¿‡åº¦å¤æ‚çš„å¹¶å‘ä¸Šä¼ å’Œæ–­ç‚¹ç»­ä¼ é€»è¾‘

// ç§»é™¤æœªä½¿ç”¨çš„chunkResultç»“æ„ä½“å’Œç›¸å…³æ–¹æ³•

// uploadSingleChunkWithHash å¹¶å‘ç‰ˆæœ¬çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼Œè¿”å›å“ˆå¸Œå’Œæ•°æ®ç”¨äºç´¯ç§¯
func (f *Fs) uploadSingleChunkWithHash(ctx context.Context, srcObj fs.Object, _ string, chunkIndex, chunkSize, fileSize, totalChunks int64) (string, []byte, error) {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ %d/%dï¼ˆå«å“ˆå¸Œï¼‰ï¼ŒèŒƒå›´: %d-%dï¼Œå¤§å°: %d", partNumber, totalChunks, chunkStart, chunkEnd-1, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡æ•°æ®æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return "", nil, fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æ•°æ®æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// è¯»å–åˆ†ç‰‡æ•°æ®åˆ°å†…å­˜ï¼ˆç”¨äºå“ˆå¸Œè®¡ç®—å’Œä¸Šä¼ ï¼‰
	chunkData, err := io.ReadAll(chunkReader)
	if err != nil {
		return "", nil, fmt.Errorf("è¯»å–åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	if int64(len(chunkData)) != actualChunkSize {
		return "", nil, fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, len(chunkData))
	}

	// è®¡ç®—åˆ†ç‰‡MD5å“ˆå¸Œ
	chunkHasher := md5.New()
	chunkHasher.Write(chunkData)
	chunkHash := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return "", nil, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥ åˆ†ç‰‡ %d: %w", partNumber, err)
	}

	// æ„é€ åˆ†ç‰‡ä¸Šä¼ URL
	uploadURL := fmt.Sprintf("%s/upload/v2/file/slice", uploadDomain)

	// ä½¿ç”¨æµå¼ä¸Šä¼ é¿å…å¤§å†…å­˜å ç”¨
	err = f.uploadPartStream(ctx, uploadURL, bytes.NewReader(chunkData), actualChunkSize)
	if err != nil {
		return "", nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d/%d å¹¶å‘ä¸Šä¼ æˆåŠŸï¼ŒMD5: %s", partNumber, totalChunks, chunkHash)
	return chunkHash, chunkData, nil
}

// singleStepUpload ä½¿ç”¨123äº‘ç›˜çš„å•æ­¥ä¸Šä¼ APIä¸Šä¼ å°æ–‡ä»¶ï¼ˆ<1GBï¼‰
// è¿™ä¸ªAPIä¸“é—¨ä¸ºå°æ–‡ä»¶è®¾è®¡ï¼Œä¸€æ¬¡HTTPè¯·æ±‚å³å¯å®Œæˆä¸Šä¼ ï¼Œæ•ˆç‡æ›´é«˜
func (f *Fs) singleStepUpload(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	startTime := time.Now()
	fs.Debugf(f, "ä½¿ç”¨å•æ­¥ä¸Šä¼ APIï¼Œæ–‡ä»¶: %s, å¤§å°: %d bytes, MD5: %s", fileName, len(data), md5Hash)

	// ä¿®å¤ï¼šå…ˆæ£€æŸ¥ç§’ä¼ ï¼Œé¿å…ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“
	fs.Debugf(f, "Checking instant upload before single-step upload...")
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, int64(len(data)))
	if err != nil {
		fs.Debugf(f, "Instant upload check failed, continuing with single-step upload: %v", err)
	} else if isInstant {
		duration := time.Since(startTime)
		fs.Debugf(f, "Single-step instant upload successful! FileID: %d, duration: %v", createResp.Data.FileID, duration)
		return f.createObjectFromUpload(fileName, int64(len(data)), md5Hash, createResp.Data.FileID, time.Now()), nil
	}

	fs.Debugf(f, "Instant upload failed, starting actual single-step upload...")

	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " å•æ­¥ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		fs.Debugf(f, "âš ï¸ å•æ­¥ä¸Šä¼ : é¢„éªŒè¯å‘ç°çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œä½†ç»§ç»­å°è¯•ï¼ˆå¯èƒ½æ˜¯APIå·®å¼‚ï¼‰", parentFileID)
		// ä¸ç«‹å³è¿”å›é”™è¯¯ï¼Œè®©APIè°ƒç”¨æ¥æœ€ç»ˆå†³å®šï¼Œå› ä¸ºä¸åŒAPIå¯èƒ½æœ‰ä¸åŒçš„éªŒè¯é€»è¾‘
	} else {
		fs.Debugf(f, " å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•ID %d é¢„éªŒè¯é€šè¿‡", parentFileID)
	}

	// éªŒè¯æ–‡ä»¶å¤§å°é™åˆ¶
	if len(data) > singleStepUploadLimit {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡å•æ­¥ä¸Šä¼ é™åˆ¶%d bytes: %d bytes", singleStepUploadLimit, len(data))
	}

	// ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åéªŒè¯å’Œæ¸…ç†
	cleanedFileName, warning := f.validateAndCleanFileNameUnified(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// ä½¿ç”¨æ ‡å‡†APIè°ƒç”¨è¿›è¡Œå•æ­¥ä¸Šä¼ 
	var uploadResp struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			FileID    int64 `json:"fileID"`
			Completed bool  `json:"completed"`
		} `json:"data"`
	}

	// æ„é€ å•æ­¥ä¸Šä¼ è¯·æ±‚
	requestBody := map[string]any{
		"parentFileId": parentFileID,
		"filename":     fileName,
		"etag":         md5Hash,
		"size":         len(data),
	}

	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/single/create", "POST", requestBody, &uploadResp)
	if err != nil {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	// æ£€æŸ¥APIå“åº”ç 
	if uploadResp.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if uploadResp.Code == 1 && strings.Contains(uploadResp.Message, "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œæ¸…ç†ç¼“å­˜", parentFileID)
			// ç§»é™¤ç¼“å­˜æ¸…ç†

			// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯é€»è¾‘ï¼šå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID
			fs.Infof(f, "ğŸ”„ ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥")
			correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
			if err != nil {
				return nil, fmt.Errorf("è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
			}

			// å¦‚æœè·å¾—äº†ä¸åŒçš„çˆ¶ç›®å½•IDï¼Œé‡æ–°å°è¯•å•æ­¥ä¸Šä¼ 
			if correctParentID != parentFileID {
				fs.Infof(f, "ğŸ”„ å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œé‡æ–°å°è¯•å•æ­¥ä¸Šä¼ ", correctParentID, parentFileID)
				return f.singleStepUpload(ctx, data, correctParentID, fileName, md5Hash)
			}

			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
		}
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸Šä¼ å®Œæˆ
	if !uploadResp.Data.Completed {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ æœªå®Œæˆï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ")
	}

	duration := time.Since(startTime)
	speed := float64(len(data)) / duration.Seconds() / 1024 / 1024 // MB/s

	// å…³é”®ä¿®å¤ï¼šæ ¹æ®ä¸Šä¼ é€Ÿåº¦åˆ¤æ–­æ˜¯å¦ä¸ºç§’ä¼ 
	// ç§’ä¼ é€šå¸¸åœ¨å‡ ç§’å†…å®Œæˆï¼Œé€Ÿåº¦ä¼šéå¸¸å¿«ï¼ˆ>100MB/sï¼‰
	if duration < 5*time.Second && speed > 100 {
		fs.Errorf(f, "ğŸ‰ ã€é‡è¦è°ƒè¯•ã€‘å•æ­¥ä¸Šä¼ ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶ID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	} else {
		fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	}

	// è¿”å›Object
	return f.createObject(fileName, uploadResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
}

// validateAndCleanFileNameUnified ç»Ÿä¸€çš„éªŒè¯å’Œæ¸…ç†å…¥å£ç‚¹
// è¿™æ˜¯æ¨èçš„æ–‡ä»¶åå¤„ç†æ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ä¸Šä¼ è·¯å¾„çš„ä¸€è‡´æ€§
func (f *Fs) validateAndCleanFileNameUnified(fileName string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(fileName); err == nil {
		return fileName, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(fileName)

	// éªŒè¯æ¸…ç†åçš„æ–‡ä»¶å
	if err := validateFileName(cleanedName); err != nil {
		// å¦‚æœæ¸…ç†åä»ç„¶æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åç§°
		cleanedName = "cleaned_file"
		warning = fmt.Errorf("åŸå§‹æ–‡ä»¶å '%s' æ— æ³•æ¸…ç†ä¸ºæœ‰æ•ˆåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§° '%s'", fileName, cleanedName)
	} else {
		warning = fmt.Errorf("æ–‡ä»¶å '%s' åŒ…å«æ— æ•ˆå­—ç¬¦ï¼Œå·²æ¸…ç†ä¸º '%s'", fileName, cleanedName)
	}

	return cleanedName, warning
}

// getUploadDomain è·å–ä¸Šä¼ åŸŸåï¼Œæ”¯æŒåŠ¨æ€è·å–å’Œç¼“å­˜
func (f *Fs) getUploadDomain(ctx context.Context) (string, error) {

	// å°è¯•åŠ¨æ€è·å–ä¸Šä¼ åŸŸå - ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„
	var response struct {
		Code    int      `json:"code"`
		Data    []string `json:"data"` // ä¿®æ­£ï¼šdataæ˜¯å­—ç¬¦ä¸²æ•°ç»„
		Message string   `json:"message"`
		TraceID string   `json:"x-traceID"`
	}

	// ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„è°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	err := f.makeAPICallWithRest(ctx, "/upload/v2/file/domain", "GET", nil, &response)
	if err == nil && response.Code == 0 && len(response.Data) > 0 {
		domain := response.Data[0] // å–ç¬¬ä¸€ä¸ªåŸŸå
		fs.Debugf(f, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåæˆåŠŸ: %s", domain)

		fs.Debugf(f, "ä¸Šä¼ åŸŸåè·å–æˆåŠŸï¼Œä¸ä½¿ç”¨ç¼“å­˜: %s", domain)

		return domain, nil
	}

	// å¦‚æœåŠ¨æ€è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå
	fs.Debugf(f, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå: %v", err)
	defaultDomains := []string{
		"https://openapi-upload.123242.com",
		"https://openapi-upload.123pan.com", // å¤‡ç”¨åŸŸå
	}

	// ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„åŸŸå
	for _, domain := range defaultDomains {
		// è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
		fs.Debugf(f, "ä½¿ç”¨é»˜è®¤ä¸Šä¼ åŸŸå: %s", domain)
		return domain, nil
	}

	return defaultDomains[0], nil
}

// deleteFile é€šè¿‡ç§»åŠ¨åˆ°å›æ”¶ç«™æ¥åˆ é™¤æ–‡ä»¶
func (f *Fs) deleteFile(ctx context.Context, fileID int64) error {
	fs.Debugf(f, "åˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)

	reqBody := map[string]any{
		"fileIDs": []int64{fileID},
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/trash", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("åˆ é™¤æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ é™¤å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸåˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)
	return nil
}

// moveFile å°†æ–‡ä»¶ç§»åŠ¨åˆ°ä¸åŒçš„ç›®å½•
func (f *Fs) moveFile(ctx context.Context, fileID, toParentFileID int64) error {
	fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)

	reqBody := map[string]any{
		"fileIDs":        []int64{fileID},
		"toParentFileID": toParentFileID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/move", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("ç§»åŠ¨æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("ç§»åŠ¨å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)
	return nil
}

// renameFile é‡å‘½åæ–‡ä»¶ - ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI
func (f *Fs) renameFile(ctx context.Context, fileID int64, fileName string) error {
	fs.Debugf(f, "é‡å‘½åæ–‡ä»¶%dä¸º: %s", fileID, fileName)

	// éªŒè¯æ–°æ–‡ä»¶å
	if err := validateFileName(fileName); err != nil {
		return fmt.Errorf("é‡å‘½åæ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}
	time.Sleep(2 * time.Second)
	// ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI: PUT /api/v1/file/name
	reqBody := map[string]any{
		"fileId":   fileID,
		"fileName": fileName,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// å®ç°é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
	maxRetries := 3
	var err error
	for attempt := 1; attempt <= maxRetries; attempt++ {
		fs.Debugf(f, "é‡å‘½åæ–‡ä»¶å°è¯• %d/%d", attempt, maxRetries)

		err = f.makeAPICallWithRest(ctx, "/api/v1/file/name", "PUT", reqBody, &response)
		if err == nil {
			fs.Debugf(f, "é‡å‘½åæ–‡ä»¶æˆåŠŸ: %d -> %s", fileID, fileName)
			return nil
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯"æ–‡ä»¶æœªæ‰¾åˆ°"é”™è¯¯
		if strings.Contains(err.Error(), "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶") || strings.Contains(err.Error(), "APIé”™è¯¯ 1") {
			if attempt < maxRetries {
				waitTime := time.Duration(attempt*2) * time.Second
				fs.Debugf(f, "æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç­‰å¾…%våé‡è¯• (å°è¯• %d/%d)", waitTime, attempt, maxRetries)
				time.Sleep(waitTime)
				continue
			}
		}

		// å…¶ä»–é”™è¯¯æˆ–æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
		break
	}

	return fmt.Errorf("é‡å‘½åæ–‡ä»¶å¤±è´¥ (å°è¯•%dæ¬¡): %w", maxRetries, err)
}

// handlePartialFileConflict å¤„ç†partialæ–‡ä»¶é‡å‘½åå†²çª
// å½“partialæ–‡ä»¶å°è¯•é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶åæ—¶å‘ç”Ÿå†²çªæ—¶è°ƒç”¨
func (f *Fs) handlePartialFileConflict(ctx context.Context, fileID int64, partialName string, parentID int64, targetName string) (*Object, error) {
	fs.Debugf(f, "å¤„ç†partialæ–‡ä»¶å†²çª: %s -> %s (çˆ¶ç›®å½•: %d)", partialName, targetName, parentID)

	// ç§»é™¤.partialåç¼€è·å–åŸå§‹æ–‡ä»¶å
	originalName := strings.TrimSuffix(partialName, ".partial")
	if targetName == "" {
		targetName = originalName
	}

	// æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
	exists, existingFileID, err := f.fileExistsInDirectory(ctx, parentID, targetName)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if exists {
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼ŒID: %d", existingFileID)

		// å¦‚æœå­˜åœ¨çš„æ–‡ä»¶å°±æ˜¯å½“å‰æ–‡ä»¶ï¼ˆå¯èƒ½å·²ç»é‡å‘½åæˆåŠŸï¼‰
		if existingFileID == fileID {
			fs.Debugf(f, "æ–‡ä»¶å·²æˆåŠŸé‡å‘½åï¼Œè¿”å›æˆåŠŸå¯¹è±¡")
			return f.createObject(targetName, fileID, -1, "", time.Now(), false), nil
		}

		// å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶åå†²çªï¼Œç”Ÿæˆå”¯ä¸€åç§°")
		uniqueName, err := f.generateUniqueFileName(ctx, parentID, targetName)
		if err != nil {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
			return nil, fmt.Errorf("ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %w", err)
		}

		fs.Logf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", targetName, uniqueName)
		targetName = uniqueName
	}

	// å°è¯•é‡å‘½åpartialæ–‡ä»¶ä¸ºç›®æ ‡åç§°
	fs.Debugf(f, "é‡å‘½åpartialæ–‡ä»¶: %d -> %s", fileID, targetName)
	err = f.renameFile(ctx, fileID, targetName)
	if err != nil {
		// å¦‚æœé‡å‘½åä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²ç»æœ‰æ­£ç¡®åç§°
		if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
			strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
			fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œæ£€æŸ¥æ–‡ä»¶å½“å‰çŠ¶æ€")

			// å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰æ­£ç¡®åç§°
			exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, parentID, targetName)
			if checkErr == nil && exists && existingFileID == fileID {
				fs.Debugf(f, "æ–‡ä»¶å®é™…å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæˆåŠŸ")
				return &Object{
					fs:          f,
					remote:      targetName,
					hasMetaData: true,
					id:          strconv.FormatInt(fileID, 10),
					size:        -1, // éœ€è¦é‡æ–°è·å–
					modTime:     time.Now(),
					isDir:       false,
				}, nil
			}
		}

		return nil, fmt.Errorf("partialæ–‡ä»¶é‡å‘½åå¤±è´¥: %w", err)
	}

	fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åæˆåŠŸ: %s", targetName)
	return &Object{
		fs:          f,
		remote:      targetName,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        -1, // éœ€è¦é‡æ–°è·å–
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// getUserInfo è·å–ç”¨æˆ·ä¿¡æ¯åŒ…æ‹¬å­˜å‚¨é…é¢ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
func (f *Fs) getUserInfo(ctx context.Context) (*UserInfoResp, error) {
	var response UserInfoResp

	// ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯æ–¹æ³•ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶å’Œé‡è¯•æœºåˆ¶
	err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
	if err != nil {
		return nil, err
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	return &response, nil
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†HTTPæ–¹æ³•ï¼Œæ”¯æŒè‡ªåŠ¨QPSé™åˆ¶å’Œç»Ÿä¸€é”™è¯¯å¤„ç†

// ç§»é™¤initializeOptionså‡½æ•°ï¼Œå·²åˆå¹¶åˆ°NewFsä¸­

// checkInstantUpload ç»Ÿä¸€çš„ç§’ä¼ æ£€æŸ¥å‡½æ•°
func (f *Fs) checkInstantUpload(ctx context.Context, parentFileID int64, fileName, md5Hash string, fileSize int64) (*UploadCreateResp, bool, error) {
	fs.Debugf(f, "Checking instant upload...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, false, fmt.Errorf("failed to create upload session: %w", err)
	}

	if createResp.Data.Reuse {
		fs.Debugf(f, "Instant upload successful! Size: %s, MD5: %s", fs.SizeSuffix(fileSize), md5Hash)
		return createResp, true, nil
	}

	fs.Debugf(f, "Need actual upload, size: %s", fs.SizeSuffix(fileSize))
	return createResp, false, nil
}

// createObjectFromUpload ä»ä¸Šä¼ å“åº”åˆ›å»ºObjectå¯¹è±¡
func (f *Fs) createObjectFromUpload(fileName string, fileSize int64, md5Hash string, fileID int64, modTime time.Time) *Object {
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          fmt.Sprintf("%d", fileID),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       false,
	}
}

// createObject é€šç”¨çš„Objectåˆ›å»ºå‡½æ•°ï¼Œç»Ÿä¸€æ‰€æœ‰Objectåˆ›å»ºé€»è¾‘
func (f *Fs) createObject(remote string, fileID int64, size int64, md5Hash string, modTime time.Time, isDir bool) *Object {
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        size,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       isDir,
	}
}

// createBasicFs åˆ›å»ºåŸºç¡€æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
func createBasicFs(ctx context.Context, name, originalName, normalizedRoot string, opt *Options, m configmap.Mapper) *Fs {
	// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(ctx)

	f := &Fs{
		name:         name,
		originalName: originalName,
		root:         normalizedRoot,
		opt:          *opt,
		m:            m,
		rootFolderID: opt.RootFolderID,
		rst:          rest.NewClient(client).SetRoot(openAPIRootURL), // rcloneæ ‡å‡†restå®¢æˆ·ç«¯
	}

	fs.Debugf(f, " createBasicFså®Œæˆ: features=%p", f.features)
	return f
}

// ç§»é™¤initializePacerså‡½æ•°ï¼Œå·²åˆå¹¶åˆ°NewFsä¸­
/*
func initializePacers(ctx context.Context, f *Fs, opt *Options) error {
	// v2 API pacer - é«˜é¢‘ç‡ (~14 QPS for api/v2/file/list)
	// ä½¿ç”¨é»˜è®¤çš„åˆ—è¡¨APIé¢‘ç‡é™åˆ¶
	f.listPacer = createPacer(ctx, listV2APIMinSleep, maxSleep, decayConstant)

	// ä¸¥æ ¼API pacer - ä¸­ç­‰é¢‘ç‡ (~4 QPS for create, async_result, etc.)
	strictPacerSleep := fileMoveMinSleep
	if opt.StrictPacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.StrictPacerMinSleep)
	}
	f.strictPacer = createPacer(ctx, strictPacerSleep, maxSleep, decayConstant)

	// v2åˆ†ç‰‡ä¸Šä¼ API pacer (~5 QPS for upload/v2/file/slice)
	uploadPacerSleep := uploadV2SliceMinSleep
	if opt.UploadPacerMinSleep > 0 {
		uploadPacerSleep = time.Duration(opt.UploadPacerMinSleep)
	}
	f.uploadPacer = createPacer(ctx, uploadPacerSleep, maxSleep, decayConstant)

	// ä¸‹è½½å’Œé«˜é¢‘ç‡API pacer (~16 QPS for v2 upload APIs)
	downloadPacerSleep := downloadInfoMinSleep
	if opt.DownloadPacerMinSleep > 0 {
		downloadPacerSleep = time.Duration(opt.DownloadPacerMinSleep)
	}
	f.downloadPacer = createPacer(ctx, downloadPacerSleep, maxSleep, decayConstant)

	fs.Infof(f, "ğŸ“Š QPSé™åˆ¶é…ç½® - åˆ—è¡¨API: %.1f QPS, ä¸¥æ ¼API: %.1f QPS, ä¸Šä¼ API: %.1f QPS, ä¸‹è½½API: %.1f QPS",
		1000.0/float64(listV2APIMinSleep.Milliseconds()),
		1000.0/float64(strictPacerSleep.Milliseconds()),
		1000.0/float64(uploadPacerSleep.Milliseconds()),
		1000.0/float64(downloadPacerSleep.Milliseconds()))

	return nil
}
*/

// registerCleanupHooks æ³¨å†Œèµ„æºæ¸…ç†é’©å­
func registerCleanupHooks(f *Fs) {
	// æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„èµ„æºæ¸…ç†é’©å­
	// ç¡®ä¿å³ä½¿ç¨‹åºå¼‚å¸¸é€€å‡ºä¹Ÿèƒ½æ¸…ç†ä¸´æ—¶æ–‡ä»¶
	atexit.Register(func() {
		fs.Debugf(f, "ğŸš¨ ç¨‹åºé€€å‡ºä¿¡å·æ£€æµ‹åˆ°ï¼Œå¼€å§‹æ¸…ç†123ç½‘ç›˜èµ„æº...")

		// ç§»é™¤ç¼“å­˜æ¸…ç†ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜
	})
	fs.Debugf(f, "ğŸ›¡ï¸ ç¨‹åºé€€å‡ºæ¸…ç†é’©å­æ³¨å†ŒæˆåŠŸ")
}

// ç§»é™¤initializeFeatureså‡½æ•°ï¼Œå·²åˆå¹¶åˆ°NewFsä¸­ä½¿ç”¨æ ‡å‡†Fillæ¨¡å¼

// initializeAuthentication åˆå§‹åŒ–è®¤è¯
func initializeAuthentication(ctx context.Context, f *Fs, m configmap.Mapper) error {
	// Check if we have saved token in config file
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %vï¼ˆè¿‡æœŸæ—¶é—´ %vï¼‰", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if tokenLoaded {
		// Check if token is expired or will expire soon
		if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
			fs.Debugf(f, "ä»¤ç‰Œå·²è¿‡æœŸæˆ–å³å°†è¿‡æœŸï¼Œç«‹å³åˆ·æ–°")
			tokenRefreshNeeded = true
		}
	} else {
		// No token, so login
		fs.Debugf(f, "æœªæ‰¾åˆ°ä»¤ç‰Œï¼Œå°è¯•ç™»å½•")
		err := f.GetAccessToken(ctx)
		if err != nil {
			return fmt.Errorf("initial login failed: %w", err)
		}
		// Save token to config after successful login
		f.saveToken(ctx, m)
		fs.Debugf(f, "ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Try to refresh the token if needed
	if tokenRefreshNeeded {
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œå°è¯•å®Œæ•´ç™»å½•: %v", err)
			// If refresh fails, try full login
			err = f.GetAccessToken(ctx)
			if err != nil {
				return fmt.Errorf("login failed after token refresh failure: %w", err)
			}
		}
		// Save the refreshed/new token
		f.saveToken(ctx, m)
		fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°/ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Setup token renewer for automatic refresh
	fs.Debugf(f, "è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨")
	f.setupTokenRenewer(ctx, m)

	return nil
}

// handleRootDirectory å¤„ç†æ ¹ç›®å½•æŸ¥æ‰¾æˆ–æ–‡ä»¶è·¯å¾„
func handleRootDirectory(ctx context.Context, f *Fs, root string) (fs.Fs, error) {
	// Find the root directory
	err := f.dirCache.FindRoot(ctx, false)
	if err != nil {
		// Assume it is a file
		newRoot, remote := dircache.SplitPath(root)
		// åˆ›å»ºæ–°çš„Fså®ä¾‹ï¼Œé¿å…å¤åˆ¶é”å€¼
		tempF := &Fs{
			name:          f.name,
			root:          newRoot,
			opt:           f.opt,
			features:      f.features, // ä¿®å¤ï¼šå¤åˆ¶featureså­—æ®µ
			m:             f.m,
			rootFolderID:  f.rootFolderID,
			rst:           f.rst, // ä¿®å¤ï¼šä¹Ÿå¤åˆ¶rstå­—æ®µ
			listPacer:     f.listPacer,
			strictPacer:   f.strictPacer,
			uploadPacer:   f.uploadPacer,
			downloadPacer: f.downloadPacer,
			// ç§»é™¤ç¼“å­˜å’Œç»Ÿä¸€ç»„ä»¶å­—æ®µï¼Œä¸å†ä½¿ç”¨
		}
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// ğŸ”§ ä¿®å¤ï¼šå¯¹äºbackendå‘½ä»¤ï¼Œä¸è¿”å›ErrorIsFileï¼Œè€Œæ˜¯è¿”å›æ­£å¸¸çš„Fså®ä¾‹
		// è¿™æ ·å¯ä»¥è®©backendå‘½ä»¤æ­£å¸¸å·¥ä½œï¼ŒåŒæ—¶ä¿æŒæ–‡ä»¶å¯¹è±¡çš„å¼•ç”¨
		fs.Debugf(tempF, "æ–‡ä»¶è·¯å¾„å¤„ç†ï¼šåˆ›å»ºæ–‡ä»¶æ¨¡å¼Fså®ä¾‹ï¼Œæ–‡ä»¶: %s", remote)
		return tempF, nil
	}

	return f, nil
}

// newFs ä»è·¯å¾„æ„é€ Fsï¼Œæ ¼å¼ä¸º container:path
func newFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	// Parse config into Options struct
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, err
	}

	// Validate and normalize options
	if err := validateOptions(opt); err != nil {
		return nil, err
	}
	normalizeOptions(opt)

	// Parse root path
	originalName := name
	normalizedRoot := strings.Trim(root, "/")

	// Create HTTP client and rest client
	client := fshttp.NewClient(ctx)

	// Create Fs object
	f := &Fs{
		name:         name,
		originalName: originalName,
		root:         normalizedRoot,
		opt:          *opt,
		m:            m,
		rootFolderID: opt.RootFolderID,
		rst:          rest.NewClient(client).SetRoot(openAPIRootURL),
	}

	// Initialize pacers
	f.listPacer = createPacer(ctx, listV2APIMinSleep, maxSleep, decayConstant)
	strictPacerSleep := fileMoveMinSleep
	if opt.StrictPacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.StrictPacerMinSleep)
	}
	f.strictPacer = createPacer(ctx, strictPacerSleep, maxSleep, decayConstant)
	uploadPacerSleep := uploadV2SliceMinSleep // ä½¿ç”¨ä¸Šä¼ APIçš„é»˜è®¤é™åˆ¶
	if opt.UploadPacerMinSleep > 0 {
		uploadPacerSleep = time.Duration(opt.UploadPacerMinSleep)
	}
	f.uploadPacer = createPacer(ctx, uploadPacerSleep, maxSleep, decayConstant)
	downloadPacerSleep := downloadInfoMinSleep // ä½¿ç”¨ä¸‹è½½APIçš„é»˜è®¤é™åˆ¶
	if opt.DownloadPacerMinSleep > 0 {
		downloadPacerSleep = time.Duration(opt.DownloadPacerMinSleep)
	}
	f.downloadPacer = createPacer(ctx, downloadPacerSleep, maxSleep, decayConstant)

	// ç§»é™¤å¤æ‚çš„ç¼“å­˜å’Œç»Ÿä¸€ç»„ä»¶åˆå§‹åŒ–ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶

	// Initialize features
	f.features = (&fs.Features{
		ReadMimeType:            true,
		CanHaveEmptyDirectories: true,
		BucketBased:             false,
		SetTier:                 false,
		GetTier:                 false,
		DuplicateFiles:          false,
		ReadMetadata:            true,
		WriteMetadata:           false,
		UserMetadata:            false,
		PartialUploads:          false,
		NoMultiThreading:        false,
		SlowModTime:             true,
		SlowHash:                false,
	}).Fill(ctx, f)

	// Initialize directory cache
	f.dirCache = dircache.New(normalizedRoot, f.rootFolderID, f)

	// Initialize authentication
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %vï¼ˆè¿‡æœŸæ—¶é—´ %vï¼‰", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if !tokenLoaded {
		tokenRefreshNeeded = true
	} else if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		tokenRefreshNeeded = true
	}

	if tokenRefreshNeeded {
		if err := f.refreshTokenIfNecessary(ctx, true, false); err != nil {
			return nil, fmt.Errorf("åˆå§‹åŒ–è®¤è¯å¤±è´¥: %w", err)
		}
	}

	// Find the root directory
	err = f.dirCache.FindRoot(ctx, false)
	if err != nil {
		// Assume it is a file
		newRoot, remote := dircache.SplitPath(root)
		// Create new Fs instance to avoid copying mutex
		tempF := &Fs{
			name:          f.name,
			originalName:  f.originalName,
			root:          newRoot,
			opt:           f.opt,
			features:      f.features,
			rst:           f.rst,
			token:         f.token,
			tokenExpiry:   f.tokenExpiry,
			tokenRenewer:  f.tokenRenewer,
			m:             f.m,
			rootFolderID:  f.rootFolderID,
			listPacer:     f.listPacer,
			uploadPacer:   f.uploadPacer,
			downloadPacer: f.downloadPacer,
			strictPacer:   f.strictPacer,
		}
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// return an error with an fs which points to the parent
		return tempF, fs.ErrorIsFile
	}
	return f, nil
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨NewObject: %s", remote)

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "è§„èŒƒåŒ–åçš„è¿œç¨‹è·¯å¾„: %s -> %s", remote, normalizedRemote)

	// Get the full path
	var fullPath string
	if f.root == "" {
		fullPath = normalizedRemote
	} else if normalizedRemote == "" {
		// å½“remoteä¸ºç©ºæ—¶ï¼ŒfullPathå°±æ˜¯rootæœ¬èº«
		fullPath = f.root
	} else {
		fullPath = normalizePath(f.root + "/" + normalizedRemote)
	}

	if fullPath == "" {
		fullPath = "/"
	}

	// Get file ID
	fileID, err := f.pathToFileID(ctx, fullPath)
	if err != nil {
		if err == fs.ErrorObjectNotFound {
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, " NewObject pathToFileIDç»“æœ: fullPath=%s -> fileID=%s", fullPath, fileID)

	// Get file details
	fileInfo, err := f.getFileInfo(ctx, fileID)
	if err != nil {
		return nil, err
	}

	// Check if it's a file (not directory)
	// Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	if fileInfo.Type == 1 {
		return nil, fs.ErrorNotAFile
	}

	// ç¡®å®šæ­£ç¡®çš„remoteè·¯å¾„
	objectRemote := remote
	if objectRemote == "" && f.root != "" {
		// å½“remoteä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºremote
		objectRemote = fileInfo.Filename
		fs.Debugf(f, " NewObjectè®¾ç½®remote: %s -> %s", remote, objectRemote)
	}

	fileIDInt, _ := strconv.ParseInt(fileID, 10, 64)
	o := f.createObject(objectRemote, fileIDInt, fileInfo.Size, fileInfo.Etag, time.Now(), false)

	return o, nil
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	return fs.ModTimeNotSupported
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨Put: %s", src.Remote())

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(src.Remote())
	fs.Debugf(f, "Putæ“ä½œè§„èŒƒåŒ–è·¯å¾„: %s -> %s", src.Remote(), normalizedRemote)

	// Use dircache to find parent directory
	fs.Debugf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•è·¯å¾„: %s", normalizedRemote)
	leaf, parentID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		fs.Errorf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %v", err)
		return nil, err
	}
	fileName := leaf
	fs.Debugf(f, "æ‰¾åˆ°çˆ¶ç›®å½•ID: %s, æ–‡ä»¶å: %s", parentID, fileName)

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return nil, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯parentFileIDæ˜¯å¦çœŸçš„å­˜åœ¨
	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d æ˜¯å¦å­˜åœ¨", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		fs.Errorf(f, "éªŒè¯çˆ¶ç›®å½•IDå¤±è´¥: %v", err)
		// å°è¯•æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		fs.Debugf(f, "æ¸…ç†ç›®å½•ç¼“å­˜å¹¶é‡è¯•")
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else if !exists {
		fs.Errorf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•é‡å»ºç›®å½•ç¼“å­˜", parentFileID)
		// æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºç›®å½•ç¼“å­˜åæŸ¥æ‰¾å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºåè§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡å»ºåæ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// Use streaming optimization for cross-cloud transfers
	obj, err := f.streamingPut(ctx, in, src, parentFileID, fileName)

	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}

	return obj, err
}

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	// In a real implementation, you would fetch metadata if not already available.
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// In a real implementation, you would fetch metadata if not already available.
	return o.size
}

// Hash returns the MD5 checksum
func (o *Object) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t != fshash.MD5 {
		return "", fshash.ErrUnsupported
	}
	// In a real implementation, you would fetch metadata if not already available.
	return o.md5sum, nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// In a real implementation, you would fetch metadata if not already available.
	return o.id
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	// è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šæ£€æµ‹å¤§æ–‡ä»¶å¹¶å¯ç”¨å¤šçº¿ç¨‹ä¸‹è½½ï¼ˆå‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
	// ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç¦ç”¨å¹¶å‘ä¸‹è½½é€‰é¡¹ï¼Œé¿å…é‡å¤å¹¶å‘
	hasDisableOption := false
	for _, option := range options {
		if option.String() == "DisableConcurrentDownload" {
			hasDisableOption = true
			break
		}
	}

	if !hasDisableOption && o.size >= 10*1024*1024 { // 10MBé˜ˆå€¼
		return o.openWithConcurrency(ctx, options...)
	}

	var resp *http.Response

	// Use pacer for download with retry (use strict pacer for download URL requests)
	err := o.fs.strictPacer.Call(func() (bool, error) {
		// Get download URL (may need refresh)
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, nil, err)
		}

		// Create HTTP request
		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, fmt.Errorf("failed to create download request: %w", err)
		}

		// Handle range requests
		var start, end int64 = 0, -1
		for _, option := range options {
			switch x := option.(type) {
			case *fs.RangeOption:
				start, end = x.Start, x.End
			case *fs.SeekOption:
				start = x.Offset
			}
		}

		if start > 0 || end >= 0 {
			rangeHeader := fmt.Sprintf("bytes=%d-", start)
			if end >= 0 {
				rangeHeader = fmt.Sprintf("bytes=%d-%d", start, end)
			}
			req.Header.Set("Range", rangeHeader)
			fs.Debugf(o.fs, "è¯·æ±‚èŒƒå›´: %s", rangeHeader)
		}

		// Set user agent
		req.Header.Set("User-Agent", o.fs.opt.UserAgent)
		// Note: Don't set timeout here as it will be applied to the entire download

		// Make the request using rclone standard HTTP client
		client := fshttp.NewClient(ctx)
		resp, err = client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// Check status code
		switch resp.StatusCode {
		case http.StatusOK, http.StatusPartialContent:
			return false, nil // Success
		case http.StatusFound, http.StatusMovedPermanently, http.StatusTemporaryRedirect, http.StatusPermanentRedirect:
			// Let the HTTP client handle redirects automatically
			// This should not happen if the client is configured correctly
			resp.Body.Close()
			return false, fmt.Errorf("unexpected redirect response %d - client should handle this automatically", resp.StatusCode)
		case http.StatusRequestedRangeNotSatisfiable:
			resp.Body.Close()
			return false, fmt.Errorf("range not satisfiable")
		case http.StatusNotFound:
			resp.Body.Close()
			return false, fs.ErrorObjectNotFound
		default:
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("download failed with status %d: %s", resp.StatusCode, string(body)))
		}
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// ç§»é™¤shouldUseConcurrentDownloadæ–¹æ³•ï¼Œé€»è¾‘å·²å†…è”

// openWithConcurrency ä½¿ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨æ‰“å¼€æ–‡ä»¶ï¼ˆç”¨äºè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// Start concurrent download
	fs.Debugf(o, "Starting concurrent download for: %s", o.Remote())

	// Use custom concurrent download implementation
	return o.openWithCustomConcurrency(ctx, options...)
}

// openNormal æ™®é€šçš„æ‰“å¼€æ–¹æ³•ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
func (o *Object) openNormal(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	var resp *http.Response

	// ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½
	err := o.fs.downloadPacer.Call(func() (bool, error) {
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: %w", err))
		}

		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, err
		}

		// å¤„ç†Rangeé€‰é¡¹
		fs.FixRangeOption(options, o.size)
		fs.OpenOptionAddHTTPHeaders(req.Header, options)

		// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
		client := fshttp.NewClient(ctx)
		resp, err = client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err))
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d, å“åº”: %s", resp.StatusCode, string(body)))
		}

		return false, nil
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// openWithCustomConcurrency ä½¿ç”¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½ï¼ˆå½“ç»Ÿä¸€ä¸‹è½½å™¨ä¸å¯ç”¨æ—¶ï¼‰
func (o *Object) openWithCustomConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Infof(o, "ğŸš€ 123ç½‘ç›˜å¯åŠ¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "123_custom_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// ä½¿ç”¨ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é€»è¾‘
	err = o.downloadWithCustomConcurrency(ctx, tempFile, options...)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(o, "âœ… 123ç½‘ç›˜è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(o.size))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return &ConcurrentDownloadReader{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// downloadWithCustomConcurrency è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å®ç°
func (o *Object) downloadWithCustomConcurrency(ctx context.Context, tempFile *os.File, _ ...fs.OpenOption) error {
	// ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é…ç½®
	chunkSize := int64(50 * 1024 * 1024) // 50MBåˆ†ç‰‡
	maxConcurrency := 6                  // 6çº¿ç¨‹å¹¶å‘

	fileSize := o.size
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	if numChunks < int64(maxConcurrency) {
		maxConcurrency = int(numChunks)
	}

	fs.Infof(o, "ğŸ“Š 123ç½‘ç›˜è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// è·å–ä¸‹è½½URL
	downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
	if err != nil {
		return fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
	}

	// åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡
	var wg sync.WaitGroup
	errChan := make(chan error, maxConcurrency)
	semaphore := make(chan struct{}, maxConcurrency)

	for i := int64(0); i < numChunks; i++ {
		start := i * chunkSize
		end := start + chunkSize - 1
		if end >= fileSize {
			end = fileSize - 1
		}

		wg.Add(1)
		go func(chunkIndex, start, end int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// ä¸‹è½½åˆ†ç‰‡
			err := o.downloadChunk(ctx, downloadURL, tempFile, start, end, chunkIndex)
			if err != nil {
				errChan <- fmt.Errorf("åˆ†ç‰‡%dä¸‹è½½å¤±è´¥: %w", chunkIndex, err)
			}
		}(i, start, end)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	for err := range errChan {
		return err
	}

	return nil
}

// downloadChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡
func (o *Object) downloadChunk(ctx context.Context, url string, tempFile *os.File, start, end, chunkIndex int64) error {
	// åˆ›å»ºHTTPè¯·æ±‚
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}

	// è®¾ç½®Rangeå¤´
	req.Header.Set("Range", fmt.Sprintf("bytes=%d-%d", start, end))
	req.Header.Set("User-Agent", o.fs.opt.UserAgent)

	// æ‰§è¡Œè¯·æ±‚ï¼Œä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(ctx)
	resp, err := client.Do(req)
	if err != nil {
		return fmt.Errorf("æ‰§è¡Œä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode != http.StatusPartialContent && resp.StatusCode != http.StatusOK {
		return fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	// è¯»å–æ•°æ®
	expectedSize := end - start + 1
	buffer := make([]byte, expectedSize)
	totalRead := int64(0)

	for totalRead < expectedSize {
		n, err := resp.Body.Read(buffer[totalRead:])
		if n > 0 {
			totalRead += int64(n)
		}
		if err != nil {
			if err == io.EOF {
				break
			}
			return fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®å¤§å°
	if totalRead != expectedSize {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, totalRead)
	}

	// å†™å…¥åˆ°æ–‡ä»¶
	writtenBytes, err := tempFile.WriteAt(buffer[:totalRead], start)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯å†™å…¥çš„å­—èŠ‚æ•°
	if int64(writtenBytes) != totalRead {
		return fmt.Errorf("åˆ†ç‰‡å†™å…¥ä¸å®Œæ•´: æœŸæœ›å†™å…¥%dï¼Œå®é™…å†™å…¥%d", totalRead, writtenBytes)
	}

	fs.Debugf(o, " 123ç½‘ç›˜åˆ†ç‰‡%dä¸‹è½½å®Œæˆ: %s (bytes=%d-%d)",
		chunkIndex, fs.SizeSuffix(totalRead), start, end)

	return nil
}

// Update the object with new content.
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œå¤šçº¿ç¨‹ä¸Šä¼ 
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	fs.Debugf(o.fs, "è°ƒç”¨Update: %s", o.remote)

	startTime := time.Now()

	// Use dircache to find parent directory
	leaf, parentID, err := o.fs.dirCache.FindPath(ctx, o.remote, false)
	if err != nil {
		return err
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(o.fs, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedFileName
	}

	// ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡Œæ›´æ–°
	fs.Debugf(o.fs, "ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡ŒUpdateæ“ä½œ")
	newObj, err := o.fs.streamingPut(ctx, in, src, parentFileID, leaf)

	// è®°å½•ä¸Šä¼ å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(startTime)
	if err != nil {
		fs.Errorf(o.fs, "Updateæ“ä½œå¤±è´¥: %v", err)
		return err
	}

	// æ›´æ–°å½“å‰å¯¹è±¡çš„å…ƒæ•°æ®
	o.size = newObj.size
	o.md5sum = newObj.md5sum
	o.modTime = newObj.modTime
	o.id = newObj.id

	fs.Debugf(o.fs, "Updateæ“ä½œæˆåŠŸï¼Œè€—æ—¶: %v", duration)

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	fs.Debugf(o.fs, "è°ƒç”¨Remove: %s", o.remote)

	// æ£€æŸ¥æ–‡ä»¶IDæ˜¯å¦æœ‰æ•ˆ
	if o.id == "" {
		fs.Debugf(o.fs, "æ–‡ä»¶IDä¸ºç©ºï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶: %s", o.remote)
		// å¯¹äºpartialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬æ— æ³•åˆ é™¤ï¼Œä½†ä¹Ÿä¸åº”è¯¥æŠ¥é”™
		// å› ä¸ºè¿™äº›æ–‡ä»¶å¯èƒ½æ ¹æœ¬ä¸å­˜åœ¨äºæœåŠ¡å™¨ä¸Š
		return nil
	}

	// Convert file ID to int64
	fileID, err := parseFileID(o.id)
	if err != nil {
		fs.Debugf(o.fs, "è§£ææ–‡ä»¶IDå¤±è´¥ï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶: %s, é”™è¯¯: %v", o.remote, err)
		// å¯¹äºæ— æ³•è§£æçš„æ–‡ä»¶IDï¼Œæˆ‘ä»¬ä¹Ÿä¸æŠ¥é”™ï¼Œå› ä¸ºè¿™é€šå¸¸æ˜¯partialæ–‡ä»¶
		return nil
	}

	err = o.fs.deleteFile(ctx, fileID)
	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}
	return err
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// DirCacher interface implementation for dircache

// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the parent directoryï¼Œä½¿ç”¨rcloneå…¨å±€é…ç½®çš„æ£€æŸ¥å™¨æ•°é‡
	listChunk := fs.GetConfig(ctx).Checkers
	if listChunk <= 0 {
		listChunk = 100 // é»˜è®¤å€¼
	}
	response, err := f.ListFile(ctx, int(parentFileID), listChunk, "", "", 0)
	if err != nil {
		return "", false, err
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true
			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}
			return foundID, found, nil
		}
	}

	return "", false, nil
}

// findLeafWithForceRefresh å¼ºåˆ¶åˆ·æ–°ç¼“å­˜åæŸ¥æ‰¾å¶èŠ‚ç‚¹
// ç”¨äºè§£å†³ç¼“å­˜ä¸ä¸€è‡´å¯¼è‡´çš„ç›®å½•æŸ¥æ‰¾å¤±è´¥é—®é¢˜
func (f *Fs) findLeafWithForceRefresh(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// ç§»é™¤ç¼“å­˜æ¸…ç†

	// ç›´æ¥è°ƒç”¨APIè·å–æœ€æ–°çš„ç›®å½•åˆ—è¡¨ï¼Œè·³è¿‡ç¼“å­˜
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	// ä½¿ç”¨rcloneå…¨å±€é…ç½®çš„æ£€æŸ¥å™¨æ•°é‡ä½œä¸ºåˆ—è¡¨é™åˆ¶
	listChunk := fs.GetConfig(context.Background()).Checkers
	if listChunk <= 0 {
		listChunk = 100 // é»˜è®¤å€¼
	}
	params.Add("limit", fmt.Sprintf("%d", listChunk))

	endpoint := "/api/v2/file/list?" + params.Encode()

	var response ListResponse
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", false, fmt.Errorf("å¼ºåˆ¶åˆ·æ–°APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf in fresh data
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true

			// ç§»é™¤ç¼“å­˜ä¿å­˜

			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}

			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°å¶èŠ‚ç‚¹: %s -> %s", leaf, foundID)
			return foundID, found, nil
		}
	}

	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æœªæ‰¾åˆ°å¶èŠ‚ç‚¹: %s", leaf)
	return "", false, nil
}

// CreateDir creates a directory with the given name in the parent folder pathID.
// Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	fs.Debugf(f, "åˆ›å»ºç›®å½•: pathID=%s, leaf=%s", pathID, leaf)

	// éªŒè¯å‚æ•°
	if leaf == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if pathID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®å½•å
	cleanedDirName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(f, "ç›®å½•åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedDirName
	}

	// Create the directory using the existing createDirectory method
	dirID, err := f.createDirectory(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}

	return dirID, nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs          = (*Fs)(nil)
	_ fs.Object      = (*Object)(nil)
	_ fs.Mover       = (*Fs)(nil)
	_ fs.DirMover    = (*Fs)(nil)
	_ fs.Copier      = (*Fs)(nil)
	_ fs.Abouter     = (*Fs)(nil)
	_ fs.Purger      = (*Fs)(nil)
	_ fs.Shutdowner  = (*Fs)(nil)
	_ fs.ObjectInfo  = (*Object)(nil)
	_ fs.IDer        = (*Object)(nil)
	_ fs.SetModTimer = (*Object)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, _ configmap.Mapper) bool {
	if f.opt.Token == "" {
		fs.Debugf(f, "é…ç½®ä¸­æœªæ‰¾åˆ°ä»¤ç‰Œï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰")
		return false
	}

	var tokenData TokenJSON                                                // Use the new TokenJSON struct
	var err error                                                          // Declare err here
	if err = json.Unmarshal([]byte(f.opt.Token), &tokenData); err != nil { // Unmarshal unobscured token
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œå¤±è´¥ï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰: %v", err)
		return false
	}

	f.token = tokenData.AccessToken
	f.tokenExpiry, err = time.Parse(time.RFC3339, tokenData.Expiry) // Use tokenData.Expiry
	if err != nil {
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œè¿‡æœŸæ—¶é—´å¤±è´¥: %v", err)
		return false
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "é…ç½®ä¸­çš„ä»¤ç‰Œä¸å®Œæ•´")
		return false
	}

	fs.Debugf(f, "ä»é…ç½®æ–‡ä»¶åŠ è½½ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)
	return true
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(_ context.Context, m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - æä¾›çš„æ˜ å°„å™¨ä¸ºç©º")
		return
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	tokenData := TokenJSON{ // Use the new TokenJSON struct
		AccessToken: f.token,
		Expiry:      f.tokenExpiry.Format(time.RFC3339), // Use Expiry
	}
	tokenJSON, err := json.Marshal(tokenData)
	if err != nil {
		fs.Errorf(f, "ä¿å­˜ä»¤ç‰Œæ—¶åºåˆ—åŒ–å¤±è´¥: %v", err)
		return
	}

	m.Set("token", string(tokenJSON))
	// m.Set does not return an error, so we assume it succeeds.
	// Any errors related to config saving would be handled by the underlying config system.

	fs.Debugf(f, "ä½¿ç”¨åŸå§‹åç§°%qä¿å­˜ä»¤ç‰Œåˆ°é…ç½®æ–‡ä»¶", f.originalName)
}

// setupTokenRenewer initializes the token renewer to automatically refresh tokens
func (f *Fs) setupTokenRenewer(ctx context.Context, m configmap.Mapper) {
	// Only set up renewer if we have valid tokens
	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	// Create a renewal transaction function
	transaction := func() error {
		fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨è§¦å‘ï¼Œåˆ·æ–°ä»¤ç‰Œ")
		// Use non-global function to avoid deadlocks
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Errorf(f, "æ›´æ–°å™¨ä¸­åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %v", err)
			return err
		}

		return nil // saveToken is already called in refreshTokenIfNecessary
	}

	// Create minimal OAuth config
	config := &oauthutil.Config{
		TokenURL: openAPIRootURL + "/api/v1/access_token", // Use the correct token URL for 123Pan
	}

	// Create a token source using the existing token
	token := &oauth2.Token{
		AccessToken: f.token,
		// 123Pan API does not seem to return a refresh token in the initial access token response,
		// so we rely on re-logging in if the access token expires.
		// RefreshToken: f.refreshToken, // Not available for 123Pan
		Expiry:    f.tokenExpiry,
		TokenType: "Bearer",
	}

	// Save token to config so it can be accessed by TokenSource
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨ä¿å­˜ä»¤ç‰Œå¤±è´¥: %v", err)
		return
	}

	// Create a client with the token source
	_, ts, err := oauthutil.NewClientWithBaseClient(ctx, f.originalName, m, config, getHTTPClient(ctx, f.originalName, m))
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨åˆ›å»ºä»¤ç‰Œæºå¤±è´¥: %v", err)
		return
	}

	// Create token renewer that will trigger when the token is about to expire
	f.tokenRenewer = oauthutil.NewRenew(f.originalName, ts, transaction)
	f.tokenRenewer.Start() // Start the renewer immediately
	fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨å·²åˆå§‹åŒ–å¹¶å¯åŠ¨ï¼Œä½¿ç”¨åŸå§‹åç§°%q", f.originalName)
}

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	f.tokenMu.Lock()

	// Check if another thread has successfully refreshed while we were waiting
	if !refreshTokenExpired && !forceRefresh && f.token != "" && time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		fs.Debugf(f, "è·å–é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡åˆ·æ–°")
		f.tokenMu.Unlock()
		return nil
	}
	defer f.tokenMu.Unlock()

	// Perform the actual token refresh
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token during refresh: %w", err)
	}

	f.token = accessToken
	f.tokenExpiry = expiredAt

	// Save the refreshed token to config
	f.saveToken(ctx, f.m)

	return nil
}

// GetAccessToken fetches a new access token from the 123Pan API.
func (f *Fs) GetAccessToken(ctx context.Context) error {
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token: %w", err)
	}
	f.token = accessToken
	f.tokenExpiry = expiredAt
	return nil
}

type ClientCredentials struct {
	ClientID     string `json:"clientID"`
	ClientSecret string `json:"clientSecret"`
}

type TokenJSON struct {
	AccessToken string `json:"access_token"`
	Expiry      string `json:"expiry"`
}

type AccessTokenData struct {
	AccessToken string `json:"accessToken"`
	ExpiredAt   string `json:"expiredAt"`
}

type Response struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     AccessTokenData
	XTraceID string `json:"x-traceID"`
}

func GetAccessToken(clientID, clientSecret string) (string, time.Time, error) {
	credentials := ClientCredentials{
		ClientID:     clientID,
		ClientSecret: clientSecret,
	}
	payload, err := json.Marshal(credentials)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to marshal payload: %v", err)
	}
	req, err := http.NewRequest("POST", "https://open-api.123pan.com/api/v1/access_token", bytes.NewBuffer(payload))
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to create request: %v", err)
	}
	req.Header.Set("Platform", "open_platform")
	req.Header.Set("Content-Type", "application/json")
	client := fshttp.NewClient(context.Background())
	resp, err := client.Do(req)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to send request: %v", err)
	}
	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return "", time.Time{}, fmt.Errorf("unexpected status code: %d", resp.StatusCode)
	}
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to read response body: %v", err)
	}
	var result Response
	if err = json.Unmarshal(body, &result); err != nil {
		return "", time.Time{}, fmt.Errorf("failed to unmarshal response: %v", err)
	}
	expiredAt, err := time.Parse(time.RFC3339, result.Data.ExpiredAt)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to parse expiration time: %v", err)
	}
	return result.Data.AccessToken, expiredAt, nil
}

// Name of the remote (e.g. "123")
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (e.g. "bucket" or "bucket/dir")
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("123 root '%s'", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	return f.features
}

// Hashes returns the supported hash sets.
func (f *Fs) Hashes() fshash.Set {
	return fshash.Set(fshash.MD5)
}

// About gets quota information
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	fs.Debugf(f, "è°ƒç”¨About")

	userInfo, err := f.getUserInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get user info: %w", err)
	}

	usage := &fs.Usage{}
	if userInfo.Data.SpacePermanent > 0 {
		usage.Total = fs.NewUsageValue(userInfo.Data.SpacePermanent)
	}
	if userInfo.Data.SpaceUsed > 0 {
		usage.Used = fs.NewUsageValue(userInfo.Data.SpaceUsed)
	}
	if userInfo.Data.SpacePermanent > 0 && userInfo.Data.SpaceUsed > 0 {
		free := userInfo.Data.SpacePermanent - userInfo.Data.SpaceUsed
		if free > 0 {
			usage.Free = fs.NewUsageValue(free)
		}
	}

	return usage, nil
}

// Purge deletes all the files in the directory
func (f *Fs) Purge(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨Purgeåˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List all files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return fmt.Errorf("æ¸…ç†ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return err
		}

		if response.Code != 0 {
			return fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Collect all files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Delete all files
	for _, file := range allFiles {
		err := f.deleteFile(ctx, file.FileID)
		if err != nil {
			fs.Errorf(f, "åˆ é™¤æ–‡ä»¶å¤±è´¥ %s: %v", file.Filename, err)
			// Continue with other files
		}
	}

	return nil
}

// Shutdown the backend
func (f *Fs) Shutdown(ctx context.Context) error {
	fs.Debugf(f, "è°ƒç”¨å…³é—­")

	// Stop token renewer if it exists
	if f.tokenRenewer != nil {
		f.tokenRenewer.Stop()
		f.tokenRenewer = nil
	}

	// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼ˆå·²ç®€åŒ–ï¼‰

	// ç§»é™¤ç¼“å­˜å…³é—­ä»£ç ï¼Œä¸å†ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜

	return nil
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "è°ƒç”¨åˆ—è¡¨ï¼Œç›®å½•: %s", dir)

	// ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœdirä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
	if dir == "" && f.root != "" {
		// æ£€æŸ¥rootæ˜¯å¦æŒ‡å‘ä¸€ä¸ªæ–‡ä»¶
		rootObj, err := f.NewObject(ctx, "")
		if err == nil {
			// rootæŒ‡å‘ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨: %s", f.root)
			return fs.DirEntries{rootObj}, nil
		}
		// å¦‚æœNewObjectå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºrootæŒ‡å‘ç›®å½•
		if err == fs.ErrorNotAFile {
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘ç›®å½•ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s", f.root)
		} else {
			fs.Debugf(f, "rootè·¯å¾„æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s, é”™è¯¯: %v", f.root, err)
		}
	}

	// ä½¿ç”¨ç›®å½•ç¼“å­˜æŸ¥æ‰¾ç›®å½•ID
	fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•ID: %s", dir)
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•å¤±è´¥ %s: %v", dir, err)
		return nil, err
	}
	fs.Debugf(f, "æ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %s", dirID, dir)

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return nil, fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return nil, fmt.Errorf("åˆ—è¡¨ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return nil, err
		}

		if response.Code != 0 {
			return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Filter out trashed files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Convert to fs.DirEntries
	for _, file := range allFiles {
		// ä¿®å¤ï¼šæ¸…ç†APIè¿”å›çš„æ–‡ä»¶åï¼Œå»é™¤å‰åç©ºæ ¼å¹¶è¿›è¡ŒURLè§£ç 
		cleanedFilename := strings.TrimSpace(file.Filename)

		// URLè§£ç æ–‡ä»¶åï¼ˆå¤„ç†%3Aç­‰ç¼–ç ï¼‰
		if decodedName, err := url.QueryUnescape(cleanedFilename); err == nil {
			cleanedFilename = decodedName
		}
		if cleanedFilename != file.Filename {
			fs.Debugf(f, " æ¸…ç†æ–‡ä»¶å: [%s] -> [%s]", file.Filename, cleanedFilename)
		}

		remote := cleanedFilename
		if dir != "" {
			remote = strings.Trim(dir+"/"+cleanedFilename, "/")
		}

		if file.Type == 1 { // Directory
			// Cache the directory ID for future use
			f.dirCache.Put(remote, strconv.FormatInt(file.FileID, 10))
			d := fs.NewDir(remote, time.Time{})
			entries = append(entries, d)
		} else { // File
			o := &Object{
				fs:          f,
				remote:      remote,
				hasMetaData: true,
				id:          strconv.FormatInt(file.FileID, 10),
				size:        file.Size,
				md5sum:      file.Etag,
				modTime:     time.Now(), // We'll parse this properly later
				isDir:       false,
			}
			entries = append(entries, o)
		}
	}

	return entries, nil
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ›å»ºç›®å½•: %s", dir)

	// Use dircache to create the directory
	_, err := f.dirCache.FindDir(ctx, dir, true)
	if err == nil {
		// ç§»é™¤ç¼“å­˜æ¸…ç†
	}
	return err
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	fileID, err := strconv.ParseInt(dirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid directory ID: %s", dirID)
	}

	// Delete the directory
	err = f.deleteFile(ctx, fileID)
	if err != nil {
		return err
	}

	// Remove from cache
	f.dirCache.FlushDir(dir)

	// ç§»é™¤ç¼“å­˜æ¸…ç†

	return nil
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨ç§»åŠ¨ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Moveæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		return nil, fs.ErrorCantMove
	}

	// Use dircache to find destination directory
	dstName, dstDirID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	cleanedDstName, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "ç§»åŠ¨æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		dstName = cleanedDstName
	}

	// è½¬æ¢ç›®æ ‡ç›®å½•ID
	dstParentID, err := strconv.ParseInt(dstDirID, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid destination directory ID: %s", dstDirID)
	}

	// ç”Ÿæˆå”¯ä¸€çš„ç›®æ ‡æ–‡ä»¶åï¼Œé¿å…å†²çª
	uniqueDstName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
	if err != nil {
		fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åç§°: %v", err)
		uniqueDstName = dstName
	}

	if uniqueDstName != dstName {
		fs.Logf(f, "æ£€æµ‹åˆ°æ–‡ä»¶åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", dstName, uniqueDstName)
		dstName = uniqueDstName
	}

	// Convert IDs to int64
	fileID, err := strconv.ParseInt(srcObj.id, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid file ID: %s", srcObj.id)
	}

	// Get source directory ID for comparison
	_, srcDirID, err := f.dirCache.FindPath(ctx, srcObj.Remote(), false)
	if err != nil {
		return nil, fmt.Errorf("failed to find source directory: %w", err)
	}

	// è·å–æºæ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰
	srcBaseName := filepath.Base(srcObj.Remote())
	fs.Debugf(f, "æºæ–‡ä»¶åŸºç¡€åç§°: %s, ç›®æ ‡æ–‡ä»¶å: %s", srcBaseName, dstName)

	// Check if we're moving to the same directory
	if srcDirID == dstDirID {
		// Same directory - only rename if name changed
		if dstName != srcBaseName {
			fs.Debugf(f, "åŒç›®å½•ç§»åŠ¨ï¼Œä»…é‡å‘½å %s ä¸º %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				return nil, err
			}
		} else {
			fs.Debugf(f, "åŒç›®å½•åŒå - æ— éœ€æ“ä½œ")
		}
	} else {
		// ä¸åŒç›®å½• - å…ˆç§»åŠ¨ï¼Œç„¶åæ ¹æ®éœ€è¦é‡å‘½å
		fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶ä»ç›®å½• %s åˆ° %s", srcDirID, dstDirID)

		// åœ¨è·¨ç›®å½•ç§»åŠ¨æ—¶ï¼Œå…ˆæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶
		if dstName == srcBaseName {
			// å¦‚æœç›®æ ‡æ–‡ä»¶åä¸æºæ–‡ä»¶åç›¸åŒï¼Œæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡ä»¶
			exists, existingFileID, err := f.fileExistsInDirectory(ctx, dstParentID, dstName)
			if err != nil {
				fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err)
			} else if exists {
				fs.Debugf(f, "ç›®æ ‡ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ŒID: %d", existingFileID)
				if existingFileID == fileID {
					fs.Debugf(f, "æ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
					// æ–‡ä»¶å·²ç»åœ¨æ­£ç¡®ä½ç½®ï¼Œç›´æ¥è¿”å›æˆåŠŸ
					return &Object{
						fs:          f,
						remote:      remote,
						hasMetaData: srcObj.hasMetaData,
						id:          srcObj.id,
						size:        srcObj.size,
						md5sum:      srcObj.md5sum,
						modTime:     srcObj.modTime,
						isDir:       srcObj.isDir,
					}, nil
				} else {
					fs.Debugf(f, "ç›®æ ‡ç›®å½•å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°")
					uniqueName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
					if err != nil {
						fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
					} else {
						dstName = uniqueName
						fs.Logf(f, "ä½¿ç”¨å”¯ä¸€æ–‡ä»¶åé¿å…å†²çª: %s", dstName)
					}
				}
			}
		}

		err = f.moveFile(ctx, fileID, dstParentID)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½• - ä¿®å¤å­—ç¬¦ä¸²åŒ¹é…é—®é¢˜
			if strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸­") ||
				strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
				fs.Debugf(f, "æ–‡ä»¶å¯èƒ½å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œæ£€æŸ¥çŠ¶æ€")
				exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, srcBaseName)
				if checkErr == nil && exists && existingFileID == fileID {
					fs.Debugf(f, "ç¡®è®¤æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œç»§ç»­é‡å‘½åæ­¥éª¤")
					// æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œè·³è¿‡ç§»åŠ¨æ­¥éª¤
				} else {
					// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
					if strings.HasSuffix(srcBaseName, ".partial") {
						fs.Debugf(f, "Partialæ–‡ä»¶ç§»åŠ¨å†²çªï¼Œå°è¯•å¤„ç†æ–‡ä»¶åå†²çª")
						return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
					}
					return nil, err
				}
			} else {
				return nil, err
			}
		}

		// If the name changed, rename the file
		if dstName != srcBaseName {
			fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åæ–‡ä»¶: %s -> %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				// æ£€æŸ¥é‡å‘½åå¤±è´¥çš„åŸå›  - å¢å¼ºé”™è¯¯åŒ¹é…
				if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
					strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
					fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œç›®å½•ä¸­å·²æœ‰é‡åæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡æ–‡ä»¶")
					exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, dstName)
					if checkErr == nil && exists && existingFileID == fileID {
						fs.Debugf(f, "æ–‡ä»¶å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæ“ä½œå®é™…å·²æˆåŠŸ")
						// æ–‡ä»¶å·²ç»æœ‰æ­£ç¡®çš„åç§°ï¼Œè®¤ä¸ºæ“ä½œæˆåŠŸ
					} else {
						// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
						if strings.HasSuffix(srcBaseName, ".partial") {
							fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œå°è¯•ç‰¹æ®Šå¤„ç†")
							return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
						}
						return nil, err
					}
				} else {
					return nil, err
				}
			}
		}
	}

	// ç§»é™¤ç¼“å­˜æ¸…ç†

	// Return the moved object
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: srcObj.hasMetaData,
		id:          srcObj.id,
		size:        srcObj.size,
		md5sum:      srcObj.md5sum,
		modTime:     srcObj.modTime,
		isDir:       srcObj.isDir,
	}, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	fs.Debugf(f, "è°ƒç”¨ç›®å½•ç§»åŠ¨ %s åˆ° %s", srcRemote, dstRemote)

	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(f, "æ— æ³•ç§»åŠ¨ç›®å½• - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return fs.ErrorCantDirMove
	}

	// Find source directory ID
	srcDirID, err := srcFs.dirCache.FindDir(ctx, srcRemote, false)
	if err != nil {
		return err
	}

	// Find destination parent directory
	dstName, dstParentID, err := f.dirCache.FindPath(ctx, dstRemote, true)
	if err != nil {
		return err
	}

	// Convert IDs to int64
	srcFileID, err := strconv.ParseInt(srcDirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid source directory ID: %s", srcDirID)
	}

	dstParentFileID, err := strconv.ParseInt(dstParentID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid destination parent ID: %s", dstParentID)
	}

	// è·å–æºç›®å½•çš„åŸºç¡€åç§°
	srcBaseName := filepath.Base(srcRemote)
	fs.Debugf(f, "æºç›®å½•åŸºç¡€åç§°: %s, ç›®æ ‡ç›®å½•å: %s", srcBaseName, dstName)
	fs.Debugf(f, "æºç›®å½•ID: %d, ç›®æ ‡çˆ¶ç›®å½•ID: %d", srcFileID, dstParentFileID)

	// æ£€æŸ¥æ˜¯å¦å°è¯•ç§»åŠ¨åˆ°åŒä¸€ä¸ªçˆ¶ç›®å½•
	srcParentID, err := f.getParentID(ctx, srcFileID)
	if err != nil {
		fs.Debugf(f, "è·å–æºç›®å½•çˆ¶IDå¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "æºç›®å½•å½“å‰çˆ¶ID: %d", srcParentID)
		if srcParentID == dstParentFileID && dstName == srcBaseName {
			fs.Debugf(f, "ç›®å½•å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
			return nil
		}
	}

	// Move the directory
	err = f.moveFile(ctx, srcFileID, dstParentFileID)
	if err != nil {
		return err
	}

	// If the name changed, rename the directory
	if dstName != srcBaseName {
		fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åç›®å½•: %s -> %s", srcBaseName, dstName)
		err = f.renameFile(ctx, srcFileID, dstName)
		if err != nil {
			return err
		}
	}

	// Update cache
	srcFs.dirCache.FlushDir(srcRemote)
	f.dirCache.FlushDir(dstRemote)

	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨å¤åˆ¶ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Copyæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•å¤åˆ¶ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantCopy
	}

	// Use dircache to find destination directory
	dstName, _, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	_, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "å¤åˆ¶æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		// Note: cleaned name is not used in current implementation
	}

	// 123Pan doesn't have a direct copy API, use download+upload
	fs.Debugf(f, "123ç½‘ç›˜ä¸æ”¯æŒæœåŠ¡å™¨ç«¯å¤åˆ¶ï¼Œä½¿ç”¨ä¸‹è½½+ä¸Šä¼ æ–¹å¼")
	return f.copyViaDownloadUpload(ctx, srcObj, remote)
}

// copyViaDownloadUpload copies a file by downloading and re-uploading
func (f *Fs) copyViaDownloadUpload(ctx context.Context, srcObj *Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "é€šè¿‡ä¸‹è½½+ä¸Šä¼ å¤åˆ¶ %s åˆ° %s", srcObj.remote, remote)

	// Open source file for reading
	src, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file: %w", err)
	}
	defer src.Close()

	// Create a new object info for the destination
	dstInfo := &ObjectInfo{
		fs:      f,
		remote:  remote,
		size:    srcObj.size,
		md5sum:  srcObj.md5sum,
		modTime: srcObj.modTime,
	}

	// Upload to destination
	return f.Put(ctx, src, dstInfo)
}

// ObjectInfo implements fs.ObjectInfo for copy operations
type ObjectInfo struct {
	fs      *Fs
	remote  string
	size    int64
	md5sum  string
	modTime time.Time
}

func (oi *ObjectInfo) Fs() fs.Info                           { return oi.fs }
func (oi *ObjectInfo) Remote() string                        { return oi.remote }
func (oi *ObjectInfo) Size() int64                           { return oi.size }
func (oi *ObjectInfo) ModTime(ctx context.Context) time.Time { return oi.modTime }
func (oi *ObjectInfo) Storable() bool                        { return true }
func (oi *ObjectInfo) String() string                        { return oi.remote }
func (oi *ObjectInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t == fshash.MD5 {
		return oi.md5sum, nil
	}
	return "", fshash.ErrUnsupported
}

// getHTTPClient makes an http client according to the options
// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯é…ç½®
func getHTTPClient(ctx context.Context, _ string, _ configmap.Mapper) *http.Client {
	// ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	return fshttp.NewClient(ctx)
}

// getNetworkQuality è¯„ä¼°å½“å‰ç½‘ç»œè´¨é‡ï¼Œè¿”å›0.0-1.0çš„è´¨é‡åˆ†æ•°
func (f *Fs) getNetworkQuality() float64 {
	// ç®€åŒ–ç½‘ç»œè´¨é‡è¯„ä¼° - ä½¿ç”¨å›ºå®šçš„è‰¯å¥½ç½‘ç»œè´¨é‡å‡è®¾
	return 0.8 // é»˜è®¤å‡è®¾ç½‘ç»œè´¨é‡è‰¯å¥½
}

// getAdaptiveTimeout æ ¹æ®æ–‡ä»¶å¤§å°ã€ä¼ è¾“ç±»å‹è®¡ç®—è‡ªé€‚åº”è¶…æ—¶æ—¶é—´
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†è¶…æ—¶è®¡ç®—
func (f *Fs) getAdaptiveTimeout(fileSize int64, transferType string) time.Duration {
	// ç®€åŒ–è¶…æ—¶è®¡ç®— - ä½¿ç”¨å›ºå®šçš„åˆç†è¶…æ—¶æ—¶é—´
	baseTimeout := 1200 * time.Second // 20åˆ†é’ŸåŸºç¡€è¶…æ—¶

	// æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆæ›´ç²¾ç»†çš„è®¡ç®—ï¼‰
	// æ¯50MBå¢åŠ 30ç§’è¶…æ—¶æ—¶é—´ï¼Œå¯¹å¤§æ–‡ä»¶æ›´å‹å¥½
	sizeBasedTimeout := time.Duration(fileSize/50/1024/1024) * 30 * time.Second

	// æ ¹æ®ä¼ è¾“ç±»å‹å’Œç½‘ç»œæ¡ä»¶è°ƒæ•´
	var typeMultiplier float64 = 1.0
	switch transferType {
	case "chunked_upload":
		typeMultiplier = 1.5 // åˆ†ç‰‡ä¸Šä¼ éœ€è¦é€‚ä¸­æ—¶é—´
	case "stream_download":
		typeMultiplier = 1.2 // æµå¼ä¸‹è½½éœ€è¦é€‚ä¸­æ—¶é—´
	case "single_step":
		typeMultiplier = 0.8 // å•æ­¥ä¸Šä¼ æ—¶é—´è¾ƒçŸ­
	case "concurrent_upload":
		typeMultiplier = 2.0 // å¹¶å‘ä¸Šä¼ éœ€è¦æ›´é•¿æ—¶é—´
	default:
		typeMultiplier = 1.0
	}

	// è€ƒè™‘ç½‘ç»œè´¨é‡
	networkQuality := f.getNetworkQuality()
	var qualityMultiplier float64 = 1.0
	if networkQuality < 0.3 { // ç½‘ç»œè´¨é‡å¾ˆå·®
		qualityMultiplier = 3.0
	} else if networkQuality < 0.5 { // ç½‘ç»œè´¨é‡å·®
		qualityMultiplier = 2.0
	} else if networkQuality < 0.7 { // ç½‘ç»œè´¨é‡ä¸€èˆ¬
		qualityMultiplier = 1.5
	} else {
		qualityMultiplier = 1.0 // ç½‘ç»œè´¨é‡è‰¯å¥½
	}

	adaptiveTimeout := baseTimeout + time.Duration(float64(sizeBasedTimeout)*typeMultiplier*qualityMultiplier)

	// è®¾ç½®æ›´åˆç†çš„è¾¹ç•Œ
	minTimeout := 2 * time.Minute // æœ€å°2åˆ†é’Ÿ
	maxTimeout := 4 * time.Hour   // æœ€å¤§4å°æ—¶ï¼Œæ”¯æŒè¶…å¤§æ–‡ä»¶

	if adaptiveTimeout < minTimeout {
		adaptiveTimeout = minTimeout
	}
	if adaptiveTimeout > maxTimeout {
		adaptiveTimeout = maxTimeout
	}

	fs.Debugf(f, "è‡ªé€‚åº”è¶…æ—¶è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç±»å‹=%s, ç½‘ç»œè´¨é‡=%.2f, è¶…æ—¶æ—¶é—´=%v",
		fs.SizeSuffix(fileSize), transferType, networkQuality, adaptiveTimeout)

	return adaptiveTimeout
}

// detectNetworkSpeed æ£€æµ‹ç½‘ç»œä¼ è¾“é€Ÿåº¦ï¼Œç”¨äºåŠ¨æ€ä¼˜åŒ–å‚æ•°
func (f *Fs) detectNetworkSpeed(_ context.Context) int64 {
	fs.Debugf(f, "å¼€å§‹æ£€æµ‹ç½‘ç»œé€Ÿåº¦")

	// ç®€åŒ–ç½‘ç»œé€Ÿåº¦æ£€æµ‹ - ä½¿ç”¨å›ºå®šçš„åˆç†é€Ÿåº¦ä¼°ç®—

	// ä½¿ç”¨å¿«é€Ÿä¼°ç®—æ–¹æ³•ï¼Œé¿å…å¤§æ–‡ä»¶åˆå§‹åŒ–å»¶è¿Ÿ
	// åŸºäºç½‘ç»œè´¨é‡è¿›è¡Œå¿«é€Ÿä¼°ç®—ï¼Œä¸è¿›è¡Œå®é™…ç½‘ç»œæµ‹è¯•
	networkQuality := f.getNetworkQuality()

	// åŸºç¡€é€Ÿåº¦ä¼°ç®—ï¼ˆåŸºäºå…¸å‹ç½‘ç»œç¯å¢ƒï¼‰
	var baseSpeed int64 = 10 * 1024 * 1024 // 10MB/s åŸºç¡€é€Ÿåº¦

	// æ ¹æ®ç½‘ç»œè´¨é‡è°ƒæ•´é€Ÿåº¦ä¼°ç®—
	if networkQuality >= 0.8 {
		baseSpeed = 20 * 1024 * 1024 // 20MB/s é«˜è´¨é‡ç½‘ç»œ
	} else if networkQuality >= 0.6 {
		baseSpeed = 15 * 1024 * 1024 // 15MB/s è‰¯å¥½ç½‘ç»œ
	} else if networkQuality >= 0.4 {
		baseSpeed = 8 * 1024 * 1024 // 8MB/s ä¸€èˆ¬ç½‘ç»œ
	} else {
		baseSpeed = 5 * 1024 * 1024 // 5MB/s è¾ƒå·®ç½‘ç»œ
	}

	fs.Debugf(f, "ç½‘ç»œé€Ÿåº¦ä¼°ç®—: %s/s (è´¨é‡=%.2f)",
		fs.SizeSuffix(baseSpeed), networkQuality)

	return baseSpeed
}

// getOptimalConcurrency æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†å¹¶å‘è®¡ç®—
func (f *Fs) getOptimalConcurrency(fileSize int64, networkSpeed int64) int {
	// ç®€åŒ–å¹¶å‘è®¡ç®— - ä½¿ç”¨rcloneå…¨å±€é…ç½®çš„ä¼ è¾“æ•°é‡
	baseConcurrency := fs.GetConfig(context.Background()).Transfers
	if baseConcurrency <= 0 {
		baseConcurrency = 4 // é»˜è®¤å¹¶å‘æ•°
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´å¹¶å‘æ•° - ä¼˜åŒ–å¤§æ–‡ä»¶å¹¶å‘ç­–ç•¥
	var sizeFactor float64 = 1.0
	if fileSize > 20*1024*1024*1024 { // >20GB - è¶…å¤§æ–‡ä»¶
		sizeFactor = 3.0 // æ˜¾è‘—æå‡å¹¶å‘æ•°
	} else if fileSize > 10*1024*1024*1024 { // >10GB - å¤§æ–‡ä»¶
		sizeFactor = 2.5
	} else if fileSize > 5*1024*1024*1024 { // >5GB - ä¸­å¤§æ–‡ä»¶
		sizeFactor = 2.0
	} else if fileSize > 2*1024*1024*1024 { // >2GB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.5
	} else if fileSize > 1*1024*1024*1024 { // >1GB - è¾ƒå¤§æ–‡ä»¶
		sizeFactor = 1.2
	} else if fileSize > 500*1024*1024 { // >500MB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.0
	} else if fileSize > 100*1024*1024 { // >100MB - å°æ–‡ä»¶
		sizeFactor = 0.8
	} else {
		sizeFactor = 0.5 // å¾ˆå°æ–‡ä»¶ä½¿ç”¨è¾ƒå°‘å¹¶å‘
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´ - æ€§èƒ½ä¼˜åŒ–ï¼šæ›´ç²¾ç»†çš„ç½‘ç»œé€Ÿåº¦åˆ†çº§å’Œè‡ªé€‚åº”è°ƒæ•´
	var speedFactor float64 = 1.0

	// æ–°å¢ï¼šç½‘ç»œè´¨é‡è‡ªé€‚åº”è°ƒæ•´
	networkLatency := f.measureNetworkLatency()
	latencyFactor := 1.0
	if networkLatency < 50 { // <50ms ä½å»¶è¿Ÿ
		latencyFactor = 1.2
	} else if networkLatency < 100 { // <100ms ä¸­ç­‰å»¶è¿Ÿ
		latencyFactor = 1.0
	} else if networkLatency < 200 { // <200ms é«˜å»¶è¿Ÿ
		latencyFactor = 0.8
	} else { // >200ms å¾ˆé«˜å»¶è¿Ÿ
		latencyFactor = 0.6
	}

	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedFactor = 2.0 * latencyFactor // ä»1.8æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.7 * latencyFactor // ä»1.5æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.4 * latencyFactor // ä»1.2æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - ä¸­é€Ÿç½‘ç»œ
		speedFactor = 1.1 * latencyFactor // ä»1.0æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 10*1024*1024 { // >10Mbps - ä¸­ä½é€Ÿç½‘ç»œ
		speedFactor = 0.9 * latencyFactor // ä»0.8æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else {
		speedFactor = 0.7 * latencyFactor // ä»0.6æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	optimalConcurrency := int(float64(baseConcurrency) * sizeFactor * speedFactor)

	// ç”¨æˆ·è¦æ±‚ï¼š123ç½‘ç›˜å•æ–‡ä»¶å¹¶å‘é™åˆ¶åˆ°4
	minConcurrency := 1
	maxConcurrency := 4 // ç”¨æˆ·è¦æ±‚çš„å¹¶å‘ä¸Šé™

	// ä¸å†å…è®¸è¶…å¤§æ–‡ä»¶ä½¿ç”¨æ›´é«˜å¹¶å‘æ•°ï¼Œç»Ÿä¸€é™åˆ¶ä¸º4

	if optimalConcurrency < minConcurrency {
		optimalConcurrency = minConcurrency
	}
	if optimalConcurrency > maxConcurrency {
		optimalConcurrency = maxConcurrency
	}

	fs.Debugf(f, "ğŸš€ ä¼˜åŒ–å¹¶å‘æ•°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€å¹¶å‘=%d, å¤§å°å› å­=%.1f, é€Ÿåº¦å› å­=%.1f, æœ€ä¼˜å¹¶å‘=%d",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed), baseConcurrency, sizeFactor, speedFactor, optimalConcurrency)

	return optimalConcurrency
}

// getOptimalChunkSize æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†åˆ†ç‰‡å¤§å°è®¡ç®—
func (f *Fs) getOptimalChunkSize(fileSize int64, networkSpeed int64) int64 {
	fs.Debugf(f, " å¼€å§‹è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°: æ–‡ä»¶å¤§å°=%s", fs.SizeSuffix(fileSize))

	// ç®€åŒ–åˆ†ç‰‡å¤§å°è®¡ç®— - ä½¿ç”¨rcloneå…¨å±€é…ç½®çš„å¤šçº¿ç¨‹åˆ†ç‰‡å¤§å°
	baseChunk := int64(fs.GetConfig(context.Background()).MultiThreadChunkSize)
	if baseChunk <= 0 {
		baseChunk = int64(defaultChunkSize) // 100MB
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´åˆ†ç‰‡å¤§å° - æ€§èƒ½ä¼˜åŒ–ï¼šæå‡åˆ†ç‰‡å¤§å°å€æ•°
	var speedMultiplier float64 = 1.0
	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 4.0 // 400MBåˆ†ç‰‡ï¼ˆä»3.0æå‡ï¼‰
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 3.0 // 300MBåˆ†ç‰‡ï¼ˆä»2.0æå‡ï¼‰
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é€Ÿç½‘ç»œ
		speedMultiplier = 2.0 // 200MBåˆ†ç‰‡ï¼ˆä»1.5æå‡ï¼‰
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - æ™®é€šç½‘ç»œ
		speedMultiplier = 1.5 // 150MBåˆ†ç‰‡ï¼ˆä»1.0æå‡ï¼‰
	} else { // <20Mbps - æ…¢é€Ÿç½‘ç»œ
		speedMultiplier = 0.8 // 80MBåˆ†ç‰‡ï¼ˆä»0.5æå‡ï¼‰
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´ - æ€§èƒ½ä¼˜åŒ–ï¼šæ›´ç²¾ç»†çš„æ–‡ä»¶å¤§å°åˆ†çº§
	var sizeMultiplier float64 = 1.0
	if fileSize > 50*1024*1024*1024 { // >50GB
		sizeMultiplier = 2.0 // å¤§æ–‡ä»¶ä½¿ç”¨æ›´å¤§åˆ†ç‰‡ï¼ˆä»1.5æå‡ï¼‰
	} else if fileSize > 20*1024*1024*1024 { // >20GB
		sizeMultiplier = 1.8 // æ–°å¢ï¼šè¶…å¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 10*1024*1024*1024 { // >10GB
		sizeMultiplier = 1.5 // æ–°å¢ï¼šå¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 5*1024*1024*1024 { // >5GB
		sizeMultiplier = 1.3 // æ–°å¢ï¼šä¸­å¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 2*1024*1024*1024 { // >2GB
		sizeMultiplier = 1.1 // æ–°å¢ï¼šä¸­ç­‰æ–‡ä»¶åˆ†çº§
	} else if fileSize < 500*1024*1024 { // <500MB
		sizeMultiplier = 0.7 // å°æ–‡ä»¶ä½¿ç”¨æ›´å°åˆ†ç‰‡ï¼ˆä»0.5æå‡ï¼‰
	}

	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
	optimalChunkSize := int64(float64(baseChunk) * speedMultiplier * sizeMultiplier)

	// è®¾ç½®åˆç†è¾¹ç•Œ - æ€§èƒ½ä¼˜åŒ–ï¼šæå‡æœ€å¤§åˆ†ç‰‡å¤§å°é™åˆ¶
	minChunkSize := int64(minChunkSize)      // 50MB
	maxChunkSize := int64(800 * 1024 * 1024) // 800MBï¼ˆä»500MBæå‡ï¼‰

	if optimalChunkSize < minChunkSize {
		optimalChunkSize = minChunkSize
	}
	if optimalChunkSize > maxChunkSize {
		optimalChunkSize = maxChunkSize
	}

	fs.Debugf(f, "åŠ¨æ€åˆ†ç‰‡å¤§å°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€åˆ†ç‰‡=%s, æœ€ä¼˜åˆ†ç‰‡=%s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed),
		fs.SizeSuffix(baseChunk), fs.SizeSuffix(optimalChunkSize))

	return optimalChunkSize
}

// measureNetworkLatency æµ‹é‡ç½‘ç»œå»¶è¿Ÿ
// æ€§èƒ½ä¼˜åŒ–ï¼šå®ç°ç½‘ç»œå»¶è¿Ÿæ£€æµ‹ï¼Œç”¨äºè‡ªé€‚åº”å¹¶å‘è°ƒæ•´
func (f *Fs) measureNetworkLatency() int64 {
	// ç®€å•çš„å»¶è¿Ÿæµ‹é‡ï¼šå‘123ç½‘ç›˜APIå‘é€HEADè¯·æ±‚
	start := time.Now()

	// æ„å»ºæµ‹è¯•URL
	testURL := "https://www.123pan.com/api/v1/user/info"
	req, err := http.NewRequest("HEAD", testURL, nil)
	if err != nil {
		return 100 // é»˜è®¤å»¶è¿Ÿ100ms
	}

	// è®¾ç½®è¯·æ±‚å¤´
	req.Header.Set("User-Agent", f.opt.UserAgent)

	// å‘é€è¯·æ±‚ - ä½¿ç”¨rcloneæ ‡å‡†HTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(context.Background())
	resp, err := client.Do(req)
	if err != nil {
		return 150 // ç½‘ç»œé”™è¯¯æ—¶è¿”å›è¾ƒé«˜å»¶è¿Ÿ
	}
	defer resp.Body.Close()

	latency := time.Since(start).Milliseconds()
	return latency
}

// ç§»é™¤ResourcePoolï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶

// ç§»é™¤æœªä½¿ç”¨çš„DownloadProgressç»“æ„ä½“ï¼Œç®€åŒ–ä»£ç 

// ç§»é™¤NewDownloadProgresså‡½æ•°ï¼Œæœªä½¿ç”¨

// ç§»é™¤UpdateChunkProgressæ–¹æ³•ï¼Œæœªä½¿ç”¨

// ç§»é™¤GetProgressInfoæ–¹æ³•ï¼Œæœªä½¿ç”¨

// ç§»é™¤NewResourcePoolå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤ResourcePoolç›¸å…³æ–¹æ³•ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤GetTempFileå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤GetOptimizedTempFileå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤removeTempFileFromTrackingå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤PutTempFileå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤CleanupAllTempFileså’ŒCloseå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// Common utility functions

// ç§»é™¤PerformanceStatsï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç»Ÿè®¡

// ç§»é™¤UploadStrategyï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ç­–ç•¥

// ç§»é™¤UploadStrategy Stringæ–¹æ³•ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤UploadStrategySelectorï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤SelectStrategyæ–¹æ³•ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤GetStrategyReasonæ–¹æ³•ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤UnifiedUploadContextï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ä¸Šä¼ æµç¨‹

// ç§»é™¤NewUnifiedUploadContextå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤unifiedUploadå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤æ‰€æœ‰execute*Uploadå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ä¸Šä¼ æµç¨‹
// ä»¥ä¸‹å‡½æ•°å·²è¢«åˆ é™¤ï¼šexecuteSingleStepUpload, executeChunkedUpload, executeStreamingUploadç­‰
// ç§»é™¤executeSingleStepUploadå‡½æ•°ä½“ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤executeCrossCloudSingleStepUploadå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡
// ç§»é™¤executeCrossCloudSingleStepUploadå‡½æ•°ä½“ï¼Œè¿‡åº¦è®¾è®¡

// ç§»é™¤æ‰€æœ‰å‰©ä½™çš„execute*Uploadå‡½æ•°ï¼Œè¿‡åº¦è®¾è®¡ï¼Œä½¿ç”¨rcloneæ ‡å‡†ä¸Šä¼ æµç¨‹
