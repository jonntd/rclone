package _123

import (
	"bufio"
	"bytes"
	"context"
	"crypto/md5"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"hash"
	"io"
	"maps"
	"mime/multipart"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"
	"unicode/utf8"

	"github.com/rclone/rclone/backend/common"
	"github.com/rclone/rclone/lib/atexit"
	"github.com/rclone/rclone/lib/dircache"
	"github.com/rclone/rclone/lib/encoder"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/rest"
	"golang.org/x/oauth2"

	// 123ç½‘ç›˜SDK
	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/accounting"
	"github.com/rclone/rclone/fs/config"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	fshash "github.com/rclone/rclone/fs/hash"
	"github.com/rclone/rclone/lib/cache"
)

// "Mozilla/5.0 AppleWebKit/600 Safari/600 Chrome/124.0.0.0 Edg/124.0.0.0"
const (
	openAPIRootURL     = "https://open-api.123pan.com"
	defaultUserAgent   = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
	tokenRefreshWindow = 10 * time.Minute // tokenRefreshWindow å®šä¹‰äº†åœ¨ä»¤ç‰Œè¿‡æœŸå‰å¤šé•¿æ—¶é—´å°è¯•åˆ·æ–°ä»¤ç‰Œ

	// å®˜æ–¹QPSé™åˆ¶æ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
	// é«˜é¢‘ç‡API (15-20 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	listV2APIMinSleep = 200 * time.Millisecond // ~5 QPS ç”¨äº api/v2/file/list (å®˜æ–¹15 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	fileMoveMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/move (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileInfosMinSleep   = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/infos (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	userInfoMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/user/info (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	accessTokenMinSleep = 300 * time.Millisecond // ~3.3 QPS ç”¨äº api/v1/access_token (å®˜æ–¹8 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä½é¢‘ç‡API (5 QPS) - åŸºäºä¸Šä¼ é€Ÿåº¦åˆ†æå¤§å¹…ä¼˜åŒ–æ€§èƒ½
	// æ³¨æ„ï¼šuploadCreateMinSleep, mkdirMinSleep, fileTrashMinSleep å·²åˆ é™¤ï¼ˆæœªä½¿ç”¨ï¼‰

	downloadInfoMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº api/v1/file/download_info (ä¿æŒä¸å˜)

	// æœ€ä½é¢‘ç‡API (1 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	// ç‰¹æ®ŠAPI (ä¿å®ˆä¼°è®¡) - åŸºäº48ç§’/åˆ†ç‰‡é—®é¢˜å¤§å¹…ä¼˜åŒ–åˆ†ç‰‡ä¸Šä¼ æ€§èƒ½
	uploadV2SliceMinSleep = 50 * time.Millisecond // ~20.0 QPS ç”¨äº upload/v2/file/slice (æ¿€è¿›æå‡ï¼Œè§£å†³ä¸Šä¼ æ…¢é—®é¢˜)

	maxSleep      = 30 * time.Second // 429é€€é¿çš„æœ€å¤§ç¡çœ æ—¶é—´
	decayConstant = 2

	// æ–‡ä»¶ä¸Šä¼ ç›¸å…³å¸¸é‡
	singleStepUploadLimit = 1024 * 1024 * 1024 // 1GB - å•æ­¥ä¸Šä¼ APIçš„æ–‡ä»¶å¤§å°é™åˆ¶ï¼ˆä¿®å¤ï¼šåŸæ¥é”™è¯¯è®¾ç½®ä¸º50MBï¼‰
	maxMemoryBufferSize   = 1024 * 1024 * 1024 // 1GB - å†…å­˜ç¼“å†²çš„æœ€å¤§å¤§å°ï¼ˆä»512MBæå‡ï¼‰
	maxFileNameBytes      = 255                // æ–‡ä»¶åçš„æœ€å¤§å­—èŠ‚é•¿åº¦ï¼ˆUTF-8ç¼–ç ï¼‰

	// æ–‡ä»¶å†²çªå¤„ç†ç­–ç•¥å¸¸é‡å·²ç§»é™¤ï¼Œå½“å‰ä½¿ç”¨APIé»˜è®¤è¡Œä¸º

	// ä¸Šä¼ ç›¸å…³å¸¸é‡ - ä¼˜åŒ–å¤§æ–‡ä»¶ä¼ è¾“æ€§èƒ½
	defaultChunkSize    = 100 * fs.Mebi // å¢åŠ é»˜è®¤åˆ†ç‰‡å¤§å°åˆ°100MB
	minChunkSize        = 50 * fs.Mebi  // å¢åŠ æœ€å°åˆ†ç‰‡å¤§å°åˆ°50MB
	maxChunkSize        = 500 * fs.Mebi // è®¾ç½®æœ€å¤§åˆ†ç‰‡å¤§å°ä¸º500MB
	defaultUploadCutoff = 100 * fs.Mebi // é™ä½åˆ†ç‰‡ä¸Šä¼ é˜ˆå€¼
	maxUploadParts      = 10000

	// è¿æ¥å’Œè¶…æ—¶è®¾ç½® - é’ˆå¯¹å¤§æ–‡ä»¶ä¼ è¾“ä¼˜åŒ–çš„å‚æ•°
	defaultConnTimeout = 60 * time.Second   // è¿æ¥è¶…æ—¶60ç§’ï¼Œæ”¯æŒå¤§æ–‡ä»¶ä¼ è¾“
	defaultTimeout     = 1200 * time.Second // æ€»ä½“è¶…æ—¶20åˆ†é’Ÿï¼Œæ”¯æŒå¤§æ–‡ä»¶ä¼ è¾“

	// ç»Ÿä¸€æ–‡ä»¶å¤§å°åˆ¤æ–­å¸¸é‡ï¼Œé¿å…é­”æ³•æ•°å­—
	memoryBufferThreshold       = 50 * 1024 * 1024       // 50MBå†…å­˜ç¼“å†²é˜ˆå€¼
	streamingTransferThreshold  = 100 * 1024 * 1024      // 100MBæµå¼ä¼ è¾“é˜ˆå€¼
	concurrentDownloadThreshold = 2 * 1024 * 1024 * 1024 // 2GBå¹¶å‘ä¸‹è½½é˜ˆå€¼
	smallFileThreshold          = 50 * 1024 * 1024       // 50MBå°æ–‡ä»¶é˜ˆå€¼

	// æ–‡ä»¶åéªŒè¯ç›¸å…³å¸¸é‡
	maxFileNameLength = 256          // 123ç½‘ç›˜æ–‡ä»¶åæœ€å¤§é•¿åº¦ï¼ˆåŒ…æ‹¬æ‰©å±•åï¼‰
	invalidChars      = `"\/:*?|><\` // 123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
	replacementChar   = "_"          // ç”¨äºæ›¿æ¢éæ³•å­—ç¬¦çš„å®‰å…¨å­—ç¬¦

	// ç½‘ç»œé€Ÿåº¦ç¼“å­˜ç›¸å…³å¸¸é‡
	NetworkSpeedCacheTTL = 5 * time.Minute // ç½‘ç»œé€Ÿåº¦ç¼“å­˜æœ‰æ•ˆæœŸ

	// ç¼“å­˜å’Œæ¸…ç†ç›¸å…³å¸¸é‡
	DefaultCacheCleanupInterval = 5 * time.Minute // é»˜è®¤ç¼“å­˜æ¸…ç†é—´éš”
	DefaultCacheCleanupTimeout  = 2 * time.Minute // é»˜è®¤ç¼“å­˜æ¸…ç†è¶…æ—¶
	BufferCleanupThreshold      = 5 * time.Minute // ç¼“å†²åŒºæ¸…ç†é˜ˆå€¼

	// é…ç½®éªŒè¯ç›¸å…³å¸¸é‡
	MaxListChunk          = 10000 // æœ€å¤§åˆ—è¡¨å—å¤§å°
	MaxUploadPartsLimit   = 10000 // æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°é™åˆ¶
	MaxConcurrentLimit    = 100   // æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	DefaultListChunk      = 1000  // é»˜è®¤åˆ—è¡¨å—å¤§å°
	DefaultMaxUploadParts = 1000  // é»˜è®¤æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°

	// ç½‘ç»œè´¨é‡è¯„ä¼°å¸¸é‡
	LatencyThreshold          = 50              // å»¶è¿Ÿé˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
	LatencyScoreRange         = 1000            // å»¶è¿Ÿåˆ†æ•°è®¡ç®—èŒƒå›´
	QualityAdjustmentInterval = 5 * time.Minute // è´¨é‡è°ƒæ•´é—´éš”

	// ç¼“å­˜ç®¡ç†å¸¸é‡
	PreloadQueueCapacity    = 1000 // é¢„åŠ è½½é˜Ÿåˆ—å®¹é‡
	MaxHotFiles             = 100  // æœ€å¤§çƒ­ç‚¹æ–‡ä»¶æ•°
	HotFileCleanupBatchSize = 20   // çƒ­ç‚¹æ–‡ä»¶æ¸…ç†æ‰¹æ¬¡å¤§å°

	// ç½‘ç»œæ£€æµ‹å¸¸é‡
	NetworkTestSize = 512 * 1024 // ç½‘ç»œæµ‹è¯•æ–‡ä»¶å¤§å°ï¼ˆ512KBï¼‰
)

// Options å®šä¹‰æ­¤åç«¯çš„é…ç½®é€‰é¡¹
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ ‡æ³¨äº†å¯ä»¥è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®çš„é€‰é¡¹
type Options struct {
	// 123ç½‘ç›˜ç‰¹å®šçš„è®¤è¯é…ç½®
	ClientID     string `config:"client_id"`
	ClientSecret string `config:"client_secret"`
	Token        string `config:"token"`
	UserAgent    string `config:"user_agent"`
	RootFolderID string `config:"root_folder_id"`

	// æ–‡ä»¶æ“ä½œé…ç½® (éƒ¨åˆ†å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	ListChunk      int           `config:"list_chunk"`    // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Checkers
	ChunkSize      fs.SizeSuffix `config:"chunk_size"`    // å¯è€ƒè™‘ä½¿ç”¨fs.Config.ChunkSize
	UploadCutoff   fs.SizeSuffix `config:"upload_cutoff"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.MultiThreadCutoff
	MaxUploadParts int           `config:"max_upload_parts"`

	// ç½‘ç»œå’Œè¶…æ—¶é…ç½® (å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	PacerMinSleep     fs.Duration `config:"pacer_min_sleep"`
	ListPacerMinSleep fs.Duration `config:"list_pacer_min_sleep"`
	ConnTimeout       fs.Duration `config:"conn_timeout"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.ConnectTimeout
	Timeout           fs.Duration `config:"timeout"`      // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Timeout

	// å¹¶å‘æ§åˆ¶é…ç½® (å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	MaxConcurrentUploads   int         `config:"max_concurrent_uploads"`   // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Transfers
	MaxConcurrentDownloads int         `config:"max_concurrent_downloads"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Checkers
	ProgressUpdateInterval fs.Duration `config:"progress_update_interval"`
	EnableProgressDisplay  bool        `config:"enable_progress_display"`

	// 123ç½‘ç›˜ç‰¹å®šçš„QPSæ§åˆ¶é€‰é¡¹
	UploadPacerMinSleep   fs.Duration `config:"upload_pacer_min_sleep"`
	DownloadPacerMinSleep fs.Duration `config:"download_pacer_min_sleep"`
	StrictPacerMinSleep   fs.Duration `config:"strict_pacer_min_sleep"`

	// ç¼–ç é…ç½®
	Enc encoder.MultiEncoder `config:"encoding"` // æ–‡ä»¶åç¼–ç è®¾ç½®

	// ç¼“å­˜ä¼˜åŒ–é…ç½® - æ–°å¢
	CacheMaxSize       fs.SizeSuffix `config:"cache_max_size"`       // æœ€å¤§ç¼“å­˜å¤§å°
	CacheTargetSize    fs.SizeSuffix `config:"cache_target_size"`    // æ¸…ç†ç›®æ ‡å¤§å°
	EnableSmartCleanup bool          `config:"enable_smart_cleanup"` // å¯ç”¨æ™ºèƒ½æ¸…ç†
	CleanupStrategy    string        `config:"cleanup_strategy"`     // æ¸…ç†ç­–ç•¥
}

// Fs è¡¨ç¤ºè¿œç¨‹123ç½‘ç›˜é©±åŠ¨å™¨å®ä¾‹
type Fs struct {
	name         string       // æ­¤è¿œç¨‹å®ä¾‹çš„åç§°
	originalName string       // æœªä¿®æ”¹çš„åŸå§‹é…ç½®åç§°
	root         string       // å½“å‰æ“ä½œçš„æ ¹è·¯å¾„
	opt          Options      // è§£æåçš„é…ç½®é€‰é¡¹
	features     *fs.Features // å¯é€‰åŠŸèƒ½ç‰¹æ€§

	// APIå®¢æˆ·ç«¯å’Œèº«ä»½éªŒè¯ç›¸å…³
	client       *http.Client     // ç”¨äºAPIè°ƒç”¨çš„HTTPå®¢æˆ·ç«¯ï¼ˆä¿ç•™ç”¨äºç‰¹æ®Šæƒ…å†µï¼‰
	rst          *rest.Client     // rcloneæ ‡å‡†restå®¢æˆ·ç«¯ï¼ˆæ¨èä½¿ç”¨ï¼‰
	token        string           // ç¼“å­˜çš„è®¿é—®ä»¤ç‰Œ
	tokenExpiry  time.Time        // ç¼“å­˜è®¿é—®ä»¤ç‰Œçš„è¿‡æœŸæ—¶é—´
	tokenMu      sync.Mutex       // ä¿æŠ¤tokenå’ŒtokenExpiryçš„äº’æ–¥é”
	tokenRenewer *oauthutil.Renew // ä»¤ç‰Œè‡ªåŠ¨æ›´æ–°å™¨

	// ç¼“å­˜å¹¶å‘å®‰å…¨é”
	cacheMu sync.RWMutex // ä¿æŠ¤æ‰€æœ‰ç¼“å­˜æ“ä½œçš„è¯»å†™é”

	// é…ç½®å’ŒçŠ¶æ€ä¿¡æ¯
	m            configmap.Mapper // é…ç½®æ˜ å°„å™¨
	rootFolderID string           // æ ¹æ–‡ä»¶å¤¹çš„ID

	// æ€§èƒ½å’Œé€Ÿç‡é™åˆ¶ - é’ˆå¯¹ä¸åŒAPIé™åˆ¶çš„å¤šä¸ªè°ƒé€Ÿå™¨
	listPacer     *fs.Pacer // æ–‡ä»¶åˆ—è¡¨APIçš„è°ƒé€Ÿå™¨ï¼ˆçº¦2 QPSï¼‰
	strictPacer   *fs.Pacer // ä¸¥æ ¼APIï¼ˆå¦‚user/info, move, deleteï¼‰çš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	uploadPacer   *fs.Pacer // ä¸Šä¼ APIçš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	downloadPacer *fs.Pacer // ä¸‹è½½APIçš„è°ƒé€Ÿå™¨ï¼ˆ2 QPSï¼‰

	// ç›®å½•ç¼“å­˜
	dirCache *dircache.DirCache

	// ç®€åŒ–çš„BadgerDBç¼“å­˜ç®¡ç†
	parentIDCache *cache.BadgerCache // çˆ¶ç›®å½•IDç¼“å­˜
	dirListCache  *cache.BadgerCache // ç›®å½•åˆ—è¡¨ç¼“å­˜
	pathToIDCache *cache.BadgerCache // è·¯å¾„åˆ°IDæ˜ å°„ç¼“å­˜

	// æ€§èƒ½ä¼˜åŒ–ç›¸å…³ - ä½¿ç”¨rcloneæ ‡å‡†ç»„ä»¶æ›¿ä»£è¿‡åº¦å¼€å‘çš„ç®¡ç†å™¨

	resumeManager         common.UnifiedResumeManager         // ç»Ÿä¸€çš„æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
	errorHandler          *common.UnifiedErrorHandler         // ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å™¨
	memoryOptimizer       *common.MemoryOptimizer             // å†…å­˜ä¼˜åŒ–å™¨
	concurrentDownloader  *common.UnifiedConcurrentDownloader // ç»Ÿä¸€çš„å¹¶å‘ä¸‹è½½å™¨
	crossCloudCoordinator *common.CrossCloudCoordinator       // è·¨äº‘ä¼ è¾“åè°ƒå™¨

	// ç¼“å­˜æ—¶é—´é…ç½® - ä½¿ç”¨ç»Ÿä¸€é…ç½®ç»“æ„
	cacheConfig common.UnifiedCacheConfig

	// ä¸Šä¼ åŸŸåç¼“å­˜ï¼Œé¿å…é¢‘ç¹APIè°ƒç”¨
	uploadDomainCache     string
	uploadDomainCacheTime time.Time
}

// CacheConfig ç¼“å­˜æ—¶é—´é…ç½®
type CacheConfig struct {
	ParentIDCacheTTL    time.Duration // parentFileIDéªŒè¯ç¼“å­˜TTLï¼Œé»˜è®¤5åˆ†é’Ÿ
	DirListCacheTTL     time.Duration // ç›®å½•åˆ—è¡¨ç¼“å­˜TTLï¼Œé»˜è®¤3åˆ†é’Ÿ
	DownloadURLCacheTTL time.Duration // ä¸‹è½½URLç¼“å­˜TTLï¼Œé»˜è®¤åŠ¨æ€ï¼ˆæ ¹æ®APIè¿”å›ï¼‰
	PathToIDCacheTTL    time.Duration // è·¯å¾„æ˜ å°„ç¼“å­˜TTLï¼Œé»˜è®¤12åˆ†é’Ÿ
}

// DefaultCacheConfig è¿”å›é»˜è®¤çš„ç¼“å­˜é…ç½®
func DefaultCacheConfig() CacheConfig {
	unifiedTTL := 5 * time.Minute // ç»Ÿä¸€TTLç­–ç•¥
	return CacheConfig{
		ParentIDCacheTTL:    unifiedTTL, // ç»Ÿä¸€ä¸º5åˆ†é’Ÿ
		DirListCacheTTL:     unifiedTTL, // ä»3åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œä¸å…¶ä»–ç¼“å­˜ä¿æŒä¸€è‡´
		DownloadURLCacheTTL: 0,          // åŠ¨æ€TTLï¼Œæ ¹æ®APIè¿”å›çš„è¿‡æœŸæ—¶é—´
		PathToIDCacheTTL:    unifiedTTL, // ä»12åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œé¿å…æŒ‡å‘è¿‡æœŸç¼“å­˜
	}
}

// ParentIDCacheEntry çˆ¶ç›®å½•IDç¼“å­˜æ¡ç›®
type ParentIDCacheEntry struct {
	ParentID  int64     `json:"parent_id"`
	Valid     bool      `json:"valid"`
	ExpiresAt time.Time `json:"expires_at"`
	CreatedAt time.Time `json:"created_at"`
}

// DirListCacheEntry ç›®å½•åˆ—è¡¨ç¼“å­˜æ¡ç›®
type DirListCacheEntry struct {
	FileList   []FileListInfoRespDataV2 `json:"file_list"`
	LastFileID int64                    `json:"last_file_id"`
	TotalCount int                      `json:"total_count"`
	CachedAt   time.Time                `json:"cached_at"`
	ParentID   int64                    `json:"parent_id"`
	Version    int64                    `json:"version"`  // ç¼“å­˜ç‰ˆæœ¬å·
	Checksum   string                   `json:"checksum"` // æ•°æ®æ ¡éªŒå’Œ
}

// PathToIDCacheEntry è·¯å¾„åˆ°FileIDæ˜ å°„ç¼“å­˜æ¡ç›®
type PathToIDCacheEntry struct {
	Path     string    `json:"path"`
	FileID   string    `json:"file_id"`
	IsDir    bool      `json:"is_dir"`
	ParentID string    `json:"parent_id"`
	CachedAt time.Time `json:"cached_at"`
}

// ProgressReadCloser åŒ…è£…ReadCloserä»¥æä¾›è¿›åº¦è·Ÿè¸ªå’Œèµ„æºç®¡ç†
type ProgressReadCloser struct {
	io.ReadCloser
	fs              *Fs
	totalSize       int64
	transferredSize int64
	startTime       time.Time // ä¸‹è½½å¼€å§‹æ—¶é—´
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (prc *ProgressReadCloser) Read(p []byte) (n int, err error) {
	n, err = prc.ReadCloser.Read(p)
	if n > 0 {
		prc.transferredSize += int64(n)
		// rcloneå†…ç½®çš„accountingä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦è·Ÿè¸ª
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£ï¼ŒåŒæ—¶æ¸…ç†èµ„æº
func (prc *ProgressReadCloser) Close() error {
	// å…³é—­åº•å±‚ReadCloser
	return prc.ReadCloser.Close()
}

// Object æè¿°123ç½‘ç›˜ä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å¯¹è±¡
type Object struct {
	fs          *Fs       // çˆ¶æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	remote      string    // è¿œç¨‹è·¯å¾„
	hasMetaData bool      // æ˜¯å¦å·²åŠ è½½å…ƒæ•°æ®
	id          string    // æ–‡ä»¶/æ–‡ä»¶å¤¹ID
	size        int64     // æ–‡ä»¶å¤§å°
	md5sum      string    // MD5å“ˆå¸Œå€¼
	modTime     time.Time // ä¿®æ”¹æ—¶é—´
	isDir       bool      // æ˜¯å¦ä¸ºç›®å½•
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file     *os.File
	tempPath string
}

// Read å®ç°io.Readeræ¥å£
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	var err error
	if r.file != nil {
		err = r.file.Close()
		r.file = nil
	}
	if r.tempPath != "" {
		if removeErr := os.Remove(r.tempPath); removeErr != nil {
			fs.Debugf(nil, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", r.tempPath, removeErr)
		} else {
			fs.Debugf(nil, "å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: %s", r.tempPath)
		}
		r.tempPath = ""
	}
	return err
}

// æ–‡ä»¶åéªŒè¯ç›¸å…³çš„å…¨å±€å˜é‡
var (
	// invalidCharsRegex ç”¨äºåŒ¹é…123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
	invalidCharsRegex = regexp.MustCompile(`["\/:*?|><\\]`)
)

func validateFileName(name string) error {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„è·¯å¾„æ£€æŸ¥
	if strings.Contains(name, "/") || strings.Contains(name, "\\") {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½åŒ…å«è·¯å¾„åˆ†éš”ç¬¦")
	}

	// åŸºç¡€æ£€æŸ¥ï¼šç©ºå€¼å’Œç©ºæ ¼
	if strings.TrimSpace(name) == "" {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½ä¸ºç©ºæˆ–å…¨éƒ¨æ˜¯ç©ºæ ¼")
	}

	// UTF-8æœ‰æ•ˆæ€§æ£€æŸ¥
	if !utf8.ValidString(name) {
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«æ— æ•ˆçš„UTF-8å­—ç¬¦")
	}

	// 123ç½‘ç›˜ç‰¹å®šçš„é•¿åº¦æ£€æŸ¥ï¼ˆå­—ç¬¦æ•°ï¼‰
	runeCount := utf8.RuneCountInString(name)
	if runeCount > maxFileNameLength {
		return fmt.Errorf("æ–‡ä»¶åé•¿åº¦è¶…è¿‡é™åˆ¶ï¼šå½“å‰%dä¸ªå­—ç¬¦ï¼Œæœ€å¤§å…è®¸%dä¸ªå­—ç¬¦",
			runeCount, maxFileNameLength)
	}

	// å­—èŠ‚é•¿åº¦æ£€æŸ¥
	byteLength := len([]byte(name))
	if byteLength > maxFileNameBytes {
		return fmt.Errorf("æ–‡ä»¶åå­—èŠ‚é•¿åº¦è¶…è¿‡%d: %d", maxFileNameBytes, byteLength)
	}

	// ç¦ç”¨å­—ç¬¦æ£€æŸ¥
	if invalidCharsRegex.MatchString(name) {
		invalidFound := invalidCharsRegex.FindAllString(name, -1)
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«ä¸å…è®¸çš„å­—ç¬¦ï¼š%vï¼Œ123ç½‘ç›˜ä¸å…è®¸ä½¿ç”¨ä»¥ä¸‹å­—ç¬¦ï¼š%s",
			invalidFound, invalidChars)
	}

	return nil
}

// cleanFileName æ¸…ç†æ–‡ä»¶åï¼Œå°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºå®‰å…¨å­—ç¬¦
// åŒæ—¶ç¡®ä¿æ–‡ä»¶åé•¿åº¦ä¸è¶…è¿‡é™åˆ¶
func cleanFileName(name string) string {
	if name == "" {
		return "æœªå‘½åæ–‡ä»¶"
	}

	// æ›¿æ¢éæ³•å­—ç¬¦
	cleaned := invalidCharsRegex.ReplaceAllString(name, replacementChar)

	// å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†
	if utf8.RuneCountInString(cleaned) > maxFileNameLength {
		// ä¿ç•™æ–‡ä»¶æ‰©å±•å
		ext := filepath.Ext(cleaned)
		nameWithoutExt := strings.TrimSuffix(cleaned, ext)

		// è®¡ç®—å¯ç”¨çš„æ–‡ä»¶åé•¿åº¦ï¼ˆæ€»é•¿åº¦ - æ‰©å±•åé•¿åº¦ï¼‰
		maxNameLength := maxFileNameLength - utf8.RuneCountInString(ext)
		if maxNameLength < 1 {
			// å¦‚æœæ‰©å±•åå¤ªé•¿ï¼Œåªä¿ç•™éƒ¨åˆ†æ‰©å±•å
			maxNameLength = maxFileNameLength - 10 // ä¿ç•™è‡³å°‘10ä¸ªå­—ç¬¦ç»™æ–‡ä»¶å
			maxNameLength = max(maxNameLength, 1)
			extRunes := []rune(ext)
			if len(extRunes) > 10 {
				ext = string(extRunes[:10])
			}
		}

		// æˆªæ–­æ–‡ä»¶åä¸»ä½“éƒ¨åˆ†
		nameRunes := []rune(nameWithoutExt)
		if len(nameRunes) > maxNameLength {
			nameWithoutExt = string(nameRunes[:maxNameLength])
		}

		cleaned = nameWithoutExt + ext
	}

	return cleaned
}

// validateAndCleanFileName éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶åçš„ä¾¿æ·å‡½æ•°
// å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
func validateAndCleanFileName(name string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(name); err == nil {
		return name, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(name)

	// è¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
	warning = fmt.Errorf("æ–‡ä»¶åå·²è‡ªåŠ¨æ¸…ç†ï¼šåŸå§‹åç§° '%s' -> æ¸…ç†ååç§° '%s'", name, cleanedName)
	return cleanedName, warning
}

// generateUniqueFileName ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª
func (f *Fs) generateUniqueFileName(ctx context.Context, parentFileID int64, baseName string) (string, error) {
	// é¦–å…ˆæ£€æŸ¥åŸå§‹æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
	exists, err := f.fileExists(ctx, parentFileID, baseName)
	if err != nil {
		return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
	}

	if !exists {
		return baseName, nil
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
	ext := filepath.Ext(baseName)
	nameWithoutExt := strings.TrimSuffix(baseName, ext)

	for i := 1; i <= 999; i++ {
		newName := fmt.Sprintf("%s (%d)%s", nameWithoutExt, i, ext)

		// éªŒè¯æ–°æ–‡ä»¶åé•¿åº¦
		if len([]byte(newName)) > maxFileNameBytes {
			// å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­åŸºç¡€åç§°
			suffixBytes := make([]byte, 0, 20) // é¢„åˆ†é…è¶³å¤Ÿçš„å®¹é‡
			suffixBytes = fmt.Appendf(suffixBytes, " (%d)%s", i, ext)
			maxBaseLength := maxFileNameBytes - len(suffixBytes)
			if maxBaseLength > 0 {
				truncatedBase := string([]byte(nameWithoutExt)[:maxBaseLength])
				newName = fmt.Sprintf("%s (%d)%s", truncatedBase, i, ext)
			} else {
				// å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œä½¿ç”¨ç®€åŒ–åç§°
				newName = fmt.Sprintf("file_%d%s", i, ext)
			}
		}

		exists, err := f.fileExists(ctx, parentFileID, newName)
		if err != nil {
			return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
		}

		if !exists {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
			return newName, nil
		}
	}

	// å¦‚æœå°è¯•äº†999æ¬¡éƒ½æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€åç§°ï¼Œä½¿ç”¨æ—¶é—´æˆ³
	timestamp := time.Now().Unix()
	newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, timestamp, ext)
	fs.Debugf(f, "ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
	return newName, nil
}

// fileExists æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExists(ctx context.Context, parentFileID int64, fileName string) (bool, error) {
	// ä½¿ç”¨ListFile APIæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", 0)
	if err != nil {
		return false, err
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			return true, nil
		}
	}

	return false, nil
}

// verifyParentFileID éªŒè¯çˆ¶ç›®å½•IDæ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨BadgerDBç¼“å­˜å‡å°‘APIè°ƒç”¨
func (f *Fs) verifyParentFileID(ctx context.Context, parentFileID int64) (bool, error) {
	// é¦–å…ˆæ£€æŸ¥BadgerDBç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
	cacheKey := common.GenerateParentIDCacheKey(parentFileID)
	if f.parentIDCache != nil {
		// ä½¿ç”¨è¯»é”ä¿æŠ¤ç¼“å­˜è¯»å–
		f.cacheMu.RLock()
		cached, found := f.parentIDCache.GetBool(cacheKey)
		f.cacheMu.RUnlock()

		if found {
			fs.Debugf(f, "çˆ¶ç›®å½•ID %d BadgerDBç¼“å­˜éªŒè¯é€šè¿‡: %v", parentFileID, cached)
			return cached, nil
		}
	}

	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID: %d", parentFileID)

	// æ‰§è¡Œå®é™…çš„APIéªŒè¯ï¼Œä½¿ç”¨ListFile APIï¼ˆè¿™æ˜¯æœ€é€šç”¨çš„éªŒè¯æ–¹å¼ï¼‰
	response, err := f.ListFile(ctx, int(parentFileID), 1, "", "", 0)
	if err != nil {
		fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %v", parentFileID, err)
		// ç½‘ç»œé”™è¯¯ä¸åº”è¯¥ç¼“å­˜ä¸ºæ— æ•ˆï¼Œé¿å…è¯¯åˆ¤
		return false, err
	}

	isValid := response.Code == 0
	if !isValid {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼ŒAPIè¿”å›: code=%d, message=%s", parentFileID, response.Code, response.Message)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼šåªç¼“å­˜æˆåŠŸçš„éªŒè¯ç»“æœï¼Œå¤±è´¥çš„ç»“æœä½¿ç”¨è¾ƒçŸ­TTL
	if f.parentIDCache != nil {
		// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥
		f.cacheMu.Lock()
		var cacheTTL time.Duration
		if isValid {
			cacheTTL = f.cacheConfig.ParentIDCacheTTL // æˆåŠŸç»“æœä½¿ç”¨æ­£å¸¸TTL
		} else {
			cacheTTL = time.Minute * 1 // å¤±è´¥ç»“æœä½¿ç”¨çŸ­TTLï¼Œé¿å…é•¿æœŸè¯¯åˆ¤
		}

		err := f.parentIDCache.SetBool(cacheKey, isValid, cacheTTL)
		f.cacheMu.Unlock()

		if err != nil {
			fs.Debugf(f, "ä¿å­˜çˆ¶ç›®å½•ID %d ç¼“å­˜å¤±è´¥: %v", parentFileID, err)
		} else {
			fs.Debugf(f, "å·²ä¿å­˜çˆ¶ç›®å½•ID %d åˆ°BadgerDBç¼“å­˜: valid=%v, TTL=%v", parentFileID, isValid, cacheTTL)
		}
	}

	return isValid, nil
}

// getCorrectParentFileID è·å–ä¸Šä¼ APIè®¤å¯çš„æ­£ç¡®çˆ¶ç›®å½•ID
// ç»Ÿä¸€ç‰ˆæœ¬ï¼šä¸ºå•æ­¥ä¸Šä¼ å’Œåˆ†ç‰‡ä¸Šä¼ æä¾›ä¸€è‡´çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
func (f *Fs) getCorrectParentFileID(ctx context.Context, cachedParentID int64) (int64, error) {
	fs.Debugf(f, " ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥ï¼Œç¼“å­˜ID: %d", cachedParentID)

	// ç­–ç•¥1ï¼šæ¸…ç†ç›¸å…³ç¼“å­˜ï¼Œä½†ä¿æŒæ›´ç²¾ç¡®çš„æ¸…ç†èŒƒå›´
	fs.Infof(f, "ğŸ”„ æ¸…ç†ç›¸å…³ç¼“å­˜ï¼Œé‡æ–°è·å–ç›®å½•ç»“æ„")
	f.clearParentIDCache(cachedParentID)
	f.clearDirListCache(cachedParentID)

	// åªé‡ç½®dirCacheï¼Œä¸å®Œå…¨æ¸…ç©ºï¼Œä¿æŒå…¶ä»–æœ‰æ•ˆç¼“å­˜
	if f.dirCache != nil {
		fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", cachedParentID)
		f.dirCache.ResetRoot()
	}

	// ç­–ç•¥2ï¼šå°è¯•é‡æ–°éªŒè¯åŸå§‹IDï¼ˆæ¸…ç†ç¼“å­˜åå¯èƒ½æ¢å¤ï¼‰
	fs.Debugf(f, "ğŸ”„ é‡æ–°éªŒè¯åŸå§‹ç›®å½•ID: %d", cachedParentID)
	exists, err := f.verifyParentFileID(ctx, cachedParentID)
	if err == nil && exists {
		fs.Infof(f, "âœ… åŸå§‹ç›®å½•ID %d åœ¨æ¸…ç†ç¼“å­˜åéªŒè¯æˆåŠŸ", cachedParentID)
		return cachedParentID, nil
	}

	// ç­–ç•¥3ï¼šå°è¯•éªŒè¯æ ¹ç›®å½•æ˜¯å¦å¯ç”¨
	// æ ¹æ®å®˜æ–¹APIç¤ºä¾‹ï¼Œå¾ˆå¤šæƒ…å†µä¸‹å¯ä»¥ä½¿ç”¨æ ¹ç›®å½• (parentFileID: 0)
	fs.Debugf(f, "ğŸ§ª å°è¯•éªŒè¯æ ¹ç›®å½• (parentFileID: 0) æ˜¯å¦å¯ç”¨")
	rootExists, err := f.verifyParentFileID(ctx, 0)
	if err == nil && rootExists {
		fs.Infof(f, "âœ… æ ¹ç›®å½•éªŒè¯æˆåŠŸï¼Œä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºå›é€€æ–¹æ¡ˆ")
		return 0, nil
	}

	// ç­–ç•¥4ï¼šå¦‚æœæ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œä»ç„¶å›é€€åˆ°æ ¹ç›®å½•ï¼ˆå¼ºåˆ¶ç­–ç•¥ï¼‰
	// è¿™æ˜¯å› ä¸ºæ ¹ç›®å½•(0)åœ¨123ç½‘ç›˜APIä¸­æ€»æ˜¯å­˜åœ¨çš„
	fs.Infof(f, "âš ï¸ æ‰€æœ‰éªŒè¯ç­–ç•¥å¤±è´¥ï¼Œå¼ºåˆ¶ä½¿ç”¨æ ¹ç›®å½• (parentFileID: 0)")
	return 0, nil
}

// clearParentIDCache æ¸…ç†parentFileIDç¼“å­˜ä¸­çš„ç‰¹å®šæ¡ç›®
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearParentIDCache(parentFileID ...int64) {
	if f.parentIDCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	if len(parentFileID) > 0 {
		// æ¸…ç†æŒ‡å®šçš„parentFileID
		for _, id := range parentFileID {
			cacheKey := common.GenerateParentIDCacheKey(id)
			if err := f.parentIDCache.Delete(cacheKey); err != nil {
				fs.Debugf(f, "æ¸…ç†çˆ¶ç›®å½•ID %d ç¼“å­˜å¤±è´¥: %v", id, err)
			} else {
				fs.Debugf(f, "å·²æ¸…ç†çˆ¶ç›®å½•ID %d çš„ç¼“å­˜", id)
			}
		}
	} else {
		// æ¸…ç†æ‰€æœ‰ç¼“å­˜
		if err := f.parentIDCache.Clear(); err != nil {
			fs.Debugf(f, "æ¸…ç†æ‰€æœ‰çˆ¶ç›®å½•IDç¼“å­˜å¤±è´¥: %v", err)
		} else {
			fs.Debugf(f, "å·²æ¸…ç†æ‰€æœ‰çˆ¶ç›®å½•IDç¼“å­˜")
		}
	}
}

// invalidateRelatedCaches æ™ºèƒ½ç¼“å­˜å¤±æ•ˆ - æ ¹æ®æ“ä½œç±»å‹æ¸…ç†ç›¸å…³ç¼“å­˜
// è½»é‡çº§ä¼˜åŒ–ï¼šç®€åŒ–ç¼“å­˜å¤±æ•ˆé€»è¾‘ï¼Œæé«˜å¯é æ€§
func (f *Fs) invalidateRelatedCaches(path string, operation string) {
	fs.Debugf(f, "è½»é‡çº§ç¼“å­˜å¤±æ•ˆ: æ“ä½œ=%s, è·¯å¾„=%s", operation, path)

	// ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼šå¯¹æ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½è¿›è¡Œå…¨é¢ç¼“å­˜æ¸…ç†
	// è™½ç„¶ä¼šå½±å“ä¸€äº›æ€§èƒ½ï¼Œä½†ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œé¿å…å¤æ‚çš„æ¡ä»¶åˆ¤æ–­

	switch operation {
	case "upload", "put", "mkdir", "delete", "remove", "rmdir", "rename", "move":
		// ç»Ÿä¸€å¤„ç†ï¼šæ¸…ç†å½“å‰è·¯å¾„å’Œçˆ¶è·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
		f.clearAllRelatedCaches(path)

		// é¢å¤–æ¸…ç†çˆ¶ç›®å½•ç¼“å­˜
		parentPath := filepath.Dir(path)
		if parentPath != "." && parentPath != "/" {
			f.clearAllRelatedCaches(parentPath)
		}

		fs.Debugf(f, "å·²æ¸…ç†è·¯å¾„ %s å’Œçˆ¶è·¯å¾„ %s çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜", path, parentPath)
	}
}

// clearAllRelatedCaches æ¸…ç†æŒ‡å®šè·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
// è½»é‡çº§ä¼˜åŒ–ï¼šæ–°å¢ç»Ÿä¸€ç¼“å­˜æ¸…ç†å‡½æ•°
func (f *Fs) clearAllRelatedCaches(path string) {
	// æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜
	f.clearPathToIDCache(path)

	// æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šæ¸…ç†æ‰€æœ‰ï¼‰
	if f.dirListCache != nil {
		f.cacheMu.Lock()
		err := f.dirListCache.Clear()
		f.cacheMu.Unlock()
		if err != nil {
			fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥: %v", err)
		}
	}

	// æ¸…ç†çˆ¶ç›®å½•IDç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šæ¸…ç†æ‰€æœ‰ï¼‰
	if f.parentIDCache != nil {
		f.cacheMu.Lock()
		err := f.parentIDCache.Clear()
		f.cacheMu.Unlock()
		if err != nil {
			fs.Debugf(f, "æ¸…ç†çˆ¶ç›®å½•IDç¼“å­˜å¤±è´¥: %v", err)
		}
	}
}

// getDirListFromCache ä»ç¼“å­˜è·å–ç›®å½•åˆ—è¡¨
// ä¿®å¤ç‰ˆæœ¬ï¼šé¿å…å±é™©çš„é”å‡çº§ï¼Œä½¿ç”¨å¼‚æ­¥æ¸…ç†
func (f *Fs) getDirListFromCache(parentFileID int64, lastFileID int64) (*DirListCacheEntry, bool) {
	if f.dirListCache == nil {
		return nil, false
	}

	cacheKey := common.GenerateDirListCacheKey(fmt.Sprintf("%d", parentFileID), fmt.Sprintf("%d", lastFileID))

	// ä¿®å¤ï¼šä½¿ç”¨çº¯è¯»æ“ä½œï¼Œä¸åœ¨è¯»é”ä¸­è¿›è¡Œå†™æ“ä½œ
	f.cacheMu.RLock()
	var entry DirListCacheEntry
	found, err := f.dirListCache.Get(cacheKey, &entry)
	f.cacheMu.RUnlock() // ç«‹å³é‡Šæ”¾è¯»é”ï¼Œä¸ä½¿ç”¨defer

	if err != nil {
		fs.Debugf(f, "è·å–ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return nil, false
	}

	if !found {
		return nil, false
	}

	// å…³é”®ä¿®å¤ï¼šåœ¨é”å¤–è¿›è¡ŒéªŒè¯ï¼Œå¦‚æœéªŒè¯å¤±è´¥åˆ™å¼‚æ­¥æ¸…ç†
	if !fastValidateCacheEntry(entry.FileList) {
		fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å¿«é€Ÿæ ¡éªŒå¤±è´¥: %s", cacheKey)

		// ä¿®å¤ï¼šå¼‚æ­¥æ¸…ç†æŸåçš„ç¼“å­˜ï¼Œé¿å…é”å‡çº§
		go f.asyncCleanInvalidCache(cacheKey)
		return nil, false
	}

	// å®Œæ•´æ ¡éªŒï¼ˆå¦‚æœéœ€è¦ï¼‰
	if entry.Checksum != "" && len(entry.FileList) > 100 {
		if !validateCacheEntry(entry.FileList, entry.Checksum) {
			fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å®Œæ•´æ ¡éªŒå¤±è´¥: %s", cacheKey)

			// ä¿®å¤ï¼šå¼‚æ­¥æ¸…ç†æŸåçš„ç¼“å­˜
			go f.asyncCleanInvalidCache(cacheKey)
			return nil, false
		}
	}

	fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å‘½ä¸­: parentID=%d, lastFileID=%d, æ–‡ä»¶æ•°=%d",
		parentFileID, lastFileID, len(entry.FileList))
	return &entry, true
}

// asyncCleanInvalidCache å¼‚æ­¥æ¸…ç†æ— æ•ˆç¼“å­˜
// å…³é”®ä¿®å¤ï¼šå°†å†™æ“ä½œåˆ†ç¦»åˆ°ä¸“é—¨çš„å‡½æ•°ä¸­ï¼Œé¿å…é”å‡çº§
func (f *Fs) asyncCleanInvalidCache(cacheKey string) {
	if f.dirListCache == nil {
		return
	}

	// æ·»åŠ å»¶è¿Ÿï¼Œé¿å…é¢‘ç¹æ¸…ç†
	time.Sleep(100 * time.Millisecond)

	// ä¿®å¤ï¼šä½¿ç”¨ä¸“é—¨çš„å†™é”ï¼Œç®€å•ç›´æ¥
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	// å†æ¬¡æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶å­˜åœ¨ï¼ˆå¯èƒ½å·²è¢«å…¶ä»–goroutineæ¸…ç†ï¼‰
	var entry DirListCacheEntry
	found, err := f.dirListCache.Get(cacheKey, &entry)
	if err != nil || !found {
		return // ç¼“å­˜å·²ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†
	}

	// å†æ¬¡éªŒè¯ï¼Œç¡®ä¿ç¡®å®éœ€è¦æ¸…ç†
	if !fastValidateCacheEntry(entry.FileList) {
		err := f.dirListCache.Delete(cacheKey)
		if err != nil {
			fs.Debugf(f, "æ¸…ç†æ— æ•ˆç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		} else {
			fs.Debugf(f, "å·²æ¸…ç†æ— æ•ˆç¼“å­˜: %s", cacheKey)
		}
	}
}

// saveDirListToCache ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) saveDirListToCache(parentFileID int64, lastFileID int64, fileList []FileListInfoRespDataV2, nextLastFileID int64) {
	if f.dirListCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	cacheKey := common.GenerateDirListCacheKey(fmt.Sprintf("%d", parentFileID), fmt.Sprintf("%d", lastFileID))

	// åœ¨ä¿å­˜åˆ°ç¼“å­˜å‰å¯¹æ–‡ä»¶åè¿›è¡ŒURLè§£ç 
	decodedFileList := make([]FileListInfoRespDataV2, len(fileList))
	for i, file := range fileList {
		decodedFile := file
		// URLè§£ç æ–‡ä»¶å
		if decodedName, err := url.QueryUnescape(strings.TrimSpace(file.Filename)); err == nil {
			decodedFile.Filename = decodedName
		}
		decodedFileList[i] = decodedFile
	}

	entry := DirListCacheEntry{
		FileList:   decodedFileList, // ä½¿ç”¨è§£ç åçš„æ–‡ä»¶åˆ—è¡¨
		LastFileID: nextLastFileID,
		TotalCount: len(decodedFileList),
		CachedAt:   time.Now(),
		ParentID:   parentFileID,
		Version:    generateCacheVersion(),
		Checksum:   calculateChecksum(decodedFileList),
	}

	// ä½¿ç”¨é…ç½®çš„ç›®å½•åˆ—è¡¨ç¼“å­˜TTL
	if err := f.dirListCache.Set(cacheKey, entry, f.cacheConfig.DirListCacheTTL); err != nil {
		fs.Debugf(f, "ä¿å­˜ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜: parentID=%d, æ–‡ä»¶æ•°=%d, TTL=%v", parentFileID, len(fileList), f.cacheConfig.DirListCacheTTL)
	}
}

// clearDirListCache æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearDirListCache(parentFileID int64) {
	if f.dirListCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	// æ¸…ç†æŒ‡å®šçˆ¶ç›®å½•çš„æ‰€æœ‰åˆ†é¡µç¼“å­˜
	prefix := fmt.Sprintf("dirlist_%d_", parentFileID)
	if err := f.dirListCache.DeletePrefix(prefix); err != nil {
		fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", prefix, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜: parentID=%d", parentFileID)
	}
}

// getPathToIDFromCache ä»ç¼“å­˜è·å–è·¯å¾„åˆ°FileIDçš„æ˜ å°„
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) getPathToIDFromCache(path string) (string, bool, bool) {
	if f.pathToIDCache == nil {
		return "", false, false
	}

	cacheKey := common.GeneratePathToIDCacheKey(path)
	var entry PathToIDCacheEntry

	// ä½¿ç”¨è¯»é”ä¿æŠ¤ç¼“å­˜è¯»å–
	f.cacheMu.RLock()
	found, err := f.pathToIDCache.Get(cacheKey, &entry)
	f.cacheMu.RUnlock()

	if err != nil {
		fs.Debugf(f, "è·å–è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return "", false, false
	}

	if found {
		fs.Debugf(f, "è·¯å¾„æ˜ å°„ç¼“å­˜å‘½ä¸­: %s -> %s (dir: %v)", path, entry.FileID, entry.IsDir)
		return entry.FileID, entry.IsDir, true
	}

	return "", false, false
}

// savePathToIDToCache ä¿å­˜è·¯å¾„åˆ°FileIDçš„æ˜ å°„åˆ°ç¼“å­˜
func (f *Fs) savePathToIDToCache(path, fileID, parentID string, isDir bool) {
	if f.pathToIDCache == nil {
		return
	}

	cacheKey := common.GeneratePathToIDCacheKey(path)
	entry := PathToIDCacheEntry{
		Path:     path,
		FileID:   fileID,
		IsDir:    isDir,
		ParentID: parentID,
		CachedAt: time.Now(),
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥
	f.cacheMu.Lock()
	err := f.pathToIDCache.Set(cacheKey, entry, f.cacheConfig.PathToIDCacheTTL)
	f.cacheMu.Unlock()

	if err != nil {
		fs.Debugf(f, "ä¿å­˜è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜è·¯å¾„æ˜ å°„åˆ°ç¼“å­˜: %s -> %s (dir: %v), TTL=%v", path, fileID, isDir, f.cacheConfig.PathToIDCacheTTL)
	}
}

// clearPathToIDCache æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearPathToIDCache(path string) {
	if f.pathToIDCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤
	f.cacheMu.Lock()
	cacheKey := fmt.Sprintf("path_to_id_%s", path)
	err := f.pathToIDCache.Delete(cacheKey)
	f.cacheMu.Unlock()

	if err != nil {
		fs.Debugf(f, "æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜: %s", path)
	}
}

// getTaskIDForPreuploadID è·å–preuploadIDå¯¹åº”çš„ç»Ÿä¸€TaskID
// ä¿®å¤ï¼šä¼˜åŒ–å›é€€ç­–ç•¥ï¼Œå‡å°‘é”™è¯¯æ—¥å¿—
func (f *Fs) getTaskIDForPreuploadID(preuploadID string, remotePath string, fileSize int64) string {
	// ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€TaskIDï¼Œä½†åœ¨å‚æ•°ä¸å®Œæ•´æ—¶é™é»˜å›é€€
	if remotePath != "" && fileSize > 0 {
		// ä½¿ç”¨ç»Ÿä¸€TaskIDç”Ÿæˆæ–¹å¼
		taskID := common.GenerateTaskID123(remotePath, fileSize)
		fs.Debugf(f, " ç”Ÿæˆç»Ÿä¸€TaskID: %s (è·¯å¾„: %s, å¤§å°: %d)", taskID, remotePath, fileSize)
		return taskID
	}

	// ä¿®å¤ï¼šé™é»˜å›é€€åˆ°preuploadIDæ–¹å¼ï¼Œé¿å…é”™è¯¯æ—¥å¿—åˆ·å±
	fallbackTaskID := fmt.Sprintf("123_fallback_%s", preuploadID)
	fs.Debugf(f, "âš ï¸ ä½¿ç”¨å›é€€TaskID: %s (è·¯å¾„: '%s', å¤§å°: %d)", fallbackTaskID, remotePath, fileSize)
	return fallbackTaskID
}

// tryMigrateTaskID å°è¯•è¿ç§»æ—§çš„TaskIDåˆ°æ–°çš„ç»Ÿä¸€TaskID
// ç®€åŒ–ï¼šä¼˜åŒ–è¿ç§»é€»è¾‘ï¼Œå‡å°‘ä¸å¿…è¦çš„æ£€æŸ¥å’Œæ“ä½œ
func (f *Fs) tryMigrateTaskID(oldTaskID, newTaskID string) error {
	// ç®€åŒ–ï¼šå¿«é€Ÿæ£€æŸ¥ï¼Œé¿å…ä¸å¿…è¦çš„æ“ä½œ
	if f.resumeManager == nil || oldTaskID == newTaskID || oldTaskID == "" || newTaskID == "" {
		return nil
	}

	// ä¼˜åŒ–ï¼šå…ˆæ£€æŸ¥æ–°TaskIDæ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„æ—§æ•°æ®æŸ¥è¯¢
	if newInfo, err := f.resumeManager.LoadResumeInfo(newTaskID); err == nil && newInfo != nil {
		// æ–°TaskIDå·²æœ‰æ•°æ®ï¼Œæ¸…ç†æ—§æ•°æ®å³å¯
		f.resumeManager.DeleteResumeInfo(oldTaskID)
		return nil
	}

	// ç®€åŒ–ï¼šå°è¯•åŠ è½½æ—§æ•°æ®å¹¶è¿ç§»
	if oldInfo, err := f.resumeManager.LoadResumeInfo(oldTaskID); err == nil && oldInfo != nil {
		if r123, ok := oldInfo.(*common.ResumeInfo123); ok {
			r123.TaskID = newTaskID
			if err := f.resumeManager.SaveResumeInfo(r123); err == nil {
				f.resumeManager.DeleteResumeInfo(oldTaskID) // å¿½ç•¥åˆ é™¤é”™è¯¯
				fs.Debugf(f, " TaskIDè¿ç§»æˆåŠŸ: %s -> %s", oldTaskID, newTaskID)
			}
		}
	}

	return nil
}

// getTaskIDWithMigration è·å–TaskIDå¹¶è‡ªåŠ¨å¤„ç†è¿ç§»
// æ–°å¢ï¼šç»Ÿä¸€TaskIDè·å–å’Œè¿ç§»é€»è¾‘ï¼Œå‡å°‘é‡å¤ä»£ç 
func (f *Fs) getTaskIDWithMigration(preuploadID, remotePath string, fileSize int64) string {
	// è·å–ç»Ÿä¸€TaskID
	taskID := f.getTaskIDForPreuploadID(preuploadID, remotePath, fileSize)

	// ç®€åŒ–ï¼šåªæœ‰åœ¨æœ‰å®Œæ•´ä¿¡æ¯æ—¶æ‰å°è¯•è¿ç§»
	if remotePath != "" && fileSize > 0 && !strings.HasPrefix(taskID, "123_fallback_") {
		oldTaskID := fmt.Sprintf("123_%s", preuploadID)
		if oldTaskID != taskID {
			f.tryMigrateTaskID(oldTaskID, taskID) // å¿½ç•¥è¿ç§»é”™è¯¯ï¼Œä¸å½±å“ä¸»æµç¨‹
		}
	}

	return taskID
}

// calculatePollingStrategy è®¡ç®—æ™ºèƒ½è½®è¯¢ç­–ç•¥
// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è½®è¯¢å‚æ•°å’Œè¿ç»­å¤±è´¥é˜ˆå€¼
func (f *Fs) calculatePollingStrategy(fileSize int64) (maxRetries int, baseInterval time.Duration, maxConsecutiveFailures int) {
	// åŸºç¡€è½®è¯¢é—´éš”ï¼ˆæŒ‰å®˜æ–¹æ–‡æ¡£è¦æ±‚ï¼‰
	baseInterval = 1 * time.Second

	// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´æœ€å¤§é‡è¯•æ¬¡æ•°å’Œè¿ç»­å¤±è´¥é˜ˆå€¼
	switch {
	case fileSize < 100*1024*1024: // å°äº100MB
		maxRetries = 180           // 3åˆ†é’Ÿ
		maxConsecutiveFailures = 8 // å°æ–‡ä»¶å®¹é”™æ€§ç¨ä½
	case fileSize < 500*1024*1024: // å°äº500MB
		maxRetries = 300            // 5åˆ†é’Ÿ
		maxConsecutiveFailures = 12 // ä¸­ç­‰æ–‡ä»¶é€‚ä¸­å®¹é”™
	case fileSize < 1*1024*1024*1024: // å°äº1GB
		maxRetries = 600            // 10åˆ†é’Ÿ
		maxConsecutiveFailures = 15 // å¤§æ–‡ä»¶æé«˜å®¹é”™æ€§
	case fileSize < 5*1024*1024*1024: // å°äº5GB
		maxRetries = 900            // 15åˆ†é’Ÿ
		maxConsecutiveFailures = 20 // è¶…å¤§æ–‡ä»¶é«˜å®¹é”™æ€§
	default: // 5GBä»¥ä¸Š
		maxRetries = 1200           // 20åˆ†é’Ÿ
		maxConsecutiveFailures = 25 // å·¨å¤§æ–‡ä»¶æœ€é«˜å®¹é”™æ€§
	}

	fs.Debugf(f, "æ–‡ä»¶å¤§å°: %s, è½®è¯¢ç­–ç•¥: æœ€å¤§%dæ¬¡, é—´éš”%v, è¿ç»­å¤±è´¥é˜ˆå€¼%d",
		fs.SizeSuffix(fileSize), maxRetries, baseInterval, maxConsecutiveFailures)

	return maxRetries, baseInterval, maxConsecutiveFailures
}

// calculateDynamicInterval è®¡ç®—åŠ¨æ€è½®è¯¢é—´éš”
// ä¼˜åŒ–ï¼šæ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°ã€å°è¯•æ¬¡æ•°å’Œé”™è¯¯ç±»å‹æ™ºèƒ½è°ƒæ•´é—´éš”
func (f *Fs) calculateDynamicInterval(baseInterval time.Duration, consecutiveFailures, attempt int, lastErr error) time.Duration {
	// åŸºç¡€é—´éš”
	interval := baseInterval

	// ä¼˜åŒ–ï¼šæ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´é€€é¿ç­–ç•¥
	isNetworkError := lastErr != nil && common.IsNetworkError(lastErr)

	// æ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°å¢åŠ å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ï¼‰
	if consecutiveFailures > 0 {
		backoffMultiplier := 1 << uint(consecutiveFailures) // 2^consecutiveFailures

		// ä¼˜åŒ–ï¼šç½‘ç»œé”™è¯¯ä½¿ç”¨æ›´æ¸©å’Œçš„é€€é¿ç­–ç•¥
		if isNetworkError {
			if backoffMultiplier > 4 {
				backoffMultiplier = 4 // ç½‘ç»œé”™è¯¯æœ€å¤§4å€å»¶è¿Ÿ
			}
		} else {
			if backoffMultiplier > 8 {
				backoffMultiplier = 8 // å…¶ä»–é”™è¯¯æœ€å¤§8å€å»¶è¿Ÿ
			}
		}

		interval = time.Duration(backoffMultiplier) * baseInterval
	}

	// ä¼˜åŒ–ï¼šé•¿æ—¶é—´è½®è¯¢æ—¶çš„æ™ºèƒ½é—´éš”è°ƒæ•´
	if attempt > 60 { // 1åˆ†é’Ÿå
		if isNetworkError {
			interval = interval * 3 // ç½‘ç»œé”™è¯¯å¢åŠ æ›´å¤šå»¶è¿Ÿ
		} else {
			interval = interval * 2 // å…¶ä»–é”™è¯¯é€‚åº¦å¢åŠ 
		}
	}
	if attempt > 300 { // 5åˆ†é’Ÿå
		interval = interval * 2 // è¿›ä¸€æ­¥å¢åŠ é—´éš”
	}

	// æœ€å¤§é—´éš”é™åˆ¶ä¸º30ç§’
	if interval > 30*time.Second {
		interval = 30 * time.Second
	}

	return interval
}

// å·²ç§»é™¤ï¼šisNetworkError å‡½æ•°å·²è¿ç§»åˆ° common.IsNetworkError
// ä½¿ç”¨ç»Ÿä¸€çš„ç½‘ç»œé”™è¯¯æ£€æµ‹æœºåˆ¶ï¼Œæé«˜å‡†ç¡®æ€§å’Œä¸€è‡´æ€§

// isRetryableError åˆ¤æ–­å“åº”é”™è¯¯ä»£ç æ˜¯å¦å¯é‡è¯•
// å¢å¼ºï¼šåŸºäº123ç½‘ç›˜APIæ–‡æ¡£çš„é”™è¯¯ä»£ç åˆ†ç±»
func (f *Fs) isRetryableError(code int) bool {
	retryableCodes := []int{
		20101, // æœåŠ¡å™¨ç¹å¿™
		20103, // ä¸´æ—¶é”™è¯¯
		20104, // ç³»ç»Ÿç»´æŠ¤
		20105, // æœåŠ¡æš‚ä¸å¯ç”¨
		500,   // å†…éƒ¨æœåŠ¡å™¨é”™è¯¯
		502,   // ç½‘å…³é”™è¯¯
		503,   // æœåŠ¡ä¸å¯ç”¨
		504,   // ç½‘å…³è¶…æ—¶
	}

	for _, retryableCode := range retryableCodes {
		if code == retryableCode {
			return true
		}
	}

	return false
}

// verifyChunkIntegrity ç®€åŒ–çš„åˆ†ç‰‡å®Œæ•´æ€§éªŒè¯
// ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€TaskIDç”Ÿæˆæœºåˆ¶ï¼Œæ”¯æŒTaskIDè¿ç§»
func (f *Fs) verifyChunkIntegrity(_ context.Context, preuploadID string, partNumber int64, remotePath string, fileSize int64) (bool, error) {
	// ç®€åŒ–éªŒè¯é€»è¾‘ï¼šä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€ç®¡ç†å™¨çš„çŠ¶æ€
	if f.resumeManager != nil {
		// ç®€åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€çš„TaskIDè·å–å’Œè¿ç§»å‡½æ•°
		taskID := f.getTaskIDWithMigration(preuploadID, remotePath, fileSize)

		isCompleted, err := f.resumeManager.IsChunkCompleted(taskID, partNumber-1) // partNumberä»1å¼€å§‹ï¼ŒchunkIndexä»0å¼€å§‹
		if err != nil {
			fs.Debugf(f, "æ£€æŸ¥åˆ†ç‰‡ %d çŠ¶æ€å¤±è´¥: %vï¼Œå‡è®¾éœ€è¦é‡æ–°ä¸Šä¼ ", partNumber, err)
			return false, nil // å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œæ ‡è®°ä¸ºéœ€è¦é‡æ–°ä¸Šä¼ 
		}

		if isCompleted {
			fs.Debugf(f, "åˆ†ç‰‡ %d åœ¨ç»Ÿä¸€ç®¡ç†å™¨ä¸­æ ‡è®°ä¸ºå·²å®Œæˆ", partNumber)
			return true, nil
		} else {
			fs.Debugf(f, "åˆ†ç‰‡ %d åœ¨ç»Ÿä¸€ç®¡ç†å™¨ä¸­æ ‡è®°ä¸ºæœªå®Œæˆ", partNumber)
			return false, nil
		}
	}

	// å›é€€æ–¹æ¡ˆï¼šå¦‚æœç»Ÿä¸€ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¿å®ˆç­–ç•¥
	fs.Debugf(f, "ç»Ÿä¸€ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œåˆ†ç‰‡ %d æ ‡è®°ä¸ºéœ€è¦é‡æ–°ä¸Šä¼ ", partNumber)
	return false, nil
}

// resumeUploadWithIntegrityCheck ç®€åŒ–çš„æ–­ç‚¹ç»­ä¼ å®Œæ•´æ€§æ£€æŸ¥
// ä¼˜åŒ–ï¼šå‡å°‘ä¸å¿…è¦çš„éªŒè¯ï¼Œæé«˜æ–­ç‚¹ç»­ä¼ å¯é æ€§
func (f *Fs) resumeUploadWithIntegrityCheck(ctx context.Context, progress *UploadProgress) error {
	uploadedCount := progress.GetUploadedCount()
	if uploadedCount == 0 {
		fs.Debugf(f, "æ²¡æœ‰å·²ä¸Šä¼ çš„åˆ†ç‰‡ï¼Œè·³è¿‡å®Œæ•´æ€§æ£€æŸ¥")
		return nil
	}

	fs.Debugf(f, "å¼€å§‹ç®€åŒ–çš„åˆ†ç‰‡å®Œæ•´æ€§æ£€æŸ¥ï¼Œå·²ä¸Šä¼ åˆ†ç‰‡æ•°: %d", uploadedCount)

	// æ™ºèƒ½éªŒè¯ç­–ç•¥ï¼šåªåœ¨ç‰¹å®šæƒ…å†µä¸‹è¿›è¡ŒéªŒè¯
	shouldVerify := false

	// æƒ…å†µ1ï¼šå¦‚æœç»Ÿä¸€ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œéœ€è¦éªŒè¯
	if f.resumeManager == nil {
		shouldVerify = true
		fs.Debugf(f, "ç»Ÿä¸€ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œå¯ç”¨å®Œæ•´æ€§éªŒè¯")
	} else {
		// æƒ…å†µ2ï¼šæ£€æŸ¥ç»Ÿä¸€ç®¡ç†å™¨å’ŒUploadProgressçš„çŠ¶æ€æ˜¯å¦ä¸€è‡´
		// ä¿®å¤ï¼šä½¿ç”¨å›é€€TaskIDæ–¹å¼ï¼ˆæš‚æ—¶ä¿æŒå…¼å®¹æ€§ï¼‰
		taskID := fmt.Sprintf("123_%s", progress.PreuploadID)
		fs.Debugf(f, "âš ï¸ å®Œæ•´æ€§éªŒè¯ä½¿ç”¨å›é€€TaskID: %s", taskID)
		managerCount, err := f.resumeManager.GetCompletedChunkCount(taskID)
		if err != nil {
			shouldVerify = true
			fs.Debugf(f, "æ— æ³•è·å–ç»Ÿä¸€ç®¡ç†å™¨ä¸­çš„åˆ†ç‰‡æ•°é‡ï¼Œå¯ç”¨å®Œæ•´æ€§éªŒè¯: %v", err)
		} else if managerCount != int64(uploadedCount) {
			shouldVerify = true
			fs.Debugf(f, "ç»Ÿä¸€ç®¡ç†å™¨åˆ†ç‰‡æ•°(%d)ä¸UploadProgressåˆ†ç‰‡æ•°(%d)ä¸ä¸€è‡´ï¼Œå¯ç”¨å®Œæ•´æ€§éªŒè¯",
				managerCount, uploadedCount)
		}
	}

	if !shouldVerify {
		fs.Debugf(f, "çŠ¶æ€ä¸€è‡´ï¼Œè·³è¿‡è¯¦ç»†çš„å®Œæ•´æ€§éªŒè¯")
		return nil
	}

	// æ‰§è¡Œç®€åŒ–çš„éªŒè¯ï¼šåªéªŒè¯çŠ¶æ€ä¸ä¸€è‡´çš„åˆ†ç‰‡
	fs.Debugf(f, "æ‰§è¡Œç®€åŒ–çš„å®Œæ•´æ€§éªŒè¯")
	corruptedChunks := make([]int64, 0)
	uploadedParts := progress.GetUploadedParts()

	for partNumber := range uploadedParts {
		if uploadedParts[partNumber] {
			// ä¿®å¤ï¼šä¼ é€’è¿œç¨‹è·¯å¾„å’Œæ–‡ä»¶å¤§å°å‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskID
			valid, err := f.verifyChunkIntegrity(ctx, progress.PreuploadID, partNumber, progress.FilePath, progress.FileSize)
			if err != nil {
				fs.Debugf(f, "éªŒè¯åˆ†ç‰‡ %d æ—¶å‡ºé”™: %vï¼Œæ ‡è®°ä¸ºéœ€è¦é‡æ–°ä¸Šä¼ ", partNumber, err)
				corruptedChunks = append(corruptedChunks, partNumber)
			} else if !valid {
				fs.Debugf(f, "åˆ†ç‰‡ %d éªŒè¯å¤±è´¥ï¼Œæ ‡è®°ä¸ºéœ€è¦é‡æ–°ä¸Šä¼ ", partNumber)
				corruptedChunks = append(corruptedChunks, partNumber)
			}
		}
	}

	// æ™ºèƒ½æ¢å¤ï¼šå¤„ç†éªŒè¯å¤±è´¥çš„åˆ†ç‰‡
	if len(corruptedChunks) > 0 {
		// å¦‚æœæŸååˆ†ç‰‡è¿‡å¤šï¼ˆè¶…è¿‡50%ï¼‰ï¼Œå¯èƒ½æ˜¯ç³»ç»Ÿæ€§é—®é¢˜ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥
		if len(corruptedChunks) > uploadedCount/2 {
			fs.Logf(f, "âš ï¸ å‘ç°å¤§é‡æŸååˆ†ç‰‡(%d/%d)ï¼Œå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§é—®é¢˜ï¼Œå»ºè®®é‡æ–°å¼€å§‹ä¸Šä¼ ",
				len(corruptedChunks), uploadedCount)
		} else {
			fs.Logf(f, "å‘ç° %d ä¸ªæŸååˆ†ç‰‡ï¼Œå°†é‡æ–°ä¸Šä¼ : %v", len(corruptedChunks), corruptedChunks)
		}

		// æ¸…é™¤æŸåçš„åˆ†ç‰‡æ ‡è®°
		for _, partNumber := range corruptedChunks {
			progress.RemoveUploaded(partNumber)
		}

		// ä¿å­˜æ›´æ–°åçš„è¿›åº¦
		err := f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜æ›´æ–°åçš„è¿›åº¦å¤±è´¥: %v", err)
		}
	} else {
		fs.Debugf(f, "æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡éªŒè¯é€šè¿‡")
	}

	return nil
}

// getParentID è·å–æ–‡ä»¶æˆ–ç›®å½•çš„çˆ¶ç›®å½•ID
func (f *Fs) getParentID(ctx context.Context, fileID int64) (int64, error) {
	fs.Debugf(f, "å°è¯•è·å–æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID", fileID)

	// ç›´æ¥é€šè¿‡APIæŸ¥è¯¢ï¼Œå› ä¸ºç¼“å­˜æœç´¢åŠŸèƒ½æœªå®ç°

	// å¦‚æœç¼“å­˜ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡APIæŸ¥è¯¢
	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…APIè·å–æ–‡ä»¶ä¿¡æ¯
	var response struct {
		Code int `json:"code"`
		Data struct {
			ParentFileID int64 `json:"parentFileId"`
		} `json:"data"`
		Message string `json:"message"`
	}

	url := fmt.Sprintf("/api/v1/file/info?fileId=%d", fileID)
	err := f.makeAPICallWithRest(ctx, url, "GET", nil, &response)
	if err != nil {
		return 0, fmt.Errorf("è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return 0, fmt.Errorf("APIè¿”å›é”™è¯¯: %s", response.Message)
	}

	fs.Debugf(f, "é€šè¿‡APIè·å–åˆ°æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID: %d", fileID, response.Data.ParentFileID)
	return response.Data.ParentFileID, nil
}

// fileExistsInDirectory æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExistsInDirectory(ctx context.Context, parentID int64, fileName string) (bool, int64, error) {
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• %d ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶: %s", parentID, fileName)

	response, err := f.ListFile(ctx, int(parentID), 100, "", "", 0)
	if err != nil {
		return false, 0, err
	}

	if response.Code != 0 {
		return false, 0, fmt.Errorf("ListFile APIé”™è¯¯: code=%d, message=%s", response.Code, response.Message)
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			fs.Debugf(f, "æ‰¾åˆ°æ–‡ä»¶ %sï¼ŒID: %d", fileName, file.FileID)
			return true, int64(file.FileID), nil
		}
	}

	fs.Debugf(f, "ç›®å½• %d ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: %s", parentID, fileName)
	return false, 0, nil
}

// normalizePath è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†123ç½‘ç›˜è·¯å¾„çš„ç‰¹æ®Šè¦æ±‚
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†è·¯å¾„å¤„ç†åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè¦æ±‚
// dircacheè¦æ±‚ï¼šè·¯å¾„ä¸åº”è¯¥ä»¥/å¼€å¤´æˆ–ç»“å°¾
func normalizePath(path string) string {
	if path == "" {
		return ""
	}

	// ä½¿ç”¨filepath.Cleanè¿›è¡ŒåŸºç¡€è·¯å¾„æ¸…ç†
	path = filepath.Clean(path)

	// å¤„ç†filepath.Cleançš„ç‰¹æ®Šè¿”å›å€¼
	if path == "." {
		return ""
	}

	// ç§»é™¤å¼€å¤´çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimPrefix(path, "/")
	path = strings.TrimPrefix(path, "\\") // å¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦

	// ç§»é™¤ç»“å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimSuffix(path, "/")
	path = strings.TrimSuffix(path, "\\")

	// å°†åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆæ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦ï¼‰
	path = strings.ReplaceAll(path, "\\", "/")

	// æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç»“æœä¸ä»¥/å¼€å¤´æˆ–ç»“å°¾
	path = strings.Trim(path, "/")

	return path
}

// isRemoteSource æ£€æŸ¥æºå¯¹è±¡æ˜¯å¦æ¥è‡ªè¿œç¨‹äº‘ç›˜ï¼ˆéæœ¬åœ°æ–‡ä»¶ï¼‰
func (f *Fs) isRemoteSource(src fs.ObjectInfo) bool {
	// æ£€æŸ¥æºå¯¹è±¡çš„ç±»å‹ï¼Œå¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œåˆ™è®¤ä¸ºæ˜¯è¿œç¨‹æº
	// è¿™å¯ä»¥é€šè¿‡æ£€æŸ¥æºå¯¹è±¡çš„Fs()æ–¹æ³•è¿”å›çš„ç±»å‹æ¥åˆ¤æ–­
	srcFs := src.Fs()
	if srcFs == nil {
		fs.Debugf(f, " isRemoteSource: srcFsä¸ºnilï¼Œè¿”å›false")
		return false
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	fsType := srcFs.Name()

	// ä¿®å¤ï¼šæ­£ç¡®è¯†åˆ«æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	// æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„åç§°å¯èƒ½æ˜¯ "local" æˆ– "local{suffix}"
	isLocal := fsType == "local" || strings.HasPrefix(fsType, "local{")
	isRemote := !isLocal && fsType != ""

	fs.Debugf(f, " isRemoteSourceæ£€æµ‹: fsType='%s', isLocal=%v, isRemote=%v", fsType, isLocal, isRemote)

	// å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œç›´æ¥è¿”å›false
	if isLocal {
		fs.Debugf(f, " æ˜ç¡®è¯†åˆ«ä¸ºæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ: %s", fsType)
		return false
	}

	// ç‰¹åˆ«æ£€æµ‹115ç½‘ç›˜å’Œå…¶ä»–äº‘ç›˜
	if strings.Contains(fsType, "115") || strings.Contains(fsType, "pan") {
		fs.Debugf(f, " æ˜ç¡®è¯†åˆ«ä¸ºäº‘ç›˜æº: %s", fsType)
		return true
	}

	return isRemote
}

// init å‡½æ•°ç”¨äºå‘Fsæ³¨å†Œ123ç½‘ç›˜åç«¯
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "123",
		Description: "123ç½‘ç›˜é©±åŠ¨å™¨",
		NewFs:       newFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name:     "client_id",
			Help:     "123ç½‘ç›˜APIå®¢æˆ·ç«¯IDã€‚",
			Required: true,
		}, {
			Name:      "client_secret",
			Help:      "123ç½‘ç›˜APIå®¢æˆ·ç«¯å¯†é’¥ã€‚",
			Required:  true,
			Sensitive: true,
		}, {
			Name:      "token",
			Help:      "OAuthè®¿é—®ä»¤ç‰Œï¼ˆJSONæ ¼å¼ï¼‰ã€‚é€šå¸¸ç•™ç©ºã€‚",
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Help:     "APIè¯·æ±‚çš„User-Agentå¤´ã€‚",
			Default:  defaultUserAgent,
			Advanced: true,
		}, {
			Name:     "root_folder_id",
			Help:     "æ ¹æ–‡ä»¶å¤¹çš„IDã€‚ç•™ç©ºè¡¨ç¤ºæ ¹ç›®å½•ã€‚",
			Default:  "0",
			Advanced: true,
		}, {
			Name:     "list_chunk",
			Help:     "æ¯ä¸ªåˆ—è¡¨è¯·æ±‚ä¸­è·å–çš„æ–‡ä»¶æ•°é‡ã€‚",
			Default:  100,
			Advanced: true,
		}, {
			Name:     "pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆuser/info, move, deleteï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  userInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "list_pacer_min_sleep",
			Help:     "æ–‡ä»¶åˆ—è¡¨APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  listV2APIMinSleep,
			Advanced: true,
		}, {
			Name:     "conn_timeout",
			Help:     "APIè¯·æ±‚çš„è¿æ¥è¶…æ—¶æ—¶é—´ã€‚",
			Default:  defaultConnTimeout,
			Advanced: true,
		}, {
			Name:     "timeout",
			Help:     "APIè¯·æ±‚çš„æ•´ä½“è¶…æ—¶æ—¶é—´ã€‚",
			Default:  defaultTimeout,
			Advanced: true,
		}, {
			Name:     "chunk_size",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ çš„å—å¤§å°ã€‚",
			Default:  defaultChunkSize,
			Advanced: true,
		}, {
			Name:     "upload_cutoff",
			Help:     "åˆ‡æ¢åˆ°å¤šéƒ¨åˆ†ä¸Šä¼ çš„é˜ˆå€¼ã€‚",
			Default:  defaultUploadCutoff,
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ ä¸­çš„æœ€å¤§åˆ†ç‰‡æ•°ã€‚",
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name:     "max_concurrent_uploads",
			Help:     "æœ€å¤§å¹¶å‘ä¸Šä¼ æ•°é‡ã€‚å‡å°‘æ­¤å€¼å¯é™ä½å†…å­˜ä½¿ç”¨å’Œæé«˜ç¨³å®šæ€§ã€‚",
			Default:  8, // åŸºäº48ç§’/åˆ†ç‰‡é—®é¢˜ä¼˜åŒ–ï¼šä»4æ”¹ä¸º8ï¼Œè¿›ä¸€æ­¥æå‡ä¸Šä¼ æ€§èƒ½
			Advanced: true,
		}, {
			Name:     "max_concurrent_downloads",
			Help:     "æœ€å¤§å¹¶å‘ä¸‹è½½æ•°é‡ã€‚æé«˜æ­¤å€¼å¯æå‡ä¸‹è½½é€Ÿåº¦ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ã€‚",
			Default:  4, // ä»1æ”¹ä¸º4ï¼Œæå‡ä¸‹è½½æ€§èƒ½
			Advanced: true,
		}, {
			Name:     "progress_update_interval",
			Help:     "è¿›åº¦æ›´æ–°é—´éš”ã€‚",
			Default:  fs.Duration(5 * time.Second),
			Advanced: true,
		}, {
			Name:     "enable_progress_display",
			Help:     "æ˜¯å¦å¯ç”¨è¿›åº¦æ˜¾ç¤ºã€‚",
			Default:  true,
			Advanced: true,
		}, {
			Name:     "upload_pacer_min_sleep",
			Help:     "ä¸Šä¼ APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚è®¾ç½®æ›´é«˜çš„å€¼å¯ä»¥é¿å…429é”™è¯¯ã€‚",
			Default:  uploadV2SliceMinSleep,
			Advanced: true,
		}, {
			Name:     "download_pacer_min_sleep",
			Help:     "ä¸‹è½½APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  downloadInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "strict_pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆmove, deleteç­‰ï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  fileMoveMinSleep,
			Advanced: true,
		}, {
			Name:     config.ConfigEncoding,
			Help:     config.ConfigEncodingHelp,
			Advanced: true,
			// é»˜è®¤åŒ…å«Slashå’ŒInvalidUtf8ç¼–ç ï¼Œé€‚é…ä¸­æ–‡æ–‡ä»¶åå’Œç‰¹æ®Šå­—ç¬¦
			Default: (encoder.EncodeCtl |
				encoder.EncodeLeftSpace |
				encoder.EncodeRightSpace |
				encoder.EncodeSlash | // æ–°å¢ï¼šé»˜è®¤ç¼–ç æ–œæ 
				encoder.EncodeInvalidUtf8), // æ–°å¢ï¼šç¼–ç æ— æ•ˆUTF-8å­—ç¬¦
		}, {
			Name:     "cache_max_size",
			Help:     "ç¼“å­˜æ¸…ç†è§¦å‘å‰çš„æœ€å¤§ç¼“å­˜å¤§å°ã€‚è®¾ç½®ä¸º0ç¦ç”¨åŸºäºå¤§å°çš„æ¸…ç†ã€‚",
			Default:  fs.SizeSuffix(100 << 20), // 100MB
			Advanced: true,
		}, {
			Name:     "cache_target_size",
			Help:     "æ¸…ç†åçš„ç›®æ ‡ç¼“å­˜å¤§å°ã€‚åº”å°äºcache_max_sizeã€‚",
			Default:  fs.SizeSuffix(64 << 20), // 64MB
			Advanced: true,
		}, {
			Name:     "enable_smart_cleanup",
			Help:     "å¯ç”¨åŸºäºLRUç­–ç•¥çš„æ™ºèƒ½ç¼“å­˜æ¸…ç†ï¼Œè€Œä¸æ˜¯ç®€å•çš„åŸºäºå¤§å°çš„æ¸…ç†ã€‚",
			Default:  false,
			Advanced: true,
		}, {
			Name:     "cleanup_strategy",
			Help:     "ç¼“å­˜æ¸…ç†ç­–ç•¥ï¼š'size'ï¼ˆåŸºäºå¤§å°ï¼‰ã€'lru'ï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ã€'priority_lru'ï¼ˆä¼˜å…ˆçº§+LRUï¼‰ã€'time'ï¼ˆåŸºäºæ—¶é—´ï¼‰ã€‚",
			Default:  "size",
			Advanced: true,
		}},
	})
}

var commandHelp = []fs.CommandHelp{{
	Name:  "getdownloadurlua",
	Short: "é€šè¿‡æ–‡ä»¶è·¯å¾„è·å–ä¸‹è½½URL",
	Long: `æ­¤å‘½ä»¤ä½¿ç”¨æ–‡ä»¶è·¯å¾„æ£€ç´¢æ–‡ä»¶çš„ä¸‹è½½URLã€‚
ç”¨æ³•:
rclone backend getdownloadurlua 123:path/to/file VidHub/1.7.24
è¯¥å‘½ä»¤è¿”å›æŒ‡å®šæ–‡ä»¶çš„ä¸‹è½½URLã€‚è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ã€‚`,
}, {
	Name:  "cache-cleanup",
	Short: "æ‰‹åŠ¨è§¦å‘ç¼“å­˜æ¸…ç†",
	Long: `æ‰‹åŠ¨è§¦å‘123ç½‘ç›˜ç¼“å­˜æ¸…ç†æ“ä½œã€‚
ç”¨æ³•:
rclone backend cache-cleanup 123: --strategy=lru
æ”¯æŒçš„æ¸…ç†ç­–ç•¥: size, lru, priority_lru, time, clear
è¯¥å‘½ä»¤è¿”å›æ¸…ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯ã€‚`,
}, {
	Name:  "cache-stats",
	Short: "æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯",
	Long: `è·å–123ç½‘ç›˜ç¼“å­˜çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
ç”¨æ³•:
rclone backend cache-stats 123:
è¯¥å‘½ä»¤è¿”å›æ‰€æœ‰ç¼“å­˜å®ä¾‹çš„ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…æ‹¬å‘½ä¸­ç‡ã€å¤§å°ç­‰ã€‚`,
}, {
	Name:  "cache-config",
	Short: "æŸ¥çœ‹å½“å‰ç¼“å­˜é…ç½®",
	Long: `æŸ¥çœ‹123ç½‘ç›˜å½“å‰çš„ç¼“å­˜é…ç½®å‚æ•°ã€‚
ç”¨æ³•:
rclone backend cache-config 123:
è¯¥å‘½ä»¤è¿”å›å½“å‰çš„ç¼“å­˜é…ç½®å’Œç”¨æˆ·é…ç½®ã€‚`,
}, {
	Name:  "cache-reset",
	Short: "é‡ç½®ç¼“å­˜é…ç½®ä¸ºé»˜è®¤å€¼",
	Long: `å°†123ç½‘ç›˜ç¼“å­˜é…ç½®é‡ç½®ä¸ºé»˜è®¤å€¼ã€‚
ç”¨æ³•:
rclone backend cache-reset 123:
è¯¥å‘½ä»¤ä¼šé‡ç½®æ‰€æœ‰ç¼“å­˜é…ç½®å‚æ•°ã€‚`,
}, {
	Name:  "media-sync",
	Short: "åŒæ­¥åª’ä½“åº“å¹¶åˆ›å»ºä¼˜åŒ–çš„.strmæ–‡ä»¶",
	Long: `å°†123ç½‘ç›˜ä¸­çš„è§†é¢‘æ–‡ä»¶åŒæ­¥åˆ°æœ¬åœ°ç›®å½•ï¼Œåˆ›å»ºå¯¹åº”çš„.strmæ–‡ä»¶ã€‚
.strmæ–‡ä»¶å°†åŒ…å«ä¼˜åŒ–çš„fileIdæ ¼å¼ï¼Œæ”¯æŒç›´æ¥æ’­æ”¾å’Œåª’ä½“åº“ç®¡ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend media-sync 123:Movies /local/media/movies
rclone backend media-sync 123:Videos /local/media/videos -o min-size=200M -o strm-format=true

æ”¯æŒçš„è§†é¢‘æ ¼å¼: mp4, mkv, avi, mov, wmv, flv, webm, m4v, 3gp, ts, m2ts
.strmæ–‡ä»¶å†…å®¹æ ¼å¼: 123://fileId (å¯é€šè¿‡strm-formaté€‰é¡¹è°ƒæ•´)`,
	Opts: map[string]string{
		"min-size":    "æœ€å°æ–‡ä»¶å¤§å°è¿‡æ»¤ï¼Œå°äºæ­¤å¤§å°çš„æ–‡ä»¶å°†è¢«å¿½ç•¥ (é»˜è®¤: 100M)",
		"strm-format": ".strmæ–‡ä»¶å†…å®¹æ ¼å¼: true(ä¼˜åŒ–æ ¼å¼)/false(è·¯å¾„æ ¼å¼) (é»˜è®¤: trueï¼Œå…¼å®¹: fileid/path)",
		"include":     "åŒ…å«çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš” (é»˜è®¤: mp4,mkv,avi,mov,wmv,flv,webm,m4v,3gp,ts,m2ts)",
		"exclude":     "æ’é™¤çš„æ–‡ä»¶æ‰©å±•åï¼Œé€—å·åˆ†éš”",
		"update-mode": "æ›´æ–°æ¨¡å¼: full/incremental (é»˜è®¤: full)",
		"dry-run":     "é¢„è§ˆæ¨¡å¼ï¼Œæ˜¾ç¤ºå°†è¦åˆ›å»ºçš„æ–‡ä»¶ä½†ä¸å®é™…åˆ›å»º (true/false)",
		"target-path": "ç›®æ ‡è·¯å¾„ï¼Œå¦‚æœä¸åœ¨å‚æ•°ä¸­æŒ‡å®šåˆ™å¿…é¡»é€šè¿‡æ­¤é€‰é¡¹æä¾›",
	},
}, {
	Name:  "get-download-url",
	Short: "é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL",
	Long: `é€šè¿‡123ç½‘ç›˜çš„fileIdæˆ–.strmæ–‡ä»¶å†…å®¹è·å–å®é™…çš„ä¸‹è½½URLã€‚
æ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼Œç‰¹åˆ«é€‚ç”¨äºåª’ä½“æœåŠ¡å™¨å’Œ.strmæ–‡ä»¶å¤„ç†ã€‚

ç”¨æ³•ç¤ºä¾‹:
rclone backend get-download-url 123: "123://17995550"
rclone backend get-download-url 123: "17995550"
rclone backend get-download-url 123: "/path/to/file.mp4"
rclone backend get-download-url 123: "123://17995550" -o user-agent="Custom-UA"

è¾“å…¥æ ¼å¼æ”¯æŒ:
- 123://fileId æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
- çº¯fileId
- æ–‡ä»¶è·¯å¾„ (è‡ªåŠ¨è§£æä¸ºfileId)`,
	Opts: map[string]string{
		"user-agent": "è‡ªå®šä¹‰User-Agentå­—ç¬¦ä¸²ï¼ˆå¯é€‰ï¼‰",
	},
},
}

// Command æ‰§è¡Œåç«¯ç‰¹å®šçš„å‘½ä»¤ã€‚
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {

	case "getdownloadurlua":
		// ğŸ”§ ä¿®å¤ï¼šæ”¯æŒä¸¤ç§æ ¼å¼
		// æ ¼å¼1: rclone backend getdownloadurlua 123: "/path" "UA" (ä¸¤ä¸ªå‚æ•°)
		// æ ¼å¼2: rclone backend getdownloadurlua "123:/path" "UA" (ä¸€ä¸ªå‚æ•°ï¼Œè·¯å¾„åœ¨f.rootä¸­)

		var path, ua string

		if len(arg) >= 2 {
			// æ ¼å¼1ï¼šä¸¤ä¸ªå‚æ•°
			path = arg[0]
			ua = arg[1]
		} else if len(arg) >= 1 {
			// æ ¼å¼2ï¼šä¸€ä¸ªå‚æ•°ï¼Œéœ€è¦é‡æ„å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
			ua = arg[0]

			// åœ¨æ–‡ä»¶æ¨¡å¼ä¸‹ï¼Œéœ€è¦ç»„åˆçˆ¶ç›®å½•è·¯å¾„å’Œæ–‡ä»¶å
			// 123åç«¯åœ¨handleRootDirectoryä¸­ä¼šåˆ†å‰²è·¯å¾„ï¼Œf.rootæ˜¯çˆ¶ç›®å½•
			// éœ€è¦ä»åŸå§‹è·¯å¾„é‡æ„å®Œæ•´è·¯å¾„
			if f.root == "" {
				path = "/"
			} else {
				path = "/" + strings.Trim(f.root, "/")
			}
			fs.Debugf(f, "123åç«¯æ–‡ä»¶æ¨¡å¼ï¼šä½¿ç”¨è·¯å¾„: %s", path)
		} else {
			return nil, fmt.Errorf("éœ€è¦æä¾›User-Agentå‚æ•°")
		}

		return f.getDownloadURLByUA(ctx, path, ua)

	case "cache-info":
		// ä½¿ç”¨ç»Ÿä¸€ç¼“å­˜æŸ¥çœ‹å™¨
		caches := map[string]cache.PersistentCache{
			"parent_ids": f.parentIDCache,
			"dir_list":   f.dirListCache,
			"path_to_id": f.pathToIDCache,
		}
		viewer := cache.NewUnifiedCacheViewer("123", f, caches)

		// æ ¹æ®å‚æ•°å†³å®šè¿”å›æ ¼å¼
		format := "tree"
		if formatOpt, ok := opt["format"]; ok {
			format = formatOpt
		}

		switch format {
		case "tree":
			return viewer.GenerateDirectoryTreeText()
		case "stats":
			return viewer.GetCacheStats(), nil
		case "info":
			return viewer.GetCacheInfo()
		default:
			return viewer.GenerateDirectoryTreeText()
		}

	case "cache-cleanup":
		// ğŸ”§ æ–°å¢ï¼šæ‰‹åŠ¨è§¦å‘ç¼“å­˜æ¸…ç†
		strategy := "size"
		if strategyOpt, ok := opt["strategy"]; ok {
			strategy = strategyOpt
		}
		return f.manualCacheCleanup(ctx, strategy)

	case "cache-stats":
		// ğŸ”§ æ–°å¢ï¼šæŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
		return f.getCacheStatistics(ctx)

	case "cache-config":
		// ğŸ”§ æ–°å¢ï¼šæŸ¥çœ‹å½“å‰ç¼“å­˜é…ç½®
		return f.getCacheConfiguration(ctx)

	case "cache-reset":
		// ğŸ”§ æ–°å¢ï¼šé‡ç½®ç¼“å­˜é…ç½®ä¸ºé»˜è®¤å€¼
		return f.resetCacheConfiguration(ctx)

	case "media-sync":
		// ğŸ¬ æ–°å¢ï¼šåª’ä½“åº“åŒæ­¥åŠŸèƒ½
		return f.mediaSyncCommand(ctx, arg, opt)

	case "get-download-url":
		// ğŸ”— æ–°å¢ï¼šé€šè¿‡fileIdè·å–ä¸‹è½½URL
		return f.getDownloadURLCommand(ctx, arg, opt)

	default:
		return nil, fs.ErrorCommandNotFound
	}
}

type ListRequest struct {
	ParentFileID int    `json:"parentFileId"`
	Limit        int    `json:"limit"`
	SearchData   string `json:"searchData,omitempty"`
	SearchMode   string `json:"searchMode,omitempty"`
	LastFileID   int    `json:"lastFileID,omitempty"`
}

type ListResponse struct {
	Code    int                   `json:"code"`
	Message string                `json:"message"`
	Data    GetFileListRespDataV2 `json:"data"` // æ›´æ”¹ä¸ºç‰¹å®šç±»å‹
}

// GetFileListRespDataV2 è¡¨ç¤ºæ–‡ä»¶åˆ—è¡¨å“åº”çš„æ•°æ®ç»“æ„
type GetFileListRespDataV2 struct {
	// -1ä»£è¡¨æœ€åä¸€é¡µï¼ˆæ— éœ€å†ç¿»é¡µæŸ¥è¯¢ï¼‰, å…¶ä»–ä»£è¡¨ä¸‹ä¸€é¡µå¼€å§‹çš„æ–‡ä»¶idï¼Œæºå¸¦åˆ°è¯·æ±‚å‚æ•°ä¸­
	LastFileId int64 `json:"lastFileId"`
	// æ–‡ä»¶åˆ—è¡¨
	FileList []FileListInfoRespDataV2 `json:"fileList"`
}

// FileListInfoRespDataV2 è¡¨ç¤ºæ¥è‡ªAPIå“åº”çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹é¡¹ç›®
type FileListInfoRespDataV2 struct {
	// æ–‡ä»¶ID
	FileID int64 `json:"fileID"`
	// æ–‡ä»¶å
	Filename string `json:"filename"`
	// 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	Type int `json:"type"`
	// æ–‡ä»¶å¤§å°
	Size int64 `json:"size"`
	// md5
	Etag string `json:"etag"`
	// æ–‡ä»¶å®¡æ ¸çŠ¶æ€, å¤§äº 100 ä¸ºå®¡æ ¸é©³å›æ–‡ä»¶
	Status int `json:"status"`
	// ç›®å½•ID
	ParentFileID int64 `json:"parentFileID"`
	// æ–‡ä»¶åˆ†ç±», 0-æœªçŸ¥ 1-éŸ³é¢‘ 2-è§†é¢‘ 3-å›¾ç‰‡
	Category int `json:"category"`
}

func (f *Fs) ListFile(ctx context.Context, parentFileID, limit int, searchData, searchMode string, lastFileID int) (*ListResponse, error) {
	fs.Debugf(f, "è°ƒç”¨ListFileï¼Œå‚æ•°ï¼šparentFileID=%d, limit=%d, lastFileID=%d", parentFileID, limit, lastFileID)

	// åªæœ‰åœ¨æ²¡æœ‰æœç´¢æ¡ä»¶æ—¶æ‰ä½¿ç”¨ç¼“å­˜
	if searchData == "" && searchMode == "" {
		// å°è¯•ä»ç¼“å­˜è·å–
		if cached, found := f.getDirListFromCache(int64(parentFileID), int64(lastFileID)); found {
			// æ„é€ ç¼“å­˜å“åº”
			result := &ListResponse{
				Code:    0,
				Message: "success",
				Data: GetFileListRespDataV2{
					LastFileId: cached.LastFileID,
					FileList:   cached.FileList,
				},
			}
			fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å‘½ä¸­: parentFileID=%d, fileCount=%d", parentFileID, len(cached.FileList))
			return result, nil
		}
	}

	// æ„é€ æŸ¥è¯¢å‚æ•°
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", limit))

	if searchData != "" {
		params.Add("searchData", searchData)
	}
	if searchMode != "" {
		params.Add("searchMode", searchMode)
	}
	if lastFileID != 0 {
		params.Add("lastFileID", fmt.Sprintf("%d", lastFileID))
	}

	// æ„é€ å¸¦æŸ¥è¯¢å‚æ•°çš„ç«¯ç‚¹
	endpoint := "/api/v2/file/list?" + params.Encode()
	fs.Debugf(f, "APIç«¯ç‚¹: %s%s", openAPIRootURL, endpoint)

	// ç›´æ¥ä½¿ç”¨v2 APIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var result ListResponse
	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &result)
	if err != nil {
		fs.Debugf(f, "ListFile APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "ListFile APIå“åº”: code=%d, message=%s, fileCount=%d", result.Code, result.Message, len(result.Data.FileList))

	// åªæœ‰åœ¨æ²¡æœ‰æœç´¢æ¡ä»¶ä¸”APIè°ƒç”¨æˆåŠŸæ—¶æ‰ç¼“å­˜ç»“æœ
	if searchData == "" && searchMode == "" && result.Code == 0 {
		f.saveDirListToCache(int64(parentFileID), int64(lastFileID), result.Data.FileList, result.Data.LastFileId)
	}

	return &result, nil
}

func (f *Fs) pathToFileID(ctx context.Context, filePath string) (string, error) {
	fs.Debugf(f, " pathToFileIDå¼€å§‹: filePath=%s", filePath)

	// æ ¹ç›®å½•
	if filePath == "/" {
		return "0", nil
	}
	if len(filePath) > 1 && strings.HasSuffix(filePath, "/") {
		filePath = filePath[:len(filePath)-1]
	}

	// å°è¯•ä»ç¼“å­˜è·å–è·¯å¾„æ˜ å°„
	if cachedFileID, _, found := f.getPathToIDFromCache(filePath); found {
		fs.Debugf(f, "è·¯å¾„æ˜ å°„ç¼“å­˜å‘½ä¸­: %s -> %s", filePath, cachedFileID)
		return cachedFileID, nil
	}
	currentID := "0"
	parts := strings.Split(filePath, "/")
	if len(parts) > 0 && parts[0] == "" { // å¦‚æœè·¯å¾„ä»¥/å¼€å¤´ï¼Œç§»é™¤ç©ºå­—ç¬¦ä¸²
		parts = parts[1:]
	}
	for _, part := range parts {
		if part == "" {
			continue
		}
		findPart := false
		next := "0"           // æ ¹æ®APIä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºnext
		maxIterations := 1000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š1000æ¬¡è¿­ä»£
		iteration := 0
		for {
			iteration++
			if iteration > maxIterations {
				return "", fmt.Errorf("æŸ¥æ‰¾è·¯å¾„ %s è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", part, maxIterations)
			}
			parentFileID, err := strconv.Atoi(currentID)
			if err != nil {
				return "", fmt.Errorf("invalid parentFileId: %s", currentID)
			}

			lastFileIDInt, err := strconv.Atoi(next)
			if err != nil {
				return "", fmt.Errorf("invalid next token: %s", next)
			}

			var response *ListResponse
			// ä½¿ç”¨åˆ—è¡¨è°ƒé€Ÿå™¨è¿›è¡Œé€Ÿç‡é™åˆ¶
			err = f.listPacer.Call(func() (bool, error) {
				response, err = f.ListFile(ctx, parentFileID, 100, "", "", lastFileIDInt)
				if err != nil {
					return shouldRetry(ctx, nil, err)
				}
				return false, nil
			})
			if err != nil {
				return "", fmt.Errorf("list file failed: %w", err)
			}
			if response.Code != 200 && response.Code != 0 {
				return "", fmt.Errorf("API returned error code %d: %s", response.Code, response.Message)
			}
			infoList := response.Data.FileList
			if len(infoList) == 0 {
				break
			}
			for _, item := range infoList {
				if item.Filename == part {
					currentID = strconv.FormatInt(item.FileID, 10)
					findPart = true

					// è®°å½•æ‰¾åˆ°çš„é¡¹ç›®ç±»å‹ä¿¡æ¯ï¼Œç”¨äºåç»­ç¼“å­˜
					isDir := (item.Type == 1) // Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
					fs.Debugf(f, "pathToFileIDæ‰¾åˆ°é¡¹ç›®: %s -> ID=%s, Type=%d, isDir=%v", part, currentID, item.Type, isDir)

					// å¦‚æœè¿™æ˜¯è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œç«‹å³ç¼“å­˜ç±»å‹ä¿¡æ¯
					if len(parts) > 0 && part == parts[len(parts)-1] {
						currentPath := "/" + strings.Join(parts, "/")
						f.savePathToIDToCache(currentPath, currentID, "0", isDir)
						fs.Debugf(f, "ç«‹å³ç¼“å­˜è·¯å¾„ç±»å‹: %s -> ID=%s, isDir=%v", currentPath, currentID, isDir)
					}

					break
				}
			}
			if findPart {
				break
			}

			nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
			if nextRaw == "-1" {
				break
			} else {
				next = nextRaw
				// æ— éœ€æ‰‹åŠ¨ç¡çœ  - è°ƒé€Ÿå™¨å¤„ç†é€Ÿç‡é™åˆ¶
			}
		}
		if !findPart {
			return "", fs.ErrorObjectNotFound
		}
	}

	if currentID == "0" && filePath != "/" {
		return "", fs.ErrorObjectNotFound
	}

	// ç¼“å­˜è·¯å¾„æ˜ å°„ç»“æœå·²åœ¨è·¯å¾„è§£æå¾ªç¯ä¸­æ­£ç¡®å¤„ç†

	fs.Debugf(f, " pathToFileIDç»“æœ: filePath=%s -> currentID=%s", filePath, currentID)
	return currentID, nil
}

type FileDetail struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

type FileInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		List []FileDetail `json:"list"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

type FileDetailResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID       int64  `json:"fileID"`
		Filename     string `json:"filename"`
		Type         int    `json:"type"`
		Size         int64  `json:"size"`
		ETag         string `json:"etag"`
		Status       int    `json:"status"`
		ParentFileID int64  `json:"parentFileID"`
		CreateAt     string `json:"createAt"`
		Trashed      int    `json:"trashed"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

// FileInfo è¡¨ç¤º'list'æ•°ç»„ä¸­å•ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
type FileInfo struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

// FileInfosResponse æ˜¯APIå“åº”çš„é¡¶çº§ç»“æ„

// å®šä¹‰Dataç»“æ„ä½“ï¼Œæ˜ å°„dataå­—æ®µçš„å†…å®¹
type Data struct {
	FileList []FileInfo `json:"fileList"`
}

// å®šä¹‰FileInfosResponseç»“æ„ä½“ï¼Œæ˜ å°„æ•´ä¸ªå“åº”JSON
type FileInfosResponse struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     Data   `json:"data"`
	XTraceID string `json:"x-traceID"`
}

// å®šä¹‰FileInfoRequestç»“æ„ä½“ï¼Œç”¨äºå‘é€è¯·æ±‚çš„payload
type FileInfoRequest struct {
	FileIDs []int64 `json:"fileIDs"`
}

type DownloadInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		DownloadURL string `json:"downloadUrl"` // ç›´é“¾åœ°å€
		ExpireTime  string `json:"expireTime"`  // è¿‡æœŸæ—¶é—´
	} `json:"data"`
}

type UploadCreateResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID      int64  `json:"fileID"`
		PreuploadID string `json:"preuploadID"`
		Reuse       bool   `json:"reuse"`
		SliceSize   int64  `json:"sliceSize"`
	} `json:"data"`
}

// UploadProgress è·Ÿè¸ªå¤šéƒ¨åˆ†ä¸Šä¼ çš„è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
type UploadProgress struct {
	PreuploadID   string         `json:"preuploadID"`
	TotalParts    int64          `json:"totalParts"`
	ChunkSize     int64          `json:"chunkSize"`
	FileSize      int64          `json:"fileSize"`
	UploadedParts map[int64]bool `json:"uploadedParts"` // partNumber -> uploaded
	FilePath      string         `json:"filePath"`      // local file path for resume
	MD5Hash       string         `json:"md5Hash"`       // file MD5 for verification
	CreatedAt     time.Time      `json:"createdAt"`
	mu            sync.RWMutex   `json:"-"` // ä¿æŠ¤UploadedPartsçš„å¹¶å‘è®¿é—®
}

// SetUploaded çº¿ç¨‹å®‰å…¨åœ°æ ‡è®°åˆ†ç‰‡å·²ä¸Šä¼ 
func (up *UploadProgress) SetUploaded(partNumber int64) {
	up.mu.Lock()
	defer up.mu.Unlock()
	up.UploadedParts[partNumber] = true
}

// IsUploaded çº¿ç¨‹å®‰å…¨åœ°æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼ 
func (up *UploadProgress) IsUploaded(partNumber int64) bool {
	up.mu.RLock()
	defer up.mu.RUnlock()
	return up.UploadedParts[partNumber]
}

// GetUploadedCount çº¿ç¨‹å®‰å…¨åœ°è·å–å·²ä¸Šä¼ åˆ†ç‰‡æ•°é‡
func (up *UploadProgress) GetUploadedCount() int {
	up.mu.RLock()
	defer up.mu.RUnlock()
	return len(up.UploadedParts)
}

// RemoveUploaded çº¿ç¨‹å®‰å…¨åœ°ç§»é™¤å·²ä¸Šä¼ æ ‡è®°ï¼ˆç”¨äºé‡æ–°ä¸Šä¼ æŸåçš„åˆ†ç‰‡ï¼‰
func (up *UploadProgress) RemoveUploaded(partNumber int64) {
	up.mu.Lock()
	defer up.mu.Unlock()
	delete(up.UploadedParts, partNumber)
}

// GetUploadedParts çº¿ç¨‹å®‰å…¨åœ°è·å–å·²ä¸Šä¼ åˆ†ç‰‡åˆ—è¡¨çš„å‰¯æœ¬
func (up *UploadProgress) GetUploadedParts() map[int64]bool {
	up.mu.RLock()
	defer up.mu.RUnlock()

	// è¿”å›å‰¯æœ¬ä»¥é¿å…å¤–éƒ¨ä¿®æ”¹
	result := make(map[int64]bool)
	maps.Copy(result, up.UploadedParts)
	return result
}

type UserInfoResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		UID            int64  `json:"uid"`
		Username       string `json:"username"`
		DisplayName    string `json:"displayName"`
		HeadImage      string `json:"headImage"`
		Passport       string `json:"passport"`
		Mail           string `json:"mail"`
		SpaceUsed      int64  `json:"spaceUsed"`
		SpacePermanent int64  `json:"spacePermanent"`
		SpaceTemp      int64  `json:"spaceTemp"`
		SpaceTempExpr  int64  `json:"spaceTempExpr"` // ä¿®å¤ï¼šAPIè¿”å›æ•°å­—è€Œéå­—ç¬¦ä¸²
		Vip            bool   `json:"vip"`
		DirectTraffic  int64  `json:"directTraffic"`
		IsHideUID      bool   `json:"isHideUID"`
	} `json:"data"`
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, userAgent string) (string, error) {
	// Ensure token is valid before making API calls
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return "", err
	}

	// è®°å½•ä½¿ç”¨çš„ User-Agent
	if userAgent != "" {
		fs.Debugf(f, "ğŸŒ 123ç½‘ç›˜ä½¿ç”¨è‡ªå®šä¹‰User-Agent: %s", userAgent)
	} else {
		fs.Debugf(f, "ğŸŒ 123ç½‘ç›˜ä½¿ç”¨é»˜è®¤User-Agent")
	}

	if filePath == "" {
		filePath = f.root
	}

	fileID, err := f.pathToFileID(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to get file ID for path %q: %w", filePath, err)
	}

	// æ³¨æ„ï¼š123ç½‘ç›˜å½“å‰çš„getDownloadURLæ–¹æ³•ä¸ç›´æ¥æ”¯æŒè‡ªå®šä¹‰UA
	// ä½†æˆ‘ä»¬è®°å½•äº†UAå‚æ•°ï¼Œä¸ºå°†æ¥çš„å®ç°åšå‡†å¤‡
	fs.Debugf(f, "ğŸ”„ 123ç½‘ç›˜é€šè¿‡è·¯å¾„è·å–ä¸‹è½½URL: è·¯å¾„=%s, fileId=%s", filePath, fileID)

	// Use the standard getDownloadURL method
	return f.getDownloadURL(ctx, fileID)
}

// getDownloadURLCommand é€šè¿‡fileIdæˆ–.strmå†…å®¹è·å–ä¸‹è½½URL
func (f *Fs) getDownloadURLCommand(ctx context.Context, args []string, opt map[string]string) (interface{}, error) {
	if len(args) < 1 {
		return nil, fmt.Errorf("éœ€è¦æä¾›fileIdã€123://fileIdæ ¼å¼æˆ–æ–‡ä»¶è·¯å¾„")
	}

	input := args[0]
	fs.Debugf(f, "ğŸ”— å¤„ç†ä¸‹è½½URLè¯·æ±‚: %s", input)

	var fileID string
	var err error

	// è§£æè¾“å…¥æ ¼å¼
	if strings.HasPrefix(input, "123://") {
		// 123://fileId æ ¼å¼ (æ¥è‡ª.strmæ–‡ä»¶)
		fileID = strings.TrimPrefix(input, "123://")
		fs.Debugf(f, "âœ… è§£æ.strmæ ¼å¼: fileId=%s", fileID)
	} else if strings.HasPrefix(input, "/") {
		// æ–‡ä»¶è·¯å¾„æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ä¸ºfileId
		fs.Debugf(f, "ğŸ” è§£ææ–‡ä»¶è·¯å¾„: %s", input)
		fileID, err = f.pathToFileID(ctx, input)
		if err != nil {
			return nil, fmt.Errorf("è·¯å¾„è½¬æ¢fileIdå¤±è´¥ %q: %w", input, err)
		}
		fs.Debugf(f, "âœ… è·¯å¾„è½¬æ¢æˆåŠŸ: %s -> %s", input, fileID)
	} else {
		// å‡è®¾æ˜¯çº¯fileId
		fileID = input
		fs.Debugf(f, "âœ… ä½¿ç”¨çº¯fileId: %s", fileID)
	}

	// éªŒè¯fileIdæ ¼å¼
	if fileID == "" {
		return nil, fmt.Errorf("æ— æ•ˆçš„fileId: %s", input)
	}

	// è·å–ä¸‹è½½URL
	userAgent := opt["user-agent"]
	if userAgent != "" {
		// å¦‚æœæä¾›äº†è‡ªå®šä¹‰UAï¼Œä½¿ç”¨getDownloadURLByUAæ–¹æ³•
		fs.Debugf(f, "ğŸŒ ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URL: fileId=%s, UA=%s", fileID, userAgent)

		// éœ€è¦å…ˆå°†fileIDè½¬æ¢å›è·¯å¾„ï¼Œå› ä¸ºgetDownloadURLByUAéœ€è¦è·¯å¾„å‚æ•°
		var filePath string
		if strings.HasPrefix(input, "/") {
			filePath = input // å¦‚æœåŸå§‹è¾“å…¥æ˜¯è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨
		} else {
			// å¦‚æœåŸå§‹è¾“å…¥æ˜¯fileIDï¼Œæˆ‘ä»¬æ— æ³•è½»æ˜“è½¬æ¢å›è·¯å¾„ï¼Œæ‰€ä»¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•
			fs.Debugf(f, "âš ï¸ è‡ªå®šä¹‰UAä»…æ”¯æŒè·¯å¾„è¾“å…¥ï¼ŒfileIDè¾“å…¥å°†ä½¿ç”¨æ ‡å‡†æ–¹æ³•")
			downloadURL, err := f.getDownloadURL(ctx, fileID)
			if err != nil {
				return nil, fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
			}
			fs.Infof(f, "âœ… æˆåŠŸè·å–123ç½‘ç›˜ä¸‹è½½URL: fileId=%s (æ ‡å‡†æ–¹æ³•)", fileID)
			return downloadURL, nil
		}

		downloadURL, err := f.getDownloadURLByUA(ctx, filePath, userAgent)
		if err != nil {
			return nil, fmt.Errorf("ä½¿ç”¨è‡ªå®šä¹‰UAè·å–ä¸‹è½½URLå¤±è´¥: %w", err)
		}
		fs.Infof(f, "âœ… æˆåŠŸè·å–123ç½‘ç›˜ä¸‹è½½URL: fileId=%s (è‡ªå®šä¹‰UA)", fileID)
		return downloadURL, nil
	} else {
		// ä½¿ç”¨æ ‡å‡†æ–¹æ³•
		fs.Debugf(f, "ğŸŒ è·å–ä¸‹è½½URL: fileId=%s", fileID)
		downloadURL, err := f.getDownloadURL(ctx, fileID)
		if err != nil {
			return nil, fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
		}
		fs.Infof(f, "âœ… æˆåŠŸè·å–123ç½‘ç›˜ä¸‹è½½URL: fileId=%s", fileID)
		return downloadURL, nil
	}
}

// validateRequired éªŒè¯å¿…éœ€å­—æ®µä¸ä¸ºç©º
func validateRequired(fieldName, value string) error {
	if value == "" {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºç©º", fieldName)
	}
	return nil
}

// validateRange éªŒè¯æ•°å€¼åœ¨æŒ‡å®šèŒƒå›´å†…
func validateRange(fieldName string, value, min, max int) error {
	if value <= 0 || value < min || value > max {
		return fmt.Errorf("%s å¿…é¡»åœ¨ %d-%d èŒƒå›´å†…", fieldName, min, max)
	}
	return nil
}

// validateDuration éªŒè¯æ—¶é—´é…ç½®ä¸ä¸ºè´Ÿæ•°
func validateDuration(fieldName string, value time.Duration) error {
	if value < 0 {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºè´Ÿæ•°", fieldName)
	}
	return nil
}

// validateSize éªŒè¯å¤§å°é…ç½®
func validateSize(fieldName string, value int64, min, max int64) error {
	if value < min {
		return fmt.Errorf("%s ä¸èƒ½å°äº %d", fieldName, min)
	}
	if max > 0 && value > max {
		return fmt.Errorf("%s ä¸èƒ½è¶…è¿‡ %d", fieldName, max)
	}
	return nil
}

// validateOptions éªŒè¯é…ç½®é€‰é¡¹çš„æœ‰æ•ˆæ€§
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»“æ„åŒ–éªŒè¯æ–¹æ³•ï¼Œå‡å°‘é‡å¤ä»£ç 
func validateOptions(opt *Options) error {
	var errors []string

	// éªŒè¯å¿…éœ€çš„è®¤è¯ä¿¡æ¯
	if err := validateRequired("client_id", opt.ClientID); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRequired("client_secret", opt.ClientSecret); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯æ•°å€¼èŒƒå›´ - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateRange("list_chunk", opt.ListChunk, 1, MaxListChunk); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_upload_parts", opt.MaxUploadParts, 1, MaxUploadPartsLimit); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_concurrent_uploads", opt.MaxConcurrentUploads, 1, MaxConcurrentLimit); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_concurrent_downloads", opt.MaxConcurrentDownloads, 1, MaxConcurrentLimit); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯æ—¶é—´é…ç½® - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateDuration("pacer_min_sleep", time.Duration(opt.PacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("list_pacer_min_sleep", time.Duration(opt.ListPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("conn_timeout", time.Duration(opt.ConnTimeout)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("timeout", time.Duration(opt.Timeout)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("progress_update_interval", time.Duration(opt.ProgressUpdateInterval)); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯å¤§å°é…ç½® - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateSize("chunk_size", int64(opt.ChunkSize), 1, 5*1024*1024*1024); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateSize("upload_cutoff", int64(opt.UploadCutoff), 0, -1); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯UserAgenté•¿åº¦
	if opt.UserAgent != "" && len(opt.UserAgent) > 200 {
		errors = append(errors, "user_agent é•¿åº¦ä¸èƒ½è¶…è¿‡ 200 å­—ç¬¦")
	}

	// éªŒè¯RootFolderIDæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
	if opt.RootFolderID != "" {
		if _, err := parseFileID(opt.RootFolderID); err != nil {
			errors = append(errors, "root_folder_id å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•°å­—")
		}
	}

	// è¿”å›èšåˆçš„é”™è¯¯ä¿¡æ¯
	if len(errors) > 0 {
		return fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %s", strings.Join(errors, "; "))
	}

	return nil
}

// createPacer åˆ›å»ºpacerçš„å·¥å‚å‡½æ•°ï¼Œå‡å°‘é‡å¤é…ç½®ä»£ç 
func createPacer(ctx context.Context, minSleep, maxSleep time.Duration, decayConstant float64) *fs.Pacer {
	return fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(minSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))
}

// normalizeOptions æ ‡å‡†åŒ–å’Œä¿®æ­£é…ç½®é€‰é¡¹
func normalizeOptions(opt *Options) {
	// è®¾ç½®é»˜è®¤å€¼
	if opt.ListChunk <= 0 {
		opt.ListChunk = DefaultListChunk
	}
	if opt.MaxUploadParts <= 0 {
		opt.MaxUploadParts = DefaultMaxUploadParts
	}
	if opt.MaxConcurrentUploads <= 0 {
		opt.MaxConcurrentUploads = 4
	}
	if opt.MaxConcurrentDownloads <= 0 {
		opt.MaxConcurrentDownloads = 4
	}

	// è®¾ç½®åˆç†çš„é»˜è®¤è¶…æ—¶æ—¶é—´
	if time.Duration(opt.PacerMinSleep) <= 0 {
		opt.PacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.ListPacerMinSleep) <= 0 {
		opt.ListPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.ConnTimeout) <= 0 {
		opt.ConnTimeout = fs.Duration(60 * time.Second)
	}
	if time.Duration(opt.Timeout) <= 0 {
		opt.Timeout = fs.Duration(300 * time.Second)
	}
	if time.Duration(opt.ProgressUpdateInterval) <= 0 {
		opt.ProgressUpdateInterval = fs.Duration(1 * time.Second)
	}

	// ç¡®ä¿è¿›åº¦æ˜¾ç¤ºé»˜è®¤å¯ç”¨ï¼ˆGoä¸­boolé›¶å€¼æ˜¯falseï¼Œéœ€è¦æ˜¾å¼è®¾ç½®ï¼‰
	// æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç®€å•ç”¨ if !opt.EnableProgressDisplayï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½æ˜¾å¼è®¾ç½®ä¸ºfalse
	// æˆ‘ä»¬éœ€è¦æ£€æŸ¥æ˜¯å¦æ˜¯ä»é…ç½®æ–‡ä»¶åŠ è½½çš„å€¼è¿˜æ˜¯é»˜è®¤çš„é›¶å€¼
	// ç”±äºconfigstruct.Setä¼šå¤„ç†é»˜è®¤å€¼ï¼Œè¿™é‡Œåº”è¯¥å·²ç»æ˜¯æ­£ç¡®çš„å€¼äº†
	// ä½†ä¸ºäº†ç¡®ä¿å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¿æŒç°æœ‰é€»è¾‘

	// è®¾ç½®åˆç†çš„é»˜è®¤å¤§å°
	if int64(opt.ChunkSize) <= 0 {
		opt.ChunkSize = fs.SizeSuffix(100 * 1024 * 1024) // 100MB
	}
	if int64(opt.UploadCutoff) <= 0 {
		opt.UploadCutoff = fs.SizeSuffix(200 * 1024 * 1024) // 200MB
	}

	// è®¾ç½®é»˜è®¤UserAgent
	if opt.UserAgent == "" {
		opt.UserAgent = "rclone/" + fs.Version
	}
}

// calculateChecksum è®¡ç®—æ•°æ®çš„è½»é‡æ ¡éªŒå’Œï¼ˆä½¿ç”¨CRC32æ›¿ä»£SHA256ï¼‰
// å·²è¿ç§»åˆ°common.CalculateChecksumï¼Œä¿ç•™æ­¤å‡½æ•°ç”¨äºå‘åå…¼å®¹
func calculateChecksum(data any) string {
	return common.CalculateChecksum(data)
}

// generateCacheVersion ç”Ÿæˆç¼“å­˜ç‰ˆæœ¬å·
// å·²è¿ç§»åˆ°common.GenerateCacheVersionï¼Œä¿ç•™æ­¤å‡½æ•°ç”¨äºå‘åå…¼å®¹
func generateCacheVersion() int64 {
	return common.GenerateCacheVersion()
}

// validateCacheEntry éªŒè¯ç¼“å­˜æ¡ç›®çš„å®Œæ•´æ€§ï¼ˆè½»é‡çº§éªŒè¯ï¼‰
// å·²è¿ç§»åˆ°common.ValidateCacheEntryï¼Œä¿ç•™æ­¤å‡½æ•°ç”¨äºå‘åå…¼å®¹
func validateCacheEntry(data any, expectedChecksum string) bool {
	return common.ValidateCacheEntry(data, expectedChecksum)
}

// fastValidateCacheEntry å¿«é€ŸéªŒè¯ç¼“å­˜æ¡ç›®ï¼ˆä»…æ£€æŸ¥åŸºæœ¬ç»“æ„ï¼‰
func fastValidateCacheEntry(data any) bool {
	if data == nil {
		return false
	}

	// ç®€å•çš„ç»“æ„æ£€æŸ¥ï¼Œé¿å…å¤æ‚çš„æ ¡éªŒå’Œè®¡ç®—
	switch v := data.(type) {
	case []any:
		return len(v) >= 0 // æ•°ç»„ç±»å‹ï¼Œæ£€æŸ¥é•¿åº¦
	case map[string]any:
		return len(v) >= 0 // å¯¹è±¡ç±»å‹ï¼Œæ£€æŸ¥é”®æ•°é‡
	case string:
		return len(v) > 0 // å­—ç¬¦ä¸²ç±»å‹ï¼Œæ£€æŸ¥éç©º
	default:
		return true // å…¶ä»–ç±»å‹é»˜è®¤æœ‰æ•ˆ
	}
}

// shouldRetry æ ¹æ®å“åº”å’Œé”™è¯¯ç¡®å®šæ˜¯å¦é‡è¯•APIè°ƒç”¨
// ä½¿ç”¨å…¬å…±åº“çš„ç»Ÿä¸€é‡è¯•é€»è¾‘
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	// ä½¿ç”¨å…¬å…±åº“çš„HTTPé‡è¯•é€»è¾‘
	return common.ShouldRetryHTTP(ctx, resp, err)
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡ŒAPIè°ƒç”¨ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶
// è¿™æ˜¯æ¨èçš„APIè°ƒç”¨æ–¹æ³•ï¼Œæ›¿ä»£ç›´æ¥ä½¿ç”¨HTTPå®¢æˆ·ç«¯
func (f *Fs) makeAPICallWithRest(ctx context.Context, endpoint string, method string, reqBody any, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API: %s %s", method, endpoint)

	// å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿restå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
	if f.rst == nil {
		fs.Errorf(f, "âš ï¸  restå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆ›å»º")
		if f.client != nil {
			f.rst = rest.NewClient(f.client).SetRoot(openAPIRootURL)
			fs.Debugf(f, " restå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºæˆåŠŸ")
		} else {
			return fmt.Errorf("HTTPå®¢æˆ·ç«¯å’Œrestå®¢æˆ·ç«¯éƒ½æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡ŒAPIè°ƒç”¨")
		}
	}

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - æ ¹æ®å®˜æ–¹æ–‡æ¡£åªæœ‰ä¸¤ä¸ªAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		// åªæœ‰åˆ†ç‰‡ä¸Šä¼ å’Œå•æ­¥ä¸Šä¼ ä½¿ç”¨ä¸Šä¼ åŸŸåï¼ˆå®˜æ–¹æ–‡æ¡£æ˜ç¡®è¯´æ˜ï¼‰
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		// å…¶ä»–APIï¼ˆåŒ…æ‹¬createã€upload_completeï¼‰ä½¿ç”¨æ ‡å‡†APIåŸŸå
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.CallJSON(ctx, &opts, reqBody, respBody)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		return shouldRetry(ctx, resp, err)
	})

	if err != nil {
		return fmt.Errorf("APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// makeAPICallWithRestMultipartToDomain ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨åˆ°æŒ‡å®šåŸŸå
func (f *Fs) makeAPICallWithRestMultipartToDomain(ctx context.Context, domain string, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart APIåˆ°åŸŸå: %s %s %s", method, domain, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ä½¿ç”¨è°ƒé€Ÿå™¨è¿›è¡ŒAPIè°ƒç”¨
	return pacer.Call(func() (bool, error) {
		opts := rest.Opts{
			Method:  method,
			RootURL: domain,
			Path:    endpoint,
			Body:    body,
			ExtraHeaders: map[string]string{
				"Content-Type": contentType,
			},
		}

		// æ·»åŠ å¿…è¦çš„è®¤è¯å¤´
		opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
		opts.ExtraHeaders["Platform"] = "open_platform"
		opts.ExtraHeaders["User-Agent"] = f.opt.UserAgent

		var resp *http.Response
		var err error
		resp, err = f.rst.Call(ctx, &opts)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
			if fserrors.ShouldRetry(err) {
				fs.Debugf(f, "APIè°ƒç”¨å¤±è´¥ï¼Œå°†é‡è¯•: %v", err)
				return true, err
			}
			return false, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
		}
		defer resp.Body.Close()

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode >= 400 {
			body, _ := io.ReadAll(resp.Body)

			// ç‰¹æ®Šå¤„ç†401é”™è¯¯ - tokenè¿‡æœŸ
			if resp.StatusCode == http.StatusUnauthorized {
				fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
				// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
				refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
				if refreshErr != nil {
					fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
					return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
				}
				// æ›´æ–°Authorizationå¤´
				opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
				fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
				return true, nil // é‡è¯•
			}

			return false, fmt.Errorf("APIè¿”å›é”™è¯¯çŠ¶æ€ %d: %s", resp.StatusCode, string(body))
		}

		// è§£æå“åº”JSON
		if respBody != nil {
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”JSONå¤±è´¥: %w", err)
			}
		}

		return false, nil
	})
}

// makeAPICallWithRestMultipart ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨
func (f *Fs) makeAPICallWithRestMultipart(ctx context.Context, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart API: %s %s", method, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - ç‰¹å®šAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		Body:    body,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
			"Content-Type":  contentType,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.Call(ctx, &opts)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// è§£æå“åº”
		if respBody != nil {
			defer resp.Body.Close()
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”å¤±è´¥: %w", err)
			}
		}

		return false, nil
	})

	if err != nil {
		return fmt.Errorf("multipart APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// getPacerForEndpoint æ ¹æ®APIç«¯ç‚¹è¿”å›é€‚å½“çš„è°ƒé€Ÿå™¨
// åŸºäºå®˜æ–¹APIé™æµæ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
// æ›´æ–°æ—¶é—´ï¼š2025-07-04ï¼Œä½¿ç”¨æœ€æ–°å®˜æ–¹QPSé™åˆ¶è¡¨
func (f *Fs) getPacerForEndpoint(endpoint string) *fs.Pacer {
	switch {
	// é«˜é¢‘ç‡API (15-20 QPS)
	case strings.Contains(endpoint, "/api/v2/file/list"):
		return f.listPacer // ~14 QPS (å®˜æ–¹15 QPS)
	case strings.Contains(endpoint, "/upload/v2/file/get_upload_url"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"):
		return f.downloadPacer // ~16 QPS (å®˜æ–¹20 QPS)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS)
	case strings.Contains(endpoint, "/api/v1/file/move"),
		strings.Contains(endpoint, "/api/v1/file/name"),
		strings.Contains(endpoint, "/api/v1/file/infos"),
		strings.Contains(endpoint, "/api/v1/user/info"):
		return f.listPacer // ~8 QPS (å®˜æ–¹10 QPS)
	case strings.Contains(endpoint, "/api/v1/access_token"):
		return f.downloadPacer // ~6 QPS (å®˜æ–¹8 QPS)

	// ä½é¢‘ç‡API (5 QPS)
	case strings.Contains(endpoint, "/upload/v1/file/create"),
		strings.Contains(endpoint, "/upload/v2/file/create"),
		strings.Contains(endpoint, "/upload/v1/file/mkdir"),
		strings.Contains(endpoint, "/api/v1/file/trash"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"),
		strings.Contains(endpoint, "/api/v1/file/download_info"):
		return f.strictPacer // ~4 QPS (å®˜æ–¹5 QPS)

	// æœ€ä½é¢‘ç‡API (1 QPS)
	case strings.Contains(endpoint, "/api/v1/file/delete"),
		strings.Contains(endpoint, "/api/v1/file/list"),
		strings.Contains(endpoint, "/api/v1/video/transcode/list"):
		return f.strictPacer // ~0.8 QPS (å®˜æ–¹1 QPS) - ä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶

	// v2åˆ†ç‰‡ä¸Šä¼ API (ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨ä¸“ç”¨çš„uploadPacer)
	case strings.Contains(endpoint, "/upload/v2/file/slice"):
		return f.uploadPacer // ~5 QPS (åŸºäºå®˜æ–¹æ–‡æ¡£å’Œå®é™…æµ‹è¯•)

	// ä¸Šä¼ åŸŸåAPI (ç‰¹æ®Šå¤„ç†)
	case strings.Contains(endpoint, "/upload/v2/file/domain"):
		return f.strictPacer // ä¿å®ˆå¤„ç†ï¼Œé¿å…é¢‘ç¹è°ƒç”¨

	default:
		// ä¸ºäº†å®‰å…¨èµ·è§ï¼ŒæœªçŸ¥ç«¯ç‚¹é»˜è®¤ä½¿ç”¨æœ€ä¸¥æ ¼çš„è°ƒé€Ÿå™¨
		fs.Debugf(f, "âš ï¸  æœªçŸ¥APIç«¯ç‚¹ï¼Œä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶: %s", endpoint)
		return f.strictPacer // æœ€ä¸¥æ ¼çš„é™åˆ¶
	}
}

// ErrorContext é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
type ErrorContext struct {
	Operation string            // æ“ä½œç±»å‹
	Method    string            // HTTPæ–¹æ³•
	Endpoint  string            // APIç«¯ç‚¹
	FileID    string            // æ–‡ä»¶IDï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	FileName  string            // æ–‡ä»¶åï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	Extra     map[string]string // é¢å¤–ä¿¡æ¯
}

// WrapError åŒ…è£…é”™è¯¯å¹¶æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
func (f *Fs) WrapError(err error, ctx ErrorContext) error {
	if err == nil {
		return nil
	}

	var details []string
	if ctx.Operation != "" {
		details = append(details, fmt.Sprintf("æ“ä½œ=%s", ctx.Operation))
	}
	if ctx.Method != "" && ctx.Endpoint != "" {
		details = append(details, fmt.Sprintf("API=%s %s", ctx.Method, ctx.Endpoint))
	}
	if ctx.FileID != "" {
		details = append(details, fmt.Sprintf("æ–‡ä»¶ID=%s", ctx.FileID))
	}
	if ctx.FileName != "" {
		details = append(details, fmt.Sprintf("æ–‡ä»¶å=%s", ctx.FileName))
	}
	for key, value := range ctx.Extra {
		details = append(details, fmt.Sprintf("%s=%s", key, value))
	}

	if len(details) > 0 {
		return fmt.Errorf("%s [%s]: %w", ctx.Operation, strings.Join(details, ", "), err)
	}
	return fmt.Errorf("%s: %w", ctx.Operation, err)
}

// ResumeInfo æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
type ResumeInfo struct {
	PreuploadID    string           `json:"preupload_id"`
	FileName       string           `json:"file_name"`
	FileSize       int64            `json:"file_size"`
	ChunkSize      int64            `json:"chunk_size"`
	UploadedChunks map[int64]string `json:"uploaded_chunks"` // chunk_index -> etag
	LastChunkIndex int64            `json:"last_chunk_index"`
	CreatedAt      time.Time        `json:"created_at"`
	LastUpdated    time.Time        `json:"last_updated"`
	MD5Hash        string           `json:"md5_hash"`
	ParentFileID   int64            `json:"parent_file_id"`
	TotalChunks    int64            `json:"total_chunks"`
	UploadedBytes  int64            `json:"uploaded_bytes"`
}

// saveUploadProgress ä¿å­˜ä¸Šä¼ è¿›åº¦åˆ°ç»Ÿä¸€ç®¡ç†å™¨
// ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€TaskIDç”Ÿæˆæœºåˆ¶
func (f *Fs) saveUploadProgress(progress *UploadProgress) error {
	if f.resumeManager == nil {
		return fmt.Errorf("æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æœªåˆå§‹åŒ–")
	}

	// ä¿®å¤ï¼šç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è¿œç¨‹è·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨FilePathï¼Œå¦‚æœä¸ºç©ºåˆ™å°è¯•ä»å…¶ä»–åœ°æ–¹è·å–
	remotePath := progress.FilePath
	if remotePath == "" {
		// å¦‚æœFilePathä¸ºç©ºï¼Œå°è¯•ä»progressçš„å…¶ä»–å­—æ®µæ¨æ–­
		fs.Debugf(f, "âš ï¸ progress.FilePathä¸ºç©ºï¼Œä½¿ç”¨å›é€€ç­–ç•¥ç”ŸæˆTaskID")
	}

	// ç®€åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€çš„TaskIDè·å–å’Œè¿ç§»å‡½æ•°
	taskID := f.getTaskIDWithMigration(progress.PreuploadID, remotePath, progress.FileSize)

	// è½¬æ¢ä¸ºç»Ÿä¸€ç®¡ç†å™¨çš„ä¿¡æ¯æ ¼å¼
	resumeInfo := &common.ResumeInfo123{
		TaskID:              taskID,
		PreuploadID:         progress.PreuploadID,
		FileName:            "", // ä¿æŒç°æœ‰å€¼æˆ–ä»progressè·å–
		FileSize:            progress.FileSize,
		FilePath:            progress.FilePath,
		ChunkSize:           progress.ChunkSize,
		TotalChunks:         progress.TotalParts,
		CompletedChunks:     progress.UploadedParts,
		CreatedAt:           progress.CreatedAt,
		LastUpdated:         time.Now(),
		BackendSpecificData: make(map[string]any),
	}

	// ä¿å­˜åˆ°ç»Ÿä¸€ç®¡ç†å™¨
	err := f.resumeManager.SaveResumeInfo(resumeInfo)
	if err != nil {
		return fmt.Errorf("ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "ä¿å­˜ä¸Šä¼ è¿›åº¦: %d/%dä¸ªåˆ†ç‰‡å®Œæˆ (TaskID: %s)",
		progress.GetUploadedCount(), progress.TotalParts, taskID)
	return nil
}

// loadUploadProgress åŠ è½½ä¸Šä¼ è¿›åº¦ï¼Œæ”¯æŒç»Ÿä¸€TaskIDå’Œè¿ç§»
// ä¿®å¤ï¼šå¢åŠ remotePathå’ŒfileSizeå‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskID
func (f *Fs) loadUploadProgress(preuploadID string, remotePath string, fileSize int64) (*UploadProgress, error) {
	if f.resumeManager == nil {
		return nil, fmt.Errorf("æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æœªåˆå§‹åŒ–")
	}

	// ç®€åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€çš„TaskIDè·å–å’Œè¿ç§»å‡½æ•°
	taskID := f.getTaskIDWithMigration(preuploadID, remotePath, fileSize)

	// ä»ç»Ÿä¸€ç®¡ç†å™¨åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	resumeInfo, err := f.resumeManager.LoadResumeInfo(taskID)
	if err != nil {
		return nil, fmt.Errorf("åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if resumeInfo == nil {
		return nil, nil // æ²¡æœ‰æ‰¾åˆ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	}

	// ç±»å‹æ–­è¨€ä¸º123ç½‘ç›˜çš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	r123, ok := resumeInfo.(*common.ResumeInfo123)
	if !ok {
		return nil, fmt.Errorf("æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ç±»å‹ä¸åŒ¹é…")
	}

	// è½¬æ¢ä¸ºUploadProgressæ ¼å¼
	progress := &UploadProgress{
		PreuploadID:   r123.PreuploadID,
		TotalParts:    r123.TotalChunks,
		ChunkSize:     r123.ChunkSize,
		FileSize:      r123.FileSize,
		UploadedParts: r123.GetCompletedChunks(),
		FilePath:      r123.FilePath,
		CreatedAt:     r123.CreatedAt,
	}

	fs.Debugf(f, "åŠ è½½ä¸Šä¼ è¿›åº¦: %d/%dä¸ªåˆ†ç‰‡å®Œæˆ (TaskID: %s)",
		progress.GetUploadedCount(), progress.TotalParts, taskID)
	return progress, nil
}

// removeUploadProgress åˆ é™¤ä¸Šä¼ è¿›åº¦ï¼Œæ”¯æŒç»Ÿä¸€TaskID
// ä¿®å¤ï¼šå¢åŠ remotePathå’ŒfileSizeå‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskIDæ¸…ç†
func (f *Fs) removeUploadProgress(preuploadID string, remotePath string, fileSize int64) {
	if f.resumeManager == nil {
		fs.Debugf(f, "æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤è¿›åº¦")
		return
	}

	// ç®€åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€TaskIDç”Ÿæˆï¼ŒåŒæ—¶æ¸…ç†æ–°æ—§æ•°æ®
	taskID := f.getTaskIDForPreuploadID(preuploadID, remotePath, fileSize)

	// åˆ é™¤å½“å‰TaskIDçš„æ•°æ®
	if err := f.resumeManager.DeleteResumeInfo(taskID); err != nil {
		fs.Debugf(f, "åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v (TaskID: %s)", err, taskID)
	} else {
		fs.Debugf(f, "å·²åˆ é™¤ä¸Šä¼ è¿›åº¦ (TaskID: %s)", taskID)
	}

	// ç®€åŒ–ï¼šåŒæ—¶æ¸…ç†å¯èƒ½çš„æ—§TaskIDæ•°æ®
	if remotePath != "" && fileSize > 0 {
		oldTaskID := fmt.Sprintf("123_%s", preuploadID)
		if oldTaskID != taskID {
			f.resumeManager.DeleteResumeInfo(oldTaskID) // å¿½ç•¥åˆ é™¤é”™è¯¯
		}
	}
}

// FileIntegrityVerifier æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å™¨
// åŠŸèƒ½å¢å¼ºï¼šå®ç°æ–‡ä»¶å®Œæ•´æ€§éªŒè¯æœºåˆ¶
type FileIntegrityVerifier struct {
	fs *Fs
}

// NewFileIntegrityVerifier åˆ›å»ºæ–‡ä»¶å®Œæ•´æ€§éªŒè¯å™¨
func NewFileIntegrityVerifier(fs *Fs) *FileIntegrityVerifier {
	return &FileIntegrityVerifier{fs: fs}
}

// VerifyFileIntegrity éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
// åŠŸèƒ½å¢å¼ºï¼šæ”¯æŒå¤šç§å“ˆå¸Œç®—æ³•çš„æ–‡ä»¶å®Œæ•´æ€§éªŒè¯
func (fiv *FileIntegrityVerifier) VerifyFileIntegrity(ctx context.Context, filePath string, expectedMD5, expectedSHA1 string, fileSize int64) (*IntegrityResult, error) {
	result := &IntegrityResult{
		FilePath:     filePath,
		FileSize:     fileSize,
		ExpectedMD5:  expectedMD5,
		ExpectedSHA1: expectedSHA1,
		StartTime:    time.Now(),
	}

	// æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	fileInfo, err := os.Stat(filePath)
	if err != nil {
		result.Error = fmt.Sprintf("æ–‡ä»¶ä¸å­˜åœ¨: %v", err)
		return result, err
	}

	// éªŒè¯æ–‡ä»¶å¤§å°
	actualSize := fileInfo.Size()
	if actualSize != fileSize {
		result.Error = fmt.Sprintf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›=%d, å®é™…=%d", fileSize, actualSize)
		result.SizeMatch = false
		return result, fmt.Errorf("æ–‡ä»¶å¤§å°éªŒè¯å¤±è´¥")
	}
	result.SizeMatch = true

	// æ‰“å¼€æ–‡ä»¶è¿›è¡Œå“ˆå¸Œè®¡ç®—
	file, err := os.Open(filePath)
	if err != nil {
		result.Error = fmt.Sprintf("æ— æ³•æ‰“å¼€æ–‡ä»¶: %v", err)
		return result, err
	}
	defer file.Close()

	// ä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨è¿›è¡Œæµå¼å“ˆå¸Œè®¡ç®—
	hashCalculator := fiv.fs.memoryOptimizer.NewStreamingHashCalculator()
	md5HashStr, err := hashCalculator.CalculateHash(file)
	if err != nil {
		result.Error = fmt.Sprintf("è®¡ç®—MD5å¤±è´¥: %v", err)
		return result, err
	}

	// è·å–è®¡ç®—ç»“æœ
	result.ActualMD5 = md5HashStr
	result.ActualSHA1 = "" // æš‚æ—¶ä¸è®¡ç®—SHA1ï¼Œå‡å°‘è®¡ç®—å¼€é”€
	result.EndTime = time.Now()
	result.Duration = result.EndTime.Sub(result.StartTime)

	// éªŒè¯MD5
	if expectedMD5 != "" {
		result.MD5Match = strings.EqualFold(result.ActualMD5, expectedMD5)
		if !result.MD5Match {
			result.Error = fmt.Sprintf("MD5ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedMD5, result.ActualMD5)
		}
	}

	// éªŒè¯SHA1
	if expectedSHA1 != "" {
		result.SHA1Match = strings.EqualFold(result.ActualSHA1, expectedSHA1)
		if !result.SHA1Match {
			if result.Error != "" {
				result.Error += "; "
			}
			result.Error += fmt.Sprintf("SHA1ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedSHA1, result.ActualSHA1)
		}
	}

	// åˆ¤æ–­æ•´ä½“éªŒè¯ç»“æœ
	result.IsValid = result.SizeMatch &&
		(expectedMD5 == "" || result.MD5Match) &&
		(expectedSHA1 == "" || result.SHA1Match)

	if result.IsValid {
		fs.Debugf(fiv.fs, " æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡: %s (è€—æ—¶: %v)", filePath, result.Duration)
	} else {
		fs.Debugf(fiv.fs, "âŒ æ–‡ä»¶å®Œæ•´æ€§éªŒè¯å¤±è´¥: %s - %s", filePath, result.Error)
	}

	return result, nil
}

// IntegrityResult å®Œæ•´æ€§éªŒè¯ç»“æœ
type IntegrityResult struct {
	FilePath     string        `json:"file_path"`
	FileSize     int64         `json:"file_size"`
	ExpectedMD5  string        `json:"expected_md5"`
	ExpectedSHA1 string        `json:"expected_sha1"`
	ActualMD5    string        `json:"actual_md5"`
	ActualSHA1   string        `json:"actual_sha1"`
	SizeMatch    bool          `json:"size_match"`
	MD5Match     bool          `json:"md5_match"`
	SHA1Match    bool          `json:"sha1_match"`
	IsValid      bool          `json:"is_valid"`
	StartTime    time.Time     `json:"start_time"`
	EndTime      time.Time     `json:"end_time"`
	Duration     time.Duration `json:"duration"`
	Error        string        `json:"error,omitempty"`
}

// AccessInfo è®¿é—®ä¿¡æ¯
type AccessInfo struct {
	FileID        string    `json:"file_id"`
	AccessCount   int       `json:"access_count"`
	LastAccess    time.Time `json:"last_access"`
	FirstAccess   time.Time `json:"first_access"`
	AccessPattern string    `json:"access_pattern"` // "frequent", "recent", "rare"
}

// EnhancedDownloadURLEntry å¢å¼ºçš„ä¸‹è½½URLç¼“å­˜æ¡ç›®
type EnhancedDownloadURLEntry struct {
	URL           string    `json:"url"`
	ExpireTime    time.Time `json:"expire_time"`
	CachedAt      time.Time `json:"cached_at"`
	AccessCount   int       `json:"access_count"`
	LastAccess    time.Time `json:"last_access"`
	FileSize      int64     `json:"file_size"`
	Priority      int       `json:"priority"`       // ç¼“å­˜ä¼˜å…ˆçº§ 1-10
	PreloadReason string    `json:"preload_reason"` // é¢„åŠ è½½åŸå› 
}

// parseFileID é€šç”¨çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
func parseFileID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseFileIDWithContext å¸¦ä¸Šä¸‹æ–‡çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
func parseFileIDWithContext(idStr, context string) (int64, error) {
	id, err := parseFileID(idStr)
	if err != nil {
		return 0, fmt.Errorf("%s: %w", context, err)
	}
	return id, nil
}

// parseParentID è§£æçˆ¶ç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºçˆ¶ç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseParentID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„çˆ¶ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseDirID è§£æç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseDirID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// getFileInfo æ ¹æ®IDè·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
func (f *Fs) getFileInfo(ctx context.Context, fileID string) (*FileListInfoRespDataV2, error) {
	fs.Debugf(f, " getFileInfoå¼€å§‹: fileID=%s", fileID)

	// éªŒè¯æ–‡ä»¶ID
	_, err := parseFileIDWithContext(fileID, "è·å–æ–‡ä»¶ä¿¡æ¯")
	if err != nil {
		fs.Debugf(f, " getFileInfoæ–‡ä»¶IDéªŒè¯å¤±è´¥: %v", err)
		return nil, err
	}

	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…API - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response FileDetailResponse
	endpoint := fmt.Sprintf("/api/v1/file/detail?fileID=%s", fileID)

	fs.Debugf(f, " getFileInfoè°ƒç”¨API: %s", endpoint)
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		fs.Debugf(f, " getFileInfo APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, " getFileInfo APIå“åº”: code=%d, message=%s", response.Code, response.Message)
	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ‡å‡†æ ¼å¼
	fileInfo := &FileListInfoRespDataV2{
		FileID:       response.Data.FileID,
		Filename:     response.Data.Filename,
		Type:         response.Data.Type,
		Size:         response.Data.Size,
		Etag:         response.Data.ETag,
		Status:       response.Data.Status,
		ParentFileID: response.Data.ParentFileID,
		Category:     0, // è¯¦æƒ…å“åº”ä¸­æœªæä¾›
	}

	fs.Debugf(f, " getFileInfoæˆåŠŸ: fileID=%s, filename=%s, type=%d, size=%d", fileID, fileInfo.Filename, fileInfo.Type, fileInfo.Size)
	return fileInfo, nil
}

// getDownloadURL è·å–æ–‡ä»¶çš„ä¸‹è½½URL
func (f *Fs) getDownloadURL(ctx context.Context, fileID string) (string, error) {
	// ä¿®å¤URLé¢‘ç¹è·å–ï¼šå‡å°‘APIè°ƒç”¨é¢‘ç‡
	fs.Debugf(f, "123ç½‘ç›˜è·å–ä¸‹è½½URL: fileID=%s", fileID)

	var response DownloadInfoResponse
	endpoint := fmt.Sprintf("/api/v1/file/download_info?fileID=%s", fileID)

	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, " 123ç½‘ç›˜ä¸‹è½½URLè·å–æˆåŠŸ: fileID=%s", fileID)

	return response.Data.DownloadURL, nil
}

// createDirectory åˆ›å»ºæ–°ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™è¿”å›ç°æœ‰ç›®å½•çš„ID
func (f *Fs) createDirectory(ctx context.Context, parentID, name string) (string, error) {
	// éªŒè¯å‚æ•°
	if name == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if parentID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯ç›®å½•å
	if err := validateFileName(name); err != nil {
		return "", fmt.Errorf("ç›®å½•åéªŒè¯å¤±è´¥: %w", err)
	}

	// å°†çˆ¶ç›®å½•IDè½¬æ¢ä¸ºint64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return "", err
	}

	// é¦–å…ˆæ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„APIè°ƒç”¨
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• '%s' æ˜¯å¦å·²å­˜åœ¨äºçˆ¶ç›®å½• %s", name, parentID)
	existingID, found, err := f.FindLeaf(ctx, parentID, name)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç°æœ‰ç›®å½•æ—¶å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
	} else if found {
		fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼ŒID: %s", name, existingID)
		return existingID, nil
	}

	// å¦‚æœå¸¸è§„æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼ˆé˜²æ­¢ç¼“å­˜é—®é¢˜ï¼‰
	if !found && err == nil {
		fs.Debugf(f, "å¸¸è§„æŸ¥æ‰¾æœªæ‰¾åˆ°ç›®å½• '%s'ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾", name)
		existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
		if err != nil {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
		} else if found {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°ç›®å½• '%s'ï¼ŒID: %s", name, existingID)
			return existingID, nil
		}
	}

	// ç›®å½•ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»º
	fs.Debugf(f, "ç›®å½• '%s' ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º", name)
	reqBody := map[string]any{
		"parentID": strconv.FormatInt(parentFileID, 10),
		"name":     name,
	}

	// è¿›è¡ŒAPIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    any    `json:"data"` // ä½¿ç”¨anyå› ä¸ºä¸ç¡®å®šå…·ä½“ç»“æ„
	}

	err = f.makeAPICallWithRest(ctx, "/upload/v1/file/mkdir", "POST", reqBody, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯
		if response.Code == 1 && strings.Contains(response.Message, "å·²ç»æœ‰åŒåæ–‡ä»¶å¤¹") {
			fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼Œå¼ºåˆ¶æ¸…ç†ç¼“å­˜åæŸ¥æ‰¾ç°æœ‰ç›®å½•ID", name)

			// å¼ºåˆ¶æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜ï¼Œç¡®ä¿è·å–æœ€æ–°çŠ¶æ€
			f.clearDirListCache(parentFileID)

			// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç¼“å­˜æ¸…ç†å®Œæˆ
			time.Sleep(100 * time.Millisecond)

			// æŸ¥æ‰¾ç°æœ‰ç›®å½•çš„ID - ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼
			existingID, found, err := f.findLeafWithForceRefresh(ctx, parentID, name)
			if err != nil {
				return "", fmt.Errorf("æŸ¥æ‰¾ç°æœ‰ç›®å½•å¤±è´¥: %w", err)
			}
			if !found {
				// å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼ˆå¯èƒ½æ˜¯APIå»¶è¿Ÿï¼‰
				fs.Debugf(f, "ç¬¬ä¸€æ¬¡æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•é‡è¯•æŸ¥æ‰¾ç›®å½• '%s'", name)
				for retry := range 3 {
					time.Sleep(time.Duration(retry+1) * 200 * time.Millisecond)
					existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
					if err == nil && found {
						break
					}
					fs.Debugf(f, "é‡è¯•ç¬¬%dæ¬¡æŸ¥æ‰¾ç›®å½• '%s': found=%v, err=%v", retry+1, name, found, err)
				}

				if !found {
					return "", fmt.Errorf("ç›®å½•å·²å­˜åœ¨ä½†æ— æ³•æ‰¾åˆ°å…¶IDï¼Œå¯èƒ½æ˜¯APIåŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜")
				}
			}

			fs.Debugf(f, "æ‰¾åˆ°ç°æœ‰ç›®å½•ID: %s", existingID)
			return existingID, nil
		}
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// åˆ›å»ºæˆåŠŸï¼Œä½†APIå¯èƒ½ä¸è¿”å›ç›®å½•IDï¼Œéœ€è¦é€šè¿‡æŸ¥æ‰¾è·å–
	fs.Debugf(f, "ç›®å½• '%s' åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹æŸ¥æ‰¾æ–°ç›®å½•ID", name)

	// æ¸…ç†ç¼“å­˜ä»¥ç¡®ä¿èƒ½æ‰¾åˆ°æ–°åˆ›å»ºçš„ç›®å½•
	f.clearDirListCache(parentFileID)

	// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨ç«¯åŒæ­¥å®Œæˆ
	time.Sleep(200 * time.Millisecond)

	// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•çš„IDï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
	var newDirID string
	var foundNew bool
	var errNew error

	// å°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼Œå¤„ç†æœåŠ¡å™¨ç«¯åŒæ­¥å»¶è¿Ÿ
	maxRetries := 5
	for attempt := range maxRetries {
		if attempt > 0 {
			// é€’å¢ç­‰å¾…æ—¶é—´ï¼š200ms, 400ms, 600ms, 800ms
			waitTime := time.Duration(attempt*200) * time.Millisecond
			fs.Debugf(f, "ç¬¬%dæ¬¡é‡è¯•æŸ¥æ‰¾æ–°ç›®å½• '%s'ï¼Œç­‰å¾… %v", attempt+1, name, waitTime)
			time.Sleep(waitTime)
		}

		// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼Œè·³è¿‡ç¼“å­˜ç›´æ¥ä»APIè·å–æœ€æ–°æ•°æ®
		newDirID, foundNew, errNew = f.findLeafWithForceRefresh(ctx, parentID, name)
		if errNew != nil {
			fs.Debugf(f, "ç¬¬%dæ¬¡æŸ¥æ‰¾æ–°ç›®å½•å¤±è´¥: %v", attempt+1, errNew)
			continue
		}

		if foundNew {
			fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æˆåŠŸæ‰¾åˆ°æ–°åˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", attempt+1, name, newDirID)
			break
		}

		fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æœªæ‰¾åˆ°æ–°ç›®å½• '%s'", attempt+1, name)
	}

	// æœ€ç»ˆæ£€æŸ¥ç»“æœ
	if errNew != nil {
		return "", fmt.Errorf("æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•å¤±è´¥ï¼Œå·²é‡è¯•%dæ¬¡: %w", maxRetries, errNew)
	}
	if !foundNew {
		return "", fmt.Errorf("æ–°åˆ›å»ºçš„ç›®å½•æœªæ‰¾åˆ°ï¼Œå·²é‡è¯•%dæ¬¡ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨åŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜", maxRetries)
	}

	fs.Debugf(f, "æˆåŠŸåˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", name, newDirID)
	return newDirID, nil
}

// createUpload åˆ›å»ºä¸Šä¼ ä¼šè¯
func (f *Fs) createUpload(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " åˆ†ç‰‡ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		// ç»Ÿä¸€å¤„ç†ç­–ç•¥ï¼šå¦‚æœé¢„éªŒè¯å¤±è´¥ï¼Œå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID
		fs.Errorf(f, "âš ï¸ åˆ†ç‰‡ä¸Šä¼ : é¢„éªŒè¯å‘ç°çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID", parentFileID)

		// ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
		realParentFileID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}

		if realParentFileID != parentFileID {
			fs.Infof(f, "ğŸ”„ åˆ†ç‰‡ä¸Šä¼ : å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œä½¿ç”¨æ­£ç¡®IDç»§ç»­ä¸Šä¼ ", realParentFileID, parentFileID)
			parentFileID = realParentFileID
		} else {
			fs.Debugf(f, "âš ï¸ åˆ†ç‰‡ä¸Šä¼ : ä½¿ç”¨æ¸…ç†ç¼“å­˜åçš„çˆ¶ç›®å½•ID: %d ç»§ç»­å°è¯•", parentFileID)
		}
	} else {
		fs.Debugf(f, " åˆ†ç‰‡ä¸Šä¼ : çˆ¶ç›®å½•ID %d é¢„éªŒè¯é€šè¿‡", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹APIæ–‡æ¡£çš„æ­£ç¡®å‚æ•°å parentFileID (å¤§å†™D)
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸º123 APIåŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API - createä½¿ç”¨æ ‡å‡†APIåŸŸå
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		return nil, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯é€»è¾‘ï¼šæ£€æµ‹parentFileIDä¸å­˜åœ¨é”™è¯¯ï¼Œä½¿ç”¨ç»Ÿä¸€ä¿®å¤ç­–ç•¥
	if response.Code == 1 && strings.Contains(response.Message, "parentFileIDä¸å­˜åœ¨") {
		fs.Errorf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œæ¸…ç†ç¼“å­˜", parentFileID)

		// ä½¿ç”¨ä¸å•æ­¥ä¸Šä¼ å®Œå…¨ä¸€è‡´çš„ç»Ÿä¸€çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥
		fs.Infof(f, "ğŸ”„ ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥")
		correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
		if err != nil {
			return nil, fmt.Errorf("è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}

		// å¦‚æœè·å¾—äº†ä¸åŒçš„çˆ¶ç›®å½•IDï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯
		if correctParentID != parentFileID {
			fs.Infof(f, "ğŸ”„ å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯", correctParentID, parentFileID)

			// ä½¿ç”¨æ­£ç¡®çš„ç›®å½•IDé‡æ–°æ„å»ºè¯·æ±‚
			reqBody["parentFileID"] = correctParentID

			// é‡æ–°è°ƒç”¨API
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("ä½¿ç”¨æ­£ç¡®ç›®å½•IDé‡è¯•APIè°ƒç”¨å¤±è´¥: %w", err)
			}
		} else {
			// å³ä½¿æ˜¯ç›¸åŒçš„IDï¼Œä¹Ÿè¦é‡æ–°å°è¯•ï¼Œå› ä¸ºç¼“å­˜å·²ç»æ¸…ç†
			fs.Infof(f, "ğŸ”„ ä½¿ç”¨æ¸…ç†ç¼“å­˜åçš„çˆ¶ç›®å½•ID: %dï¼Œé‡æ–°å°è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯", correctParentID)
			reqBody["parentFileID"] = correctParentID
			err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
			if err != nil {
				return nil, fmt.Errorf("æ¸…ç†ç¼“å­˜åé‡è¯•APIè°ƒç”¨å¤±è´¥: %w", err)
			}
		}
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, "åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	return &response, nil
}

// createUploadV2 ä¸“é—¨ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åˆ›å»ºä¼šè¯ï¼Œå¼ºåˆ¶ä½¿ç”¨v2 API
func (f *Fs) createUploadV2(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// éªŒè¯æ–‡ä»¶åç¬¦åˆAPIè¦æ±‚
	if err := validateFileName(filename); err != nil {
		return nil, fmt.Errorf("æ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}

	// é¦–å…ˆéªŒè¯parentFileIDæ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "createUploadV2: éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		return nil, fmt.Errorf("çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®å¤ï¼šä½¿ç”¨å®˜æ–¹APIæ–‡æ¡£çš„æ­£ç¡®å‚æ•°å parentFileID (å¤§å†™D)
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// å¼ºåˆ¶ä½¿ç”¨v2 APIï¼Œä¸ä½¿ç”¨ç‰ˆæœ¬å›é€€ - ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•
	bodyBytes, _ := json.Marshal(reqBody)
	fs.Debugf(f, "è°ƒç”¨v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯: %s", string(bodyBytes))
	// æ ¹æ®å®˜æ–¹æ–‡æ¡£ä½¿ç”¨v2ç«¯ç‚¹å’Œæ ‡å‡†APIåŸŸå
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		fs.Errorf(f, "v2 APIè°ƒç”¨å¤±è´¥: %v", err)

		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if strings.Contains(err.Error(), "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸  çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå¯èƒ½ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦æ¸…ç†ç¼“å­˜é‡è¯•", parentFileID)
			// æ¸…ç†ç›¸å…³ç¼“å­˜
			f.clearParentIDCache(parentFileID)
			f.clearDirListCache(parentFileID)
			// é‡ç½®ç›®å½•ç¼“å­˜
			if f.dirCache != nil {
				fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", parentFileID)
				f.dirCache.ResetRoot()
			}
			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: %w", err)
		}

		return nil, fmt.Errorf("v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// è¯¦ç»†è®°å½•APIå“åº”
	fs.Debugf(f, "v2 APIå“åº”: Code=%d, Message='%s', FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Code, response.Message, response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	if response.Code != 0 {
		fs.Errorf(f, "v2 APIè¿”å›é”™è¯¯: Code=%d, Message='%s'", response.Code, response.Message)
		return nil, fmt.Errorf("v2 API error %d: %s", response.Code, response.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸ºç§’ä¼ 
	if response.Data.Reuse {
		fs.Debugf(f, "ğŸš€ v2 APIç§’ä¼ æˆåŠŸ: FileID=%d, æ–‡ä»¶å·²å­˜åœ¨äºæœåŠ¡å™¨", response.Data.FileID)
		return &response, nil
	}

	// éç§’ä¼ æƒ…å†µï¼ŒéªŒè¯PreuploadIDä¸ä¸ºç©º
	if response.Data.PreuploadID == "" {
		fs.Errorf(f, "âŒ v2 APIå“åº”å¼‚å¸¸: éç§’ä¼ æƒ…å†µä¸‹PreuploadIDä¸ºç©ºï¼Œå®Œæ•´å“åº”: %+v", response)
		return nil, fmt.Errorf("v2 APIè¿”å›çš„é¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, " v2åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.SliceSize)

	return &response, nil
}

// uploadFile ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹
func (f *Fs) uploadFile(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64) error {
	return f.uploadFileWithPath(ctx, in, createResp, size, "")
}

// uploadFileWithPath ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileWithPath(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64, filePath string) error {
	// ç¡®å®šå—å¤§å°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		chunkSize = int64(f.opt.ChunkSize)
	}

	// ç¡®ä¿å—å¤§å°åœ¨é™åˆ¶èŒƒå›´å†…
	if chunkSize < int64(minChunkSize) {
		chunkSize = int64(minChunkSize)
	}
	if chunkSize > int64(maxChunkSize) {
		chunkSize = int64(maxChunkSize)
	}

	// å¯¹äºå°æ–‡ä»¶ï¼Œå…¨éƒ¨è¯»å…¥å†…å­˜ï¼ˆæ— éœ€æ–­ç‚¹ç»­ä¼ ï¼‰
	if size <= chunkSize {
		data, err := io.ReadAll(in)
		if err != nil {
			return fmt.Errorf("failed to read file data: %w", err)
		}

		if int64(len(data)) != size {
			return fmt.Errorf("size mismatch: expected %d, got %d", size, len(data))
		}

		// å•éƒ¨åˆ†ä¸Šä¼ 
		return f.uploadSinglePart(ctx, createResp.Data.PreuploadID, data)
	}

	// å¤§æ–‡ä»¶çš„å¤šéƒ¨åˆ†ä¸Šä¼ ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
	return f.uploadMultiPartWithResume(ctx, in, createResp.Data.PreuploadID, size, chunkSize, filePath)
}

// uploadSinglePart å•éƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadSinglePart(ctx context.Context, preuploadID string, data []byte) error {
	// è®¡ç®—åˆ†ç‰‡MD5
	chunkHash := fmt.Sprintf("%x", md5.Sum(data))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err := f.uploadPartWithMultipart(ctx, preuploadID, 1, chunkHash, data)
	if err != nil {
		return fmt.Errorf("failed to upload part: %w", err)
	}

	// å®Œæˆä¸Šä¼  - ä½¿ç”¨å¸¦æ–‡ä»¶å¤§å°çš„ç‰ˆæœ¬ä»¥æ”¯æŒå¢å¼ºéªŒè¯
	_, err = f.completeUploadWithResultAndSize(ctx, preuploadID, int64(len(data)))
	return err
}

// uploadMultiPartWithResume å¤šéƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadMultiPartWithResume(ctx context.Context, in io.Reader, preuploadID string, size, chunkSize int64, filePath string) error {
	// é˜²æ­¢é™¤é›¶é”™è¯¯
	if chunkSize <= 0 {
		return fmt.Errorf("æ— æ•ˆçš„åˆ†ç‰‡å¤§å°: %d", chunkSize)
	}

	uploadNums := (size + chunkSize - 1) / chunkSize

	// é™åˆ¶åˆ†ç‰‡æ•°é‡
	if uploadNums > int64(f.opt.MaxUploadParts) {
		return fmt.Errorf("file too large: would require %d parts, max allowed is %d", uploadNums, f.opt.MaxUploadParts)
	}

	// ä¿®å¤ï¼šå°è¯•åŠ è½½ç°æœ‰è¿›åº¦ï¼Œä¼ é€’æ–‡ä»¶è·¯å¾„å’Œå¤§å°ä¿¡æ¯
	progress, err := f.loadUploadProgress(preuploadID, filePath, size)
	if err != nil {
		fs.Debugf(f, "åŠ è½½ä¸Šä¼ è¿›åº¦å¤±è´¥: %v", err)
		progress = nil
	}

	// å¦‚æœä¸å­˜åœ¨è¿›åº¦æˆ–å‚æ•°ä¸åŒ¹é…ï¼Œåˆ›å»ºæ–°è¿›åº¦
	if progress == nil || progress.TotalParts != uploadNums || progress.ChunkSize != chunkSize || progress.FileSize != size {
		fs.Debugf(f, "ä¸º%såˆ›å»ºæ–°çš„ä¸Šä¼ è¿›åº¦", preuploadID)
		progress = &UploadProgress{
			PreuploadID:   preuploadID,
			TotalParts:    uploadNums,
			ChunkSize:     chunkSize,
			FileSize:      size,
			UploadedParts: make(map[int64]bool),
			FilePath:      filePath,
			CreatedAt:     time.Now(),
		}
	} else {
		fs.Debugf(f, "æ¢å¤ä¸Šä¼ %s: %d/%dä¸ªåˆ†ç‰‡å·²ä¸Šä¼ ",
			preuploadID, progress.GetUploadedCount(), progress.TotalParts)
	}

	// æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»æ–‡ä»¶æ¢å¤ï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶è·¯å¾„ä¸”æ–‡ä»¶å­˜åœ¨ï¼‰
	var fileReader *os.File
	canResumeFromFile := filePath != "" && progress.GetUploadedCount() > 0

	if canResumeFromFile {
		fileReader, err = os.Open(filePath)
		if err != nil {
			fs.Debugf(f, "æ— æ³•æ‰“å¼€æ–‡ä»¶è¿›è¡Œæ–­ç‚¹ç»­ä¼ ï¼Œå›é€€åˆ°æµå¼ä¼ è¾“: %v", err)
			canResumeFromFile = false
		} else {
			defer fileReader.Close()
			// ä½¿ç”¨æ–‡ä»¶è¯»å–å™¨è€Œä¸æ˜¯æµä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ 
			in = fileReader
		}
	}

	buffer := make([]byte, chunkSize)

	for partIndex := range uploadNums {
		partNumber := partIndex + 1

		// å¦‚æœæ­¤åˆ†ç‰‡å·²ä¸Šä¼ åˆ™è·³è¿‡ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		if progress.IsUploaded(partNumber) {
			fs.Debugf(f, "è·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡ %d/%d", partNumber, uploadNums)

			// å¦‚æœå¯ä»¥å®šä½ï¼ˆæ–‡ä»¶è¯»å–å™¨ï¼‰ï¼Œå®šä½åˆ°ä¸‹ä¸€åˆ†ç‰‡
			if canResumeFromFile && fileReader != nil {
				nextOffset := (partIndex + 1) * chunkSize
				if nextOffset < size {
					_, err := fileReader.Seek(nextOffset, io.SeekStart)
					if err != nil {
						return fmt.Errorf("failed to seek to part %d: %w", partNumber+1, err)
					}
				}
			} else {
				// ä»éœ€è¦è¯»å–å¹¶ä¸¢å¼ƒæ•°æ®ä»¥æ¨è¿›è¯»å–å™¨
				if partIndex == uploadNums-1 {
					remaining := size - partIndex*chunkSize
					_, err := io.ReadFull(in, make([]byte, remaining))
					if err != nil {
						return fmt.Errorf("failed to skip part %d data: %w", partNumber, err)
					}
				} else {
					_, err := io.ReadFull(in, buffer)
					if err != nil {
						return fmt.Errorf("failed to skip part %d data: %w", partNumber, err)
					}
				}
			}
			continue
		}

		// å¦‚æœå¯ä»¥å®šä½ä¸”è¿™ä¸æ˜¯ç¬¬ä¸€ä¸ªæœªä¸Šä¼ çš„åˆ†ç‰‡ï¼Œå®šä½åˆ°æ­£ç¡®ä½ç½®
		if canResumeFromFile && fileReader != nil {
			currentOffset := partIndex * chunkSize
			_, err := fileReader.Seek(currentOffset, io.SeekStart)
			if err != nil {
				return fmt.Errorf("failed to seek to part %d: %w", partNumber, err)
			}
		}

		// è¯»å–å—æ•°æ®
		var partData []byte
		if partIndex == uploadNums-1 {
			// æœ€åä¸€åˆ†ç‰‡ - è¯»å–å‰©ä½™æ•°æ®
			remaining := size - partIndex*chunkSize
			partData = make([]byte, remaining)
			_, err := io.ReadFull(in, partData)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
		} else {
			// å¸¸è§„åˆ†ç‰‡
			_, err := io.ReadFull(in, buffer)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
			partData = buffer
		}

		// è®¡ç®—åˆ†ç‰‡MD5
		chunkHash := fmt.Sprintf("%x", md5.Sum(partData))

		// å…³é”®ä¿®å¤ï¼šç§»é™¤å¤šå±‚é‡è¯•åµŒå¥—ï¼Œç›´æ¥è°ƒç”¨uploadPartWithMultipart
		// uploadPartWithMultipartå†…éƒ¨å·²ä½¿ç”¨ç»Ÿä¸€é‡è¯•ç®¡ç†å™¨
		err = f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkHash, partData)
		if err != nil {
			// è¿”å›é”™è¯¯å‰ä¿å­˜è¿›åº¦
			saveErr := f.saveUploadProgress(progress)
			if saveErr != nil {
				fs.Debugf(f, "ä¸Šä¼ é”™è¯¯åä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
			}
			return fmt.Errorf("failed to upload part %d: %w", partNumber, err)
		}

		// æ ‡è®°æ­¤åˆ†ç‰‡å·²ä¸Šä¼ å¹¶ä¿å­˜è¿›åº¦ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		progress.SetUploaded(partNumber)
		err = f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜ä¸Šä¼ è¿›åº¦å¤±è´¥: %v", err)
		}

		fs.Debugf(f, "å·²ä¸Šä¼ åˆ†ç‰‡ %d/%d", partNumber, uploadNums)
	}

	// å®Œæˆä¸Šä¼  - ä½¿ç”¨å¸¦æ–‡ä»¶å¤§å°çš„ç‰ˆæœ¬ä»¥æ”¯æŒå¢å¼ºéªŒè¯
	_, err = f.completeUploadWithResultAndSize(ctx, preuploadID, size)
	if err != nil {
		// è¿”å›é”™è¯¯å‰ä¿å­˜è¿›åº¦
		saveErr := f.saveUploadProgress(progress)
		if saveErr != nil {
			fs.Debugf(f, "å®Œæˆé”™è¯¯åä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
		}
		return err
	}

	// ä¿®å¤ï¼šä¸Šä¼ æˆåŠŸå®Œæˆï¼Œåˆ é™¤è¿›åº¦æ–‡ä»¶ï¼Œä¼ é€’æ–‡ä»¶è·¯å¾„å’Œå¤§å°ä¿¡æ¯
	f.removeUploadProgress(preuploadID, filePath, size)
	return nil
}

// uploadPartWithMultipart ä½¿ç”¨æ­£ç¡®çš„multipart/form-dataæ ¼å¼ä¸Šä¼ åˆ†ç‰‡
// ä¿®å¤ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€é‡è¯•ç®¡ç†å™¨ï¼Œé¿å…å¤šå±‚é‡è¯•åµŒå¥—
func (f *Fs) uploadPartWithMultipart(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	if len(data) == 0 {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(data))

	// ç®€åŒ–é‡è¯•é€»è¾‘ - ä½¿ç”¨rcloneæ ‡å‡†çš„fs.Paceré‡è¯•æœºåˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		err := f.uploadPartDirectly(ctx, preuploadID, partNumber, chunkHash, data)
		return fserrors.ShouldRetry(err), err
	})
}

// uploadPartDirectly ç›´æ¥ä¸Šä¼ åˆ†ç‰‡ï¼Œä¸åŒ…å«é‡è¯•é€»è¾‘
// å…³é”®ä¿®å¤ï¼šå°†é‡è¯•é€»è¾‘ä»ä¸šåŠ¡é€»è¾‘ä¸­åˆ†ç¦»
func (f *Fs) uploadPartDirectly(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	// ä¿®å¤ï¼šç§»é™¤ uploadPacer.Callï¼Œç›´æ¥æ‰§è¡Œï¼Œé‡è¯•ç”±ç»Ÿä¸€ç®¡ç†å™¨å¤„ç†
	// ä¼˜åŒ–ï¼šç¼“å­˜ä¸Šä¼ åŸŸåï¼Œé¿å…æ¯ä¸ªåˆ†ç‰‡éƒ½é‡æ–°è·å–
	uploadDomain, err := f.getCachedUploadDomain(ctx)
	if err != nil {
		return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
	}

	// åˆ›å»ºmultipartè¡¨å•æ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	writer.WriteField("preuploadID", preuploadID)
	writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
	writer.WriteField("sliceMD5", chunkHash)

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
	}

	_, err = part.Write(data)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	err = writer.Close()
	if err != nil {
		return fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// ä¿®å¤å¡ä½é—®é¢˜ï¼šä¸ºåˆ†ç‰‡ä¸Šä¼ æ·»åŠ è¶…æ—¶æ§åˆ¶
	uploadCtx, cancel := context.WithTimeout(ctx, 5*time.Minute) // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š5åˆ†é’Ÿ
	defer cancel()

	// ä½¿ç”¨ä¸Šä¼ åŸŸåè°ƒç”¨åˆ†ç‰‡ä¸Šä¼ API
	err = f.makeAPICallWithRestMultipartToDomain(uploadCtx, uploadDomain, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d ä¸Šä¼ æˆåŠŸ", partNumber)
	return nil
}

// UploadCompleteResult ä¸Šä¼ å®Œæˆç»“æœ
type UploadCompleteResult struct {
	FileID int64  `json:"fileID"`
	Etag   string `json:"etag"`
}

// ChunkAnalysis åˆ†ç‰‡åˆ†æç»“æœ
type ChunkAnalysis struct {
	TotalChunks    int
	UploadedChunks int
	FailedChunks   int
	MissingChunks  int
	CanResume      bool
}

// completeUploadWithResultAndSize å®Œæˆå¤šéƒ¨åˆ†ä¸Šä¼ å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒæ™ºèƒ½è½®è¯¢ç­–ç•¥
// å¢å¼ºï¼šå®ç°åŠ¨æ€è½®è¯¢é—´éš”å’Œæ™ºèƒ½é”™è¯¯å¤„ç†
func (f *Fs) completeUploadWithResultAndSize(ctx context.Context, preuploadID string, fileSize int64) (*UploadCompleteResult, error) {
	fs.Debugf(f, " ä½¿ç”¨å¢å¼ºçš„ä¸Šä¼ éªŒè¯é€»è¾‘ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))

	reqBody := map[string]any{
		"preuploadID": preuploadID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			Completed bool   `json:"completed"`
			FileID    int64  `json:"fileID"`
			Etag      string `json:"etag"`
		} `json:"data"`
	}

	// ä¼˜åŒ–ï¼šæ™ºèƒ½è½®è¯¢ç­–ç•¥ï¼Œæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´å‚æ•°å’Œé˜ˆå€¼
	maxRetries, baseInterval, maxConsecutiveFailures := f.calculatePollingStrategy(fileSize)
	fs.Debugf(f, "æ™ºèƒ½è½®è¯¢ç­–ç•¥: æœ€å¤§é‡è¯•%dæ¬¡, åŸºç¡€é—´éš”%v, è¿ç»­å¤±è´¥é˜ˆå€¼%d",
		maxRetries, baseInterval, maxConsecutiveFailures)

	// ä¼˜åŒ–ï¼šæ™ºèƒ½è½®è¯¢é€»è¾‘ï¼Œæ”¯æŒåŠ¨æ€é—´éš”å’Œé”™è¯¯åˆ†ç±»
	consecutiveFailures := 0
	lastProgressTime := time.Now()
	var lastError error // è®°å½•æœ€åä¸€æ¬¡é”™è¯¯ï¼Œç”¨äºåŠ¨æ€é—´éš”è®¡ç®—

	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// ä¼˜åŒ–ï¼šåŠ¨æ€è½®è¯¢é—´éš”ï¼Œæ ¹æ®è¿ç»­å¤±è´¥æ¬¡æ•°å’Œé”™è¯¯ç±»å‹è°ƒæ•´
			interval := f.calculateDynamicInterval(baseInterval, consecutiveFailures, attempt, lastError)
			fs.Debugf(f, "è½®è¯¢é—´éš”: %v (è¿ç»­å¤±è´¥: %d, é”™è¯¯ç±»å‹: %T)", interval, consecutiveFailures, lastError)

			select {
			case <-ctx.Done():
				return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯è¢«å–æ¶ˆ: %w", ctx.Err())
			case <-time.After(interval):
				// ç»§ç»­è½®è¯¢
			}
		}

		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			lastError = err // è®°å½•é”™è¯¯ï¼Œç”¨äºä¸‹æ¬¡åŠ¨æ€é—´éš”è®¡ç®—
			consecutiveFailures++
			fs.Debugf(f, "ä¸Šä¼ å®ŒæˆAPIè°ƒç”¨å¤±è´¥ (å°è¯• %d/%d, è¿ç»­å¤±è´¥: %d/%d): %v",
				attempt+1, maxRetries, consecutiveFailures, maxConsecutiveFailures, err)

			// ä¼˜åŒ–ï¼šé›†æˆ UnifiedErrorHandler çš„æ™ºèƒ½é‡è¯•ç­–ç•¥
			if f.errorHandler != nil {
				shouldRetry, retryDelay := f.errorHandler.HandleErrorWithRetry(ctx, err, "upload_complete", attempt, maxRetries)
				if shouldRetry {
					if retryDelay > 0 {
						fs.Debugf(f, "UnifiedErrorHandlerå»ºè®®å»¶è¿Ÿ %v åé‡è¯•", retryDelay)
						select {
						case <-ctx.Done():
							return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯è¢«å–æ¶ˆ: %w", ctx.Err())
						case <-time.After(retryDelay):
						}
					}
					continue
				}
			}

			// ä¼˜åŒ–ï¼šä½¿ç”¨åŠ¨æ€è¿ç»­å¤±è´¥é˜ˆå€¼ï¼Œç½‘ç»œé”™è¯¯ç‰¹æ®Šå¤„ç†
			if common.IsNetworkError(err) && consecutiveFailures < maxConsecutiveFailures/2 {
				fs.Debugf(f, "ç½‘ç»œé”™è¯¯ï¼Œç»§ç»­é‡è¯• (è¿ç»­å¤±è´¥: %d/%d)", consecutiveFailures, maxConsecutiveFailures/2)
				continue // ç½‘ç»œé”™è¯¯çš„å®¹é”™æ€§æ›´é«˜
			}
			if consecutiveFailures >= maxConsecutiveFailures {
				return nil, fmt.Errorf("è¿ç»­APIè°ƒç”¨å¤±è´¥æ¬¡æ•°è¿‡å¤š (%d/%d): %w",
					consecutiveFailures, maxConsecutiveFailures, err)
			}
			continue
		}

		// é‡ç½®è¿ç»­å¤±è´¥è®¡æ•°
		consecutiveFailures = 0
		lastProgressTime = time.Now()

		if response.Code == 0 && response.Data.Completed {
			totalElapsed := time.Since(lastProgressTime)
			fs.Infof(f, "âœ… ä¸Šä¼ éªŒè¯æˆåŠŸå®Œæˆ (æ€»è€—æ—¶: %v, å°è¯•æ¬¡æ•°: %d/%d)",
				totalElapsed, attempt+1, maxRetries)
			return &UploadCompleteResult{
				FileID: response.Data.FileID,
				Etag:   response.Data.Etag,
			}, nil
		}

		if response.Code == 0 && !response.Data.Completed {
			// ä¼˜åŒ–ï¼šcompleted=falseçŠ¶æ€çš„æ™ºèƒ½å¤„ç†å’Œè¯¦ç»†æ—¥å¿—
			elapsed := time.Since(lastProgressTime)
			progress := float64(attempt+1) / float64(maxRetries) * 100

			fs.Debugf(f, "ğŸ“‹ æœåŠ¡å™¨è¿”å›completed=falseï¼Œç»§ç»­è½®è¯¢ (è¿›åº¦: %.1f%%, %d/%d, å·²ç­‰å¾…: %v)",
				progress, attempt+1, maxRetries, elapsed)

			// ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è­¦å‘Šé˜ˆå€¼
			warningThreshold := 5 * time.Minute
			if fileSize > 1*1024*1024*1024 { // å¤§äº1GB
				warningThreshold = 15 * time.Minute
			}

			if elapsed > warningThreshold {
				fs.Logf(f, "âš ï¸ éªŒè¯ç­‰å¾…æ—¶é—´è¿‡é•¿(%v)ï¼Œæ–‡ä»¶å¤§å°: %sï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜",
					elapsed, fs.SizeSuffix(fileSize))
			}
			continue
		}

		// å¢å¼ºï¼šé”™è¯¯ä»£ç åˆ†ç±»å¤„ç†
		if f.isRetryableError(response.Code) {
			fs.Debugf(f, "âš ï¸ é‡åˆ°å¯é‡è¯•é”™è¯¯ %d: %sï¼Œç»§ç»­è½®è¯¢", response.Code, response.Message)
			continue
		}

		// å…¶ä»–é”™è¯¯ï¼Œç›´æ¥è¿”å›
		return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯å¤±è´¥: API error %d: %s", response.Code, response.Message)
	}

	return nil, fmt.Errorf("ä¸Šä¼ éªŒè¯è¶…æ—¶ï¼Œå·²è½®è¯¢%dæ¬¡", maxRetries)
}

// uploadPartStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ åˆ†ç‰‡ï¼Œé¿å…å¤§å†…å­˜å ç”¨
func (f *Fs) uploadPartStream(ctx context.Context, uploadURL string, reader io.Reader, size int64) error {
	req, err := http.NewRequestWithContext(ctx, "PUT", uploadURL, reader)
	if err != nil {
		return fmt.Errorf("failed to create stream upload request: %w", err)
	}

	req.ContentLength = size
	req.Header.Set("Content-Type", "application/octet-stream")

	// ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶æœºåˆ¶
	adaptiveTimeout := f.getAdaptiveTimeout(size, "chunked_upload")
	var cancel context.CancelFunc
	ctx, cancel = context.WithTimeout(ctx, adaptiveTimeout)
	defer cancel()
	req = req.WithContext(ctx)

	resp, err := f.client.Do(req)
	if err != nil {
		return fmt.Errorf("stream upload request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusCreated {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("stream upload failed with status %d: %s", resp.StatusCode, string(body))
	}

	return nil
}

// streamingPut ç»Ÿä¸€çš„æµå¼ä¸Šä¼ å…¥å£ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“ä¼˜åŒ–
func (f *Fs) streamingPut(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è¿›å…¥streamingPutå‡½æ•°ï¼Œæ–‡ä»¶: %s", fileName)

	// é€šç”¨è·¨äº‘ä¼ è¾“æ£€æµ‹ï¼šæ£€æµ‹æ˜¯å¦æ¥è‡ªå…¶ä»–äº‘å­˜å‚¨æœåŠ¡
	srcFsString := src.Fs().String()
	currentFsString := f.String()

	// æ£€æµ‹è·¨äº‘ä¼ è¾“ï¼šæºå’Œç›®æ ‡æ˜¯ä¸åŒçš„æ–‡ä»¶ç³»ç»Ÿ
	if srcFsString != currentFsString {
		// è¿›ä¸€æ­¥æ£€æµ‹æ˜¯å¦ä¸ºäº‘å­˜å‚¨åˆ°äº‘å­˜å‚¨çš„ä¼ è¾“ï¼ˆæ’é™¤æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼‰
		if !strings.Contains(srcFsString, "local") && !strings.Contains(currentFsString, "local") {
			fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s â†’ %sï¼Œå¯ç”¨ä¼˜åŒ–ä¼ è¾“æ¨¡å¼", srcFsString, currentFsString)
			return f.handleCrossCloudTransfer(ctx, in, src, parentFileID, fileName)
		}
	}

	// è·¨äº‘ä¼ è¾“æ£€æµ‹å’Œå¤„ç†
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“: %s â†’ 123ç½‘ç›˜ (å¤§å°: %s)", src.Fs().Name(), fileName, fs.SizeSuffix(src.Size()))
		fs.Infof(f, "ğŸ“¤ ç›´æ¥ä½¿ç”¨æºäº‘ç›˜æ•°æ®æµè¿›è¡Œ123ç½‘ç›˜ä¸Šä¼ ...")
		// å…³é”®ä¿®å¤ï¼šä½¿ç”¨ç›´æ¥ä¼ è¾“è€Œä¸æ˜¯æœ¬åœ°åŒ–ç¼“å­˜ï¼Œé¿å…äºŒæ¬¡ä¸‹è½½
		return f.handleCrossCloudTransfer(ctx, in, src, parentFileID, fileName)
	}

	// æœ¬åœ°æ–‡ä»¶ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å…¥å£
	fs.Debugf(f, "æœ¬åœ°æ–‡ä»¶ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å…¥å£è¿›è¡Œæµå¼ä¸Šä¼ ")
	return f.unifiedUpload(ctx, in, src, parentFileID, fileName)
}

// handleCrossCloudTransfer ç»Ÿä¸€çš„è·¨äº‘ä¼ è¾“å¤„ç†å‡½æ•°
// ä¼˜åŒ–ï¼šé›†æˆè·¨äº‘ä¼ è¾“åè°ƒå™¨ï¼Œé¿å…é‡å¤ä¸‹è½½å’ŒçŠ¶æ€æ··ä¹±
func (f *Fs) handleCrossCloudTransfer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸŒ å¼€å§‹ç»Ÿä¸€è·¨äº‘ä¼ è¾“å¤„ç†: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// ä¼˜åŒ–ï¼šä½¿ç”¨è·¨äº‘ä¼ è¾“åè°ƒå™¨ç®¡ç†ä¼ è¾“çŠ¶æ€
	if f.crossCloudCoordinator != nil {
		transfer, err := f.crossCloudCoordinator.StartTransfer(ctx, src, f, fileName)
		if err != nil {
			fs.Errorf(f, "å¯åŠ¨è·¨äº‘ä¼ è¾“åè°ƒå¤±è´¥: %v", err)
			// å›é€€åˆ°åŸæœ‰é€»è¾‘
		} else {
			defer func() {
				// æ ¹æ®æœ€ç»ˆç»“æœæ›´æ–°ä¼ è¾“çŠ¶æ€
				if err != nil {
					f.crossCloudCoordinator.CompleteTransfer(transfer.TransferID, false, err)
				} else {
					f.crossCloudCoordinator.CompleteTransfer(transfer.TransferID, true, nil)
				}
			}()
		}
	}

	// æ™ºèƒ½é‡è¯•æ„ŸçŸ¥ï¼šæ£€æµ‹æ˜¯å¦ä¸ºé‡è¯•æƒ…å†µï¼Œé¿å…é‡å¤ä¸‹è½½
	if cachedMD5, found := f.getCachedMD5(src); found {
		fs.Infof(f, "ğŸ¯ æ£€æµ‹åˆ°é‡è¯•æƒ…å†µï¼Œæ–‡ä»¶å·²ä¸‹è½½è¿‡ï¼Œç›´æ¥å°è¯•ä¸Šä¼  (MD5: %s)", cachedMD5)
		return f.uploadWithKnownMD5FromRetry(ctx, src, parentFileID, fileName, cachedMD5)
	}

	// ä¼˜åŒ–ï¼šæ£€æŸ¥åè°ƒå™¨ä¸­æ˜¯å¦æœ‰å·²ä¸‹è½½çš„æ–‡ä»¶
	if f.crossCloudCoordinator != nil {
		if reader, size, cleanup, err := f.crossCloudCoordinator.CheckExistingDownload(ctx, src); err == nil && reader != nil {
			fs.Infof(f, "ğŸ¯ åè°ƒå™¨å‘ç°å·²ä¸‹è½½æ–‡ä»¶ï¼Œé¿å…é‡å¤ä¸‹è½½ (å¤§å°: %s)", fs.SizeSuffix(size))
			defer cleanup()
			// ä¿®å¤é€’å½’ï¼šç›´æ¥å¤„ç†å·²ä¸‹è½½çš„æ–‡ä»¶ï¼Œä¸è°ƒç”¨unifiedUploadé¿å…é€’å½’
			return f.uploadFromDownloadedData(ctx, reader, src, parentFileID, fileName)
		}
	}

	// é¢„ä¸‹è½½æ•°æ®å¤„ç†ï¼šå¦‚æœæœ‰Readerï¼Œç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®
	if in != nil {
		fs.Infof(f, "ğŸ“¤ ä½¿ç”¨å·²ä¸‹è½½æ•°æ®è¿›è¡Œä¸Šä¼ ï¼Œé¿å…é‡å¤ä¸‹è½½")

		// å°è¯•ç§’ä¼ æ£€æŸ¥ï¼ˆå¦‚æœæºå¯¹è±¡æ”¯æŒMD5ï¼‰
		if srcObj, ok := src.(fs.Object); ok {
			if srcHash, err := srcObj.Hash(ctx, fshash.MD5); err == nil && srcHash != "" {
				fs.Infof(f, "ğŸ” å°è¯•ç§’ä¼ æ£€æŸ¥ï¼ŒMD5: %s", srcHash)
				// ç§’ä¼ åŠŸèƒ½æš‚æœªå®ç°ï¼Œç»§ç»­æ­£å¸¸ä¸Šä¼ 
			}
		}

		// ä¿®å¤é€’å½’ï¼šç›´æ¥å¤„ç†å·²ä¸‹è½½çš„æ•°æ®ï¼Œä¸è°ƒç”¨unifiedUploadé¿å…é€’å½’
		return f.uploadFromDownloadedData(ctx, in, src, parentFileID, fileName)
	}

	// ç®€åŒ–ä¼ è¾“å¤„ç†ï¼šç›´æ¥å®ç°ä¸‹è½½â†’è®¡ç®—MD5â†’ä¸Šä¼ æµç¨‹ï¼Œé¿å…å¤æ‚çš„ç­–ç•¥é€‰æ‹©å¯¼è‡´é€’å½’
	fs.Infof(f, "ğŸ”„ æ‰§è¡Œç®€åŒ–è·¨äº‘ä¼ è¾“æµç¨‹ï¼šä¸‹è½½â†’è®¡ç®—MD5â†’ä¸Šä¼ ")

	// è·å–åº•å±‚çš„fs.Object
	var srcObj fs.Object
	if obj, ok := src.(fs.Object); ok {
		srcObj = obj
	} else if overrideRemote, ok := src.(*fs.OverrideRemote); ok {
		srcObj = overrideRemote.UnWrap()
		if srcObj == nil {
			return nil, fmt.Errorf("æ— æ³•ä»fs.OverrideRemoteä¸­è§£åŒ…å‡ºfs.Object")
		}
	} else {
		srcObj = fs.UnWrapObjectInfo(src)
		if srcObj == nil {
			return nil, fmt.Errorf("æºå¯¹è±¡ç±»å‹ %T æ— æ³•è§£åŒ…ä¸ºfs.Objectï¼Œæ— æ³•è¿›è¡Œè·¨äº‘ä¼ è¾“", src)
		}
	}

	// ç›´æ¥å®ç°ç®€åŒ–çš„è·¨äº‘ä¼ è¾“ï¼šæ‰“å¼€æºæ–‡ä»¶â†’è¯»å–æ•°æ®â†’è®¡ç®—MD5â†’ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¥ æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œè·¨äº‘ä¼ è¾“")
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer srcReader.Close()

	// ç›´æ¥ä½¿ç”¨uploadFromDownloadedDataå¤„ç†æ•°æ®æµ
	return f.uploadFromDownloadedData(ctx, srcReader, src, parentFileID, fileName)
}

// getCachedUploadDomain è·å–ç¼“å­˜çš„ä¸Šä¼ åŸŸåï¼Œé¿å…é¢‘ç¹APIè°ƒç”¨
// æ€§èƒ½ä¼˜åŒ–ï¼šåŸºäºæ—¥å¿—åˆ†æï¼Œæ¯ä¸ªåˆ†ç‰‡éƒ½é‡æ–°è·å–åŸŸåæµªè´¹æ—¶é—´
func (f *Fs) getCachedUploadDomain(ctx context.Context) (string, error) {
	// æ£€æŸ¥ç¼“å­˜çš„ä¸Šä¼ åŸŸåæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
	if f.uploadDomainCache != "" && time.Since(f.uploadDomainCacheTime) < 5*time.Minute {
		fs.Debugf(f, "ğŸš€ ä½¿ç”¨ç¼“å­˜çš„ä¸Šä¼ åŸŸå: %s", f.uploadDomainCache)
		return f.uploadDomainCache, nil
	}

	// ç¼“å­˜è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è·å–
	fs.Debugf(f, "ğŸ”„ ä¸Šä¼ åŸŸåç¼“å­˜è¿‡æœŸï¼Œé‡æ–°è·å–")
	domain, err := f.getUploadDomain(ctx)
	if err != nil {
		return "", err
	}

	// æ›´æ–°ç¼“å­˜
	f.uploadDomainCache = domain
	f.uploadDomainCacheTime = time.Now()

	fs.Debugf(f, " ä¸Šä¼ åŸŸåç¼“å­˜æ›´æ–°: %s", domain)
	return domain, nil
}

// uploadFromDownloadedData ä»å·²ä¸‹è½½çš„æ•°æ®ç›´æ¥ä¸Šä¼ ï¼Œé¿å…é€’å½’è°ƒç”¨
func (f *Fs) uploadFromDownloadedData(ctx context.Context, reader io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ“¤ ä»å·²ä¸‹è½½æ•°æ®ç›´æ¥ä¸Šä¼ : %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: è®¡ç®—MD5å“ˆå¸Œï¼ˆ123ç½‘ç›˜APIå¿…éœ€ï¼‰
	fs.Infof(f, "ğŸ” è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ...")
	md5StartTime := time.Now()

	// è¯»å–æ‰€æœ‰æ•°æ®å¹¶è®¡ç®—MD5
	data, err := io.ReadAll(reader)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–å·²ä¸‹è½½æ•°æ®å¤±è´¥: %w", err)
	}

	if int64(len(data)) != fileSize {
		return nil, fmt.Errorf("æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(int64(len(data))))
	}

	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration := time.Since(md5StartTime)
	fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆ: %sï¼Œè€—æ—¶: %v", md5Hash, md5Duration.Round(time.Second))

	// æ­¥éª¤2: å°è¯•ç§’ä¼ 
	fs.Infof(f, "âš¡ å°è¯•123ç½‘ç›˜ç§’ä¼ ...")
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		fs.Debugf(f, "ç§’ä¼ æ£€æŸ¥å¤±è´¥: %v", err)
	} else if isInstant {
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼èŠ‚çœä¸Šä¼ æ—¶é—´")
		return f.createObjectFromUpload(fileName, fileSize, md5Hash, createResp.Data.FileID, src.ModTime(ctx)), nil
	}

	// æ­¥éª¤3: ç§’ä¼ å¤±è´¥ï¼Œåˆ›å»ºä¸Šä¼ ä¼šè¯å¹¶å®é™…ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ : %s", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆç°åœ¨æœ‰äº†MD5ï¼‰
	createResp, err = f.createUploadV2(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// å‡†å¤‡ä¸Šä¼ è¿›åº¦ä¿¡æ¯
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		defaultNetworkSpeed := int64(20 * 1024 * 1024) // 20MB/s
		chunkSize = f.getOptimalChunkSize(fileSize, defaultNetworkSpeed)
	}
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	progress, err := f.prepareUploadProgress(createResp.Data.PreuploadID, totalChunks, chunkSize, fileSize, fileName, md5Hash, src.Remote())
	if err != nil {
		return nil, fmt.Errorf("å‡†å¤‡ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å†…å­˜ä¸­çš„æ•°æ®è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ 
	uploadStartTime := time.Now()
	concurrencyParams := f.calculateConcurrencyParams(fileSize)
	maxConcurrency := concurrencyParams.optimal

	result, err := f.v2UploadChunksWithConcurrency(ctx, bytes.NewReader(data), &localFileInfo{
		name:    fileName,
		size:    fileSize,
		modTime: src.ModTime(ctx),
	}, createResp, progress, fileName, maxConcurrency)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(fileSize) / uploadDuration.Seconds() / (1024 * 1024) // MB/s

	fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“å®Œæˆ: %sï¼ŒMD5è®¡ç®—: %vï¼Œä¸Šä¼ : %vï¼Œä¸Šä¼ é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(fileSize), md5Duration.Round(time.Second), uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// calculateOptimalChunkSize è®¡ç®—å»ºè®®çš„åˆ†ç‰‡å¤§å°ï¼ˆä»…ç”¨äºä¼°ç®—ï¼‰
// æ³¨æ„ï¼šå®é™…ä¸Šä¼ æ—¶ä¼šä¸¥æ ¼ä½¿ç”¨APIè¿”å›çš„SliceSizeï¼Œæ­¤å‡½æ•°ä»…ç”¨äºæ€§èƒ½ä¼°ç®—
func (f *Fs) calculateOptimalChunkSize(fileSize int64) int64 {
	// åŠ¨æ€åˆ†ç‰‡ç­–ç•¥ï¼šåŸºäºæ–‡ä»¶å¤§å°æä¾›å»ºè®®ï¼Œä½†å®é™…ä½¿ç”¨APIè¿”å›çš„SliceSize
	// è¿™ä¸ªå‡½æ•°ä¸»è¦ç”¨äºæ€§èƒ½ä¼°ç®—å’Œè¿›åº¦è®¡ç®—ï¼Œä¸å½±å“å®é™…ä¸Šä¼ åˆ†ç‰‡å¤§å°
	switch {
	case fileSize < 50*1024*1024: // <50MB
		return 16 * 1024 * 1024 // 16MBå»ºè®®åˆ†ç‰‡
	case fileSize < 200*1024*1024: // <200MB
		return 32 * 1024 * 1024 // 32MBå»ºè®®åˆ†ç‰‡
	case fileSize < 1*1024*1024*1024: // <1GB
		return 64 * 1024 * 1024 // 64MBå»ºè®®åˆ†ç‰‡ï¼ˆå¸¸è§APIè¿”å›å€¼ï¼‰
	case fileSize < 5*1024*1024*1024: // <5GB
		return 128 * 1024 * 1024 // 128MBå»ºè®®åˆ†ç‰‡
	default: // >5GB
		return 256 * 1024 * 1024 // 256MBå»ºè®®åˆ†ç‰‡
	}
}

// uploadWithKnownMD5FromRetry åœ¨é‡è¯•æƒ…å†µä¸‹ä½¿ç”¨å·²çŸ¥MD5ç›´æ¥ä¸Šä¼ 
// ä¿®å¤é‡å¤ä¸‹è½½ï¼šé¿å…é‡æ–°ä¸‹è½½ï¼Œç›´æ¥å°è¯•ä¸Šä¼ 
func (f *Fs) uploadWithKnownMD5FromRetry(ctx context.Context, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	fs.Infof(f, "ğŸš€ é‡è¯•ä¸Šä¼ ï¼Œä½¿ç”¨å·²çŸ¥MD5: %s", md5Hash)

	// å°è¯•ç§’ä¼ 
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		fs.Debugf(f, "é‡è¯•æ—¶åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %v", err)
		// å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´ä¸‹è½½æµç¨‹
		fs.Infof(f, "âš ï¸ é‡è¯•ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°å®Œæ•´ä¸‹è½½æµç¨‹")
		return f.handleCrossCloudTransfer(ctx, nil, src, parentFileID, fileName)
	}

	// æ£€æŸ¥æ˜¯å¦ç§’ä¼ æˆåŠŸ
	if createResp.Data.Reuse {
		fs.Infof(f, "âœ… é‡è¯•æ—¶ç§’ä¼ æˆåŠŸ: %s", fileName)
		return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
	}

	// å¦‚æœä¸èƒ½ç§’ä¼ ï¼Œè¯´æ˜éœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶ï¼Œå›é€€åˆ°å®Œæ•´æµç¨‹
	fs.Infof(f, "âš ï¸ é‡è¯•æ—¶æ— æ³•ç§’ä¼ ï¼Œéœ€è¦é‡æ–°ä¸‹è½½æ–‡ä»¶è¿›è¡Œä¸Šä¼ ")
	return f.handleCrossCloudTransfer(ctx, nil, src, parentFileID, fileName)
}

// selectOptimalCrossCloudStrategy é€‰æ‹©æœ€ä¼˜çš„è·¨äº‘ä¼ è¾“ç­–ç•¥
// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šç»Ÿä¸€ç­–ç•¥é€‰æ‹©ï¼Œé¿å…é‡å¤æ‰“å¼€æºæ–‡ä»¶
func (f *Fs) selectOptimalCrossCloudStrategy(ctx context.Context, srcObj fs.Object, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ¯ é€‰æ‹©è·¨äº‘ä¼ è¾“ç­–ç•¥: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// å†…å­˜ä¼˜åŒ–ï¼šæ ¹æ®æ–‡ä»¶å¤§å°å’Œå†…å­˜ä½¿ç”¨æƒ…å†µé€‰æ‹©æœ€ä¼˜ä¼ è¾“ç­–ç•¥
	switch {
	case fileSize <= memoryBufferThreshold && f.memoryOptimizer.ShouldUseMemoryBuffer(fileSize): // å†…å­˜ç¼“å†²ç­–ç•¥
		fs.Infof(f, "ğŸ“ é€‰æ‹©å†…å­˜ç¼“å†²ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.memoryBufferedCrossCloudTransferDirect(ctx, srcObj, src, parentFileID, fileName)
	case fileSize <= streamingTransferThreshold: // æµå¼ä¼ è¾“ç­–ç•¥
		fs.Infof(f, "ğŸŒŠ é€‰æ‹©æµå¼ä¼ è¾“ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.streamingCrossCloudTransferDirect(ctx, srcObj, src, parentFileID, fileName)
	case fileSize <= concurrentDownloadThreshold: // å¹¶å‘ä¸‹è½½ç­–ç•¥
		fs.Infof(f, "ğŸš€ é€‰æ‹©å¹¶å‘ä¸‹è½½ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.concurrentDownloadCrossCloudTransfer(ctx, srcObj, parentFileID, fileName)
	default: // å¤§æ–‡ä»¶ - ä¼˜åŒ–çš„ç£ç›˜ç­–ç•¥
		fs.Infof(f, "ğŸ’¾ é€‰æ‹©ä¼˜åŒ–ç£ç›˜ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.optimizedDiskCrossCloudTransferDirect(ctx, srcObj, src, parentFileID, fileName)
	}
}

// streamingCrossCloudTransferDirect æµå¼è·¨äº‘ä¼ è¾“ï¼ˆ50-100MBæ–‡ä»¶ï¼‰
func (f *Fs) streamingCrossCloudTransferDirect(ctx context.Context, srcObj fs.Object, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸŒŠ å¼€å§‹æµå¼è·¨äº‘ä¼ è¾“ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: æ‰“å¼€æºæ–‡ä»¶æµ
	startTime := time.Now()
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer srcReader.Close()

	// æ­¥éª¤2: ä½¿ç”¨æµå¼å¤„ç†å™¨å¤„ç†æ•°æ®
	streamProcessor := f.memoryOptimizer.NewStreamProcessor()
	defer f.memoryOptimizer.ReleaseStreamProcessor(streamProcessor)

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæµå¼å¤„ç†
	tempFile, err := f.memoryOptimizer.OptimizedTempFile("streaming_transfer_", fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.memoryOptimizer.ReleaseTempFile(tempFile)

	// æ­¥éª¤3: æµå¼å¤åˆ¶æ•°æ®å¹¶è®¡ç®—å“ˆå¸Œ
	fs.Infof(f, "ğŸ” å¼€å§‹æµå¼å¤„ç†å’Œå“ˆå¸Œè®¡ç®—...")
	md5StartTime := time.Now()

	copiedSize, md5Hash, err := streamProcessor.StreamCopy(tempFile.File(), srcReader)
	if err != nil {
		return nil, fmt.Errorf("æµå¼å¤„ç†å¤±è´¥: %w", err)
	}

	downloadDuration := time.Since(startTime)
	md5Duration := time.Since(md5StartTime)

	fs.Infof(f, "ğŸ“¥ æµå¼å¤„ç†å®Œæˆ: %s, ä¸‹è½½ç”¨æ—¶: %v, MD5ç”¨æ—¶: %v",
		fs.SizeSuffix(copiedSize), downloadDuration, md5Duration)

	// æ­¥éª¤4: é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¹¶ä¸Šä¼ 
	_, err = tempFile.File().Seek(0, 0)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	return f.uploadFromTempFile(ctx, tempFile.File(), md5Hash, src, parentFileID, fileName, downloadDuration, md5Duration, startTime)
}

// uploadFromTempFile ä»ä¸´æ—¶æ–‡ä»¶ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadFromTempFile(ctx context.Context, tempFile *os.File, md5Hash string, src fs.ObjectInfo, parentFileID int64, fileName string, downloadDuration, md5Duration time.Duration, startTime time.Time) (*Object, error) {
	fileSize := src.Size()

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, err
	}

	if isInstant {
		// ç§’ä¼ æˆåŠŸ
		totalDuration := time.Since(startTime)
		fs.Infof(f, "âš¡ 123ç½‘ç›˜ç§’ä¼ æˆåŠŸ: %s, æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v, MD5: %v)",
			fs.SizeSuffix(fileSize), totalDuration, downloadDuration, md5Duration)

		return f.createObjectFromUpload(fileName, fileSize, md5Hash, createResp.Data.FileID, src.ModTime(ctx)), nil
	}

	// æ­¥éª¤4: éœ€è¦å®é™…ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ å¼€å§‹å®é™…ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	// ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œä¸Šä¼ 
	err = f.uploadFile(ctx, tempFile, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœ
	result, err := f.completeUploadWithResultAndSize(ctx, createResp.Data.PreuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadedObj := f.createObject(fileName, result.FileID, fileSize, md5Hash, src.ModTime(ctx), false)

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	fs.Infof(f, "âœ… 123ç½‘ç›˜æµå¼ä¼ è¾“å®Œæˆ: %s, æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v, MD5: %v, ä¸Šä¼ : %v)",
		fs.SizeSuffix(fileSize), totalDuration, downloadDuration, md5Duration, uploadDuration)

	return uploadedObj, nil
}

// concurrentDownloadCrossCloudTransfer å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“
func (f *Fs) concurrentDownloadCrossCloudTransfer(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fileSize := srcObj.Size()
	fs.Infof(f, "ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// ç®€åŒ–è·¨äº‘ä¼ è¾“é€»è¾‘ï¼šç›´æ¥æ‰§è¡Œä¸‹è½½-ä¸Šä¼ æµç¨‹

	// ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šåˆ›å»ºè·¨äº‘ä¼ è¾“çš„Transferå¯¹è±¡ï¼Œé›†æˆåˆ°rclone accountingç³»ç»Ÿ
	stats := f.getGlobalStats(ctx)
	var downloadTransfer *accounting.Transfer
	if stats != nil {
		// è·å–æºæ–‡ä»¶ç³»ç»Ÿ
		var srcFs fs.Fs
		if fsInfo := srcObj.Fs(); fsInfo != nil {
			// å°è¯•è½¬æ¢ä¸ºFsæ¥å£
			if fs, ok := fsInfo.(fs.Fs); ok {
				srcFs = fs
			}
		}

		// åˆ›å»ºä¸‹è½½é˜¶æ®µçš„Transferå¯¹è±¡
		downloadTransfer = stats.NewTransferRemoteSize(
			fileName, // ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œä¸æ·»åŠ "ä¸‹è½½é˜¶æ®µ"æ ‡è®°
			fileSize,
			srcFs,
			f,
		)
		defer downloadTransfer.Done(ctx, nil)

		// è°ƒè¯•æ—¥å¿—å·²ä¼˜åŒ–ï¼šå¯¹è±¡åˆ›å»ºè¯¦æƒ…ä»…åœ¨è¯¦ç»†æ¨¡å¼ä¸‹è¾“å‡º
	}

	// åŸºäºæ—¥å¿—åˆ†æä¼˜åŒ–åˆ†ç‰‡å‚æ•°
	// æ—¥å¿—æ˜¾ç¤ºï¼š32Miåˆ†ç‰‡ï¼Œ50åˆ†ç‰‡ï¼Œ4å¹¶å‘ï¼Œé€Ÿåº¦131.912Mi/sï¼ˆä¼˜ç§€ï¼‰
	// ä½†ä¸Šä¼ é˜¶æ®µç”¨æ—¶6åˆ†47ç§’ï¼Œæ˜¯ä¸»è¦ç“¶é¢ˆï¼Œéœ€è¦ä¼˜åŒ–ä¸Šä¼ ç­–ç•¥
	chunkSize := f.calculateOptimalChunkSize(fileSize) // åŠ¨æ€è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
	if fileSize < chunkSize*2 {
		// æ–‡ä»¶å¤ªå°ï¼Œä¸å€¼å¾—å¹¶å‘ä¸‹è½½
		fs.Debugf(f, "æ–‡ä»¶å¤ªå°ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½")
		// ä¿®å¤å¤šé‡å¹¶å‘ä¸‹è½½ï¼šä½¿ç”¨ç‰¹æ®Šé€‰é¡¹é¿å…è§¦å‘æºå¯¹è±¡çš„å¹¶å‘ä¸‹è½½
		srcReader, err := f.openSourceWithoutConcurrency(ctx, srcObj)
		if err != nil {
			return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer srcReader.Close()
		return f.optimizedDiskCrossCloudTransfer(ctx, srcReader, srcObj, parentFileID, fileName)
	}

	numChunks := (fileSize + chunkSize - 1) / chunkSize
	maxConcurrency := int64(4) // ç”¨æˆ·è¦æ±‚ï¼š123ç½‘ç›˜å•æ–‡ä»¶å¹¶å‘é™åˆ¶åˆ°4
	maxConcurrency = min(numChunks, maxConcurrency)

	fs.Infof(f, "ğŸ“Š ä¼˜åŒ–åå¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºç»„è£…
	tempFile, err := os.CreateTemp("", "concurrent_download_*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// å¹¶å‘ä¸‹è½½å„ä¸ªåˆ†ç‰‡
	startTime := time.Now()
	err = f.downloadChunksConcurrently(ctx, srcObj, tempFile, chunkSize, numChunks, maxConcurrency)
	downloadDuration := time.Since(startTime)

	if err != nil {
		// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šæ™ºèƒ½å›é€€æœºåˆ¶ï¼Œé¿å…ä¸å¿…è¦çš„é‡æ–°ä¸‹è½½
		fs.Debugf(f, "å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œåˆ†æå¤±è´¥åŸå› : %v", err)

		// ä½¿ç”¨ç»Ÿä¸€é”™è¯¯å¤„ç†å™¨åˆ†æé”™è¯¯ç±»å‹
		if f.errorHandler != nil {
			wrappedErr := f.errorHandler.HandleSpecificError(err, "è·¨äº‘ä¼ è¾“")
			if strings.Contains(strings.ToLower(wrappedErr.Error()), "ç›®å½•ç»“æ„") ||
				strings.Contains(strings.ToLower(wrappedErr.Error()), "ä¸Šä¼ ") {
				fs.Errorf(f, "ğŸš¨ æ£€æµ‹åˆ°ä¸Šä¼ ç›¸å…³é”™è¯¯ï¼Œä¸åº”è¯¥é‡æ–°ä¸‹è½½æºæ–‡ä»¶: %v", wrappedErr)
				return nil, fmt.Errorf("è·¨äº‘ä¼ è¾“ä¸Šä¼ é˜¶æ®µå¤±è´¥: %w", wrappedErr)
			}
		}

		// æ£€æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†æ•°æ®å·²ä¸‹è½½æˆåŠŸ
		stat, statErr := tempFile.Stat()
		if statErr == nil && stat.Size() > 0 {
			partialSize := stat.Size()
			completionRate := float64(partialSize) / float64(fileSize) * 100

			fs.Infof(f, "ğŸ”„ æ£€æµ‹åˆ°éƒ¨åˆ†ä¸‹è½½æ•°æ®: %s (%.1f%%)ï¼Œå°è¯•æ™ºèƒ½æ¢å¤",
				fs.SizeSuffix(partialSize), completionRate)

			// å¦‚æœå·²ä¸‹è½½è¶…è¿‡30%ï¼Œå°è¯•æ–­ç‚¹ç»­ä¼ æ¢å¤
			if completionRate > 30.0 {
				fs.Infof(f, "ğŸš€ å°è¯•æ–­ç‚¹ç»­ä¼ æ¢å¤...")
				if resumeErr := f.attemptResumeDownload(ctx, srcObj, tempFile, chunkSize, numChunks, maxConcurrency); resumeErr == nil {
					fs.Infof(f, "âœ… æ–­ç‚¹ç»­ä¼ æ¢å¤æˆåŠŸ")
					downloadDuration = time.Since(startTime) // æ›´æ–°æ€»ä¸‹è½½æ—¶é—´
				} else {
					fs.Debugf(f, "æ–­ç‚¹ç»­ä¼ æ¢å¤å¤±è´¥: %vï¼Œè°¨æ…å›é€€", resumeErr)
					// ä¿®å¤ï¼šè°¨æ…å›é€€ï¼Œé¿å…é‡å¤ä¸‹è½½
					fs.Errorf(f, "âš ï¸ æ–­ç‚¹ç»­ä¼ æ¢å¤å¤±è´¥ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œæˆ–é‡è¯•æ•´ä¸ªæ“ä½œ")
					return nil, fmt.Errorf("æ–­ç‚¹ç»­ä¼ æ¢å¤å¤±è´¥ï¼Œé¿å…é‡å¤ä¸‹è½½: %w", resumeErr)
				}
			} else {
				fs.Debugf(f, "éƒ¨åˆ†ä¸‹è½½æ•°æ®ä¸è¶³30%%ï¼Œè°¨æ…å¤„ç†")
				// ä¿®å¤ï¼šé¿å…ç«‹å³é‡æ–°ä¸‹è½½
				fs.Errorf(f, "âš ï¸ éƒ¨åˆ†ä¸‹è½½æ•°æ®ä¸è¶³ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼Œå»ºè®®é‡è¯•æ•´ä¸ªæ“ä½œ")
				return nil, fmt.Errorf("éƒ¨åˆ†ä¸‹è½½æ•°æ®ä¸è¶³ï¼Œé¿å…é‡å¤ä¸‹è½½: %w", err)
			}
		} else {
			fs.Debugf(f, "æ— æœ‰æ•ˆçš„éƒ¨åˆ†ä¸‹è½½æ•°æ®ï¼Œè°¨æ…å¤„ç†")
			// ä¿®å¤ï¼šé¿å…ç«‹å³é‡æ–°ä¸‹è½½
			fs.Errorf(f, "âš ï¸ æ— æœ‰æ•ˆä¸‹è½½æ•°æ®ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼Œå»ºè®®é‡è¯•æ•´ä¸ªæ“ä½œ")
			return nil, fmt.Errorf("æ— æœ‰æ•ˆä¸‹è½½æ•°æ®ï¼Œé¿å…é‡å¤ä¸‹è½½: %w", err)
		}
	}

	fs.Infof(f, "ğŸ“¥ å¹¶å‘ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¹¶ç»§ç»­åç»­å¤„ç†
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	// ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šåˆ›å»ºä¸Šä¼ é˜¶æ®µçš„Transferå¯¹è±¡
	var uploadTransfer *accounting.Transfer
	if stats != nil {
		uploadTransfer = stats.NewTransferRemoteSize(
			fmt.Sprintf("ğŸ“¤ %s (ä¸Šä¼ é˜¶æ®µ)", fileName),
			fileSize,
			f, // æºæ˜¯ä¸´æ—¶æ–‡ä»¶ï¼Œç›®æ ‡æ˜¯123ç½‘ç›˜
			f,
		)
		defer uploadTransfer.Done(ctx, nil)
		fs.Debugf(f, " åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸Šä¼ é˜¶æ®µTransfer: %s", fileName)
	}

	// ç»§ç»­MD5è®¡ç®—å’Œä¸Šä¼ æµç¨‹
	result, err := f.continueAfterDownload(ctx, tempFile, srcObj, parentFileID, fileName, downloadDuration)
	if err != nil {
		return nil, err
	}

	// è·¨äº‘ä¼ è¾“æˆåŠŸå®Œæˆ

	return result, nil
}

// downloadChunksConcurrently å¹¶å‘ä¸‹è½½æ–‡ä»¶åˆ†ç‰‡ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) downloadChunksConcurrently(ctx context.Context, srcObj fs.Object, tempFile *os.File, chunkSize, numChunks, maxConcurrency int64) error {
	// åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
	progress := NewDownloadProgress(numChunks, srcObj.Size())

	// ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šç›´æ¥ä½¿ç”¨è·¨äº‘ä¼ è¾“çš„Transferå¯¹è±¡
	var currentAccount *accounting.Account

	{
		// å›é€€ï¼šå°è¯•ä»å…¨å±€ç»Ÿè®¡æŸ¥æ‰¾ç°æœ‰Account
		stats := accounting.GlobalStats()
		if stats != nil {
			// é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
			if acc := stats.GetInProgressAccount(srcObj.Remote()); acc != nil {
				currentAccount = acc
				fs.Debugf(f, " æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„Account: %s", srcObj.Remote())
			} else {
				// æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
				allAccounts := stats.ListInProgressAccounts()
				for _, accountName := range allAccounts {
					// æ£€æŸ¥æ˜¯å¦æ˜¯è·¨äº‘ä¼ è¾“ç›¸å…³çš„Account
					if strings.Contains(accountName, "ä¸‹è½½é˜¶æ®µ") ||
						strings.Contains(accountName, srcObj.Remote()) ||
						strings.Contains(srcObj.Remote(), strings.TrimSuffix(strings.TrimPrefix(accountName, "ğŸ“¥ "), " (ä¸‹è½½é˜¶æ®µ)")) {
						if acc := stats.GetInProgressAccount(accountName); acc != nil {
							currentAccount = acc
							fs.Debugf(f, " æ‰¾åˆ°æ¨¡ç³ŠåŒ¹é…çš„Account: %s -> %s", srcObj.Remote(), accountName)
							break
						}
					}
				}
			}
		}
	}

	// å¯åŠ¨è¿›åº¦æ›´æ–°åç¨‹ï¼ˆæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯ï¼‰
	progressCtx, cancelProgress := context.WithCancel(ctx)
	defer cancelProgress()

	go f.updateAccountExtraInfo(progressCtx, progress, srcObj.Remote(), currentAccount)

	// åˆ›å»ºå·¥ä½œæ± 
	semaphore := make(chan struct{}, maxConcurrency)
	errChan := make(chan error, numChunks)
	var wg sync.WaitGroup

	for i := range numChunks {
		wg.Add(1)
		go func(chunkIndex int64) {
			defer wg.Done()

			// æ·»åŠ panicæ¢å¤æœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§
			defer func() {
				if r := recover(); r != nil {
					fs.Errorf(f, "123ç½‘ç›˜ä¸‹è½½åˆ†ç‰‡ %d å‘ç”Ÿpanic: %v", chunkIndex, r)
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d panic: %v", chunkIndex, r)
				}
			}()

			// å…³é”®ä¿®å¤ï¼šæ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²å®Œæˆï¼Œè·³è¿‡å·²å®Œæˆçš„åˆ†ç‰‡
			if f.resumeManager != nil {
				// ä½¿ç”¨ä¸ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨ç›¸åŒçš„taskIDç”Ÿæˆæ–¹å¼
				taskID := common.GenerateTaskID123(srcObj.Remote(), srcObj.Size())
				isCompleted, err := f.resumeManager.IsChunkCompleted(taskID, chunkIndex)
				if err != nil {
					fs.Debugf(f, "æ£€æŸ¥åˆ†ç‰‡ %d çŠ¶æ€å¤±è´¥: %vï¼Œç»§ç»­ä¸‹è½½", chunkIndex, err)
				} else if isCompleted {
					fs.Debugf(f, " è·³è¿‡å·²å®Œæˆçš„åˆ†ç‰‡: %d/%d", chunkIndex, numChunks-1)
					return
				} else {
					fs.Debugf(f, "ğŸ”„ åˆ†ç‰‡ %d/%d éœ€è¦ä¸‹è½½ï¼ˆæœªå®Œæˆï¼‰", chunkIndex, numChunks-1)
				}
			} else {
				fs.Debugf(f, "ğŸ”„ åˆ†ç‰‡ %d/%d éœ€è¦ä¸‹è½½ï¼ˆæ— æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨ï¼‰", chunkIndex, numChunks-1)
			}

			// ä¿®å¤ä¿¡å·é‡æ³„æ¼ï¼šä½¿ç”¨å¸¦è¶…æ—¶çš„ä¿¡å·é‡è·å–
			chunkTimeout := 10 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š10åˆ†é’Ÿè¶…æ—¶
			chunkCtx, cancelChunk := context.WithTimeout(ctx, chunkTimeout)
			defer cancelChunk()

			// è·å–ä¿¡å·é‡ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
			select {
			case semaphore <- struct{}{}:
				// æˆåŠŸè·å–ä¿¡å·é‡ï¼Œç«‹å³è®¾ç½®deferé‡Šæ”¾
				defer func() {
					select {
					case <-semaphore:
					default:
						// å¦‚æœä¿¡å·é‡å·²ç»è¢«é‡Šæ”¾ï¼Œé¿å…é˜»å¡
					}
				}()
			case <-chunkCtx.Done():
				errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è·å–ä¿¡å·é‡è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				return
			}

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := chunkIndex * chunkSize
			end := start + chunkSize - 1
			if end >= srcObj.Size() {
				end = srcObj.Size() - 1
			}
			actualChunkSize := end - start + 1

			// ä¿®å¤ä¿¡å·é‡æ³„æ¼ï¼šä½¿ç”¨å¸¦è¶…æ—¶çš„åˆ†ç‰‡ä¸‹è½½
			chunkStartTime := time.Now()
			err := f.downloadChunk(chunkCtx, srcObj, tempFile, start, end, chunkIndex)
			chunkDuration := time.Since(chunkStartTime)

			if err != nil {
				// æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
				if chunkCtx.Err() != nil {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d è¶…æ—¶: %w", chunkIndex, chunkCtx.Err())
				} else {
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", chunkIndex, err)
				}
				return
			}

			// æ›´æ–°è¿›åº¦
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, chunkDuration)

			// å…³é”®ä¿®å¤ï¼šç§»é™¤é‡å¤çš„è¿›åº¦æŠ¥å‘Šï¼Œé¿å…åŒé‡æŠ¥å‘Šé—®é¢˜
			// è¿›åº¦æŠ¥å‘Šåº”è¯¥ç”±downloadChunkæ–¹æ³•æˆ–ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨è´Ÿè´£ï¼Œè¿™é‡Œä¸é‡å¤æŠ¥å‘Š
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d ä¸‹è½½å®Œæˆ: %s", chunkIndex, fs.SizeSuffix(actualChunkSize))

			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d ä¸‹è½½æˆåŠŸ: %d-%d (%s) | ç”¨æ—¶: %v | é€Ÿåº¦: %.2f MB/s",
				chunkIndex, start, end, fs.SizeSuffix(actualChunkSize), chunkDuration,
				float64(actualChunkSize)/chunkDuration.Seconds()/1024/1024)
		}(i)
	}

	// ä¿®å¤æ­»é”é£é™©ï¼šæ·»åŠ æ•´ä½“è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
	overallTimeout := time.Duration(numChunks) * 15 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š15åˆ†é’Ÿ
	overallTimeout = min(overallTimeout, 2*time.Hour)             // æœ€å¤§2å°æ—¶è¶…æ—¶

	// ä½¿ç”¨å¸¦è¶…æ—¶çš„ç­‰å¾…æœºåˆ¶
	done := make(chan struct{})
	go func() {
		defer close(done)
		wg.Wait()
		close(errChan)
	}()

	// ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case <-done:
		// æ­£å¸¸å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
		for err := range errChan {
			if err != nil {
				return err
			}
		}
		return nil
	case <-time.After(overallTimeout):
		fs.Errorf(f, "ğŸš¨ å¹¶å‘ä¸‹è½½æ•´ä½“è¶…æ—¶ (%v)ï¼Œå¯èƒ½å­˜åœ¨æ­»é”", overallTimeout)
		return fmt.Errorf("å¹¶å‘ä¸‹è½½æ•´ä½“è¶…æ—¶: %v", overallTimeout)
	}
}

// downloadChunk ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ†ç‰‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
func (f *Fs) downloadChunk(ctx context.Context, srcObj fs.Object, tempFile *os.File, start, end, chunkIndex int64) error {
	const maxRetries = 4
	var lastErr error

	for retry := range maxRetries {
		// åœ¨æ¯æ¬¡é‡è¯•å‰å¼ºåˆ¶æ¸…é™¤ä¸‹è½½URLç¼“å­˜ï¼ˆå‚è€ƒ115ç½‘ç›˜ä¿®å¤æ–¹æ¡ˆï¼‰
		if retry > 0 {
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡é‡è¯•ï¼Œç›´æ¥è·å–æ–°ä¸‹è½½URL")

			// æ·»åŠ é‡è¯•å»¶è¿Ÿ
			retryDelay := time.Duration(retry) * time.Second
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d é‡è¯• %d/%d: ç­‰å¾… %v åé‡è¯•", chunkIndex, retry, maxRetries-1, retryDelay)
			time.Sleep(retryDelay)
		}

		// ä½¿ç”¨Rangeé€‰é¡¹æ‰“å¼€æ–‡ä»¶åˆ†ç‰‡
		// ä¿®å¤å¤šé‡å¹¶å‘ä¸‹è½½ï¼šä½¿ç”¨ç‰¹æ®Šæ–¹æ³•é¿å…è§¦å‘æºå¯¹è±¡çš„å¹¶å‘ä¸‹è½½
		rangeOption := &fs.RangeOption{Start: start, End: end}
		var chunkReader io.ReadCloser
		var err error

		// ä¿®å¤115ç½‘ç›˜å¹¶å‘ä¸‹è½½è¢«ç¦ç”¨é—®é¢˜ï¼šç§»é™¤ä¸å¿…è¦çš„DisableConcurrentDownloadOption
		switch obj := srcObj.(type) {
		case *Object:
			// å¦‚æœæ˜¯123ç½‘ç›˜å¯¹è±¡ï¼Œç›´æ¥è°ƒç”¨openNormalé¿å…æ— é™å¾ªç¯
			chunkReader, err = obj.openNormal(ctx, rangeOption)
		case interface {
			open(context.Context, ...fs.OpenOption) (io.ReadCloser, error)
		}:
			// ä¿®å¤ï¼š115ç½‘ç›˜å¯¹è±¡ä½¿ç”¨æ­£å¸¸çš„Rangeä¸‹è½½ï¼Œä¸ç¦ç”¨å¹¶å‘ä¸‹è½½
			// è®©115ç½‘ç›˜è‡ªå·±å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼ˆåŸºäºåˆ†ç‰‡å¤§å°ï¼‰
			chunkReader, err = obj.open(ctx, rangeOption)
		default:
			// ä¿®å¤ï¼šå…¶ä»–ç±»å‹å¯¹è±¡ä¹Ÿä½¿ç”¨æ­£å¸¸çš„Rangeä¸‹è½½
			chunkReader, err = srcObj.Open(ctx, rangeOption)
		}
		if err != nil {
			lastErr = fmt.Errorf("æ‰“å¼€åˆ†ç‰‡å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d æ‰“å¼€å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// å‚è€ƒ115ç½‘ç›˜ï¼šä¿®å¤Accountè¿›åº¦è·Ÿè¸ªï¼Œä½¿ç”¨æ›´ç¨³å®šçš„æ–¹å¼
		var currentAccount *accounting.Account
		{
			// å‚è€ƒ115ç½‘ç›˜ï¼šä»å…¨å±€ç»Ÿè®¡æŸ¥æ‰¾ç°æœ‰Account
			remoteName := srcObj.Remote()
			if stats := accounting.GlobalStats(); stats != nil {
				currentAccount = stats.GetInProgressAccount(remoteName)
				if currentAccount == nil {
					// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
					allAccounts := stats.ListInProgressAccounts()
					for _, accountName := range allAccounts {
						if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
							currentAccount = stats.GetInProgressAccount(accountName)
							if currentAccount != nil {
								fs.Debugf(f, " åˆ†ç‰‡ %d æ‰¾åˆ°åŒ¹é…çš„Account: %s -> %s", chunkIndex, remoteName, accountName)
								break
							}
						}
					}
				} else {
					fs.Debugf(f, " åˆ†ç‰‡ %d æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„Account: %s", chunkIndex, remoteName)
				}
			}

			// å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ›å»ºä¸´æ—¶Account
			if currentAccount == nil {
				tempTransfer := accounting.Stats(ctx).NewTransfer(srcObj, f)
				dummyReader := io.NopCloser(strings.NewReader(""))
				_ = tempTransfer.Account(ctx, dummyReader) // Account created but not used in current implementation
				dummyReader.Close()
				fs.Debugf(f, " åˆ†ç‰‡ %d ä½¿ç”¨ä¸´æ—¶Accountè·Ÿè¸ªè¿›åº¦", chunkIndex)
			}
		}

		// è¯»å–åˆ†ç‰‡æ•°æ®å¹¶æŠ¥å‘Šè¿›åº¦
		chunkData, err := io.ReadAll(chunkReader)
		chunkReader.Close()
		if err != nil {
			lastErr = fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d è¯»å–å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// å…³é”®ä¿®å¤ï¼šç§»é™¤é‡å¤çš„è¿›åº¦æŠ¥å‘Šï¼Œç”±ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨è´Ÿè´£è¿›åº¦æŠ¥å‘Š
		// è¿™é‡Œåªè´Ÿè´£æ•°æ®ä¸‹è½½å’ŒéªŒè¯ï¼Œè¿›åº¦æŠ¥å‘Šç”±ä¸Šå±‚çš„downloadChunksConcurrentlyWithAccountç»Ÿä¸€å¤„ç†
		fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡æ•°æ®è¯»å–å®Œæˆ: %s", fs.SizeSuffix(int64(len(chunkData))))

		// ä¿®å¤å…³é”®é—®é¢˜ï¼šéªŒè¯è¯»å–çš„æ•°æ®å¤§å°
		expectedSize := end - start + 1
		actualDataSize := int64(len(chunkData))
		if actualDataSize != expectedSize {
			lastErr = fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, actualDataSize)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d æ•°æ®å¤§å°ä¸åŒ¹é… (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, lastErr)
			continue
		}

		// ä¿®å¤å…³é”®é—®é¢˜ï¼šéªŒè¯WriteAtçš„è¿”å›å€¼
		writtenBytes, err := tempFile.WriteAt(chunkData, start)
		if err != nil {
			lastErr = fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d å†™å…¥å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// å…³é”®ä¿®å¤ï¼šéªŒè¯å®é™…å†™å…¥çš„å­—èŠ‚æ•°
		if int64(writtenBytes) != actualDataSize {
			lastErr = fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡å†™å…¥ä¸å®Œæ•´: æœŸæœ›å†™å…¥%dï¼Œå®é™…å†™å…¥%d", actualDataSize, writtenBytes)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d å†™å…¥ä¸å®Œæ•´ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, lastErr)
			continue
		}

		// æˆåŠŸå®Œæˆ
		if retry > 0 {
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d é‡è¯•æˆåŠŸ (ç¬¬ %d æ¬¡å°è¯•)", chunkIndex, retry+1)
		}
		return nil
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
	return fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡ %d ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡: %w", chunkIndex, maxRetries, lastErr)
}

// continueAfterDownload ä¸‹è½½å®Œæˆåç»§ç»­MD5è®¡ç®—å’Œä¸Šä¼ æµç¨‹
func (f *Fs) continueAfterDownload(ctx context.Context, tempFile *os.File, src fs.ObjectInfo, parentFileID int64, fileName string, downloadDuration time.Duration) (*Object, error) {
	fileSize := src.Size()
	startTime := time.Now().Add(-downloadDuration) // è°ƒæ•´å¼€å§‹æ—¶é—´ä»¥åŒ…å«ä¸‹è½½æ—¶é—´

	// æ­¥éª¤2: è®¡ç®—MD5
	fs.Infof(f, "ğŸ” å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	md5StartTime := time.Now()

	_, err := tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	hasher := md5.New()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration := time.Since(md5StartTime)

	fs.Infof(f, "ğŸ” MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v", md5Hash, md5Duration)

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	createResp, isInstant, err := f.checkInstantUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, err
	}

	if isInstant {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)

		return f.createObjectFromUpload(fileName, fileSize, md5Hash, createResp.Data.FileID, time.Now()), nil
	}

	// æ­¥éª¤4: ä¸Šä¼ æ–‡ä»¶
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	err = f.uploadFile(ctx, tempFile, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	fs.Infof(f, "âœ… å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return f.createObject(fileName, createResp.Data.FileID, fileSize, md5Hash, time.Now(), false), nil
}

// MD5ç¼“å­˜ç®¡ç†å™¨
type CrossCloudMD5Cache struct {
	cache map[string]CrossCloudMD5Entry
	mutex sync.RWMutex
}

type CrossCloudMD5Entry struct {
	MD5Hash    string
	FileSize   int64
	ModTime    time.Time
	CachedTime time.Time
}

// ä¿®å¤å…¨å±€çŠ¶æ€ç«æ€æ¡ä»¶ï¼šä½¿ç”¨sync.Onceç¡®ä¿çº¿ç¨‹å®‰å…¨åˆå§‹åŒ–
var (
	globalMD5Cache     *CrossCloudMD5Cache
	globalMD5CacheOnce sync.Once
)

func getGlobalMD5Cache() *CrossCloudMD5Cache {
	globalMD5CacheOnce.Do(func() {
		globalMD5Cache = &CrossCloudMD5Cache{
			cache: make(map[string]CrossCloudMD5Entry),
		}
	})
	return globalMD5Cache
}

// getCachedMD5 å°è¯•ä»ç¼“å­˜è·å–MD5
func (f *Fs) getCachedMD5(src fs.ObjectInfo) (string, bool) {
	// ç”Ÿæˆç¼“å­˜é”®ï¼šæºæ–‡ä»¶ç³»ç»Ÿç±»å‹ + è·¯å¾„ + å¤§å° + ä¿®æ”¹æ—¶é—´
	srcFs := src.Fs()
	if srcFs == nil {
		return "", false
	}

	cacheKey := common.GenerateMD5CacheKey(srcFs.Name(), src.Remote(), src.Size(), src.ModTime(context.Background()).Unix())

	cache := getGlobalMD5Cache()
	cache.mutex.RLock()
	defer cache.mutex.RUnlock()

	entry, exists := cache.cache[cacheKey]
	if !exists {
		return "", false
	}

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
	if time.Since(entry.CachedTime) > 24*time.Hour {
		// å¼‚æ­¥æ¸…ç†è¿‡æœŸç¼“å­˜
		go func() {
			cache := getGlobalMD5Cache()
			cache.mutex.Lock()
			delete(cache.cache, cacheKey)
			cache.mutex.Unlock()
		}()
		return "", false
	}

	// éªŒè¯æ–‡ä»¶ä¿¡æ¯æ˜¯å¦åŒ¹é…
	if entry.FileSize == src.Size() && entry.ModTime.Equal(src.ModTime(context.Background())) {
		fs.Debugf(f, " MD5ç¼“å­˜å‘½ä¸­: %s -> %s", cacheKey, entry.MD5Hash)
		return entry.MD5Hash, true
	}

	return "", false
}

// cacheMD5 å°†MD5å­˜å…¥ç¼“å­˜
func (f *Fs) cacheMD5(src fs.ObjectInfo, md5Hash string) {
	srcFs := src.Fs()
	if srcFs == nil {
		return
	}

	cacheKey := common.GenerateMD5CacheKey(srcFs.Name(), src.Remote(), src.Size(), src.ModTime(context.Background()).Unix())

	entry := CrossCloudMD5Entry{
		MD5Hash:    md5Hash,
		FileSize:   src.Size(),
		ModTime:    src.ModTime(context.Background()),
		CachedTime: time.Now(),
	}

	cache := getGlobalMD5Cache()
	cache.mutex.Lock()
	cache.cache[cacheKey] = entry
	cache.mutex.Unlock()

	fs.Debugf(f, "ğŸ’¾ MD5å·²ç¼“å­˜: %s -> %s", cacheKey, md5Hash)
}

// continueWithKnownMD5 ä½¿ç”¨å·²çŸ¥MD5ç»§ç»­ä¸Šä¼ æµç¨‹
func (f *Fs) continueWithKnownMD5(ctx context.Context, tempFile *os.File, src fs.ObjectInfo, parentFileID int64, fileName string, md5Hash string, downloadDuration, md5Duration time.Duration, startTime time.Time) (*Object, error) {
	fileSize := src.Size()

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)
		return f.createObject(fileName, createResp.Data.FileID, fileSize, md5Hash, time.Now(), false), nil
	}

	// æ­¥éª¤4: ä¸Šä¼ æ–‡ä»¶
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	err = f.uploadFile(ctx, tempFile, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	// ç¼“å­˜MD5ä»¥ä¾›åç»­ä½¿ç”¨
	f.cacheMD5(src, md5Hash)

	fs.Infof(f, "âœ… ä¼˜åŒ–è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return f.createObject(fileName, createResp.Data.FileID, fileSize, md5Hash, time.Now(), false), nil
}

// optimizedDiskCrossCloudTransfer ä¼˜åŒ–çš„ç£ç›˜è·¨äº‘ä¼ è¾“ï¼ˆåŸæœ‰é€»è¾‘ä¼˜åŒ–ç‰ˆï¼‰
func (f *Fs) optimizedDiskCrossCloudTransfer(ctx context.Context, srcReader io.ReadCloser, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	startTime := time.Now()

	// æ­¥éª¤1: æ£€æŸ¥MD5ç¼“å­˜
	var md5Hash string
	var downloadDuration, md5Duration time.Duration

	if cachedMD5, found := f.getCachedMD5(src); found {
		fs.Infof(f, "ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„MD5: %s", cachedMD5)
		md5Hash = cachedMD5
		md5Duration = 0 // ç¼“å­˜å‘½ä¸­ï¼Œæ— éœ€è®¡ç®—æ—¶é—´

		// ä»éœ€è¦ä¸‹è½½æ–‡ä»¶ç”¨äºä¸Šä¼ ï¼Œä½†å¯ä»¥è·³è¿‡MD5è®¡ç®—
		tempFile, err := os.CreateTemp("", "cross_cloud_cached_*.tmp")
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer func() {
			tempFile.Close()
			os.Remove(tempFile.Name())
		}()

		downloadStartTime := time.Now()
		written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, false, fileName) // ä¸éœ€è¦è®¡ç®—MD5
		downloadDuration = time.Since(downloadStartTime)

		if err != nil {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
		}

		if written != fileSize {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
		}

		fs.Infof(f, "ğŸ“¥ ä¸‹è½½å®Œæˆ (ä½¿ç”¨ç¼“å­˜MD5): %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
			fs.SizeSuffix(fileSize), downloadDuration,
			fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

		// ç›´æ¥è·³è½¬åˆ°ä¸Šä¼ æ­¥éª¤
		return f.continueWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash, downloadDuration, md5Duration, startTime)
	}

	// æ­¥éª¤2: ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸ä¸‹è½½å¹¶è®¡ç®—MD5
	fs.Infof(f, "ğŸ’¾ MD5ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¸‹è½½å¹¶è®¡ç®—MD5")

	// åˆ›å»ºæœ¬åœ°ä¸´æ—¶æ–‡ä»¶
	tempFile, err := os.CreateTemp("", "cross_cloud_optimized_*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// å®Œæ•´ä¸‹è½½æ–‡ä»¶å¹¶è®¡ç®—MD5
	downloadStartTime := time.Now()
	written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, true, fileName)
	downloadDuration = time.Since(downloadStartTime)

	if err != nil {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}

	downloadSpeed := float64(written) / downloadDuration.Seconds() / (1024 * 1024) // MB/s
	fs.Infof(f, "âœ… æ­¥éª¤1å®Œæˆ: æ–‡ä»¶ä¸‹è½½æˆåŠŸ %sï¼Œè€—æ—¶: %vï¼Œé€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), downloadDuration.Round(time.Second), downloadSpeed)

	// æ­¥éª¤2: ä»æœ¬åœ°æ–‡ä»¶è®¡ç®—MD5
	fs.Infof(f, "ğŸ”¢ æ­¥éª¤2/4: å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	hasher := md5.New()
	md5StartTime := time.Now()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
	}

	md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration = time.Since(md5StartTime)
	fs.Infof(f, "âœ… æ­¥éª¤2å®Œæˆ: MD5è®¡ç®—æˆåŠŸ %sï¼Œè€—æ—¶: %v", md5Hash, md5Duration.Round(time.Second))

	// æ­¥éª¤3: åˆ›å»ºä¸Šä¼ ä¼šè¯å¹¶æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "ğŸš€ æ­¥éª¤3/4: åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ ...")
	createResp, err := f.createUploadV2(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// æ£€æŸ¥æ˜¯å¦è§¦å‘ç§’ä¼ 
	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ æ­¥éª¤3å®Œæˆ: è§¦å‘ç§’ä¼ åŠŸèƒ½ï¼æ€»è€—æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration.Round(time.Second), downloadDuration.Round(time.Second), md5Duration.Round(time.Second))
		return f.createObject(fileName, createResp.Data.FileID, fileSize, md5Hash, time.Now(), false), nil
	}

	// æ­¥éª¤4: æ‰§è¡Œå®é™…ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ æ­¥éª¤4/4: å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°123ç½‘ç›˜...")
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	// å‡†å¤‡ä¸Šä¼ å‚æ•°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		chunkSize = f.getOptimalChunkSize(fileSize, 20*1024*1024) // é»˜è®¤20MB/sç½‘é€Ÿ
	}
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	progress, err := f.prepareUploadProgress(createResp.Data.PreuploadID, totalChunks, chunkSize, fileSize, fileName, tempFile.Name(), src.Remote())
	if err != nil {
		return nil, fmt.Errorf("å‡†å¤‡ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
	}

	// è®¡ç®—å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)
	maxConcurrency := concurrencyParams.optimal

	// æ‰§è¡Œä¸Šä¼ 
	uploadStartTime := time.Now()
	result, err := f.v2UploadChunksWithConcurrency(ctx, tempFile, &localFileInfo{
		name:    fileName,
		size:    fileSize,
		modTime: src.ModTime(ctx),
	}, createResp, progress, fileName, maxConcurrency)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(fileSize) / uploadDuration.Seconds() / (1024 * 1024) // MB/s
	totalDuration := time.Since(startTime)

	// ç¼“å­˜MD5ä»¥ä¾›åç»­ä½¿ç”¨
	f.cacheMD5(src, md5Hash)

	fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“å®Œæˆ: %sï¼Œæ€»è€—æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)ï¼Œä¸Šä¼ é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(fileSize), totalDuration.Round(time.Second),
		downloadDuration.Round(time.Second), md5Duration.Round(time.Second),
		uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// putWithKnownMD5 å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ 
// è¿™æ˜¯æœ€é«˜æ•ˆçš„è·¯å¾„ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨123ç½‘ç›˜çš„ç§’ä¼ åŠŸèƒ½
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) putWithKnownMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	return f.putWithKnownMD5WithOptions(ctx, in, src, parentFileID, fileName, md5Hash, false)
}

// putWithKnownMD5WithOptions å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ ï¼Œæ”¯æŒè·³è¿‡å•æ­¥ä¸Šä¼ é€‰é¡¹
func (f *Fs) putWithKnownMD5WithOptions(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string, skipSingleStep bool) (*Object, error) {
	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆå°è¯•å•æ­¥ä¸Šä¼ APIï¼ˆé™¤éæ˜ç¡®è·³è¿‡ï¼‰
	fileSize := src.Size()
	if !skipSingleStep && fileSize < singleStepUploadLimit && fileSize > 0 && fileSize <= maxMemoryBufferSize {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", fileSize, singleStepUploadLimit)

		// è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
		data, err := io.ReadAll(in)
		if err != nil {
			fs.Debugf(f, "è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
		} else if int64(len(data)) == fileSize {
			// å°è¯•å•æ­¥ä¸Šä¼ 
			result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
			if err != nil {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
				// é‡æ–°åˆ›å»ºreaderç”¨äºä¼ ç»Ÿä¸Šä¼ 
				in = bytes.NewReader(data)
			} else {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
				return result, nil
			}
		}
	}

	// ä¼ ç»Ÿä¸Šä¼ æµç¨‹ï¼šä½¿ç”¨å·²çŸ¥çš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ç›¸åŒMD5çš„æ–‡ä»¶
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ /å»é‡ï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™æ˜¯æœ€ç†æƒ³çš„æƒ…å†µï¼šæ— éœ€å®é™…ä¼ è¾“æ•°æ®
	if createResp.Data.Reuse {
		fs.Debugf(f, "æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œéœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®
	fs.Debugf(f, "ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®")
	err = f.uploadFile(ctx, in, createResp, src.Size())
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	// æ³¨æ„ï¼šä½¿ç”¨å®é™…çš„æ–‡ä»¶åè€Œä¸æ˜¯src.Remote()ï¼Œå› ä¸ºsrc.Remote()å¯èƒ½åŒ…å«.partialåç¼€
	// è€Œå®é™…ä¸Šä¼ åˆ°123ç½‘ç›˜çš„æ–‡ä»¶åæ˜¯fileNameï¼ˆå·²å»é™¤.partialåç¼€ï¼‰
	return f.createObject(fileName, createResp.Data.FileID, src.Size(), md5Hash, time.Now(), false), nil
}

// putSmallFileWithMD5 å¤„ç†å°æ–‡ä»¶ï¼ˆâ‰¤10MBï¼‰çš„ä¸Šä¼ 
// é€šè¿‡ç¼“å­˜åˆ°å†…å­˜å…ˆè®¡ç®—MD5ï¼Œæœ€å¤§åŒ–åˆ©ç”¨ç§’ä¼ åŠŸèƒ½ï¼ŒåŒæ—¶æ§åˆ¶å†…å­˜ä½¿ç”¨
func (f *Fs) putSmallFileWithMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ç¼“å­˜å°æ–‡ä»¶ï¼ˆ%då­—èŠ‚ï¼‰åˆ°å†…å­˜ä»¥è®¡ç®—MD5ï¼Œå°è¯•ç§’ä¼ ", fileSize)

	// éªŒè¯æ–‡ä»¶å¤§å°çš„åˆç†æ€§
	if fileSize < 0 {
		return nil, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶å¤§å°: %d", fileSize)
	}
	if fileSize > 50*1024*1024 { // 50MBä¸Šé™
		fs.Debugf(f, "æ–‡ä»¶è¿‡å¤§ï¼ˆ%då­—èŠ‚ï¼‰ï¼Œä¸é€‚åˆå†…å­˜ç¼“å­˜ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ", fileSize)
		return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
	}

	// ç®€åŒ–å†…å­˜ç®¡ç† - ä½¿ç”¨Goæ ‡å‡†åº“ï¼Œç§»é™¤è¿‡åº¦å¤æ‚çš„å†…å­˜è·Ÿè¸ª

	// ä½¿ç”¨é™åˆ¶è¯»å–å™¨é˜²æ­¢è¯»å–è¶…è¿‡é¢„æœŸå¤§å°çš„æ•°æ®
	// è¿™å¯ä»¥é˜²æ­¢æ¶æ„æˆ–æŸåçš„æ–‡ä»¶å¯¼è‡´å†…å­˜è€—å°½
	var limitedReader io.Reader
	if fileSize > 0 {
		// å…è®¸è¯»å–æ¯”é¢„æœŸç¨å¤šä¸€ç‚¹çš„æ•°æ®æ¥æ£€æµ‹å¤§å°ä¸åŒ¹é…
		limitedReader = io.LimitReader(in, fileSize+1024) // é¢å¤–1KBç”¨äºæ£€æµ‹
	} else {
		// å¯¹äºæœªçŸ¥å¤§å°çš„æ–‡ä»¶ï¼Œè®¾ç½®åˆç†çš„ä¸Šé™
		limitedReader = io.LimitReader(in, 50*1024*1024) // 50MBä¸Šé™
	}

	// å°†æ–‡ä»¶è¯»å–åˆ°å†…å­˜ï¼Œä½¿ç”¨é™åˆ¶è¯»å–å™¨ä¿æŠ¤
	data, err := io.ReadAll(limitedReader)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–å°æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// ç«‹å³æ£€æŸ¥è¯»å–çš„æ•°æ®å¤§å°ï¼Œé˜²æ­¢å†…å­˜æµªè´¹
	actualSize := int64(len(data))
	if fileSize > 0 {
		if actualSize > fileSize {
			return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡é¢„æœŸ: å®é™… %d å­—èŠ‚ > é¢„æœŸ %d å­—èŠ‚ï¼Œå¯èƒ½æ–‡ä»¶å·²æŸå", actualSize, fileSize)
		}
		if actualSize != fileSize {
			fs.Debugf(f, "æ–‡ä»¶å¤§å°ä¸é¢„æœŸä¸ç¬¦: å®é™… %d å­—èŠ‚ï¼Œé¢„æœŸ %d å­—èŠ‚", actualSize, fileSize)
		}
	}

	// è®¡ç®—MD5å“ˆå¸Œå€¼
	// è¿™æ˜¯å…³é”®æ­¥éª¤ï¼šå…ˆè®¡ç®—MD5ï¼Œç„¶åå°è¯•ç§’ä¼ 
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "å°æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %s", md5Hash)

	// ä½¿ç”¨è®¡ç®—å‡ºçš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ 
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ æˆåŠŸï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬é¿å…äº†å®é™…çš„æ•°æ®ä¼ è¾“
	if createResp.Data.Reuse {
		fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return f.createObject(src.Remote(), createResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®
	// ä½¿ç”¨bytes.NewReaderä»å†…å­˜ä¸­çš„æ•°æ®åˆ›å»ºreader
	fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®")
	err = f.uploadFile(ctx, bytes.NewReader(data), createResp, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return f.createObject(src.Remote(), createResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
}

// streamingPutWithBuffer ä½¿ç”¨ç¼“å†²ç­–ç•¥å¤„ç†è·¨äº‘ç›˜ä¼ è¾“
// è§£å†³"æºæ–‡ä»¶ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é—®é¢˜
func (f *Fs) streamingPutWithBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ä½¿ç”¨ç¼“å†²ç­–ç•¥è¿›è¡Œè·¨äº‘ç›˜ä¼ è¾“")

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
	fileSize := src.Size()

	// å¦‚æœæ–‡ä»¶å°äº100MBï¼Œç›´æ¥è¯»å…¥å†…å­˜
	if fileSize <= 100*1024*1024 {
		fs.Debugf(f, "å°æ–‡ä»¶(%s)ï¼Œä½¿ç”¨å†…å­˜ç¼“å†²", fs.SizeSuffix(fileSize))
		return f.streamingPutWithMemoryBuffer(ctx, in, src, parentFileID, fileName)
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²", fs.SizeSuffix(fileSize))
	return f.streamingPutWithTempFile(ctx, in, src, parentFileID, fileName)
}

// streamingPutWithMemoryBuffer ä½¿ç”¨å†…å­˜ç¼“å†²å¤„ç†å°æ–‡ä»¶
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) streamingPutWithMemoryBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// è®¡ç®—MD5
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

	fs.Debugf(f, "å†…å­˜ç¼“å†²å®Œæˆï¼Œæ–‡ä»¶å¤§å°: %d, MD5: %s", len(data), md5Hash)

	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
	if len(data) < singleStepUploadLimit {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", len(data), singleStepUploadLimit)

		result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
		if err != nil {
			fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ æ–¹å¼: %v", err)
			// å¦‚æœå•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿçš„åˆ†ç‰‡ä¸Šä¼ æ–¹å¼
			// ä½¿ç”¨skipSingleStepæ ‡å¿—é˜²æ­¢æ— é™é€’å½’
			return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
		}

		fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
		return result, nil
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼
	fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes >= %dï¼Œä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼", len(data), singleStepUploadLimit)
	return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
}

// streamingPutWithTempFile ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²å¤„ç†å¤§æ–‡ä»¶
func (f *Fs) streamingPutWithTempFile(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¯¹äºè¶…å¤§æ–‡ä»¶ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥
	fileSize := src.Size()
	if fileSize > singleStepUploadLimit { // å¤§äºå•æ­¥ä¸Šä¼ é™åˆ¶çš„æ–‡ä»¶ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“
		fs.Debugf(f, "è¶…å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“ç­–ç•¥ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰", fs.SizeSuffix(fileSize))
		return f.streamingPutWithChunkedResume(ctx, in, src, parentFileID, fileName)
	}

	// å¯¹äºè¾ƒå°çš„å¤§æ–‡ä»¶ï¼Œç»§ç»­ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²ç­–ç•¥", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - ä½¿ç”¨å®‰å…¨çš„èµ„æºç®¡ç†
	tempFile, err := os.CreateTemp("", "rclone-123pan-*")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		// å®‰å…¨çš„èµ„æºæ¸…ç†ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æ‰§è¡Œ
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// è¾¹è¯»è¾¹å†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆï¼Œå†™å…¥: %d å­—èŠ‚, MD5: %s", written, md5Hash)

	// é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å·²çŸ¥MD5ä¸Šä¼ 
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// streamingPutWithTempFileForced å¼ºåˆ¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå…ˆå°è¯•æµå¼ä¼ è¾“ï¼Œå¤±è´¥æ—¶æ‰ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
func (f *Fs) streamingPutWithTempFileForced(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ä½¿ç”¨ä¼˜åŒ–çš„å›é€€ç­–ç•¥ï¼šå…ˆå°è¯•æµå¼ä¼ è¾“")

	// å°è¯•è·å–æºå¯¹è±¡è¿›è¡Œæµå¼ä¼ è¾“
	if srcObj, ok := src.(fs.Object); ok {
		// å°è¯•æµå¼ä¼ è¾“ä»¥é¿å…åŒå€æµé‡
		result, err := f.attemptStreamingUpload(ctx, srcObj, parentFileID, fileName)
		if err == nil {
			fs.Debugf(f, "æµå¼ä¼ è¾“æˆåŠŸï¼Œé¿å…äº†ä¸´æ—¶æ–‡ä»¶")
			return result, nil
		}
		fs.Debugf(f, "æµå¼ä¼ è¾“å¤±è´¥ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥: %v", err)
	}

	fs.Debugf(f, "ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ä½œä¸ºæœ€ç»ˆå›é€€æ–¹æ¡ˆ")

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - ä½¿ç”¨å®‰å…¨çš„èµ„æºç®¡ç†
	tempFile, err := os.CreateTemp("", "rclone-123pan-fallback-*")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºå›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		// å®‰å…¨çš„èµ„æºæ¸…ç†ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æ‰§è¡Œ
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "å…³é—­å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "åˆ é™¤å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// è¾¹è¯»è¾¹å†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "å›é€€ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆï¼Œå†™å…¥: %d å­—èŠ‚, MD5: %s", written, md5Hash)

	// é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡æ–°å®šä½å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å·²çŸ¥MD5ä¸Šä¼ 
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// streamingPutWithChunkedResume ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“å¤„ç†è¶…å¤§æ–‡ä»¶
// æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œèµ„æºå ç”¨æœ€å°‘ï¼ˆåªéœ€è¦å•ä¸ªåˆ†ç‰‡çš„ä¸´æ—¶å­˜å‚¨ï¼‰
// è¿™æ˜¯è¶…å¤§æ–‡ä»¶ä¼ è¾“çš„æœ€ä½³ç­–ç•¥ï¼šæ—¢èŠ‚çœèµ„æºåˆæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) streamingPutWithChunkedResume(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// ä¸ºå¤§æ–‡ä»¶æ·»åŠ ç”¨æˆ·å‹å¥½çš„åˆå§‹åŒ–æç¤º
	fileSize := src.Size()
	if fileSize > 1024*1024*1024 { // å¤§äº1GBçš„æ–‡ä»¶
		fs.Infof(f, "ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤§æ–‡ä»¶ä¸Šä¼  (%s) - å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°ï¼Œè¯·ç¨å€™...", fs.SizeSuffix(fileSize))
	}

	// éªŒè¯æºå¯¹è±¡
	srcObj, ok := src.(fs.Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
	}

	fileSize = src.Size()

	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆå…ˆåˆ›å»ºä¼šè¯è·å–APIè¦æ±‚çš„SliceSizeï¼‰
	createResp, err := f.createChunkedUploadSession(ctx, parentFileID, fileName, fileSize)
	if err != nil {
		return nil, err
	}

	// å…³é”®ä¿®å¤ï¼šä½¿ç”¨APIè¿”å›çš„SliceSizeï¼Œè€Œä¸æ˜¯åŠ¨æ€è®¡ç®—çš„åˆ†ç‰‡å¤§å°
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize <= 0 {
		// å¦‚æœAPIæ²¡æœ‰è¿”å›æœ‰æ•ˆçš„SliceSizeï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€
		fs.Debugf(f, "âš ï¸ APIæœªè¿”å›æœ‰æ•ˆSliceSize(%d)ï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€", apiSliceSize)
		apiSliceSize = 64 * 1024 * 1024 // 64MBå›é€€å¤§å°
	}

	fs.Debugf(f, "å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“ï¼šæ–‡ä»¶å¤§å° %sï¼ŒAPIè¦æ±‚åˆ†ç‰‡å¤§å° %s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(apiSliceSize))

	// å¦‚æœæ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	if createResp.Data.Reuse {
		fs.Debugf(f, "æ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
	}

	// å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“ï¼Œä½¿ç”¨APIè¦æ±‚çš„åˆ†ç‰‡å¤§å°
	return f.uploadFileInChunksWithResume(ctx, srcObj, createResp, fileSize, apiSliceSize, fileName)
}

// ChunkedUploadParams removed - was unused

// createChunkedUploadSession åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯
func (f *Fs) createChunkedUploadSession(ctx context.Context, parentFileID int64, fileName string, fileSize int64) (*UploadCreateResp, error) {
	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆä½¿ç”¨ä¸´æ—¶MD5ï¼Œåç»­ä¼šæ›´æ–°ï¼‰
	tempMD5 := fmt.Sprintf("%032x", time.Now().UnixNano())
	createResp, err := f.createUpload(ctx, parentFileID, fileName, tempMD5, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	return createResp, nil
}

// uploadFileInChunksWithResume æ‰§è¡Œåˆ†ç‰‡æµå¼ä¸Šä¼ ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileInChunksWithResume(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	// å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
	progress, err := f.prepareUploadProgress(preuploadID, totalChunks, chunkSize, fileSize, fileName, "", srcObj.Remote())
	if err != nil {
		return nil, err
	}

	// éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§
	if err := f.validateUploadedChunks(ctx, progress); err != nil {
		fs.Debugf(f, "åˆ†ç‰‡å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: %v", err)
		// ç»§ç»­æ‰§è¡Œï¼Œä½†å·²æŸåçš„åˆ†ç‰‡ä¼šè¢«é‡æ–°ä¸Šä¼ 
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)

	fs.Debugf(f, "åŠ¨æ€ä¼˜åŒ–å‚æ•° - ç½‘ç»œé€Ÿåº¦: %s/s, æœ€ä¼˜å¹¶å‘æ•°: %d, å®é™…å¹¶å‘æ•°: %d",
		fs.SizeSuffix(concurrencyParams.networkSpeed), concurrencyParams.optimal, concurrencyParams.actual)

	// é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šå¤šçº¿ç¨‹æˆ–å•çº¿ç¨‹
	if totalChunks > 2 && concurrencyParams.actual > 1 {
		fs.Debugf(f, "ä½¿ç”¨å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶å‘æ•°: %d", concurrencyParams.actual)
		return f.uploadChunksWithConcurrency(ctx, srcObj, createResp, progress, md5.New(), fileName, concurrencyParams.actual)
	}

	// å•çº¿ç¨‹ä¸Šä¼ 
	return f.uploadChunksSingleThreaded(ctx, srcObj, createResp, progress, fileSize, chunkSize, fileName)
}

// ConcurrencyParams å¹¶å‘å‚æ•°
type ConcurrencyParams struct {
	networkSpeed int64
	optimal      int
	actual       int
}

// prepareUploadProgress å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
func (f *Fs) prepareUploadProgress(preuploadID string, totalChunks, chunkSize, fileSize int64, fileName, filePath, remotePath string) (*UploadProgress, error) {
	// ä½¿ç”¨ç»Ÿä¸€çš„æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
	if f.resumeManager == nil {
		fs.Debugf(f, "æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æœªåˆå§‹åŒ–ï¼Œåˆ›å»ºæ–°è¿›åº¦")
		return &UploadProgress{
			PreuploadID:   preuploadID,
			TotalParts:    totalChunks,
			ChunkSize:     chunkSize,
			FileSize:      fileSize,
			UploadedParts: make(map[int64]bool),
			CreatedAt:     time.Now(),
		}, nil
	}

	// ä¿®å¤ï¼šç”Ÿæˆç»Ÿä¸€çš„taskIDï¼Œä½¿ç”¨è¿œç¨‹è·¯å¾„å’Œæ–‡ä»¶å¤§å°ï¼Œä¸ä¸‹è½½ä¿æŒä¸€è‡´
	var taskID string
	if remotePath != "" {
		taskID = common.GenerateTaskID123(remotePath, fileSize)
		fs.Debugf(f, " ä½¿ç”¨ç»Ÿä¸€TaskIDç”Ÿæˆ: %s (è·¯å¾„: %s, å¤§å°: %d)", taskID, remotePath, fileSize)
	} else {
		// å›é€€åˆ°åŸæœ‰æ–¹å¼ï¼ˆå…¼å®¹æ€§ï¼‰
		taskID = fmt.Sprintf("123_%s", preuploadID)
		fs.Debugf(f, "âš ï¸ ä½¿ç”¨å›é€€TaskIDç”Ÿæˆ: %s", taskID)
	}

	// å°è¯•ä»ç»Ÿä¸€ç®¡ç†å™¨åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	resumeInfo, err := f.resumeManager.LoadResumeInfo(taskID)
	if err != nil {
		fs.Debugf(f, "åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %vï¼Œåˆ›å»ºæ–°è¿›åº¦", err)
	}

	var progress *UploadProgress

	if resumeInfo != nil {
		// ç±»å‹æ–­è¨€ä¸º123ç½‘ç›˜çš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
		if r123, ok := resumeInfo.(*common.ResumeInfo123); ok {
			// ä»ç»Ÿä¸€ç®¡ç†å™¨çš„ä¿¡æ¯è½¬æ¢ä¸ºUploadProgress
			progress = &UploadProgress{
				PreuploadID:   r123.PreuploadID,
				TotalParts:    r123.TotalChunks,
				ChunkSize:     r123.ChunkSize,
				FileSize:      r123.FileSize,
				UploadedParts: r123.GetCompletedChunks(),
				FilePath:      remotePath, // ä¿®å¤ï¼šä½¿ç”¨å½“å‰çš„remotePathè€Œä¸æ˜¯r123.FilePath
				CreatedAt:     r123.CreatedAt,
			}
			fs.Debugf(f, "æ¢å¤åˆ†ç‰‡ä¸Šä¼ è¿›åº¦: %d/%d ä¸ªåˆ†ç‰‡å·²å®Œæˆ (TaskID: %s)",
				progress.GetUploadedCount(), totalChunks, taskID)
		} else {
			fs.Debugf(f, "æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ç±»å‹ä¸åŒ¹é…ï¼Œåˆ›å»ºæ–°è¿›åº¦")
			resumeInfo = nil
		}
	}

	if resumeInfo == nil {
		// åˆ›å»ºæ–°çš„è¿›åº¦ä¿¡æ¯
		progress = &UploadProgress{
			PreuploadID:   preuploadID,
			TotalParts:    totalChunks,
			ChunkSize:     chunkSize,
			FileSize:      fileSize,
			UploadedParts: make(map[int64]bool),
			FilePath:      remotePath, // ä¿®å¤ï¼šè®¾ç½®FilePathå­—æ®µ
			CreatedAt:     time.Now(),
		}

		// åˆ›å»ºå¯¹åº”çš„ç»Ÿä¸€ç®¡ç†å™¨ä¿¡æ¯å¹¶ä¿å­˜
		newResumeInfo := &common.ResumeInfo123{
			TaskID:              taskID,
			PreuploadID:         preuploadID,
			FileName:            fileName,
			FileSize:            fileSize,
			FilePath:            filePath,
			ChunkSize:           chunkSize,
			TotalChunks:         totalChunks,
			CompletedChunks:     make(map[int64]bool),
			CreatedAt:           time.Now(),
			LastUpdated:         time.Now(),
			BackendSpecificData: make(map[string]any),
		}

		// ä¿å­˜åˆ°ç»Ÿä¸€ç®¡ç†å™¨
		if saveErr := f.resumeManager.SaveResumeInfo(newResumeInfo); saveErr != nil {
			fs.Debugf(f, "ä¿å­˜æ–°å»ºæ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %v", saveErr)
		} else {
			fs.Debugf(f, "åˆ›å»ºå¹¶ä¿å­˜æ–°çš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %dåˆ†ç‰‡ (TaskID: %s)", totalChunks, taskID)
		}
	}

	return progress, nil
}

// validateUploadedChunks éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§
func (f *Fs) validateUploadedChunks(ctx context.Context, progress *UploadProgress) error {
	if progress.GetUploadedCount() > 0 {
		fs.Debugf(f, "éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§")
		return f.resumeUploadWithIntegrityCheck(ctx, progress)
	}
	return nil
}

// calculateConcurrencyParams è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
func (f *Fs) calculateConcurrencyParams(fileSize int64) *ConcurrencyParams {
	networkSpeed := f.detectNetworkSpeed(context.Background())
	optimalConcurrency := f.getOptimalConcurrency(fileSize, networkSpeed)

	// æ£€æŸ¥ç”¨æˆ·è®¾ç½®çš„æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	maxConcurrency := optimalConcurrency
	if f.opt.MaxConcurrentUploads > 0 && f.opt.MaxConcurrentUploads < maxConcurrency {
		maxConcurrency = f.opt.MaxConcurrentUploads
	}

	return &ConcurrencyParams{
		networkSpeed: networkSpeed,
		optimal:      optimalConcurrency,
		actual:       maxConcurrency,
	}
}

// uploadChunksSingleThreaded å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) uploadChunksSingleThreaded(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, progress *UploadProgress, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	overallHasher := md5.New()

	fs.Debugf(f, "ä½¿ç”¨å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")

	// é€ä¸ªåˆ†ç‰‡å¤„ç†ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰
	for chunkIndex := range totalChunks {
		partNumber := chunkIndex + 1

		// æ£€æŸ¥æ­¤åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼  - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		if progress.IsUploaded(partNumber) {
			fs.Debugf(f, "è·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡ %d/%d", partNumber, totalChunks)

			// ä¸ºäº†è®¡ç®—æ•´ä½“MD5ï¼Œéœ€è¦è¯»å–å¹¶ä¸¢å¼ƒè¿™éƒ¨åˆ†æ•°æ®
			if err := f.processSkippedChunkForMD5(ctx, srcObj, overallHasher, chunkIndex, chunkSize, fileSize, partNumber); err != nil {
				return nil, err
			}
			continue
		}

		// ä¸Šä¼ æ–°åˆ†ç‰‡
		if err := f.uploadSingleChunkWithStream(ctx, srcObj, overallHasher, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks); err != nil {
			// ä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
			f.saveUploadProgress(progress) // å¿½ç•¥ä¿å­˜é”™è¯¯
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		// æ ‡è®°åˆ†ç‰‡å·²å®Œæˆå¹¶ä¿å­˜è¿›åº¦
		progress.SetUploaded(partNumber)
		if err := f.saveUploadProgress(progress); err != nil {
			fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
		}

		fs.Debugf(f, " åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ å¹¶è¿”å›ç»“æœ
	return f.finalizeChunkedUpload(ctx, createResp, overallHasher, fileSize, fileName, preuploadID)
}

// processSkippedChunkForMD5 å¤„ç†è·³è¿‡çš„åˆ†ç‰‡ä»¥è®¡ç®—MD5
func (f *Fs) processSkippedChunkForMD5(ctx context.Context, srcObj fs.Object, hasher hash.Hash, chunkIndex, chunkSize, fileSize int64, partNumber int64) error {
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	// è¯»å–åˆ†ç‰‡æ•°æ®ç”¨äºMD5è®¡ç®—
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	_, err = io.CopyN(hasher, chunkReader, actualChunkSize)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}

	return nil
}

// finalizeChunkedUpload å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¹¶è¿”å›ç»“æœ
func (f *Fs) finalizeChunkedUpload(ctx context.Context, _ *UploadCreateResp, hasher hash.Hash, fileSize int64, fileName, preuploadID string) (*Object, error) {
	// è®¡ç®—æœ€ç»ˆMD5
	finalMD5 := fmt.Sprintf("%x", hasher.Sum(nil))

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "åˆ†ç‰‡æµå¼ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// ä¿®å¤ï¼šæ¸…ç†è¿›åº¦æ–‡ä»¶ï¼Œä¼ é€’æ–‡ä»¶åå’Œæ–‡ä»¶å¤§å°ä¿¡æ¯
	f.removeUploadProgress(preuploadID, fileName, fileSize)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return f.createObject(fileName, result.FileID, fileSize, finalMD5, time.Now(), false), nil
}

// attemptStreamingUpload å°è¯•æµå¼ä¸Šä¼ ä»¥é¿å…åŒå€æµé‡æ¶ˆè€—
// è¿™ä¸ªæ–¹æ³•å®ç°çœŸæ­£çš„è¾¹ä¸‹è¾¹ä¸Šä¼ è¾“ï¼Œæœ€å°åŒ–æµé‡ä½¿ç”¨
func (f *Fs) attemptStreamingUpload(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "å°è¯•æµå¼ä¸Šä¼ ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶å’ŒåŒå€æµé‡")

	// ä¼˜åŒ–çš„MD5è·å–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å·²çŸ¥å“ˆå¸Œï¼Œé¿å…é‡å¤è®¡ç®—
	var md5Hash string
	if hash, err := srcObj.Hash(ctx, fshash.MD5); err == nil && hash != "" {
		md5Hash = hash
		fs.Debugf(f, "ä½¿ç”¨æºå¯¹è±¡å·²çŸ¥MD5: %s", md5Hash)
	} else {
		// å¦‚æœæ²¡æœ‰å·²çŸ¥MD5ï¼Œè®¡ç®—MD5ï¼ˆåªè¯»å–ä¸€æ¬¡ï¼‰
		md5Reader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è®¡ç®—MD5: %w", err)
		}
		defer md5Reader.Close()

		hasher := md5.New()
		_, err = io.Copy(hasher, md5Reader)
		if err != nil {
			return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
		}

		md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
		fs.Debugf(f, "æµå¼ä¼ è¾“MD5è®¡ç®—å®Œæˆ: %s", md5Hash)
	}

	// ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å·²çŸ¥MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæµå¼ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// å¦‚æœè§¦å‘ç§’ä¼ ï¼Œç›´æ¥è¿”å›æˆåŠŸ
	if createResp.Data.Reuse {
		fs.Debugf(f, "æµå¼ä¼ è¾“è§¦å‘ç§’ä¼ ï¼ŒMD5: %s", md5Hash)
		return f.createObject(fileName, createResp.Data.FileID, srcObj.Size(), md5Hash, time.Now(), false), nil
	}

	// ç¬¬ä¸‰æ­¥ï¼šé‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œå®é™…ä¸Šä¼ 
	uploadReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•é‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸Šä¼ : %w", err)
	}
	defer uploadReader.Close()

	// æ‰§è¡Œå®é™…ä¸Šä¼ 
	err = f.uploadFile(ctx, uploadReader, createResp, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("æµå¼ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "æµå¼ä¸Šä¼ æˆåŠŸå®Œæˆï¼Œæ–‡ä»¶ID: %d", createResp.Data.FileID)

	return f.createObject(fileName, createResp.Data.FileID, srcObj.Size(), md5Hash, time.Now(), false), nil
}

// uploadSingleChunkWithStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
// è¿™æ˜¯åˆ†ç‰‡æµå¼ä¼ è¾“çš„æ ¸å¿ƒï¼šè¾¹ä¸‹è½½è¾¹ä¸Šä¼ ï¼Œåªéœ€è¦åˆ†ç‰‡å¤§å°çš„ä¸´æ—¶å­˜å‚¨
func (f *Fs) uploadSingleChunkWithStream(ctx context.Context, srcObj fs.Object, overallHasher io.Writer, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) error {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¼€å§‹æµå¼ä¸Šä¼ åˆ†ç‰‡ %d/%d (åç§»: %d, å¤§å°: %d)",
		partNumber, totalChunks, chunkStart, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡èŒƒå›´çš„æºæ–‡ä»¶æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æºæ–‡ä»¶æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæ­¤åˆ†ç‰‡ï¼ˆåªéœ€è¦åˆ†ç‰‡å¤§å°çš„å­˜å‚¨ï¼‰
	tempFile, err := os.CreateTemp("", fmt.Sprintf("rclone-123pan-chunk-%d-*", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// åˆ›å»ºåˆ†ç‰‡MD5è®¡ç®—å™¨
	chunkHasher := md5.New()

	// è¾¹è¯»è¾¹å†™ï¼šåŒæ—¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ã€åˆ†ç‰‡MD5å’Œæ•´ä½“MD5
	multiWriter := io.MultiWriter(tempFile, chunkHasher, overallHasher)

	// æµå¼ä¼ è¾“åˆ†ç‰‡æ•°æ®
	written, err := io.Copy(multiWriter, chunkReader)
	if err != nil {
		return fmt.Errorf("æµå¼ä¼ è¾“åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
	}

	if written != actualChunkSize {
		return fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, written)
	}

	// é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶åˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return fmt.Errorf("é‡æ–°å®šä½åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// æ³¨æ„ï¼šæ–°çš„uploadPartWithMultipartæ–¹æ³•å†…éƒ¨ä¼šè·å–ä¸Šä¼ åŸŸåï¼Œè¿™é‡Œä¸å†éœ€è¦

	// è¯»å–ä¸´æ—¶æ–‡ä»¶æ•°æ®ç”¨äºä¸Šä¼ 
	chunkData, err := io.ReadAll(tempFile)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	chunkMD5 := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err = f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkMD5, chunkData)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}
	fs.Debugf(f, "åˆ†ç‰‡ %d æµå¼ä¸Šä¼ æˆåŠŸï¼Œå¤§å°: %d, MD5: %s", partNumber, written, chunkMD5)

	return nil
}

// uploadChunksWithConcurrency ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ï¼Œé›†æˆæµå¼å“ˆå¸Œç´¯ç§¯å™¨
func (f *Fs) uploadChunksWithConcurrency(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, progress *UploadProgress, _ io.Writer, fileName string, maxConcurrency int) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	fs.Debugf(f, "å¼€å§‹å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡ï¼Œæœ€å¤§å¹¶å‘æ•°ï¼š%d", totalChunks, maxConcurrency)

	// ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæŸ¥æ‰¾Accountå¯¹è±¡è¿›è¡Œé›†æˆ
	var currentAccount *accounting.Account
	if stats := accounting.GlobalStats(); stats != nil {
		// å°è¯•ç²¾ç¡®åŒ¹é…
		currentAccount = stats.GetInProgressAccount(fileName)
		if currentAccount == nil {
			// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
			allAccounts := stats.ListInProgressAccounts()
			for _, accountName := range allAccounts {
				if strings.Contains(accountName, fileName) || strings.Contains(fileName, accountName) {
					currentAccount = stats.GetInProgressAccount(accountName)
					if currentAccount != nil {
						break
					}
				}
			}
		}
	}

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢goroutineé•¿æ—¶é—´è¿è¡Œ
	uploadTimeout := time.Duration(totalChunks) * 10 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š10åˆ†é’Ÿ
	uploadTimeout = min(uploadTimeout, 2*time.Hour)                // æœ€å¤§2å°æ—¶è¶…æ—¶
	workerCtx, cancelWorkers := context.WithTimeout(ctx, uploadTimeout)
	defer cancelWorkers() // ç¡®ä¿å‡½æ•°é€€å‡ºæ—¶å–æ¶ˆæ‰€æœ‰worker

	// åˆ›å»ºæœ‰ç•Œå·¥ä½œé˜Ÿåˆ—å’Œç»“æœé€šé“ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
	// é˜Ÿåˆ—å¤§å°é™åˆ¶ä¸ºå¹¶å‘æ•°çš„2å€ï¼Œé¿å…è¿‡åº¦ç¼“å†²
	queueSize := maxConcurrency * 2
	queueSize = min(queueSize, int(totalChunks))
	chunkJobs := make(chan int64, queueSize)
	results := make(chan chunkResult, maxConcurrency) // ç»“æœé€šé“åªéœ€è¦å¹¶å‘æ•°å¤§å°

	// å¯åŠ¨å·¥ä½œåç¨‹
	for range maxConcurrency {
		go f.chunkUploadWorker(workerCtx, srcObj, preuploadID, chunkSize, fileSize, totalChunks, chunkJobs, results)
	}

	// å‘é€éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ä»»åŠ¡ï¼ˆä½¿ç”¨goroutineé¿å…é˜»å¡ï¼‰
	pendingChunks := int64(0)
	fs.Debugf(f, "ğŸ“¤ å¼€å§‹åˆ†å‘åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡...")
	go func() {
		defer close(chunkJobs)
		tasksSent := 0
		for chunkIndex := range totalChunks {
			partNumber := chunkIndex + 1
			if !progress.IsUploaded(partNumber) {
				select {
				case chunkJobs <- chunkIndex:
					tasksSent++
					if tasksSent%10 == 0 || tasksSent <= 5 {
						fs.Debugf(f, "ğŸ“‹ å·²åˆ†å‘ %d ä¸ªåˆ†ç‰‡ä»»åŠ¡ï¼Œå½“å‰åˆ†ç‰‡: %d/%d", tasksSent, partNumber, totalChunks)
					}
				case <-workerCtx.Done():
					fs.Debugf(f, "âš ï¸  ä¸Šä¸‹æ–‡å–æ¶ˆï¼Œåœæ­¢å‘é€åˆ†ç‰‡ä»»åŠ¡ (å·²å‘é€%dä¸ªä»»åŠ¡)", tasksSent)
					return
				}
			}
		}
		fs.Debugf(f, " åˆ†ç‰‡ä»»åŠ¡åˆ†å‘å®Œæˆï¼Œå…±å‘é€ %d ä¸ªä»»åŠ¡", tasksSent)
	}()

	// è®¡ç®—éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡æ•°é‡ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
	for chunkIndex := range totalChunks {
		partNumber := chunkIndex + 1
		if !progress.IsUploaded(partNumber) {
			pendingChunks++
		}
	}

	fs.Debugf(f, "éœ€è¦ä¸Šä¼  %d ä¸ªåˆ†ç‰‡", pendingChunks)

	// å¦‚æœæ²¡æœ‰éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ï¼Œç›´æ¥è·³è¿‡æ”¶é›†ç»“æœ
	if pendingChunks == 0 {
		fs.Debugf(f, "æ‰€æœ‰åˆ†ç‰‡å·²å®Œæˆï¼Œè·³è¿‡ä¸Šä¼ é˜¶æ®µ")
	} else {
		// æ”¶é›†ç»“æœ
		completedChunks := int64(0)
		for completedChunks < pendingChunks {
			select {
			case result := <-results:
				if result.err != nil {
					fs.Errorf(f, "åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %v", result.chunkIndex+1, result.err)
					// ä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
					saveErr := f.saveUploadProgress(progress)
					if saveErr != nil {
						fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
					}
					return nil, fmt.Errorf("åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %w", result.chunkIndex+1, result.err)
				}

				// æ ‡è®°åˆ†ç‰‡å®Œæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰- ä½¿ç”¨å†…ç½®çš„çº¿ç¨‹å®‰å…¨æ–¹æ³•
				partNumber := result.chunkIndex + 1
				progress.SetUploaded(partNumber)
				completedChunks++

				// ä¿å­˜è¿›åº¦
				err := f.saveUploadProgress(progress)
				if err != nil {
					fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
				}

				fs.Debugf(f, " åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ (å¹¶å‘)", partNumber, totalChunks)

				// ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯
				if currentAccount != nil {
					extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completedChunks, totalChunks)
					currentAccount.SetExtraInfo(extraInfo)
				}

			case <-workerCtx.Done():
				// ä¿å­˜å½“å‰è¿›åº¦
				saveErr := f.saveUploadProgress(progress)
				if saveErr != nil {
					fs.Debugf(f, "è¶…æ—¶æ—¶ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
				}

				if workerCtx.Err() == context.DeadlineExceeded {
					return nil, fmt.Errorf("ä¸Šä¼ è¶…æ—¶ï¼Œå·²å®Œæˆ %d/%d åˆ†ç‰‡", completedChunks, pendingChunks)
				}
				return nil, workerCtx.Err()
			case <-ctx.Done():
				// ä¿å­˜å½“å‰è¿›åº¦
				saveErr := f.saveUploadProgress(progress)
				if saveErr != nil {
					fs.Debugf(f, "å–æ¶ˆæ—¶ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
				}
				return nil, ctx.Err()
			}
		}
	}

	// ä½¿ç”¨æºå¯¹è±¡çš„MD5ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œæ›¿ä»£å¤æ‚çš„å“ˆå¸Œç´¯ç§¯å™¨ï¼‰
	fs.Debugf(f, "æ‰€æœ‰åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œä½¿ç”¨æºå¯¹è±¡MD5")
	finalMD5, _ := srcObj.Hash(ctx, fshash.MD5)

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆå¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// ä¿®å¤ï¼šæ¸…ç†è¿›åº¦æ–‡ä»¶ï¼Œä¼ é€’è¿œç¨‹è·¯å¾„å’Œæ–‡ä»¶å¤§å°ä¿¡æ¯
	f.removeUploadProgress(preuploadID, progress.FilePath, fileSize)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return f.createObject(fileName, result.FileID, fileSize, finalMD5, time.Now(), false), nil
}

// chunkResult åˆ†ç‰‡ä¸Šä¼ ç»“æœï¼ŒåŒ…å«å“ˆå¸Œä¿¡æ¯
type chunkResult struct {
	chunkIndex int64
	chunkHash  string // åˆ†ç‰‡MD5å“ˆå¸Œ
	chunkData  []byte // åˆ†ç‰‡æ•°æ®ï¼ˆç”¨äºå“ˆå¸Œç´¯ç§¯ï¼‰
	err        error
}

// chunkUploadWorker åˆ†ç‰‡ä¸Šä¼ å·¥ä½œåç¨‹ï¼Œæ”¯æŒå“ˆå¸Œç´¯ç§¯
func (f *Fs) chunkUploadWorker(ctx context.Context, srcObj fs.Object, preuploadID string, chunkSize, fileSize, totalChunks int64, jobs <-chan int64, results chan<- chunkResult) {
	for chunkIndex := range jobs {
		chunkHash, chunkData, err := f.uploadSingleChunkWithHash(ctx, srcObj, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks)
		results <- chunkResult{
			chunkIndex: chunkIndex,
			chunkHash:  chunkHash,
			chunkData:  chunkData,
			err:        err,
		}
	}
}

// uploadSingleChunkWithHash å¹¶å‘ç‰ˆæœ¬çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼Œè¿”å›å“ˆå¸Œå’Œæ•°æ®ç”¨äºç´¯ç§¯
func (f *Fs) uploadSingleChunkWithHash(ctx context.Context, srcObj fs.Object, _ string, chunkIndex, chunkSize, fileSize, totalChunks int64) (string, []byte, error) {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	chunkEnd = min(chunkEnd, fileSize)
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ %d/%dï¼ˆå«å“ˆå¸Œï¼‰ï¼ŒèŒƒå›´: %d-%dï¼Œå¤§å°: %d", partNumber, totalChunks, chunkStart, chunkEnd-1, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡æ•°æ®æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return "", nil, fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æ•°æ®æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// è¯»å–åˆ†ç‰‡æ•°æ®åˆ°å†…å­˜ï¼ˆç”¨äºå“ˆå¸Œè®¡ç®—å’Œä¸Šä¼ ï¼‰
	chunkData, err := io.ReadAll(chunkReader)
	if err != nil {
		return "", nil, fmt.Errorf("è¯»å–åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	if int64(len(chunkData)) != actualChunkSize {
		return "", nil, fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, len(chunkData))
	}

	// è®¡ç®—åˆ†ç‰‡MD5å“ˆå¸Œ
	chunkHasher := md5.New()
	chunkHasher.Write(chunkData)
	chunkHash := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return "", nil, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥ åˆ†ç‰‡ %d: %w", partNumber, err)
	}

	// æ„é€ åˆ†ç‰‡ä¸Šä¼ URL
	uploadURL := fmt.Sprintf("%s/upload/v2/file/slice", uploadDomain)

	// ä½¿ç”¨æµå¼ä¸Šä¼ é¿å…å¤§å†…å­˜å ç”¨
	err = f.uploadPartStream(ctx, uploadURL, bytes.NewReader(chunkData), actualChunkSize)
	if err != nil {
		return "", nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d/%d å¹¶å‘ä¸Šä¼ æˆåŠŸï¼ŒMD5: %s", partNumber, totalChunks, chunkHash)
	return chunkHash, chunkData, nil
}

// singleStepUpload ä½¿ç”¨123äº‘ç›˜çš„å•æ­¥ä¸Šä¼ APIä¸Šä¼ å°æ–‡ä»¶ï¼ˆ<1GBï¼‰
// è¿™ä¸ªAPIä¸“é—¨ä¸ºå°æ–‡ä»¶è®¾è®¡ï¼Œä¸€æ¬¡HTTPè¯·æ±‚å³å¯å®Œæˆä¸Šä¼ ï¼Œæ•ˆç‡æ›´é«˜
func (f *Fs) singleStepUpload(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	startTime := time.Now()
	fs.Debugf(f, "ä½¿ç”¨å•æ­¥ä¸Šä¼ APIï¼Œæ–‡ä»¶: %s, å¤§å°: %d bytes, MD5: %s", fileName, len(data), md5Hash)

	// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯ï¼šä½¿ç”¨ç»Ÿä¸€çš„éªŒè¯é€»è¾‘
	fs.Debugf(f, " å•æ­¥ä¸Šä¼ : ç»Ÿä¸€éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		fs.Debugf(f, "âš ï¸ å•æ­¥ä¸Šä¼ : é¢„éªŒè¯å‘ç°çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œä½†ç»§ç»­å°è¯•ï¼ˆå¯èƒ½æ˜¯APIå·®å¼‚ï¼‰", parentFileID)
		// ä¸ç«‹å³è¿”å›é”™è¯¯ï¼Œè®©APIè°ƒç”¨æ¥æœ€ç»ˆå†³å®šï¼Œå› ä¸ºä¸åŒAPIå¯èƒ½æœ‰ä¸åŒçš„éªŒè¯é€»è¾‘
	} else {
		fs.Debugf(f, " å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•ID %d é¢„éªŒè¯é€šè¿‡", parentFileID)
	}

	// éªŒè¯æ–‡ä»¶å¤§å°é™åˆ¶
	if len(data) > singleStepUploadLimit {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡å•æ­¥ä¸Šä¼ é™åˆ¶%d bytes: %d bytes", singleStepUploadLimit, len(data))
	}

	// ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åéªŒè¯å’Œæ¸…ç†
	cleanedFileName, warning := f.validateAndCleanFileNameUnified(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// ä½¿ç”¨ä¸“é—¨çš„multipartä¸Šä¼ æ–¹æ³•
	var uploadResp struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			FileID    int64 `json:"fileID"`
			Completed bool  `json:"completed"`
		} `json:"data"`
	}

	err = f.singleStepUploadMultipart(ctx, data, parentFileID, fileName, md5Hash, &uploadResp)
	if err != nil {
		return nil, err
	}

	// æ£€æŸ¥APIå“åº”ç 
	if uploadResp.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if uploadResp.Code == 1 && strings.Contains(uploadResp.Message, "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸ çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œæ¸…ç†ç¼“å­˜", parentFileID)
			// æ¸…ç†ç›¸å…³ç¼“å­˜
			f.clearParentIDCache(parentFileID)
			f.clearDirListCache(parentFileID)

			// ç»Ÿä¸€çˆ¶ç›®å½•IDéªŒè¯é€»è¾‘ï¼šå°è¯•è·å–æ­£ç¡®çš„çˆ¶ç›®å½•ID
			fs.Infof(f, "ğŸ”„ ä½¿ç”¨ç»Ÿä¸€çš„çˆ¶ç›®å½•IDä¿®å¤ç­–ç•¥")
			correctParentID, err := f.getCorrectParentFileID(ctx, parentFileID)
			if err != nil {
				return nil, fmt.Errorf("è·å–æ­£ç¡®çˆ¶ç›®å½•IDå¤±è´¥: %w", err)
			}

			// å¦‚æœè·å¾—äº†ä¸åŒçš„çˆ¶ç›®å½•IDï¼Œé‡æ–°å°è¯•å•æ­¥ä¸Šä¼ 
			if correctParentID != parentFileID {
				fs.Infof(f, "ğŸ”„ å‘ç°æ­£ç¡®çˆ¶ç›®å½•ID: %d (åŸID: %d)ï¼Œé‡æ–°å°è¯•å•æ­¥ä¸Šä¼ ", correctParentID, parentFileID)
				return f.singleStepUpload(ctx, data, correctParentID, fileName, md5Hash)
			}

			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
		}
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸Šä¼ å®Œæˆ
	if !uploadResp.Data.Completed {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ æœªå®Œæˆï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ")
	}

	duration := time.Since(startTime)
	speed := float64(len(data)) / duration.Seconds() / 1024 / 1024 // MB/s

	// å…³é”®ä¿®å¤ï¼šæ ¹æ®ä¸Šä¼ é€Ÿåº¦åˆ¤æ–­æ˜¯å¦ä¸ºç§’ä¼ 
	// ç§’ä¼ é€šå¸¸åœ¨å‡ ç§’å†…å®Œæˆï¼Œé€Ÿåº¦ä¼šéå¸¸å¿«ï¼ˆ>100MB/sï¼‰
	if duration < 5*time.Second && speed > 100 {
		fs.Errorf(f, "ğŸ‰ ã€é‡è¦è°ƒè¯•ã€‘å•æ­¥ä¸Šä¼ ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶ID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	} else {
		fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)
	}

	// è¿”å›Object
	return f.createObject(fileName, uploadResp.Data.FileID, int64(len(data)), md5Hash, time.Now(), false), nil
}

// validateAndCleanFileNameUnified ç»Ÿä¸€çš„éªŒè¯å’Œæ¸…ç†å…¥å£ç‚¹
// è¿™æ˜¯æ¨èçš„æ–‡ä»¶åå¤„ç†æ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ä¸Šä¼ è·¯å¾„çš„ä¸€è‡´æ€§
func (f *Fs) validateAndCleanFileNameUnified(fileName string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(fileName); err == nil {
		return fileName, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(fileName)

	// éªŒè¯æ¸…ç†åçš„æ–‡ä»¶å
	if err := validateFileName(cleanedName); err != nil {
		// å¦‚æœæ¸…ç†åä»ç„¶æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åç§°
		cleanedName = "cleaned_file"
		warning = fmt.Errorf("åŸå§‹æ–‡ä»¶å '%s' æ— æ³•æ¸…ç†ä¸ºæœ‰æ•ˆåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§° '%s'", fileName, cleanedName)
	} else {
		warning = fmt.Errorf("æ–‡ä»¶å '%s' åŒ…å«æ— æ•ˆå­—ç¬¦ï¼Œå·²æ¸…ç†ä¸º '%s'", fileName, cleanedName)
	}

	return cleanedName, warning
}

// getUploadDomain è·å–ä¸Šä¼ åŸŸåï¼Œæ”¯æŒåŠ¨æ€è·å–å’Œç¼“å­˜
func (f *Fs) getUploadDomain(ctx context.Context) (string, error) {

	// å°è¯•åŠ¨æ€è·å–ä¸Šä¼ åŸŸå - ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„
	var response struct {
		Code    int      `json:"code"`
		Data    []string `json:"data"` // ä¿®æ­£ï¼šdataæ˜¯å­—ç¬¦ä¸²æ•°ç»„
		Message string   `json:"message"`
		TraceID string   `json:"x-traceID"`
	}

	// ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„è°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	err := f.makeAPICallWithRest(ctx, "/upload/v2/file/domain", "GET", nil, &response)
	if err == nil && response.Code == 0 && len(response.Data) > 0 {
		domain := response.Data[0] // å–ç¬¬ä¸€ä¸ªåŸŸå
		fs.Debugf(f, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåæˆåŠŸ: %s", domain)

		fs.Debugf(f, "ä¸Šä¼ åŸŸåè·å–æˆåŠŸï¼Œä¸ä½¿ç”¨ç¼“å­˜: %s", domain)

		return domain, nil
	}

	// å¦‚æœåŠ¨æ€è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå
	fs.Debugf(f, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå: %v", err)
	defaultDomains := []string{
		"https://openapi-upload.123242.com",
		"https://openapi-upload.123pan.com", // å¤‡ç”¨åŸŸå
	}

	// ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„åŸŸå
	for _, domain := range defaultDomains {
		// è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
		fs.Debugf(f, "ä½¿ç”¨é»˜è®¤ä¸Šä¼ åŸŸå: %s", domain)
		return domain, nil
	}

	return defaultDomains[0], nil
}

// deleteFile é€šè¿‡ç§»åŠ¨åˆ°å›æ”¶ç«™æ¥åˆ é™¤æ–‡ä»¶
func (f *Fs) deleteFile(ctx context.Context, fileID int64) error {
	fs.Debugf(f, "åˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)

	reqBody := map[string]any{
		"fileIDs": []int64{fileID},
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/trash", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("åˆ é™¤æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ é™¤å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸåˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)
	return nil
}

// moveFile å°†æ–‡ä»¶ç§»åŠ¨åˆ°ä¸åŒçš„ç›®å½•
func (f *Fs) moveFile(ctx context.Context, fileID, toParentFileID int64) error {
	fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)

	reqBody := map[string]any{
		"fileIDs":        []int64{fileID},
		"toParentFileID": toParentFileID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/move", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("ç§»åŠ¨æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("ç§»åŠ¨å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)
	return nil
}

// renameFile é‡å‘½åæ–‡ä»¶ - ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI
func (f *Fs) renameFile(ctx context.Context, fileID int64, fileName string) error {
	fs.Debugf(f, "é‡å‘½åæ–‡ä»¶%dä¸º: %s", fileID, fileName)

	// éªŒè¯æ–°æ–‡ä»¶å
	if err := validateFileName(fileName); err != nil {
		return fmt.Errorf("é‡å‘½åæ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}
	time.Sleep(2 * time.Second)
	// ä½¿ç”¨æ­£ç¡®çš„123ç½‘ç›˜é‡å‘½åAPI: PUT /api/v1/file/name
	reqBody := map[string]any{
		"fileId":   fileID,
		"fileName": fileName,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// å®ç°é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
	maxRetries := 3
	var err error
	for attempt := 1; attempt <= maxRetries; attempt++ {
		fs.Debugf(f, "é‡å‘½åæ–‡ä»¶å°è¯• %d/%d", attempt, maxRetries)

		err = f.makeAPICallWithRest(ctx, "/api/v1/file/name", "PUT", reqBody, &response)
		if err == nil {
			fs.Debugf(f, "é‡å‘½åæ–‡ä»¶æˆåŠŸ: %d -> %s", fileID, fileName)
			return nil
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯"æ–‡ä»¶æœªæ‰¾åˆ°"é”™è¯¯
		if strings.Contains(err.Error(), "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶") || strings.Contains(err.Error(), "APIé”™è¯¯ 1") {
			if attempt < maxRetries {
				waitTime := time.Duration(attempt*2) * time.Second
				fs.Debugf(f, "æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç­‰å¾…%våé‡è¯• (å°è¯• %d/%d)", waitTime, attempt, maxRetries)
				time.Sleep(waitTime)
				continue
			}
		}

		// å…¶ä»–é”™è¯¯æˆ–æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
		break
	}

	return fmt.Errorf("é‡å‘½åæ–‡ä»¶å¤±è´¥ (å°è¯•%dæ¬¡): %w", maxRetries, err)
}

// handlePartialFileConflict å¤„ç†partialæ–‡ä»¶é‡å‘½åå†²çª
// å½“partialæ–‡ä»¶å°è¯•é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶åæ—¶å‘ç”Ÿå†²çªæ—¶è°ƒç”¨
func (f *Fs) handlePartialFileConflict(ctx context.Context, fileID int64, partialName string, parentID int64, targetName string) (*Object, error) {
	fs.Debugf(f, "å¤„ç†partialæ–‡ä»¶å†²çª: %s -> %s (çˆ¶ç›®å½•: %d)", partialName, targetName, parentID)

	// ç§»é™¤.partialåç¼€è·å–åŸå§‹æ–‡ä»¶å
	originalName := strings.TrimSuffix(partialName, ".partial")
	if targetName == "" {
		targetName = originalName
	}

	// æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
	exists, existingFileID, err := f.fileExistsInDirectory(ctx, parentID, targetName)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if exists {
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼ŒID: %d", existingFileID)

		// å¦‚æœå­˜åœ¨çš„æ–‡ä»¶å°±æ˜¯å½“å‰æ–‡ä»¶ï¼ˆå¯èƒ½å·²ç»é‡å‘½åæˆåŠŸï¼‰
		if existingFileID == fileID {
			fs.Debugf(f, "æ–‡ä»¶å·²æˆåŠŸé‡å‘½åï¼Œè¿”å›æˆåŠŸå¯¹è±¡")
			return f.createObject(targetName, fileID, -1, "", time.Now(), false), nil
		}

		// å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶åå†²çªï¼Œç”Ÿæˆå”¯ä¸€åç§°")
		uniqueName, err := f.generateUniqueFileName(ctx, parentID, targetName)
		if err != nil {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
			return nil, fmt.Errorf("ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %w", err)
		}

		fs.Logf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", targetName, uniqueName)
		targetName = uniqueName
	}

	// å°è¯•é‡å‘½åpartialæ–‡ä»¶ä¸ºç›®æ ‡åç§°
	fs.Debugf(f, "é‡å‘½åpartialæ–‡ä»¶: %d -> %s", fileID, targetName)
	err = f.renameFile(ctx, fileID, targetName)
	if err != nil {
		// å¦‚æœé‡å‘½åä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²ç»æœ‰æ­£ç¡®åç§°
		if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
			strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
			fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œæ£€æŸ¥æ–‡ä»¶å½“å‰çŠ¶æ€")

			// å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰æ­£ç¡®åç§°
			exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, parentID, targetName)
			if checkErr == nil && exists && existingFileID == fileID {
				fs.Debugf(f, "æ–‡ä»¶å®é™…å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæˆåŠŸ")
				return &Object{
					fs:          f,
					remote:      targetName,
					hasMetaData: true,
					id:          strconv.FormatInt(fileID, 10),
					size:        -1, // éœ€è¦é‡æ–°è·å–
					modTime:     time.Now(),
					isDir:       false,
				}, nil
			}
		}

		return nil, fmt.Errorf("partialæ–‡ä»¶é‡å‘½åå¤±è´¥: %w", err)
	}

	fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åæˆåŠŸ: %s", targetName)
	return &Object{
		fs:          f,
		remote:      targetName,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        -1, // éœ€è¦é‡æ–°è·å–
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// getUserInfo è·å–ç”¨æˆ·ä¿¡æ¯åŒ…æ‹¬å­˜å‚¨é…é¢ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
func (f *Fs) getUserInfo(ctx context.Context) (*UserInfoResp, error) {
	var response UserInfoResp

	// ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯æ–¹æ³•ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶å’Œé‡è¯•æœºåˆ¶
	err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
	if err != nil {
		return nil, err
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	return &response, nil
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†HTTPæ–¹æ³•ï¼Œæ”¯æŒè‡ªåŠ¨QPSé™åˆ¶å’Œç»Ÿä¸€é”™è¯¯å¤„ç†

// initializeOptions åˆå§‹åŒ–å’ŒéªŒè¯é…ç½®é€‰é¡¹
func initializeOptions(name, root string, m configmap.Mapper) (*Options, string, string, error) {
	// è§£æé€‰é¡¹
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, "", "", err
	}

	// éªŒè¯é…ç½®é€‰é¡¹
	if err := validateOptions(opt); err != nil {
		return nil, "", "", fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	// æ ‡å‡†åŒ–å’Œè®¾ç½®é»˜è®¤å€¼
	normalizeOptions(opt)

	// Store the original name before any modifications for config operations
	originalName := name
	if idx := strings.IndexRune(name, '{'); idx > 0 {
		originalName = name[:idx]
	}

	// è§„èŒƒåŒ–rootè·¯å¾„ï¼Œå¤„ç†ç»å¯¹è·¯å¾„æ ¼å¼
	normalizedRoot := normalizePath(root)
	fs.Debugf(nil, "NewFsè·¯å¾„è§„èŒƒåŒ–: %q -> %q", root, normalizedRoot)

	return opt, originalName, normalizedRoot, nil
}

// checkInstantUpload ç»Ÿä¸€çš„ç§’ä¼ æ£€æŸ¥å‡½æ•°
func (f *Fs) checkInstantUpload(ctx context.Context, parentFileID int64, fileName, md5Hash string, fileSize int64) (*UploadCreateResp, bool, error) {
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, false, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å¤§å°: %s, MD5: %s", fs.SizeSuffix(fileSize), md5Hash)
		return createResp, true, nil
	}

	fs.Infof(f, "ğŸ“¤ éœ€è¦å®é™…ä¸Šä¼ ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))
	return createResp, false, nil
}

// createObjectFromUpload ä»ä¸Šä¼ å“åº”åˆ›å»ºObjectå¯¹è±¡
func (f *Fs) createObjectFromUpload(fileName string, fileSize int64, md5Hash string, fileID int64, modTime time.Time) *Object {
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          fmt.Sprintf("%d", fileID),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       false,
	}
}

// createObject é€šç”¨çš„Objectåˆ›å»ºå‡½æ•°ï¼Œç»Ÿä¸€æ‰€æœ‰Objectåˆ›å»ºé€»è¾‘
func (f *Fs) createObject(remote string, fileID int64, size int64, md5Hash string, modTime time.Time, isDir bool) *Object {
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        size,
		md5sum:      md5Hash,
		modTime:     modTime,
		isDir:       isDir,
	}
}

// createBasicFs åˆ›å»ºåŸºç¡€æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
func createBasicFs(ctx context.Context, name, originalName, normalizedRoot string, opt *Options, m configmap.Mapper) *Fs {
	// åˆ›å»ºHTTPå®¢æˆ·ç«¯ - ä½¿ç”¨å…¬å…±åº“çš„ä¼˜åŒ–é…ç½®
	client := common.GetHTTPClient(ctx)

	// ä¿®å¤ï¼šé¢„å…ˆåˆå§‹åŒ–åŸºæœ¬çš„featuresï¼Œé¿å…Featuresæœªåˆå§‹åŒ–é”™è¯¯
	basicFeatures := &fs.Features{
		NoMultiThreading: false, // é»˜è®¤æ”¯æŒå¤šçº¿ç¨‹
	}

	fs.Debugf(nil, " createBasicFs: åˆå§‹åŒ–åŸºæœ¬features: %p", basicFeatures)

	f := &Fs{
		name:         name,
		originalName: originalName,
		root:         normalizedRoot,
		opt:          *opt,
		m:            m,
		rootFolderID: opt.RootFolderID,
		client:       client,                                         // ä¿ç•™ç”¨äºç‰¹æ®Šæƒ…å†µ
		rst:          rest.NewClient(client).SetRoot(openAPIRootURL), // rcloneæ ‡å‡†restå®¢æˆ·ç«¯
		features:     basicFeatures,                                  // é¢„å…ˆåˆå§‹åŒ–åŸºæœ¬features
	}

	fs.Debugf(f, " createBasicFså®Œæˆ: features=%p", f.features)
	return f
}

// initializePacers åˆå§‹åŒ–APIé™æµå™¨
func initializePacers(ctx context.Context, f *Fs, opt *Options) error {
	// v2 API pacer - é«˜é¢‘ç‡ (~14 QPS for api/v2/file/list)
	listPacerSleep := listV2APIMinSleep
	if opt.ListPacerMinSleep > 0 {
		listPacerSleep = time.Duration(opt.ListPacerMinSleep)
	}
	f.listPacer = createPacer(ctx, listPacerSleep, maxSleep, decayConstant)

	// ä¸¥æ ¼API pacer - ä¸­ç­‰é¢‘ç‡ (~4 QPS for create, async_result, etc.)
	strictPacerSleep := fileMoveMinSleep
	if opt.StrictPacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.StrictPacerMinSleep)
	} else if opt.PacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.PacerMinSleep) // å‘åå…¼å®¹
	}
	f.strictPacer = createPacer(ctx, strictPacerSleep, maxSleep, decayConstant)

	// v2åˆ†ç‰‡ä¸Šä¼ API pacer (~5 QPS for upload/v2/file/slice)
	uploadPacerSleep := uploadV2SliceMinSleep
	if opt.UploadPacerMinSleep > 0 {
		uploadPacerSleep = time.Duration(opt.UploadPacerMinSleep)
	}
	f.uploadPacer = createPacer(ctx, uploadPacerSleep, maxSleep, decayConstant)

	// ä¸‹è½½å’Œé«˜é¢‘ç‡API pacer (~16 QPS for v2 upload APIs)
	downloadPacerSleep := downloadInfoMinSleep
	if opt.DownloadPacerMinSleep > 0 {
		downloadPacerSleep = time.Duration(opt.DownloadPacerMinSleep)
	}
	f.downloadPacer = createPacer(ctx, downloadPacerSleep, maxSleep, decayConstant)

	fs.Infof(f, "ğŸ“Š QPSé™åˆ¶é…ç½® - åˆ—è¡¨API: %.1f QPS, ä¸¥æ ¼API: %.1f QPS, ä¸Šä¼ API: %.1f QPS, ä¸‹è½½API: %.1f QPS",
		1000.0/float64(listPacerSleep.Milliseconds()),
		1000.0/float64(strictPacerSleep.Milliseconds()),
		1000.0/float64(uploadPacerSleep.Milliseconds()),
		1000.0/float64(downloadPacerSleep.Milliseconds()))

	return nil
}

// initializeCaches åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
func initializeCaches(f *Fs) error {
	// åˆå§‹åŒ–ç¼“å­˜é…ç½® - ä½¿ç”¨ç»Ÿä¸€é…ç½®
	f.cacheConfig = common.DefaultUnifiedCacheConfig("123")

	// åº”ç”¨ç”¨æˆ·é…ç½®åˆ°ç¼“å­˜é…ç½®
	f.cacheConfig.MaxCacheSize = f.opt.CacheMaxSize
	f.cacheConfig.TargetCleanSize = f.opt.CacheTargetSize
	f.cacheConfig.EnableSmartCleanup = f.opt.EnableSmartCleanup
	f.cacheConfig.CleanupStrategy = f.opt.CleanupStrategy

	// ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜åˆå§‹åŒ–å™¨ - ä¼ é€’é…ç½®å‚æ•°
	return common.Initialize123Cache(f, &f.parentIDCache, &f.dirListCache, &f.pathToIDCache, &f.cacheConfig)
}

// initializeUnifiedComponents åˆå§‹åŒ–ç»Ÿä¸€ç»„ä»¶
func initializeUnifiedComponents(f *Fs) error {
	// åˆ›å»ºä¸‹è½½é€‚é…å™¨å·¥å‚å‡½æ•°
	createDownloadAdapter := func() interface{} {
		return NewPan123DownloadAdapter(f)
	}

	// ä½¿ç”¨ç»Ÿä¸€çš„ç»„ä»¶åˆå§‹åŒ–å™¨
	components, err := common.Initialize123Components(f, createDownloadAdapter)
	if err != nil {
		return fmt.Errorf("åˆå§‹åŒ–123ç½‘ç›˜ç»Ÿä¸€ç»„ä»¶å¤±è´¥: %w", err)
	}

	// å°†ç»„ä»¶åˆ†é…ç»™æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
	f.resumeManager = components.ResumeManager
	f.errorHandler = components.ErrorHandler
	f.crossCloudCoordinator = components.CrossCloudCoordinator
	f.memoryOptimizer = components.MemoryOptimizer
	f.concurrentDownloader = components.ConcurrentDownloader

	// è®°å½•åˆå§‹åŒ–ç»“æœ
	common.LogComponentInitializationResult("123", f, components, nil)

	return nil
}

// registerCleanupHooks æ³¨å†Œèµ„æºæ¸…ç†é’©å­
func registerCleanupHooks(f *Fs) {
	// æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„èµ„æºæ¸…ç†é’©å­
	// ç¡®ä¿å³ä½¿ç¨‹åºå¼‚å¸¸é€€å‡ºä¹Ÿèƒ½æ¸…ç†ä¸´æ—¶æ–‡ä»¶
	atexit.Register(func() {
		fs.Debugf(f, "ğŸš¨ ç¨‹åºé€€å‡ºä¿¡å·æ£€æµ‹åˆ°ï¼Œå¼€å§‹æ¸…ç†123ç½‘ç›˜èµ„æº...")

		// ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜æ¸…ç†å‡½æ•°
		caches := []*cache.BadgerCache{f.parentIDCache, f.dirListCache, f.pathToIDCache}
		common.CleanupCacheInstances("123", f, caches)
	})
	fs.Debugf(f, "ğŸ›¡ï¸ ç¨‹åºé€€å‡ºæ¸…ç†é’©å­æ³¨å†ŒæˆåŠŸ")
}

// initializeFeatures åˆå§‹åŒ–åŠŸèƒ½ç‰¹æ€§
func initializeFeatures(ctx context.Context, f *Fs) {
	// è°ƒè¯•æ—¥å¿—å·²ä¼˜åŒ–ï¼šFeaturesåˆå§‹åŒ–è¯¦æƒ…å·²ç®€åŒ–

	// ä¿®å¤ï¼šå…ˆåˆ›å»ºfeatureså¯¹è±¡ï¼Œç„¶åè°ƒç”¨Fillï¼Œç¡®ä¿ä¸ä¼šè¿”å›nil
	features := &fs.Features{
		ReadMimeType:            true,
		CanHaveEmptyDirectories: true,
		BucketBased:             false,
		SetTier:                 false,
		GetTier:                 false,
		DuplicateFiles:          false, // 123Pan handles duplicates
		ReadMetadata:            true,
		WriteMetadata:           false, // Not supported
		UserMetadata:            false, // Not supported
		PartialUploads:          false, // ç¦ç”¨.partialæœºåˆ¶ï¼Œé¿å…é‡å‘½åAPIé—®é¢˜
		NoMultiThreading:        false, // Supports concurrent operations
		SlowModTime:             true,  // ModTime operations are slow
		SlowHash:                false, // Hash is available from API
	}

	// è°ƒç”¨Fillæ–¹æ³•æ¥å®Œå–„features
	filledFeatures := features.Fill(ctx, f)

	if filledFeatures != nil {
		f.features = filledFeatures
	} else {
		// å¦‚æœFillè¿”å›nilï¼Œä½¿ç”¨åŸå§‹features
		f.features = features
		fs.Debugf(f, "âš ï¸ Features.Fill()è¿”å›nilï¼Œä½¿ç”¨åŸå§‹features")
	}

	fs.Infof(f, "123ç½‘ç›˜æ€§èƒ½ä¼˜åŒ–åˆå§‹åŒ–å®Œæˆ - æœ€å¤§å¹¶å‘ä¸Šä¼ : %d, æœ€å¤§å¹¶å‘ä¸‹è½½: %d, è¿›åº¦æ˜¾ç¤º: %v, åŠ¨æ€è°ƒæ•´: å¯ç”¨",
		f.opt.MaxConcurrentUploads, f.opt.MaxConcurrentDownloads, f.opt.EnableProgressDisplay)
}

// initializeAuthentication åˆå§‹åŒ–è®¤è¯
func initializeAuthentication(ctx context.Context, f *Fs, m configmap.Mapper) error {
	// Check if we have saved token in config file
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %vï¼ˆè¿‡æœŸæ—¶é—´ %vï¼‰", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if tokenLoaded {
		// Check if token is expired or will expire soon
		if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
			fs.Debugf(f, "ä»¤ç‰Œå·²è¿‡æœŸæˆ–å³å°†è¿‡æœŸï¼Œç«‹å³åˆ·æ–°")
			tokenRefreshNeeded = true
		}
	} else {
		// No token, so login
		fs.Debugf(f, "æœªæ‰¾åˆ°ä»¤ç‰Œï¼Œå°è¯•ç™»å½•")
		err := f.GetAccessToken(ctx)
		if err != nil {
			return fmt.Errorf("initial login failed: %w", err)
		}
		// Save token to config after successful login
		f.saveToken(ctx, m)
		fs.Debugf(f, "ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Try to refresh the token if needed
	if tokenRefreshNeeded {
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œå°è¯•å®Œæ•´ç™»å½•: %v", err)
			// If refresh fails, try full login
			err = f.GetAccessToken(ctx)
			if err != nil {
				return fmt.Errorf("login failed after token refresh failure: %w", err)
			}
		}
		// Save the refreshed/new token
		f.saveToken(ctx, m)
		fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°/ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Setup token renewer for automatic refresh
	fs.Debugf(f, "è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨")
	f.setupTokenRenewer(ctx, m)

	return nil
}

// handleRootDirectory å¤„ç†æ ¹ç›®å½•æŸ¥æ‰¾æˆ–æ–‡ä»¶è·¯å¾„
func handleRootDirectory(ctx context.Context, f *Fs, root string) (fs.Fs, error) {
	// Find the root directory
	err := f.dirCache.FindRoot(ctx, false)
	if err != nil {
		// Assume it is a file
		newRoot, remote := dircache.SplitPath(root)
		// åˆ›å»ºæ–°çš„Fså®ä¾‹ï¼Œé¿å…å¤åˆ¶é”å€¼
		tempF := &Fs{
			name:                 f.name,
			root:                 newRoot,
			opt:                  f.opt,
			features:             f.features, // ä¿®å¤ï¼šå¤åˆ¶featureså­—æ®µ
			m:                    f.m,
			rootFolderID:         f.rootFolderID,
			client:               f.client,
			rst:                  f.rst, // ä¿®å¤ï¼šä¹Ÿå¤åˆ¶rstå­—æ®µ
			listPacer:            f.listPacer,
			strictPacer:          f.strictPacer,
			uploadPacer:          f.uploadPacer,
			downloadPacer:        f.downloadPacer,
			parentIDCache:        f.parentIDCache,
			dirListCache:         f.dirListCache,
			pathToIDCache:        f.pathToIDCache,
			resumeManager:        f.resumeManager,
			errorHandler:         f.errorHandler,         // ä¿®å¤ï¼šå¤åˆ¶ç»Ÿä¸€ç»„ä»¶
			memoryOptimizer:      f.memoryOptimizer,      // ä¿®å¤ï¼šå¤åˆ¶ç»Ÿä¸€ç»„ä»¶
			concurrentDownloader: f.concurrentDownloader, // ä¿®å¤ï¼šå¤åˆ¶ç»Ÿä¸€ç»„ä»¶
			cacheConfig:          f.cacheConfig,
		}
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// ğŸ”§ ä¿®å¤ï¼šå¯¹äºbackendå‘½ä»¤ï¼Œä¸è¿”å›ErrorIsFileï¼Œè€Œæ˜¯è¿”å›æ­£å¸¸çš„Fså®ä¾‹
		// è¿™æ ·å¯ä»¥è®©backendå‘½ä»¤æ­£å¸¸å·¥ä½œï¼ŒåŒæ—¶ä¿æŒæ–‡ä»¶å¯¹è±¡çš„å¼•ç”¨
		fs.Debugf(tempF, "æ–‡ä»¶è·¯å¾„å¤„ç†ï¼šåˆ›å»ºæ–‡ä»¶æ¨¡å¼Fså®ä¾‹ï¼Œæ–‡ä»¶: %s", remote)
		return tempF, nil
	}

	return f, nil
}

// newFs ä»è·¯å¾„æ„é€ Fsï¼Œæ ¼å¼ä¸º container:path
func newFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	// åˆå§‹åŒ–å’ŒéªŒè¯é…ç½®é€‰é¡¹
	opt, originalName, normalizedRoot, err := initializeOptions(name, root, m)
	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŸºç¡€æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡
	f := createBasicFs(ctx, name, originalName, normalizedRoot, opt, m)

	// åˆå§‹åŒ–APIé™æµå™¨
	if err := initializePacers(ctx, f, opt); err != nil {
		return nil, fmt.Errorf("åˆå§‹åŒ–APIé™æµå™¨å¤±è´¥: %w", err)
	}

	// åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ
	if err := initializeCaches(f); err != nil {
		return nil, fmt.Errorf("åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿå¤±è´¥: %w", err)
	}

	// åˆå§‹åŒ–ç»Ÿä¸€ç»„ä»¶
	if err := initializeUnifiedComponents(f); err != nil {
		return nil, fmt.Errorf("åˆå§‹åŒ–ç»Ÿä¸€ç»„ä»¶å¤±è´¥: %w", err)
	}

	// æ³¨å†Œèµ„æºæ¸…ç†é’©å­
	registerCleanupHooks(f)

	// åˆå§‹åŒ–åŠŸèƒ½ç‰¹æ€§
	initializeFeatures(ctx, f)

	// åˆå§‹åŒ–ç›®å½•ç¼“å­˜
	f.dirCache = dircache.New(normalizedRoot, f.rootFolderID, f)

	// åˆå§‹åŒ–è®¤è¯
	if err := initializeAuthentication(ctx, f, m); err != nil {
		return nil, fmt.Errorf("åˆå§‹åŒ–è®¤è¯å¤±è´¥: %w", err)
	}

	// æŸ¥æ‰¾æ ¹ç›®å½•æˆ–å¤„ç†æ–‡ä»¶è·¯å¾„
	return handleRootDirectory(ctx, f, root)
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨NewObject: %s", remote)

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "è§„èŒƒåŒ–åçš„è¿œç¨‹è·¯å¾„: %s -> %s", remote, normalizedRemote)

	// Get the full path
	var fullPath string
	if f.root == "" {
		fullPath = normalizedRemote
	} else if normalizedRemote == "" {
		// å½“remoteä¸ºç©ºæ—¶ï¼ŒfullPathå°±æ˜¯rootæœ¬èº«
		fullPath = f.root
	} else {
		fullPath = normalizePath(f.root + "/" + normalizedRemote)
	}

	if fullPath == "" {
		fullPath = "/"
	}

	// Get file ID
	fileID, err := f.pathToFileID(ctx, fullPath)
	if err != nil {
		if err == fs.ErrorObjectNotFound {
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, " NewObject pathToFileIDç»“æœ: fullPath=%s -> fileID=%s", fullPath, fileID)

	// Get file details
	fileInfo, err := f.getFileInfo(ctx, fileID)
	if err != nil {
		return nil, err
	}

	// Check if it's a file (not directory)
	// Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	if fileInfo.Type == 1 {
		return nil, fs.ErrorNotAFile
	}

	// ç¡®å®šæ­£ç¡®çš„remoteè·¯å¾„
	objectRemote := remote
	if objectRemote == "" && f.root != "" {
		// å½“remoteä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºremote
		objectRemote = fileInfo.Filename
		fs.Debugf(f, " NewObjectè®¾ç½®remote: %s -> %s", remote, objectRemote)
	}

	fileIDInt, _ := strconv.ParseInt(fileID, 10, 64)
	o := f.createObject(objectRemote, fileIDInt, fileInfo.Size, fileInfo.Etag, time.Now(), false)

	return o, nil
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	return fs.ModTimeNotSupported
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨Put: %s", src.Remote())

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(src.Remote())
	fs.Debugf(f, "Putæ“ä½œè§„èŒƒåŒ–è·¯å¾„: %s -> %s", src.Remote(), normalizedRemote)

	// Use dircache to find parent directory
	fs.Debugf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•è·¯å¾„: %s", normalizedRemote)
	leaf, parentID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		fs.Errorf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %v", err)
		return nil, err
	}
	fileName := leaf
	fs.Debugf(f, "æ‰¾åˆ°çˆ¶ç›®å½•ID: %s, æ–‡ä»¶å: %s", parentID, fileName)

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return nil, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯parentFileIDæ˜¯å¦çœŸçš„å­˜åœ¨
	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d æ˜¯å¦å­˜åœ¨", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		fs.Errorf(f, "éªŒè¯çˆ¶ç›®å½•IDå¤±è´¥: %v", err)
		// å°è¯•æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		fs.Debugf(f, "æ¸…ç†ç›®å½•ç¼“å­˜å¹¶é‡è¯•")
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else if !exists {
		fs.Errorf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•é‡å»ºç›®å½•ç¼“å­˜", parentFileID)
		// æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		f.dirCache.ResetRoot()
		_, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºç›®å½•ç¼“å­˜åæŸ¥æ‰¾å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºåè§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡å»ºåæ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// Use streaming optimization for cross-cloud transfers
	obj, err := f.streamingPut(ctx, in, src, parentFileID, fileName)

	if err == nil {
		// ä¸Šä¼ æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		f.invalidateRelatedCaches(normalizedRemote, "upload")
	}

	return obj, err
}

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	// In a real implementation, you would fetch metadata if not already available.
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// In a real implementation, you would fetch metadata if not already available.
	return o.size
}

// Hash returns the MD5 checksum
func (o *Object) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t != fshash.MD5 {
		return "", fshash.ErrUnsupported
	}
	// In a real implementation, you would fetch metadata if not already available.
	return o.md5sum, nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// In a real implementation, you would fetch metadata if not already available.
	return o.id
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	// è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šæ£€æµ‹å¤§æ–‡ä»¶å¹¶å¯ç”¨å¤šçº¿ç¨‹ä¸‹è½½ï¼ˆå‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
	// ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰ç¦ç”¨å¹¶å‘ä¸‹è½½é€‰é¡¹ï¼Œé¿å…é‡å¤å¹¶å‘
	hasDisableOption := false
	for _, option := range options {
		if option.String() == "DisableConcurrentDownload" {
			hasDisableOption = true
			break
		}
	}

	if !hasDisableOption && o.shouldUseConcurrentDownload(ctx, options) {
		return o.openWithConcurrency(ctx, options...)
	}

	var resp *http.Response

	// Use pacer for download with retry (use strict pacer for download URL requests)
	err := o.fs.strictPacer.Call(func() (bool, error) {
		// Get download URL (may need refresh)
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, nil, err)
		}

		// Create HTTP request
		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, fmt.Errorf("failed to create download request: %w", err)
		}

		// Handle range requests
		var start, end int64 = 0, -1
		for _, option := range options {
			switch x := option.(type) {
			case *fs.RangeOption:
				start, end = x.Start, x.End
			case *fs.SeekOption:
				start = x.Offset
			}
		}

		if start > 0 || end >= 0 {
			rangeHeader := fmt.Sprintf("bytes=%d-", start)
			if end >= 0 {
				rangeHeader = fmt.Sprintf("bytes=%d-%d", start, end)
			}
			req.Header.Set("Range", rangeHeader)
			fs.Debugf(o.fs, "è¯·æ±‚èŒƒå›´: %s", rangeHeader)
		}

		// Set user agent
		req.Header.Set("User-Agent", o.fs.opt.UserAgent)
		// Note: Don't set timeout here as it will be applied to the entire download

		// Make the request
		resp, err = o.fs.client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// Check status code
		switch resp.StatusCode {
		case http.StatusOK, http.StatusPartialContent:
			return false, nil // Success
		case http.StatusFound, http.StatusMovedPermanently, http.StatusTemporaryRedirect, http.StatusPermanentRedirect:
			// Let the HTTP client handle redirects automatically
			// This should not happen if the client is configured correctly
			resp.Body.Close()
			return false, fmt.Errorf("unexpected redirect response %d - client should handle this automatically", resp.StatusCode)
		case http.StatusRequestedRangeNotSatisfiable:
			resp.Body.Close()
			return false, fmt.Errorf("range not satisfiable")
		case http.StatusNotFound:
			resp.Body.Close()
			return false, fs.ErrorObjectNotFound
		default:
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("download failed with status %d: %s", resp.StatusCode, string(body)))
		}
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// shouldUseConcurrentDownload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
func (o *Object) shouldUseConcurrentDownload(ctx context.Context, options []fs.OpenOption) bool {
	// ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨çš„åˆ¤æ–­é€»è¾‘ï¼Œé¿å…é‡å¤åˆ¤æ–­
	if o.fs.concurrentDownloader != nil {
		return o.fs.concurrentDownloader.ShouldUseConcurrentDownload(ctx, o, options)
	}

	// å›é€€é€»è¾‘ï¼šå¦‚æœç»Ÿä¸€ä¸‹è½½å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–åˆ¤æ–­
	return o.size >= 10*1024*1024 // 10MBé˜ˆå€¼
}

// openWithConcurrency ä½¿ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨æ‰“å¼€æ–‡ä»¶ï¼ˆç”¨äºè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// è°ƒè¯•ï¼šæ£€æŸ¥ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨çŠ¶æ€
	fs.Debugf(o, " openWithConcurrencyå¼€å§‹: concurrentDownloaderå¯ç”¨=%v", o.fs.concurrentDownloader != nil)

	// æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
	if o.fs.concurrentDownloader == nil {
		fs.Debugf(o, " ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨ä¸å¯ç”¨ï¼Œä½†shouldUseConcurrentDownloadè¿”å›äº†trueï¼Œä½¿ç”¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½")
		return o.openWithCustomConcurrency(ctx, options...)
	}

	if !o.fs.concurrentDownloader.ShouldUseConcurrentDownload(ctx, o, options) {
		// è°ƒè¯•æ—¥å¿—å·²ä¼˜åŒ–ï¼šå¹¶å‘ä¸‹è½½åˆ¤æ–­è¯¦æƒ…å·²ç®€åŒ–
		return o.openNormal(ctx, options...)
	}

	fs.Infof(o, "ğŸš€ 123ç½‘ç›˜å¯åŠ¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "123_unified_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// å®Œå…¨å‚è€ƒ115ç½‘ç›˜ï¼šåˆ›å»ºTransferå¯¹è±¡ç”¨äºè¿›åº¦æ˜¾ç¤º
	var downloadTransfer *accounting.Transfer
	if stats := accounting.GlobalStats(); stats != nil {
		downloadTransfer = stats.NewTransferRemoteSize(
			fmt.Sprintf("ğŸ“¥ %s (123ç½‘ç›˜ä¸‹è½½)", o.Remote()),
			o.size,
			o.fs, // æºæ˜¯123ç½‘ç›˜
			nil,  // ç›®æ ‡æœªçŸ¥
		)
		defer downloadTransfer.Done(ctx, nil)
		// è°ƒè¯•æ—¥å¿—å·²ä¼˜åŒ–ï¼šå¯¹è±¡åˆ›å»ºè¯¦æƒ…ä»…åœ¨è¯¦ç»†æ¨¡å¼ä¸‹è¾“å‡º)
	}

	// ä½¿ç”¨æ”¯æŒAccountçš„å¹¶å‘ä¸‹è½½å™¨
	var downloadedSize int64
	if downloadTransfer != nil {
		downloadedSize, err = o.fs.concurrentDownloader.DownloadToFileWithAccount(ctx, o, tempFile, downloadTransfer, options...)
	} else {
		// å›é€€åˆ°åŸæ–¹æ³•
		downloadedSize, err = o.fs.concurrentDownloader.DownloadToFile(ctx, o, tempFile, options...)
	}

	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "ç»Ÿä¸€å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// éªŒè¯ä¸‹è½½å¤§å°
	if downloadedSize != o.size {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "ä¸‹è½½å¤§å°ä¸åŒ¹é…ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: æœŸæœ›%dï¼Œå®é™…%d", o.size, downloadedSize)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(o, "âœ… 123ç½‘ç›˜ç»Ÿä¸€å¹¶å‘ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(downloadedSize))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return &ConcurrentDownloadReader{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// openNormal æ™®é€šçš„æ‰“å¼€æ–¹æ³•ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
func (o *Object) openNormal(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {

	var resp *http.Response

	// ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½
	err := o.fs.downloadPacer.Call(func() (bool, error) {
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: %w", err))
		}

		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, err
		}

		// å¤„ç†Rangeé€‰é¡¹
		fs.FixRangeOption(options, o.size)
		fs.OpenOptionAddHTTPHeaders(req.Header, options)

		resp, err = o.fs.client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err))
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d, å“åº”: %s", resp.StatusCode, string(body)))
		}

		return false, nil
	})

	if err != nil {
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// openWithCustomConcurrency ä½¿ç”¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½ï¼ˆå½“ç»Ÿä¸€ä¸‹è½½å™¨ä¸å¯ç”¨æ—¶ï¼‰
func (o *Object) openWithCustomConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Infof(o, "ğŸš€ 123ç½‘ç›˜å¯åŠ¨è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "123_custom_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// ä½¿ç”¨ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é€»è¾‘
	err = o.downloadWithCustomConcurrency(ctx, tempFile, options...)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(o, "âœ… 123ç½‘ç›˜è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(o.size))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return &ConcurrentDownloadReader{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// downloadWithCustomConcurrency è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å®ç°
func (o *Object) downloadWithCustomConcurrency(ctx context.Context, tempFile *os.File, _ ...fs.OpenOption) error {
	// ç®€åŒ–çš„å¹¶å‘ä¸‹è½½é…ç½®
	chunkSize := int64(50 * 1024 * 1024) // 50MBåˆ†ç‰‡
	maxConcurrency := 6                  // 6çº¿ç¨‹å¹¶å‘

	fileSize := o.size
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	if numChunks < int64(maxConcurrency) {
		maxConcurrency = int(numChunks)
	}

	fs.Infof(o, "ğŸ“Š 123ç½‘ç›˜è‡ªå®šä¹‰å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// è·å–ä¸‹è½½URL
	downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
	if err != nil {
		return fmt.Errorf("è·å–ä¸‹è½½URLå¤±è´¥: %w", err)
	}

	// åˆ›å»ºå¹¶å‘ä¸‹è½½ä»»åŠ¡
	var wg sync.WaitGroup
	errChan := make(chan error, maxConcurrency)
	semaphore := make(chan struct{}, maxConcurrency)

	for i := range numChunks {
		start := i * chunkSize
		end := start + chunkSize - 1
		if end >= fileSize {
			end = fileSize - 1
		}

		wg.Add(1)
		go func(chunkIndex, start, end int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// ä¸‹è½½åˆ†ç‰‡
			err := o.downloadChunk(ctx, downloadURL, tempFile, start, end, chunkIndex)
			if err != nil {
				errChan <- fmt.Errorf("åˆ†ç‰‡%dä¸‹è½½å¤±è´¥: %w", chunkIndex, err)
			}
		}(i, start, end)
	}

	// ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	for err := range errChan {
		return err
	}

	return nil
}

// downloadChunk ä¸‹è½½å•ä¸ªåˆ†ç‰‡
func (o *Object) downloadChunk(ctx context.Context, url string, tempFile *os.File, start, end, chunkIndex int64) error {
	// åˆ›å»ºHTTPè¯·æ±‚
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}

	// è®¾ç½®Rangeå¤´
	req.Header.Set("Range", fmt.Sprintf("bytes=%d-%d", start, end))
	req.Header.Set("User-Agent", o.fs.opt.UserAgent)

	// æ‰§è¡Œè¯·æ±‚
	resp, err := o.fs.client.Do(req)
	if err != nil {
		return fmt.Errorf("æ‰§è¡Œä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
	}
	defer resp.Body.Close()

	// æ£€æŸ¥å“åº”çŠ¶æ€
	if resp.StatusCode != http.StatusPartialContent && resp.StatusCode != http.StatusOK {
		return fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
	}

	// è¯»å–æ•°æ®
	expectedSize := end - start + 1
	buffer := make([]byte, expectedSize)
	totalRead := int64(0)

	for totalRead < expectedSize {
		n, err := resp.Body.Read(buffer[totalRead:])
		if n > 0 {
			totalRead += int64(n)
		}
		if err != nil {
			if err == io.EOF {
				break
			}
			return fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®å¤§å°
	if totalRead != expectedSize {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, totalRead)
	}

	// å†™å…¥åˆ°æ–‡ä»¶
	writtenBytes, err := tempFile.WriteAt(buffer[:totalRead], start)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯å†™å…¥çš„å­—èŠ‚æ•°
	if int64(writtenBytes) != totalRead {
		return fmt.Errorf("åˆ†ç‰‡å†™å…¥ä¸å®Œæ•´: æœŸæœ›å†™å…¥%dï¼Œå®é™…å†™å…¥%d", totalRead, writtenBytes)
	}

	fs.Debugf(o, " 123ç½‘ç›˜åˆ†ç‰‡%dä¸‹è½½å®Œæˆ: %s (bytes=%d-%d)",
		chunkIndex, fs.SizeSuffix(totalRead), start, end)

	return nil
}

// Update the object with new content.
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œå¤šçº¿ç¨‹ä¸Šä¼ 
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	fs.Debugf(o.fs, "è°ƒç”¨Update: %s", o.remote)

	startTime := time.Now()

	// Use dircache to find parent directory
	leaf, parentID, err := o.fs.dirCache.FindPath(ctx, o.remote, false)
	if err != nil {
		return err
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(o.fs, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedFileName
	}

	// ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡Œæ›´æ–°
	fs.Debugf(o.fs, "ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡ŒUpdateæ“ä½œ")
	newObj, err := o.fs.streamingPut(ctx, in, src, parentFileID, leaf)

	// è®°å½•ä¸Šä¼ å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(startTime)
	if err != nil {
		fs.Errorf(o.fs, "Updateæ“ä½œå¤±è´¥: %v", err)
		return err
	}

	// æ›´æ–°å½“å‰å¯¹è±¡çš„å…ƒæ•°æ®
	o.size = newObj.size
	o.md5sum = newObj.md5sum
	o.modTime = newObj.modTime
	o.id = newObj.id

	fs.Debugf(o.fs, "Updateæ“ä½œæˆåŠŸï¼Œè€—æ—¶: %v", duration)

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	fs.Debugf(o.fs, "è°ƒç”¨Remove: %s", o.remote)

	// æ£€æŸ¥æ–‡ä»¶IDæ˜¯å¦æœ‰æ•ˆ
	if o.id == "" {
		fs.Debugf(o.fs, "æ–‡ä»¶IDä¸ºç©ºï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶: %s", o.remote)
		// å¯¹äºpartialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬æ— æ³•åˆ é™¤ï¼Œä½†ä¹Ÿä¸åº”è¯¥æŠ¥é”™
		// å› ä¸ºè¿™äº›æ–‡ä»¶å¯èƒ½æ ¹æœ¬ä¸å­˜åœ¨äºæœåŠ¡å™¨ä¸Š
		return nil
	}

	// Convert file ID to int64
	fileID, err := parseFileID(o.id)
	if err != nil {
		fs.Debugf(o.fs, "è§£ææ–‡ä»¶IDå¤±è´¥ï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶: %s, é”™è¯¯: %v", o.remote, err)
		// å¯¹äºæ— æ³•è§£æçš„æ–‡ä»¶IDï¼Œæˆ‘ä»¬ä¹Ÿä¸æŠ¥é”™ï¼Œå› ä¸ºè¿™é€šå¸¸æ˜¯partialæ–‡ä»¶
		return nil
	}

	err = o.fs.deleteFile(ctx, fileID)
	if err == nil {
		// åˆ é™¤æ–‡ä»¶æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		o.fs.invalidateRelatedCaches(o.remote, "remove")
	}
	return err
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// DirCacher interface implementation for dircache

// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the parent directory
	response, err := f.ListFile(ctx, int(parentFileID), f.opt.ListChunk, "", "", 0)
	if err != nil {
		return "", false, err
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true
			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}
			return foundID, found, nil
		}
	}

	return "", false, nil
}

// findLeafWithForceRefresh å¼ºåˆ¶åˆ·æ–°ç¼“å­˜åæŸ¥æ‰¾å¶èŠ‚ç‚¹
// ç”¨äºè§£å†³ç¼“å­˜ä¸ä¸€è‡´å¯¼è‡´çš„ç›®å½•æŸ¥æ‰¾å¤±è´¥é—®é¢˜
func (f *Fs) findLeafWithForceRefresh(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// å¼ºåˆ¶æ¸…ç†ç¼“å­˜
	f.clearDirListCache(parentFileID)

	// ç›´æ¥è°ƒç”¨APIè·å–æœ€æ–°çš„ç›®å½•åˆ—è¡¨ï¼Œè·³è¿‡ç¼“å­˜
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", f.opt.ListChunk))

	endpoint := "/api/v2/file/list?" + params.Encode()

	var response ListResponse
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", false, fmt.Errorf("å¼ºåˆ¶åˆ·æ–°APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf in fresh data
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true

			// æ›´æ–°ç¼“å­˜ä»¥ä¿æŒä¸€è‡´æ€§
			f.saveDirListToCache(parentFileID, 0, response.Data.FileList, response.Data.LastFileId)

			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}

			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°å¶èŠ‚ç‚¹: %s -> %s", leaf, foundID)
			return foundID, found, nil
		}
	}

	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æœªæ‰¾åˆ°å¶èŠ‚ç‚¹: %s", leaf)
	return "", false, nil
}

// CreateDir creates a directory with the given name in the parent folder pathID.
// Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	fs.Debugf(f, "åˆ›å»ºç›®å½•: pathID=%s, leaf=%s", pathID, leaf)

	// éªŒè¯å‚æ•°
	if leaf == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if pathID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®å½•å
	cleanedDirName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(f, "ç›®å½•åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedDirName
	}

	// Create the directory using the existing createDirectory method
	dirID, err := f.createDirectory(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}

	return dirID, nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs          = (*Fs)(nil)
	_ fs.Object      = (*Object)(nil)
	_ fs.Mover       = (*Fs)(nil)
	_ fs.DirMover    = (*Fs)(nil)
	_ fs.Copier      = (*Fs)(nil)
	_ fs.Abouter     = (*Fs)(nil)
	_ fs.Purger      = (*Fs)(nil)
	_ fs.Shutdowner  = (*Fs)(nil)
	_ fs.ObjectInfo  = (*Object)(nil)
	_ fs.IDer        = (*Object)(nil)
	_ fs.SetModTimer = (*Object)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, _ configmap.Mapper) bool {
	if f.opt.Token == "" {
		fs.Debugf(f, "é…ç½®ä¸­æœªæ‰¾åˆ°ä»¤ç‰Œï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰")
		return false
	}

	var tokenData TokenJSON                                                // Use the new TokenJSON struct
	var err error                                                          // Declare err here
	if err = json.Unmarshal([]byte(f.opt.Token), &tokenData); err != nil { // Unmarshal unobscured token
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œå¤±è´¥ï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰: %v", err)
		return false
	}

	f.token = tokenData.AccessToken
	f.tokenExpiry, err = time.Parse(time.RFC3339, tokenData.Expiry) // Use tokenData.Expiry
	if err != nil {
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œè¿‡æœŸæ—¶é—´å¤±è´¥: %v", err)
		return false
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "é…ç½®ä¸­çš„ä»¤ç‰Œä¸å®Œæ•´")
		return false
	}

	fs.Debugf(f, "ä»é…ç½®æ–‡ä»¶åŠ è½½ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)
	return true
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(_ context.Context, m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - æä¾›çš„æ˜ å°„å™¨ä¸ºç©º")
		return
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	tokenData := TokenJSON{ // Use the new TokenJSON struct
		AccessToken: f.token,
		Expiry:      f.tokenExpiry.Format(time.RFC3339), // Use Expiry
	}
	tokenJSON, err := json.Marshal(tokenData)
	if err != nil {
		fs.Errorf(f, "ä¿å­˜ä»¤ç‰Œæ—¶åºåˆ—åŒ–å¤±è´¥: %v", err)
		return
	}

	m.Set("token", string(tokenJSON))
	// m.Set does not return an error, so we assume it succeeds.
	// Any errors related to config saving would be handled by the underlying config system.

	fs.Debugf(f, "ä½¿ç”¨åŸå§‹åç§°%qä¿å­˜ä»¤ç‰Œåˆ°é…ç½®æ–‡ä»¶", f.originalName)
}

// setupTokenRenewer initializes the token renewer to automatically refresh tokens
func (f *Fs) setupTokenRenewer(ctx context.Context, m configmap.Mapper) {
	// Only set up renewer if we have valid tokens
	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	// Create a renewal transaction function
	transaction := func() error {
		fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨è§¦å‘ï¼Œåˆ·æ–°ä»¤ç‰Œ")
		// Use non-global function to avoid deadlocks
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Errorf(f, "æ›´æ–°å™¨ä¸­åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %v", err)
			return err
		}

		return nil // saveToken is already called in refreshTokenIfNecessary
	}

	// Create minimal OAuth config
	config := &oauthutil.Config{
		TokenURL: openAPIRootURL + "/api/v1/access_token", // Use the correct token URL for 123Pan
	}

	// Create a token source using the existing token
	token := &oauth2.Token{
		AccessToken: f.token,
		// 123Pan API does not seem to return a refresh token in the initial access token response,
		// so we rely on re-logging in if the access token expires.
		// RefreshToken: f.refreshToken, // Not available for 123Pan
		Expiry:    f.tokenExpiry,
		TokenType: "Bearer",
	}

	// Save token to config so it can be accessed by TokenSource
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨ä¿å­˜ä»¤ç‰Œå¤±è´¥: %v", err)
		return
	}

	// Create a client with the token source
	_, ts, err := oauthutil.NewClientWithBaseClient(ctx, f.originalName, m, config, getHTTPClient(ctx, f.originalName, m))
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨åˆ›å»ºä»¤ç‰Œæºå¤±è´¥: %v", err)
		return
	}

	// Create token renewer that will trigger when the token is about to expire
	f.tokenRenewer = oauthutil.NewRenew(f.originalName, ts, transaction)
	f.tokenRenewer.Start() // Start the renewer immediately
	fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨å·²åˆå§‹åŒ–å¹¶å¯åŠ¨ï¼Œä½¿ç”¨åŸå§‹åç§°%q", f.originalName)
}

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	f.tokenMu.Lock()

	// Check if another thread has successfully refreshed while we were waiting
	if !refreshTokenExpired && !forceRefresh && f.token != "" && time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		fs.Debugf(f, "è·å–é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡åˆ·æ–°")
		f.tokenMu.Unlock()
		return nil
	}
	defer f.tokenMu.Unlock()

	// Perform the actual token refresh
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token during refresh: %w", err)
	}

	f.token = accessToken
	f.tokenExpiry = expiredAt

	// Save the refreshed token to config
	f.saveToken(ctx, f.m)

	return nil
}

// GetAccessToken fetches a new access token from the 123Pan API.
func (f *Fs) GetAccessToken(ctx context.Context) error {
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token: %w", err)
	}
	f.token = accessToken
	f.tokenExpiry = expiredAt
	return nil
}

type ClientCredentials struct {
	ClientID     string `json:"clientID"`
	ClientSecret string `json:"clientSecret"`
}

type TokenJSON struct {
	AccessToken string `json:"access_token"`
	Expiry      string `json:"expiry"`
}

type AccessTokenData struct {
	AccessToken string `json:"accessToken"`
	ExpiredAt   string `json:"expiredAt"`
}

type Response struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     AccessTokenData
	XTraceID string `json:"x-traceID"`
}

func GetAccessToken(clientID, clientSecret string) (string, time.Time, error) {
	credentials := ClientCredentials{
		ClientID:     clientID,
		ClientSecret: clientSecret,
	}
	payload, err := json.Marshal(credentials)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to marshal payload: %v", err)
	}
	req, err := http.NewRequest("POST", "https://open-api.123pan.com/api/v1/access_token", bytes.NewBuffer(payload))
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to create request: %v", err)
	}
	req.Header.Set("Platform", "open_platform")
	req.Header.Set("Content-Type", "application/json")
	client := common.GetStandardHTTPClient(context.Background())
	resp, err := client.Do(req)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to send request: %v", err)
	}
	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return "", time.Time{}, fmt.Errorf("unexpected status code: %d", resp.StatusCode)
	}
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to read response body: %v", err)
	}
	var result Response
	if err = json.Unmarshal(body, &result); err != nil {
		return "", time.Time{}, fmt.Errorf("failed to unmarshal response: %v", err)
	}
	expiredAt, err := time.Parse(time.RFC3339, result.Data.ExpiredAt)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to parse expiration time: %v", err)
	}
	return result.Data.AccessToken, expiredAt, nil
}

// Name of the remote (e.g. "123")
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (e.g. "bucket" or "bucket/dir")
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("123 root '%s'", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	fs.Debugf(f, " Featuresæ–¹æ³•è¢«è°ƒç”¨: f.features=%p", f.features)

	// ä¿®å¤ï¼šç°åœ¨featuresåœ¨createBasicFsä¸­å°±è¢«åˆå§‹åŒ–äº†ï¼Œåº”è¯¥ä¸ä¼šä¸ºnil
	if f.features == nil {
		// è¿™ç§æƒ…å†µç°åœ¨åº”è¯¥ä¸ä¼šå‘ç”Ÿï¼Œä½†ä¿ç•™ä½œä¸ºå®‰å…¨æªæ–½
		fs.Errorf(f, "âš ï¸ Featuresæ„å¤–æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
		defaultFeatures := &fs.Features{
			NoMultiThreading: false, // é»˜è®¤æ”¯æŒå¤šçº¿ç¨‹
		}
		fs.Debugf(f, " è¿”å›é»˜è®¤features: %p", defaultFeatures)
		return defaultFeatures
	}

	fs.Debugf(f, " è¿”å›å·²åˆå§‹åŒ–çš„features: %p", f.features)
	return f.features
}

// Hashes returns the supported hash sets.
func (f *Fs) Hashes() fshash.Set {
	return fshash.Set(fshash.MD5)
}

// About gets quota information
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	fs.Debugf(f, "è°ƒç”¨About")

	userInfo, err := f.getUserInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get user info: %w", err)
	}

	usage := &fs.Usage{}
	if userInfo.Data.SpacePermanent > 0 {
		usage.Total = fs.NewUsageValue(userInfo.Data.SpacePermanent)
	}
	if userInfo.Data.SpaceUsed > 0 {
		usage.Used = fs.NewUsageValue(userInfo.Data.SpaceUsed)
	}
	if userInfo.Data.SpacePermanent > 0 && userInfo.Data.SpaceUsed > 0 {
		free := userInfo.Data.SpacePermanent - userInfo.Data.SpaceUsed
		if free > 0 {
			usage.Free = fs.NewUsageValue(free)
		}
	}

	return usage, nil
}

// Purge deletes all the files in the directory
func (f *Fs) Purge(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨Purgeåˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List all files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return fmt.Errorf("æ¸…ç†ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return err
		}

		if response.Code != 0 {
			return fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Collect all files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Delete all files
	for _, file := range allFiles {
		err := f.deleteFile(ctx, file.FileID)
		if err != nil {
			fs.Errorf(f, "åˆ é™¤æ–‡ä»¶å¤±è´¥ %s: %v", file.Filename, err)
			// Continue with other files
		}
	}

	return nil
}

// Shutdown the backend
func (f *Fs) Shutdown(ctx context.Context) error {
	fs.Debugf(f, "è°ƒç”¨å…³é—­")

	// Stop token renewer if it exists
	if f.tokenRenewer != nil {
		f.tokenRenewer.Stop()
		f.tokenRenewer = nil
	}

	// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼ˆå·²ç®€åŒ–ï¼‰

	// å…³é—­ç¼“å­˜
	caches := []*cache.BadgerCache{f.parentIDCache, f.dirListCache, f.pathToIDCache}
	for i, c := range caches {
		if c != nil {
			if err := c.Close(); err != nil {
				fs.Debugf(f, "å…³é—­ç¼“å­˜%då¤±è´¥: %v", i, err)
			}
		}
	}

	return nil
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "è°ƒç”¨åˆ—è¡¨ï¼Œç›®å½•: %s", dir)

	// ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœdirä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
	if dir == "" && f.root != "" {
		// æ£€æŸ¥rootæ˜¯å¦æŒ‡å‘ä¸€ä¸ªæ–‡ä»¶
		rootObj, err := f.NewObject(ctx, "")
		if err == nil {
			// rootæŒ‡å‘ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨: %s", f.root)
			return fs.DirEntries{rootObj}, nil
		}
		// å¦‚æœNewObjectå¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºrootæŒ‡å‘ç›®å½•
		if err == fs.ErrorNotAFile {
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘ç›®å½•ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s", f.root)
		} else {
			fs.Debugf(f, "rootè·¯å¾„æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s, é”™è¯¯: %v", f.root, err)
		}
	}

	// ä½¿ç”¨ç›®å½•ç¼“å­˜æŸ¥æ‰¾ç›®å½•ID
	fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•ID: %s", dir)
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•å¤±è´¥ %s: %v", dir, err)
		return nil, err
	}
	fs.Debugf(f, "æ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %s", dirID, dir)

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return nil, fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)
	maxIterations := 10000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š10000æ¬¡è¿­ä»£
	iteration := 0

	for {
		iteration++
		if iteration > maxIterations {
			return nil, fmt.Errorf("åˆ—è¡¨ç›®å½•è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•° %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxIterations)
		}
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return nil, err
		}

		if response.Code != 0 {
			return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Filter out trashed files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Convert to fs.DirEntries
	for _, file := range allFiles {
		// ä¿®å¤ï¼šæ¸…ç†APIè¿”å›çš„æ–‡ä»¶åï¼Œå»é™¤å‰åç©ºæ ¼å¹¶è¿›è¡ŒURLè§£ç 
		cleanedFilename := strings.TrimSpace(file.Filename)

		// URLè§£ç æ–‡ä»¶åï¼ˆå¤„ç†%3Aç­‰ç¼–ç ï¼‰
		if decodedName, err := url.QueryUnescape(cleanedFilename); err == nil {
			cleanedFilename = decodedName
		}
		if cleanedFilename != file.Filename {
			fs.Debugf(f, " æ¸…ç†æ–‡ä»¶å: [%s] -> [%s]", file.Filename, cleanedFilename)
		}

		remote := cleanedFilename
		if dir != "" {
			remote = strings.Trim(dir+"/"+cleanedFilename, "/")
		}

		if file.Type == 1 { // Directory
			// Cache the directory ID for future use
			f.dirCache.Put(remote, strconv.FormatInt(file.FileID, 10))
			d := fs.NewDir(remote, time.Time{})
			entries = append(entries, d)
		} else { // File
			o := &Object{
				fs:          f,
				remote:      remote,
				hasMetaData: true,
				id:          strconv.FormatInt(file.FileID, 10),
				size:        file.Size,
				md5sum:      file.Etag,
				modTime:     time.Now(), // We'll parse this properly later
				isDir:       false,
			}
			entries = append(entries, o)
		}
	}

	return entries, nil
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ›å»ºç›®å½•: %s", dir)

	// Use dircache to create the directory
	_, err := f.dirCache.FindDir(ctx, dir, true)
	if err == nil {
		// åˆ›å»ºç›®å½•æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		f.invalidateRelatedCaches(dir, "mkdir")
	}
	return err
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	fileID, err := strconv.ParseInt(dirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid directory ID: %s", dirID)
	}

	// Delete the directory
	err = f.deleteFile(ctx, fileID)
	if err != nil {
		return err
	}

	// Remove from cache
	f.dirCache.FlushDir(dir)

	// åˆ é™¤ç›®å½•æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches(dir, "rmdir")

	return nil
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨ç§»åŠ¨ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Moveæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		return nil, fs.ErrorCantMove
	}

	// Use dircache to find destination directory
	dstName, dstDirID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	cleanedDstName, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "ç§»åŠ¨æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		dstName = cleanedDstName
	}

	// è½¬æ¢ç›®æ ‡ç›®å½•ID
	dstParentID, err := strconv.ParseInt(dstDirID, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid destination directory ID: %s", dstDirID)
	}

	// ç”Ÿæˆå”¯ä¸€çš„ç›®æ ‡æ–‡ä»¶åï¼Œé¿å…å†²çª
	uniqueDstName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
	if err != nil {
		fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åç§°: %v", err)
		uniqueDstName = dstName
	}

	if uniqueDstName != dstName {
		fs.Logf(f, "æ£€æµ‹åˆ°æ–‡ä»¶åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", dstName, uniqueDstName)
		dstName = uniqueDstName
	}

	// Convert IDs to int64
	fileID, err := strconv.ParseInt(srcObj.id, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid file ID: %s", srcObj.id)
	}

	// Get source directory ID for comparison
	_, srcDirID, err := f.dirCache.FindPath(ctx, srcObj.Remote(), false)
	if err != nil {
		return nil, fmt.Errorf("failed to find source directory: %w", err)
	}

	// è·å–æºæ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰
	srcBaseName := filepath.Base(srcObj.Remote())
	fs.Debugf(f, "æºæ–‡ä»¶åŸºç¡€åç§°: %s, ç›®æ ‡æ–‡ä»¶å: %s", srcBaseName, dstName)

	// Check if we're moving to the same directory
	if srcDirID == dstDirID {
		// Same directory - only rename if name changed
		if dstName != srcBaseName {
			fs.Debugf(f, "åŒç›®å½•ç§»åŠ¨ï¼Œä»…é‡å‘½å %s ä¸º %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				return nil, err
			}
		} else {
			fs.Debugf(f, "åŒç›®å½•åŒå - æ— éœ€æ“ä½œ")
		}
	} else {
		// ä¸åŒç›®å½• - å…ˆç§»åŠ¨ï¼Œç„¶åæ ¹æ®éœ€è¦é‡å‘½å
		fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶ä»ç›®å½• %s åˆ° %s", srcDirID, dstDirID)

		// åœ¨è·¨ç›®å½•ç§»åŠ¨æ—¶ï¼Œå…ˆæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶
		if dstName == srcBaseName {
			// å¦‚æœç›®æ ‡æ–‡ä»¶åä¸æºæ–‡ä»¶åç›¸åŒï¼Œæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡ä»¶
			exists, existingFileID, err := f.fileExistsInDirectory(ctx, dstParentID, dstName)
			if err != nil {
				fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err)
			} else if exists {
				fs.Debugf(f, "ç›®æ ‡ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ŒID: %d", existingFileID)
				if existingFileID == fileID {
					fs.Debugf(f, "æ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
					// æ–‡ä»¶å·²ç»åœ¨æ­£ç¡®ä½ç½®ï¼Œç›´æ¥è¿”å›æˆåŠŸ
					return &Object{
						fs:          f,
						remote:      remote,
						hasMetaData: srcObj.hasMetaData,
						id:          srcObj.id,
						size:        srcObj.size,
						md5sum:      srcObj.md5sum,
						modTime:     srcObj.modTime,
						isDir:       srcObj.isDir,
					}, nil
				} else {
					fs.Debugf(f, "ç›®æ ‡ç›®å½•å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°")
					uniqueName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
					if err != nil {
						fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
					} else {
						dstName = uniqueName
						fs.Logf(f, "ä½¿ç”¨å”¯ä¸€æ–‡ä»¶åé¿å…å†²çª: %s", dstName)
					}
				}
			}
		}

		err = f.moveFile(ctx, fileID, dstParentID)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½• - ä¿®å¤å­—ç¬¦ä¸²åŒ¹é…é—®é¢˜
			if strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸­") ||
				strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
				fs.Debugf(f, "æ–‡ä»¶å¯èƒ½å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œæ£€æŸ¥çŠ¶æ€")
				exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, srcBaseName)
				if checkErr == nil && exists && existingFileID == fileID {
					fs.Debugf(f, "ç¡®è®¤æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œç»§ç»­é‡å‘½åæ­¥éª¤")
					// æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œè·³è¿‡ç§»åŠ¨æ­¥éª¤
				} else {
					// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
					if strings.HasSuffix(srcBaseName, ".partial") {
						fs.Debugf(f, "Partialæ–‡ä»¶ç§»åŠ¨å†²çªï¼Œå°è¯•å¤„ç†æ–‡ä»¶åå†²çª")
						return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
					}
					return nil, err
				}
			} else {
				return nil, err
			}
		}

		// If the name changed, rename the file
		if dstName != srcBaseName {
			fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åæ–‡ä»¶: %s -> %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				// æ£€æŸ¥é‡å‘½åå¤±è´¥çš„åŸå›  - å¢å¼ºé”™è¯¯åŒ¹é…
				if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
					strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
					fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œç›®å½•ä¸­å·²æœ‰é‡åæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡æ–‡ä»¶")
					exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, dstName)
					if checkErr == nil && exists && existingFileID == fileID {
						fs.Debugf(f, "æ–‡ä»¶å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæ“ä½œå®é™…å·²æˆåŠŸ")
						// æ–‡ä»¶å·²ç»æœ‰æ­£ç¡®çš„åç§°ï¼Œè®¤ä¸ºæ“ä½œæˆåŠŸ
					} else {
						// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
						if strings.HasSuffix(srcBaseName, ".partial") {
							fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œå°è¯•ç‰¹æ®Šå¤„ç†")
							return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
						}
						return nil, err
					}
				} else {
					return nil, err
				}
			}
		}
	}

	// ç§»åŠ¨æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches(srcObj.Remote(), "move")
	f.invalidateRelatedCaches(remote, "move")

	// Return the moved object
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: srcObj.hasMetaData,
		id:          srcObj.id,
		size:        srcObj.size,
		md5sum:      srcObj.md5sum,
		modTime:     srcObj.modTime,
		isDir:       srcObj.isDir,
	}, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	fs.Debugf(f, "è°ƒç”¨ç›®å½•ç§»åŠ¨ %s åˆ° %s", srcRemote, dstRemote)

	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(f, "æ— æ³•ç§»åŠ¨ç›®å½• - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return fs.ErrorCantDirMove
	}

	// Find source directory ID
	srcDirID, err := srcFs.dirCache.FindDir(ctx, srcRemote, false)
	if err != nil {
		return err
	}

	// Find destination parent directory
	dstName, dstParentID, err := f.dirCache.FindPath(ctx, dstRemote, true)
	if err != nil {
		return err
	}

	// Convert IDs to int64
	srcFileID, err := strconv.ParseInt(srcDirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid source directory ID: %s", srcDirID)
	}

	dstParentFileID, err := strconv.ParseInt(dstParentID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid destination parent ID: %s", dstParentID)
	}

	// è·å–æºç›®å½•çš„åŸºç¡€åç§°
	srcBaseName := filepath.Base(srcRemote)
	fs.Debugf(f, "æºç›®å½•åŸºç¡€åç§°: %s, ç›®æ ‡ç›®å½•å: %s", srcBaseName, dstName)
	fs.Debugf(f, "æºç›®å½•ID: %d, ç›®æ ‡çˆ¶ç›®å½•ID: %d", srcFileID, dstParentFileID)

	// æ£€æŸ¥æ˜¯å¦å°è¯•ç§»åŠ¨åˆ°åŒä¸€ä¸ªçˆ¶ç›®å½•
	srcParentID, err := f.getParentID(ctx, srcFileID)
	if err != nil {
		fs.Debugf(f, "è·å–æºç›®å½•çˆ¶IDå¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "æºç›®å½•å½“å‰çˆ¶ID: %d", srcParentID)
		if srcParentID == dstParentFileID && dstName == srcBaseName {
			fs.Debugf(f, "ç›®å½•å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
			return nil
		}
	}

	// Move the directory
	err = f.moveFile(ctx, srcFileID, dstParentFileID)
	if err != nil {
		return err
	}

	// If the name changed, rename the directory
	if dstName != srcBaseName {
		fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åç›®å½•: %s -> %s", srcBaseName, dstName)
		err = f.renameFile(ctx, srcFileID, dstName)
		if err != nil {
			return err
		}
	}

	// Update cache
	srcFs.dirCache.FlushDir(srcRemote)
	f.dirCache.FlushDir(dstRemote)

	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨å¤åˆ¶ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "Copyæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•å¤åˆ¶ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantCopy
	}

	// Use dircache to find destination directory
	dstName, _, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	_, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "å¤åˆ¶æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		// Note: cleaned name is not used in current implementation
	}

	// 123Pan doesn't have a direct copy API, use download+upload
	fs.Debugf(f, "123ç½‘ç›˜ä¸æ”¯æŒæœåŠ¡å™¨ç«¯å¤åˆ¶ï¼Œä½¿ç”¨ä¸‹è½½+ä¸Šä¼ æ–¹å¼")
	return f.copyViaDownloadUpload(ctx, srcObj, remote)
}

// copyViaDownloadUpload copies a file by downloading and re-uploading
func (f *Fs) copyViaDownloadUpload(ctx context.Context, srcObj *Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "é€šè¿‡ä¸‹è½½+ä¸Šä¼ å¤åˆ¶ %s åˆ° %s", srcObj.remote, remote)

	// Open source file for reading
	src, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file: %w", err)
	}
	defer src.Close()

	// Create a new object info for the destination
	dstInfo := &ObjectInfo{
		fs:      f,
		remote:  remote,
		size:    srcObj.size,
		md5sum:  srcObj.md5sum,
		modTime: srcObj.modTime,
	}

	// Upload to destination
	return f.Put(ctx, src, dstInfo)
}

// ObjectInfo implements fs.ObjectInfo for copy operations
type ObjectInfo struct {
	fs      *Fs
	remote  string
	size    int64
	md5sum  string
	modTime time.Time
}

func (oi *ObjectInfo) Fs() fs.Info                           { return oi.fs }
func (oi *ObjectInfo) Remote() string                        { return oi.remote }
func (oi *ObjectInfo) Size() int64                           { return oi.size }
func (oi *ObjectInfo) ModTime(ctx context.Context) time.Time { return oi.modTime }
func (oi *ObjectInfo) Storable() bool                        { return true }
func (oi *ObjectInfo) String() string                        { return oi.remote }
func (oi *ObjectInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t == fshash.MD5 {
		return oi.md5sum, nil
	}
	return "", fshash.ErrUnsupported
}

// getHTTPClient makes an http client according to the options
// ä½¿ç”¨å…¬å…±åº“çš„æ ‡å‡†HTTPå®¢æˆ·ç«¯é…ç½®
func getHTTPClient(ctx context.Context, _ string, _ configmap.Mapper) *http.Client {
	// ä½¿ç”¨å…¬å…±åº“çš„æ ‡å‡†HTTPå®¢æˆ·ç«¯
	return common.GetStandardHTTPClient(ctx)
}

// getNetworkQuality è¯„ä¼°å½“å‰ç½‘ç»œè´¨é‡ï¼Œè¿”å›0.0-1.0çš„è´¨é‡åˆ†æ•°
func (f *Fs) getNetworkQuality() float64 {
	// ç®€åŒ–ç½‘ç»œè´¨é‡è¯„ä¼° - ä½¿ç”¨å›ºå®šçš„è‰¯å¥½ç½‘ç»œè´¨é‡å‡è®¾
	return 0.8 // é»˜è®¤å‡è®¾ç½‘ç»œè´¨é‡è‰¯å¥½
}

// getAdaptiveTimeout æ ¹æ®æ–‡ä»¶å¤§å°ã€ä¼ è¾“ç±»å‹è®¡ç®—è‡ªé€‚åº”è¶…æ—¶æ—¶é—´
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†è¶…æ—¶è®¡ç®—
func (f *Fs) getAdaptiveTimeout(fileSize int64, transferType string) time.Duration {
	// ç®€åŒ–è¶…æ—¶è®¡ç®— - ä½¿ç”¨å›ºå®šçš„åˆç†ç®—æ³•
	baseTimeout := time.Duration(f.opt.Timeout)
	if baseTimeout <= 0 {
		baseTimeout = defaultTimeout
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆæ›´ç²¾ç»†çš„è®¡ç®—ï¼‰
	// æ¯50MBå¢åŠ 30ç§’è¶…æ—¶æ—¶é—´ï¼Œå¯¹å¤§æ–‡ä»¶æ›´å‹å¥½
	sizeBasedTimeout := time.Duration(fileSize/50/1024/1024) * 30 * time.Second

	// æ ¹æ®ä¼ è¾“ç±»å‹å’Œç½‘ç»œæ¡ä»¶è°ƒæ•´
	var typeMultiplier float64 = 1.0
	switch transferType {
	case "chunked_upload":
		typeMultiplier = 1.5 // åˆ†ç‰‡ä¸Šä¼ éœ€è¦é€‚ä¸­æ—¶é—´
	case "stream_download":
		typeMultiplier = 1.2 // æµå¼ä¸‹è½½éœ€è¦é€‚ä¸­æ—¶é—´
	case "single_step":
		typeMultiplier = 0.8 // å•æ­¥ä¸Šä¼ æ—¶é—´è¾ƒçŸ­
	case "concurrent_upload":
		typeMultiplier = 2.0 // å¹¶å‘ä¸Šä¼ éœ€è¦æ›´é•¿æ—¶é—´
	default:
		typeMultiplier = 1.0
	}

	// è€ƒè™‘ç½‘ç»œè´¨é‡
	networkQuality := f.getNetworkQuality()
	var qualityMultiplier float64 = 1.0
	if networkQuality < 0.3 { // ç½‘ç»œè´¨é‡å¾ˆå·®
		qualityMultiplier = 3.0
	} else if networkQuality < 0.5 { // ç½‘ç»œè´¨é‡å·®
		qualityMultiplier = 2.0
	} else if networkQuality < 0.7 { // ç½‘ç»œè´¨é‡ä¸€èˆ¬
		qualityMultiplier = 1.5
	} else {
		qualityMultiplier = 1.0 // ç½‘ç»œè´¨é‡è‰¯å¥½
	}

	adaptiveTimeout := baseTimeout + time.Duration(float64(sizeBasedTimeout)*typeMultiplier*qualityMultiplier)

	// è®¾ç½®æ›´åˆç†çš„è¾¹ç•Œ
	minTimeout := 2 * time.Minute // æœ€å°2åˆ†é’Ÿ
	maxTimeout := 4 * time.Hour   // æœ€å¤§4å°æ—¶ï¼Œæ”¯æŒè¶…å¤§æ–‡ä»¶

	if adaptiveTimeout < minTimeout {
		adaptiveTimeout = minTimeout
	}
	if adaptiveTimeout > maxTimeout {
		adaptiveTimeout = maxTimeout
	}

	fs.Debugf(f, "è‡ªé€‚åº”è¶…æ—¶è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç±»å‹=%s, ç½‘ç»œè´¨é‡=%.2f, è¶…æ—¶æ—¶é—´=%v",
		fs.SizeSuffix(fileSize), transferType, networkQuality, adaptiveTimeout)

	return adaptiveTimeout
}

// detectNetworkSpeed æ£€æµ‹ç½‘ç»œä¼ è¾“é€Ÿåº¦ï¼Œç”¨äºåŠ¨æ€ä¼˜åŒ–å‚æ•°
func (f *Fs) detectNetworkSpeed(_ context.Context) int64 {
	fs.Debugf(f, "å¼€å§‹æ£€æµ‹ç½‘ç»œé€Ÿåº¦")

	// ç®€åŒ–ç½‘ç»œé€Ÿåº¦æ£€æµ‹ - ä½¿ç”¨å›ºå®šçš„åˆç†é€Ÿåº¦ä¼°ç®—

	// ä½¿ç”¨å¿«é€Ÿä¼°ç®—æ–¹æ³•ï¼Œé¿å…å¤§æ–‡ä»¶åˆå§‹åŒ–å»¶è¿Ÿ
	// åŸºäºç½‘ç»œè´¨é‡è¿›è¡Œå¿«é€Ÿä¼°ç®—ï¼Œä¸è¿›è¡Œå®é™…ç½‘ç»œæµ‹è¯•
	networkQuality := f.getNetworkQuality()

	// åŸºç¡€é€Ÿåº¦ä¼°ç®—ï¼ˆåŸºäºå…¸å‹ç½‘ç»œç¯å¢ƒï¼‰
	var baseSpeed int64 = 10 * 1024 * 1024 // 10MB/s åŸºç¡€é€Ÿåº¦

	// æ ¹æ®ç½‘ç»œè´¨é‡è°ƒæ•´é€Ÿåº¦ä¼°ç®—
	if networkQuality >= 0.8 {
		baseSpeed = 20 * 1024 * 1024 // 20MB/s é«˜è´¨é‡ç½‘ç»œ
	} else if networkQuality >= 0.6 {
		baseSpeed = 15 * 1024 * 1024 // 15MB/s è‰¯å¥½ç½‘ç»œ
	} else if networkQuality >= 0.4 {
		baseSpeed = 8 * 1024 * 1024 // 8MB/s ä¸€èˆ¬ç½‘ç»œ
	} else {
		baseSpeed = 5 * 1024 * 1024 // 5MB/s è¾ƒå·®ç½‘ç»œ
	}

	fs.Debugf(f, "ç½‘ç»œé€Ÿåº¦ä¼°ç®—: %s/s (è´¨é‡=%.2f)",
		fs.SizeSuffix(baseSpeed), networkQuality)

	return baseSpeed
}

// getOptimalConcurrency æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†å¹¶å‘è®¡ç®—
func (f *Fs) getOptimalConcurrency(fileSize int64, networkSpeed int64) int {
	// ç®€åŒ–å¹¶å‘è®¡ç®— - ä½¿ç”¨å›ºå®šçš„åˆç†ç®—æ³•
	baseConcurrency := f.opt.MaxConcurrentUploads
	if baseConcurrency <= 0 {
		baseConcurrency = 4 // é»˜è®¤å¹¶å‘æ•°
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´å¹¶å‘æ•° - ä¼˜åŒ–å¤§æ–‡ä»¶å¹¶å‘ç­–ç•¥
	var sizeFactor float64 = 1.0
	if fileSize > 20*1024*1024*1024 { // >20GB - è¶…å¤§æ–‡ä»¶
		sizeFactor = 3.0 // æ˜¾è‘—æå‡å¹¶å‘æ•°
	} else if fileSize > 10*1024*1024*1024 { // >10GB - å¤§æ–‡ä»¶
		sizeFactor = 2.5
	} else if fileSize > 5*1024*1024*1024 { // >5GB - ä¸­å¤§æ–‡ä»¶
		sizeFactor = 2.0
	} else if fileSize > 2*1024*1024*1024 { // >2GB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.5
	} else if fileSize > 1*1024*1024*1024 { // >1GB - è¾ƒå¤§æ–‡ä»¶
		sizeFactor = 1.2
	} else if fileSize > 500*1024*1024 { // >500MB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.0
	} else if fileSize > 100*1024*1024 { // >100MB - å°æ–‡ä»¶
		sizeFactor = 0.8
	} else {
		sizeFactor = 0.5 // å¾ˆå°æ–‡ä»¶ä½¿ç”¨è¾ƒå°‘å¹¶å‘
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´ - æ€§èƒ½ä¼˜åŒ–ï¼šæ›´ç²¾ç»†çš„ç½‘ç»œé€Ÿåº¦åˆ†çº§å’Œè‡ªé€‚åº”è°ƒæ•´
	var speedFactor float64 = 1.0

	// æ–°å¢ï¼šç½‘ç»œè´¨é‡è‡ªé€‚åº”è°ƒæ•´
	networkLatency := f.measureNetworkLatency()
	latencyFactor := 1.0
	if networkLatency < 50 { // <50ms ä½å»¶è¿Ÿ
		latencyFactor = 1.2
	} else if networkLatency < 100 { // <100ms ä¸­ç­‰å»¶è¿Ÿ
		latencyFactor = 1.0
	} else if networkLatency < 200 { // <200ms é«˜å»¶è¿Ÿ
		latencyFactor = 0.8
	} else { // >200ms å¾ˆé«˜å»¶è¿Ÿ
		latencyFactor = 0.6
	}

	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedFactor = 2.0 * latencyFactor // ä»1.8æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.7 * latencyFactor // ä»1.5æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.4 * latencyFactor // ä»1.2æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - ä¸­é€Ÿç½‘ç»œ
		speedFactor = 1.1 * latencyFactor // ä»1.0æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else if networkSpeed > 10*1024*1024 { // >10Mbps - ä¸­ä½é€Ÿç½‘ç»œ
		speedFactor = 0.9 * latencyFactor // ä»0.8æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	} else {
		speedFactor = 0.7 * latencyFactor // ä»0.6æå‡ï¼Œç»“åˆå»¶è¿Ÿè°ƒæ•´
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	optimalConcurrency := int(float64(baseConcurrency) * sizeFactor * speedFactor)

	// ç”¨æˆ·è¦æ±‚ï¼š123ç½‘ç›˜å•æ–‡ä»¶å¹¶å‘é™åˆ¶åˆ°4
	minConcurrency := 1
	maxConcurrency := 4 // ç”¨æˆ·è¦æ±‚çš„å¹¶å‘ä¸Šé™

	// ä¸å†å…è®¸è¶…å¤§æ–‡ä»¶ä½¿ç”¨æ›´é«˜å¹¶å‘æ•°ï¼Œç»Ÿä¸€é™åˆ¶ä¸º4

	if optimalConcurrency < minConcurrency {
		optimalConcurrency = minConcurrency
	}
	if optimalConcurrency > maxConcurrency {
		optimalConcurrency = maxConcurrency
	}

	fs.Debugf(f, "ğŸš€ ä¼˜åŒ–å¹¶å‘æ•°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€å¹¶å‘=%d, å¤§å°å› å­=%.1f, é€Ÿåº¦å› å­=%.1f, æœ€ä¼˜å¹¶å‘=%d",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed), baseConcurrency, sizeFactor, speedFactor, optimalConcurrency)

	return optimalConcurrency
}

// getOptimalChunkSize æ ¹æ®æ–‡ä»¶å¤§å°è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å›ºå®šçš„åˆç†åˆ†ç‰‡å¤§å°è®¡ç®—
func (f *Fs) getOptimalChunkSize(fileSize int64, networkSpeed int64) int64 {
	fs.Debugf(f, " å¼€å§‹è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°: æ–‡ä»¶å¤§å°=%s", fs.SizeSuffix(fileSize))

	// ç®€åŒ–åˆ†ç‰‡å¤§å°è®¡ç®— - ä½¿ç”¨å›ºå®šçš„åˆç†ç®—æ³•
	baseChunk := int64(f.opt.ChunkSize)
	if baseChunk <= 0 {
		baseChunk = int64(defaultChunkSize) // 100MB
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´åˆ†ç‰‡å¤§å° - æ€§èƒ½ä¼˜åŒ–ï¼šæå‡åˆ†ç‰‡å¤§å°å€æ•°
	var speedMultiplier float64 = 1.0
	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 4.0 // 400MBåˆ†ç‰‡ï¼ˆä»3.0æå‡ï¼‰
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 3.0 // 300MBåˆ†ç‰‡ï¼ˆä»2.0æå‡ï¼‰
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é€Ÿç½‘ç»œ
		speedMultiplier = 2.0 // 200MBåˆ†ç‰‡ï¼ˆä»1.5æå‡ï¼‰
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - æ™®é€šç½‘ç»œ
		speedMultiplier = 1.5 // 150MBåˆ†ç‰‡ï¼ˆä»1.0æå‡ï¼‰
	} else { // <20Mbps - æ…¢é€Ÿç½‘ç»œ
		speedMultiplier = 0.8 // 80MBåˆ†ç‰‡ï¼ˆä»0.5æå‡ï¼‰
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´ - æ€§èƒ½ä¼˜åŒ–ï¼šæ›´ç²¾ç»†çš„æ–‡ä»¶å¤§å°åˆ†çº§
	var sizeMultiplier float64 = 1.0
	if fileSize > 50*1024*1024*1024 { // >50GB
		sizeMultiplier = 2.0 // å¤§æ–‡ä»¶ä½¿ç”¨æ›´å¤§åˆ†ç‰‡ï¼ˆä»1.5æå‡ï¼‰
	} else if fileSize > 20*1024*1024*1024 { // >20GB
		sizeMultiplier = 1.8 // æ–°å¢ï¼šè¶…å¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 10*1024*1024*1024 { // >10GB
		sizeMultiplier = 1.5 // æ–°å¢ï¼šå¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 5*1024*1024*1024 { // >5GB
		sizeMultiplier = 1.3 // æ–°å¢ï¼šä¸­å¤§æ–‡ä»¶åˆ†çº§
	} else if fileSize > 2*1024*1024*1024 { // >2GB
		sizeMultiplier = 1.1 // æ–°å¢ï¼šä¸­ç­‰æ–‡ä»¶åˆ†çº§
	} else if fileSize < 500*1024*1024 { // <500MB
		sizeMultiplier = 0.7 // å°æ–‡ä»¶ä½¿ç”¨æ›´å°åˆ†ç‰‡ï¼ˆä»0.5æå‡ï¼‰
	}

	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
	optimalChunkSize := int64(float64(baseChunk) * speedMultiplier * sizeMultiplier)

	// è®¾ç½®åˆç†è¾¹ç•Œ - æ€§èƒ½ä¼˜åŒ–ï¼šæå‡æœ€å¤§åˆ†ç‰‡å¤§å°é™åˆ¶
	minChunkSize := int64(minChunkSize)      // 50MB
	maxChunkSize := int64(800 * 1024 * 1024) // 800MBï¼ˆä»500MBæå‡ï¼‰

	if optimalChunkSize < minChunkSize {
		optimalChunkSize = minChunkSize
	}
	if optimalChunkSize > maxChunkSize {
		optimalChunkSize = maxChunkSize
	}

	fs.Debugf(f, "åŠ¨æ€åˆ†ç‰‡å¤§å°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€åˆ†ç‰‡=%s, æœ€ä¼˜åˆ†ç‰‡=%s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed),
		fs.SizeSuffix(baseChunk), fs.SizeSuffix(optimalChunkSize))

	return optimalChunkSize
}

// measureNetworkLatency æµ‹é‡ç½‘ç»œå»¶è¿Ÿ
// æ€§èƒ½ä¼˜åŒ–ï¼šå®ç°ç½‘ç»œå»¶è¿Ÿæ£€æµ‹ï¼Œç”¨äºè‡ªé€‚åº”å¹¶å‘è°ƒæ•´
func (f *Fs) measureNetworkLatency() int64 {
	// ç®€å•çš„å»¶è¿Ÿæµ‹é‡ï¼šå‘123ç½‘ç›˜APIå‘é€HEADè¯·æ±‚
	start := time.Now()

	// æ„å»ºæµ‹è¯•URL
	testURL := "https://www.123pan.com/api/v1/user/info"
	req, err := http.NewRequest("HEAD", testURL, nil)
	if err != nil {
		return 100 // é»˜è®¤å»¶è¿Ÿ100ms
	}

	// è®¾ç½®è¯·æ±‚å¤´
	req.Header.Set("User-Agent", f.opt.UserAgent)

	// å‘é€è¯·æ±‚ - ä½¿ç”¨ç»Ÿä¸€çš„HTTPå®¢æˆ·ç«¯
	client := common.GetStandardHTTPClient(context.Background())
	resp, err := client.Do(req)
	if err != nil {
		return 150 // ç½‘ç»œé”™è¯¯æ—¶è¿”å›è¾ƒé«˜å»¶è¿Ÿ
	}
	defer resp.Body.Close()

	latency := time.Since(start).Milliseconds()
	return latency
}

// ResourcePool èµ„æºæ± ç®¡ç†å™¨ï¼Œç”¨äºä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œå‡å°‘GCå‹åŠ›
// æ€§èƒ½ä¼˜åŒ–ï¼šå¢å¼ºå†…å­˜æ± ç®¡ç†ï¼Œå‡å°‘å†…å­˜åˆ†é…å’ŒGCå‹åŠ›
type ResourcePool struct {
	bufferPool   sync.Pool // ç¼“å†²åŒºæ± 
	hasherPool   sync.Pool // MD5å“ˆå¸Œè®¡ç®—å™¨æ± 
	chunkPool    sync.Pool // åˆ†ç‰‡ç¼“å†²åŒºæ± ï¼ˆæ–°å¢ï¼‰
	readerPool   sync.Pool // Readeræ± ï¼ˆæ–°å¢ï¼‰
	progressPool sync.Pool // è¿›åº¦è·Ÿè¸ªå™¨æ± ï¼ˆæ–°å¢ï¼‰

	// ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ª
	tempFilesMu sync.Mutex
	tempFiles   map[string]bool // è·Ÿè¸ªåˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶
}

// DownloadProgress 123ç½‘ç›˜ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
type DownloadProgress struct {
	totalChunks     int64
	completedChunks int64
	totalBytes      int64
	downloadedBytes int64
	startTime       time.Time
	lastUpdateTime  time.Time
	chunkSizes      map[int64]int64         // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„å¤§å°
	chunkTimes      map[int64]time.Duration // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„ä¸‹è½½æ—¶é—´
	peakSpeed       float64                 // å³°å€¼é€Ÿåº¦ MB/s
	mu              sync.RWMutex
}

// NewDownloadProgress åˆ›å»ºæ–°çš„ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
func NewDownloadProgress(totalChunks, totalBytes int64) *DownloadProgress {
	return &DownloadProgress{
		totalChunks:    totalChunks,
		totalBytes:     totalBytes,
		startTime:      time.Now(),
		lastUpdateTime: time.Now(),
		chunkSizes:     make(map[int64]int64),
		chunkTimes:     make(map[int64]time.Duration),
		peakSpeed:      0,
	}
}

// UpdateChunkProgress æ›´æ–°åˆ†ç‰‡ä¸‹è½½è¿›åº¦
func (dp *DownloadProgress) UpdateChunkProgress(chunkIndex, chunkSize int64, chunkDuration time.Duration) {
	dp.mu.Lock()
	defer dp.mu.Unlock()

	// å¦‚æœè¿™ä¸ªåˆ†ç‰‡è¿˜æ²¡æœ‰è®°å½•ï¼Œå¢åŠ å®Œæˆè®¡æ•°
	if _, exists := dp.chunkSizes[chunkIndex]; !exists {
		dp.completedChunks++
		dp.downloadedBytes += chunkSize
		dp.chunkSizes[chunkIndex] = chunkSize
		dp.chunkTimes[chunkIndex] = chunkDuration
		dp.lastUpdateTime = time.Now()

		// è®¡ç®—å¹¶æ›´æ–°å³°å€¼é€Ÿåº¦
		if chunkDuration.Seconds() > 0 {
			chunkSpeed := float64(chunkSize) / chunkDuration.Seconds() / 1024 / 1024 // MB/s
			if chunkSpeed > dp.peakSpeed {
				dp.peakSpeed = chunkSpeed
			}
		}
	}
}

// GetProgressInfo è·å–å½“å‰è¿›åº¦ä¿¡æ¯
func (dp *DownloadProgress) GetProgressInfo() (percentage float64, avgSpeed float64, peakSpeed float64, eta time.Duration, completed, total int64, downloadedBytes, totalBytes int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	elapsed := time.Since(dp.startTime)
	percentage = float64(dp.completedChunks) / float64(dp.totalChunks) * 100

	if elapsed.Seconds() > 0 {
		avgSpeed = float64(dp.downloadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	peakSpeed = dp.peakSpeed

	if avgSpeed > 0 && dp.downloadedBytes > 0 {
		remainingBytes := dp.totalBytes - dp.downloadedBytes
		etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
		eta = time.Duration(etaSeconds) * time.Second
	}

	return percentage, avgSpeed, peakSpeed, eta, dp.completedChunks, dp.totalChunks, dp.downloadedBytes, dp.totalBytes
}

// updateAccountExtraInfo æ›´æ–°Accountçš„é¢å¤–è¿›åº¦ä¿¡æ¯ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) updateAccountExtraInfo(ctx context.Context, progress *DownloadProgress, remoteName string, account *accounting.Account) {
	ticker := time.NewTicker(2 * time.Second) // æ¯2ç§’æ›´æ–°ä¸€æ¬¡Accountçš„é¢å¤–ä¿¡æ¯
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ¸…é™¤é¢å¤–ä¿¡æ¯
			if account != nil {
				account.SetExtraInfo("")
			}
			return
		case <-ticker.C:
			if account == nil {
				// å°è¯•é‡æ–°è·å–Accountå¯¹è±¡
				if stats := accounting.GlobalStats(); stats != nil {
					// é¦–å…ˆå°è¯•ç”¨åŸå§‹åç§°æŸ¥æ‰¾
					if acc := stats.GetInProgressAccount(remoteName); acc != nil {
						account = acc
					} else {
						// æ·»åŠ æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
						allAccounts := stats.ListInProgressAccounts()
						for _, accountName := range allAccounts {
							if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
								if acc := stats.GetInProgressAccount(accountName); acc != nil {
									account = acc
									break
								}
							}
						}
					}
				}
				if account == nil {
					continue // å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·³è¿‡è¿™æ¬¡æ›´æ–°
				}
			}

			_, _, _, _, completed, total, _, _ := progress.GetProgressInfo()

			// é›†æˆï¼šæ„å»ºé¢å¤–ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ˜¾ç¤ºåˆ†ç‰‡è¿›åº¦
			if completed > 0 && total > 0 {
				extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completed, total)
				account.SetExtraInfo(extraInfo)
			}
		}
	}
}

// NewResourcePool åˆ›å»ºæ–°çš„èµ„æºæ± 
// æ€§èƒ½ä¼˜åŒ–ï¼šå¢å¼ºå†…å­˜æ± ç®¡ç†ï¼Œæ”¯æŒæ›´å¤šç±»å‹çš„å¯¹è±¡å¤ç”¨
func NewResourcePool() *ResourcePool {
	return &ResourcePool{
		bufferPool: sync.Pool{
			New: func() any {
				// åˆ›å»ºé»˜è®¤åˆ†ç‰‡å¤§å°çš„ç¼“å†²åŒº
				return make([]byte, defaultChunkSize)
			},
		},
		hasherPool: sync.Pool{
			New: func() any {
				return md5.New()
			},
		},
		// æ–°å¢ï¼šåˆ†ç‰‡ç¼“å†²åŒºæ± ï¼Œç”¨äºå¤§åˆ†ç‰‡çš„å†…å­˜å¤ç”¨
		chunkPool: sync.Pool{
			New: func() any {
				return make([]byte, 100*1024*1024) // 100MBåˆ†ç‰‡ç¼“å†²åŒº
			},
		},
		// æ–°å¢ï¼šReaderæ± ï¼Œå‡å°‘Readerå¯¹è±¡çš„åˆ›å»ºå¼€é”€
		readerPool: sync.Pool{
			New: func() any {
				return &bytes.Buffer{}
			},
		},
		// æ–°å¢ï¼šè¿›åº¦è·Ÿè¸ªå™¨æ± ï¼Œå¤ç”¨è¿›åº¦å¯¹è±¡
		progressPool: sync.Pool{
			New: func() any {
				return &DownloadProgress{
					chunkSizes: make(map[int64]int64),
					chunkTimes: make(map[int64]time.Duration),
				}
			},
		},
		tempFiles: make(map[string]bool),
	}
}

// GetBuffer ä»æ± ä¸­è·å–ç¼“å†²åŒº
func (rp *ResourcePool) GetBuffer() []byte {
	buf := rp.bufferPool.Get().([]byte)
	// é‡ç½®ç¼“å†²åŒºé•¿åº¦ä½†ä¿ç•™å®¹é‡
	return buf[:0]
}

// PutBuffer å°†ç¼“å†²åŒºå½’è¿˜åˆ°æ± ä¸­
// Note: buf parameter is a slice, passing by value is appropriate for slice headers
func (rp *ResourcePool) PutBuffer(buf []byte) {
	// æ£€æŸ¥ç¼“å†²åŒºå¤§å°ï¼Œé¿å…æ± ä¸­ç§¯ç´¯è¿‡å¤§çš„ç¼“å†²åŒº
	if cap(buf) <= int(maxChunkSize) {
		rp.bufferPool.Put(buf)
	}
}

// GetHasher ä»æ± ä¸­è·å–MD5å“ˆå¸Œè®¡ç®—å™¨
func (rp *ResourcePool) GetHasher() hash.Hash {
	hasher := rp.hasherPool.Get().(hash.Hash)
	hasher.Reset() // é‡ç½®å“ˆå¸Œè®¡ç®—å™¨çŠ¶æ€
	return hasher
}

// PutHasher å°†å“ˆå¸Œè®¡ç®—å™¨å½’è¿˜åˆ°æ± ä¸­
func (rp *ResourcePool) PutHasher(hasher hash.Hash) {
	rp.hasherPool.Put(hasher)
}

// GetTempFile åˆ›å»ºä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å¼‚å¸¸å¤„ç†å’Œèµ„æºç®¡ç†ä¿è¯
func (rp *ResourcePool) GetTempFile(prefix string) (*os.File, error) {
	// æ£€æŸ¥èµ„æºæ± çŠ¶æ€
	if rp == nil {
		return nil, fmt.Errorf("èµ„æºæ± æœªåˆå§‹åŒ–")
	}

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•
	tempFile, err := os.CreateTemp("", prefix+"*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	fileName := tempFile.Name()

	// ä½¿ç”¨deferç¡®ä¿å¼‚å¸¸æ—¶ä¹Ÿèƒ½æ¸…ç†èµ„æº
	var success bool
	defer func() {
		if !success {
			// åˆ›å»ºå¤±è´¥æ—¶çš„æ¸…ç†å·¥ä½œ
			if tempFile != nil {
				tempFile.Close()
			}
			rp.removeTempFileFromTracking(fileName)
			if err := os.Remove(fileName); err != nil && !os.IsNotExist(err) {
				fs.Debugf(nil, "âš ï¸  æ¸…ç†å¤±è´¥çš„ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: %s, é”™è¯¯: %v", fileName, err)
			}
		}
	}()

	// è·Ÿè¸ªä¸´æ—¶æ–‡ä»¶
	rp.tempFilesMu.Lock()
	if rp.tempFiles == nil {
		rp.tempFiles = make(map[string]bool)
	}
	rp.tempFiles[fileName] = true
	rp.tempFilesMu.Unlock()

	// è®¾ç½®æ–‡ä»¶æƒé™ï¼Œç¡®ä¿åªæœ‰å½“å‰ç”¨æˆ·å¯ä»¥è®¿é—®
	if err = tempFile.Chmod(0600); err != nil {
		return nil, fmt.Errorf("è®¾ç½®ä¸´æ—¶æ–‡ä»¶æƒé™å¤±è´¥: %w", err)
	}

	success = true
	fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ: %s", fileName)
	return tempFile, nil
}

// GetOptimizedTempFile åˆ›å»ºä¼˜åŒ–çš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶é«˜æ•ˆå¤„ç†
func (rp *ResourcePool) GetOptimizedTempFile(prefix string, expectedSize int64) (*os.File, error) {
	tempFile, err := rp.GetTempFile(prefix)
	if err != nil {
		return nil, err
	}

	// GetTempFileå·²ç»å¤„ç†äº†è·Ÿè¸ªï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è·Ÿè¸ª

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œé¢„åˆ†é…ç£ç›˜ç©ºé—´ä»¥æå‡å†™å…¥æ€§èƒ½
	if expectedSize > 100*1024*1024 { // >100MB
		err = tempFile.Truncate(expectedSize)
		if err != nil {
			tempFile.Close()
			rp.removeTempFileFromTracking(tempFile.Name())
			os.Remove(tempFile.Name())
			return nil, fmt.Errorf("é¢„åˆ†é…ä¸´æ—¶æ–‡ä»¶ç©ºé—´å¤±è´¥: %w", err)
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
		_, err = tempFile.Seek(0, 0)
		if err != nil {
			tempFile.Close()
			rp.removeTempFileFromTracking(tempFile.Name())
			os.Remove(tempFile.Name())
			return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
		}
	}

	return tempFile, nil
}

// removeTempFileFromTracking ä»è·Ÿè¸ªåˆ—è¡¨ä¸­ç§»é™¤ä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å®‰å…¨æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
func (rp *ResourcePool) removeTempFileFromTracking(fileName string) {
	if rp == nil {
		fs.Debugf(nil, "âš ï¸  èµ„æºæ± æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç§»é™¤ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ª: %s", fileName)
		return
	}

	if fileName == "" {
		fs.Debugf(nil, "âš ï¸  ä¸´æ—¶æ–‡ä»¶åä¸ºç©ºï¼Œè·³è¿‡ç§»é™¤è·Ÿè¸ª")
		return
	}

	rp.tempFilesMu.Lock()
	defer rp.tempFilesMu.Unlock()

	if rp.tempFiles == nil {
		fs.Debugf(nil, "âš ï¸  ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ªæ˜ å°„æœªåˆå§‹åŒ–")
		return
	}

	if _, exists := rp.tempFiles[fileName]; exists {
		delete(rp.tempFiles, fileName)
		fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ªç§»é™¤æˆåŠŸ: %s", fileName)
	} else {
		fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶æœªåœ¨è·Ÿè¸ªåˆ—è¡¨ä¸­: %s", fileName)
	}
}

// PutTempFile æ¸…ç†ä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å¼‚å¸¸å¤„ç†å’Œèµ„æºæ¸…ç†ä¿è¯
func (rp *ResourcePool) PutTempFile(tempFile *os.File) {
	if tempFile == nil {
		return
	}

	fileName := tempFile.Name()

	// å®‰å…¨å…³é—­æ–‡ä»¶ï¼Œä½¿ç”¨deferç¡®ä¿å³ä½¿å‡ºç°panicä¹Ÿèƒ½å…³é—­
	defer func() {
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(nil, "âš ï¸  å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", fileName, closeErr)
			}
		}
	}()

	// ä»è·Ÿè¸ªåˆ—è¡¨ä¸­ç§»é™¤
	rp.removeTempFileFromTracking(fileName)

	// å¤šæ¬¡å°è¯•åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½çš„æ–‡ä»¶é”å®šé—®é¢˜
	maxRetries := 3
	for attempt := range maxRetries {
		if err := os.Remove(fileName); err != nil {
			if os.IsNotExist(err) {
				// æ–‡ä»¶å·²ä¸å­˜åœ¨ï¼Œè®¤ä¸ºåˆ é™¤æˆåŠŸ
				fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶å·²ä¸å­˜åœ¨: %s", fileName)
				return
			}

			if attempt < maxRetries-1 {
				// ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•ï¼ˆå¯èƒ½æ˜¯æ–‡ä»¶é”å®šï¼‰
				time.Sleep(time.Duration(attempt+1) * 100 * time.Millisecond)
				fs.Debugf(nil, "ğŸ”„ ä¸´æ—¶æ–‡ä»¶åˆ é™¤é‡è¯• %d/%d: %s", attempt+1, maxRetries, fileName)
				continue
			}

			// æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸å½±å“ç¨‹åºè¿è¡Œ
			fs.Errorf(nil, "âŒ åˆ é™¤ä¸´æ—¶æ–‡ä»¶æœ€ç»ˆå¤±è´¥: %s, é”™è¯¯: %v", fileName, err)
		} else {
			fs.Debugf(nil, " ä¸´æ—¶æ–‡ä»¶åˆ é™¤æˆåŠŸ: %s", fileName)
			return
		}
	}
}

// CleanupAllTempFiles æ¸…ç†æ‰€æœ‰è·Ÿè¸ªçš„ä¸´æ—¶æ–‡ä»¶
func (rp *ResourcePool) CleanupAllTempFiles() {
	rp.tempFilesMu.Lock()
	defer rp.tempFilesMu.Unlock()

	cleanedCount := 0
	for fileName := range rp.tempFiles {
		if err := os.Remove(fileName); err != nil {
			if !os.IsNotExist(err) {
				fs.Debugf(nil, "âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", fileName, err)
			}
		} else {
			cleanedCount++
		}
		delete(rp.tempFiles, fileName)
	}

	if cleanedCount > 0 {
		fs.Debugf(nil, "ğŸ§¹ èµ„æºæ± æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† %d ä¸ªä¸´æ—¶æ–‡ä»¶", cleanedCount)
	}
}

// Close å…³é—­èµ„æºæ± ï¼Œæ¸…ç†æ‰€æœ‰èµ„æº
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç®€åŒ–å®ç°ï¼Œsync.Poolä¼šè‡ªåŠ¨ç®¡ç†èµ„æº
func (rp *ResourcePool) Close() {
	// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
	rp.CleanupAllTempFiles()

	// sync.Poolä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜æ± ï¼Œæ— éœ€æ‰‹åŠ¨æ¸…ç†
	// è¿™ä¸ªæ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†æ¥å£å…¼å®¹æ€§
}

// ===== é€šç”¨å·¥å…·å‡½æ•°æ¨¡å— =====
// è¿™äº›å‡½æ•°å¯ä»¥è¢«å¤šä¸ªåœ°æ–¹å¤ç”¨ï¼Œæé«˜ä»£ç çš„å¯ç»´æŠ¤æ€§

// PerformanceStats æ€§èƒ½ç»Ÿè®¡æ‘˜è¦
type PerformanceStats struct {
	TotalRuntime         time.Duration // æ€»è¿è¡Œæ—¶é—´
	TotalUploads         int           // æ€»ä¸Šä¼ æ•°
	SuccessfulUploads    int           // æˆåŠŸä¸Šä¼ æ•°
	FailedUploads        int           // å¤±è´¥ä¸Šä¼ æ•°
	AverageUploadSpeed   float64       // å¹³å‡ä¸Šä¼ é€Ÿåº¦ MB/s
	TotalDownloads       int           // æ€»ä¸‹è½½æ•°
	SuccessfulDownloads  int           // æˆåŠŸä¸‹è½½æ•°
	FailedDownloads      int           // å¤±è´¥ä¸‹è½½æ•°
	AverageDownloadSpeed float64       // å¹³å‡ä¸‹è½½é€Ÿåº¦ MB/s
	TotalAPICalls        int           // æ€»APIè°ƒç”¨æ•°
	APISuccessRate       float64       // APIæˆåŠŸç‡
	AverageAPILatency    time.Duration // å¹³å‡APIå»¶è¿Ÿ
	CurrentMemoryUsage   int64         // å½“å‰å†…å­˜ä½¿ç”¨
	PeakMemoryUsage      int64         // å³°å€¼å†…å­˜ä½¿ç”¨
}

// UploadStrategy ä¸Šä¼ ç­–ç•¥æšä¸¾
type UploadStrategy int

const (
	StrategyAuto       UploadStrategy = iota // è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
	StrategySingleStep                       // å•æ­¥ä¸Šä¼ API
	StrategyChunked                          // åˆ†ç‰‡ä¸Šä¼ 
	StrategyStreaming                        // æµå¼ä¸Šä¼ 
)

// String è¿”å›ç­–ç•¥çš„å­—ç¬¦ä¸²è¡¨ç¤º
func (us UploadStrategy) String() string {
	switch us {
	case StrategyAuto:
		return "è‡ªåŠ¨é€‰æ‹©"
	case StrategySingleStep:
		return "å•æ­¥ä¸Šä¼ "
	case StrategyChunked:
		return "åˆ†ç‰‡ä¸Šä¼ "
	case StrategyStreaming:
		return "æµå¼ä¸Šä¼ "
	default:
		return "æœªçŸ¥ç­–ç•¥"
	}
}

// UploadStrategySelector ä¸Šä¼ ç­–ç•¥é€‰æ‹©å™¨
type UploadStrategySelector struct {
	fileSize       int64   // æ–‡ä»¶å¤§å°
	hasKnownMD5    bool    // æ˜¯å¦æœ‰å·²çŸ¥MD5
	isRemoteSource bool    // æ˜¯å¦æ˜¯è¿œç¨‹æº
	networkSpeed   int64   // ç½‘ç»œé€Ÿåº¦
	networkQuality float64 // ç½‘ç»œè´¨é‡
}

// SelectStrategy é€‰æ‹©æœ€ä¼˜ä¸Šä¼ ç­–ç•¥
func (uss *UploadStrategySelector) SelectStrategy() UploadStrategy {
	// ç­–ç•¥é€‰æ‹©é€»è¾‘ä¼˜åŒ– - ä¿®å¤å•æ­¥ä¸Šä¼ é€‰æ‹©é—®é¢˜ï¼Œå¢åŠ è·¨äº‘ä¼ è¾“ç‰¹æ®Šå¤„ç†

	// è·¨äº‘ä¼ è¾“ç‰¹æ®Šå¤„ç†ï¼šå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°åŒ–ç­–ç•¥
	if uss.isRemoteSource {
		// é‡è¦è¯´æ˜ï¼šè·¨äº‘ä¼ è¾“å·²åœ¨crossCloudUploadWithLocalCacheä¸­å¼ºåˆ¶æœ¬åœ°åŒ–
		// æ­¤æ—¶åˆ°è¾¾è¿™é‡Œçš„åº”è¯¥éƒ½æ˜¯æœ¬åœ°æ•°æ®ï¼Œä½†ä¸ºäº†å®‰å…¨èµ·è§ï¼Œä»ç„¶ä¼˜å…ˆé€‰æ‹©ç¨³å®šç­–ç•¥
		if uss.fileSize <= singleStepUploadLimit { // 1GBä»¥ä¸‹ä½¿ç”¨å•æ­¥ä¸Šä¼ 
			return StrategySingleStep
		}
		// å…³é”®ä¿®å¤ï¼šè¶…è¿‡1GBçš„è·¨äº‘ä¼ è¾“ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ï¼ˆæ­¤æ—¶å·²æ˜¯æœ¬åœ°æ•°æ®ï¼‰
		// åˆ†ç‰‡ä¸Šä¼ æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œæ›´é€‚åˆå¤§æ–‡ä»¶
		return StrategyChunked
	}

	// 1. å°æ–‡ä»¶ï¼ˆ<1GBï¼‰-> å•æ­¥ä¸Šä¼ ï¼ˆAPIé™åˆ¶1GBï¼Œæ€§èƒ½æ›´å¥½ï¼‰
	if uss.fileSize < singleStepUploadLimit && uss.fileSize > 0 {
		return StrategySingleStep
	}

	// 2. å¤§æ–‡ä»¶ï¼ˆ>=1GBï¼‰-> åˆ†ç‰‡ä¸Šä¼ ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¹¶å‘ï¼‰
	if uss.fileSize >= singleStepUploadLimit {
		return StrategyChunked
	}

	// 3. å…¶ä»–æƒ…å†µï¼ˆç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼‰-> æµå¼ä¸Šä¼ 
	return StrategyStreaming
}

// GetStrategyReason è·å–ç­–ç•¥é€‰æ‹©çš„åŸå› è¯´æ˜
func (uss *UploadStrategySelector) GetStrategyReason(strategy UploadStrategy) string {
	switch strategy {
	case StrategySingleStep:
		if uss.isRemoteSource {
			return fmt.Sprintf("è·¨äº‘ä¼ è¾“æ–‡ä»¶(%s)ï¼Œä½¿ç”¨å•æ­¥ä¸Šä¼ é¿å…åˆ†ç‰‡éªŒè¯é—®é¢˜", fs.SizeSuffix(uss.fileSize))
		}
		return fmt.Sprintf("å°æ–‡ä»¶(%s)ï¼Œä½¿ç”¨å•æ­¥ä¸Šä¼ æ¥å£ï¼ˆAPIé™åˆ¶1GBï¼‰", fs.SizeSuffix(uss.fileSize))
	case StrategyChunked:
		if uss.isRemoteSource {
			return fmt.Sprintf("å¤§å‹è·¨äº‘ä¼ è¾“æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ é¿å…äºŒæ¬¡ä¸‹è½½", fs.SizeSuffix(uss.fileSize))
		}
		return fmt.Sprintf("å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ æ”¯æŒæ–­ç‚¹ç»­ä¼ ", fs.SizeSuffix(uss.fileSize))
	case StrategyStreaming:
		if uss.isRemoteSource {
			return fmt.Sprintf("å¤§å‹è·¨äº‘ä¼ è¾“æ–‡ä»¶(%s)ï¼Œä½¿ç”¨æµå¼ä¸Šä¼ é¿å…åˆ†ç‰‡éªŒè¯é—®é¢˜", fs.SizeSuffix(uss.fileSize))
		}
		return fmt.Sprintf("ä¸­ç­‰æ–‡ä»¶(%s)ï¼Œä½¿ç”¨æµå¼ä¸Šä¼ å¹³è¡¡æ€§èƒ½", fs.SizeSuffix(uss.fileSize))
	default:
		return "è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥"
	}
}

// UnifiedUploadContext ç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
type UnifiedUploadContext struct {
	ctx            context.Context
	in             io.Reader
	src            fs.ObjectInfo
	parentFileID   int64
	fileName       string
	strategy       UploadStrategy
	selector       *UploadStrategySelector
	startTime      time.Time
	networkSpeed   int64
	networkQuality float64
}

// NewUnifiedUploadContext åˆ›å»ºç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
func (f *Fs) NewUnifiedUploadContext(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) *UnifiedUploadContext {
	// æ£€æµ‹ç½‘ç»œæ¡ä»¶
	networkSpeed := f.detectNetworkSpeed(ctx)
	networkQuality := f.getNetworkQuality()

	// å°è¯•è·å–å·²çŸ¥MD5
	var hasKnownMD5 bool
	if hashValue, err := src.Hash(ctx, fshash.MD5); err == nil && hashValue != "" {
		hasKnownMD5 = true
	}

	// åˆ›å»ºç­–ç•¥é€‰æ‹©å™¨
	selector := &UploadStrategySelector{
		fileSize:       src.Size(),
		hasKnownMD5:    hasKnownMD5,
		isRemoteSource: f.isRemoteSource(src),
		networkSpeed:   networkSpeed,
		networkQuality: networkQuality,
	}

	// é€‰æ‹©ç­–ç•¥
	strategy := selector.SelectStrategy()

	return &UnifiedUploadContext{
		ctx:            ctx,
		in:             in,
		src:            src,
		parentFileID:   parentFileID,
		fileName:       fileName,
		strategy:       strategy,
		selector:       selector,
		startTime:      time.Now(),
		networkSpeed:   networkSpeed,
		networkQuality: networkQuality,
	}
}

// unifiedUpload ç»Ÿä¸€ä¸Šä¼ å…¥å£ï¼Œç®€åŒ–å¤æ‚çš„å›é€€ç­–ç•¥
func (f *Fs) unifiedUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// åˆ›å»ºç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
	uploadCtx := f.NewUnifiedUploadContext(ctx, in, src, parentFileID, fileName)

	// è®°å½•ç­–ç•¥é€‰æ‹©
	reason := uploadCtx.selector.GetStrategyReason(uploadCtx.strategy)
	fs.Debugf(f, "ç»Ÿä¸€ä¸Šä¼ ç­–ç•¥: %s - %s", uploadCtx.strategy.String(), reason)

	// è®°å½•ä¸Šä¼ å¼€å§‹æŒ‡æ ‡
	startTime := time.Now()

	// æ‰§è¡Œå¯¹åº”çš„ä¸Šä¼ ç­–ç•¥
	var result *Object
	var err error

	switch uploadCtx.strategy {
	case StrategySingleStep:
		result, err = f.executeSingleStepUpload(uploadCtx)
	case StrategyChunked:
		result, err = f.executeChunkedUpload(uploadCtx)
	case StrategyStreaming:
		result, err = f.executeStreamingUpload(uploadCtx)
	default:
		// é»˜è®¤å›é€€åˆ°æµå¼ä¸Šä¼ 
		fs.Debugf(f, "æœªçŸ¥ç­–ç•¥ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ")
		result, err = f.executeStreamingUpload(uploadCtx)
	}

	// ç®€åŒ–çš„æ€§èƒ½è®°å½•
	duration := time.Since(startTime)
	if err != nil {
		fs.Errorf(f, "ç»Ÿä¸€ä¸Šä¼ å¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "ç»Ÿä¸€ä¸Šä¼ æˆåŠŸï¼Œè€—æ—¶: %v", duration)
	}

	return result, err
}

// executeSingleStepUpload æ‰§è¡Œå•æ­¥ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeSingleStepUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	// æ£€æµ‹è·¨äº‘ä¼ è¾“å¹¶è®°å½•
	isRemoteSource := f.isRemoteSource(uploadCtx.src)
	if isRemoteSource {
		fs.Infof(f, "ğŸŒ æ‰§è¡Œè·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ ç­–ç•¥ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
	} else {
		fs.Debugf(f, "æ‰§è¡Œå•æ­¥ä¸Šä¼ ç­–ç•¥")
	}

	// è·å–å·²çŸ¥MD5
	md5Hash, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5)
	if err != nil || md5Hash == "" {
		if isRemoteSource {
			// è·¨äº‘ä¼ è¾“æ—¶ï¼Œå¦‚æœæ²¡æœ‰MD5ï¼Œå°è¯•è®¡ç®—
			fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“æ— å·²çŸ¥MD5ï¼Œå°è¯•ä»æ•°æ®æµè®¡ç®—")
			data, readErr := io.ReadAll(uploadCtx.in)
			if readErr != nil {
				return nil, fmt.Errorf("è·¨äº‘ä¼ è¾“è¯»å–æ•°æ®å¤±è´¥: %w", readErr)
			}

			// è®¡ç®—MD5
			hash := md5.New()
			hash.Write(data)
			md5Hash = hex.EncodeToString(hash.Sum(nil))
			fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“è®¡ç®—MD5: %s", md5Hash)

			// éªŒè¯æ–‡ä»¶å¤§å°
			if int64(len(data)) != uploadCtx.src.Size() {
				return nil, fmt.Errorf("è·¨äº‘ä¼ è¾“æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", uploadCtx.src.Size(), len(data))
			}

			// æ‰§è¡Œè·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ 
			return f.executeCrossCloudSingleStepUpload(uploadCtx, data, md5Hash)
		}
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ éœ€è¦å·²çŸ¥MD5ï¼Œä½†è·å–å¤±è´¥: %w", err)
	}

	// è¯»å–æ–‡ä»¶æ•°æ®
	data, err := io.ReadAll(uploadCtx.in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	if int64(len(data)) != uploadCtx.src.Size() {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", uploadCtx.src.Size(), len(data))
	}

	// æ‰§è¡Œå•æ­¥ä¸Šä¼ 
	result, err := f.singleStepUpload(uploadCtx.ctx, data, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash)
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ç›®å½•IDä¸å­˜åœ¨çš„é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡è¯•ä¸€æ¬¡
		if strings.Contains(err.Error(), "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•") {
			fs.Debugf(f, "å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œé‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¹¶é‡è¯•")

			// é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•ID
			leaf, parentID, findErr := f.dirCache.FindPath(uploadCtx.ctx, uploadCtx.fileName, true)
			if findErr != nil {
				return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", findErr)
			}

			newParentFileID, parseErr := parseParentID(parentID)
			if parseErr != nil {
				return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", parseErr)
			}

			fs.Debugf(f, "å•æ­¥ä¸Šä¼ : é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d (åŸID: %d)", newParentFileID, uploadCtx.parentFileID)
			uploadCtx.parentFileID = newParentFileID
			uploadCtx.fileName = leaf

			// é‡è¯•å•æ­¥ä¸Šä¼ 
			result, err = f.singleStepUpload(uploadCtx.ctx, data, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash)
			if err != nil {
				return nil, fmt.Errorf("é‡è¯•å•æ­¥ä¸Šä¼ å¤±è´¥: %w", err)
			}
		} else {
			return nil, err
		}
	}

	return result, nil
}

// executeCrossCloudSingleStepUpload æ‰§è¡Œè·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ ï¼ˆç‰¹æ®Šä¼˜åŒ–ç‰ˆæœ¬ï¼‰
func (f *Fs) executeCrossCloudSingleStepUpload(uploadCtx *UnifiedUploadContext, data []byte, md5Hash string) (*Object, error) {
	fs.Infof(f, "ğŸŒ å¼€å§‹è·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ : %s (%s)", uploadCtx.fileName, fs.SizeSuffix(int64(len(data))))

	// å…³é”®ä¿®å¤ï¼šåœ¨å•æ­¥ä¸Šä¼ å‰å…ˆæ£€æŸ¥ç§’ä¼ 
	fileSize := int64(len(data))
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, isInstant, err := f.checkInstantUpload(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash, fileSize)
	if err != nil {
		fs.Debugf(f, "ç§’ä¼ æ£€æŸ¥å¤±è´¥: %v", err)
		return nil, fmt.Errorf("ç§’ä¼ æ£€æŸ¥å¤±è´¥: %w", err)
	}

	if isInstant {
		fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“ç§’ä¼ æˆåŠŸï¼æ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))
		return f.createObjectFromUpload(uploadCtx.fileName, fileSize, md5Hash, createResp.Data.FileID, time.Now()), nil
	}

	fs.Infof(f, "ğŸ“¤ ç§’ä¼ æœªå‘½ä¸­ï¼Œå¼€å§‹å®é™…å•æ­¥ä¸Šä¼ ...")

	// ä¸ºè·¨äº‘ä¼ è¾“å¢åŠ é‡è¯•æœºåˆ¶
	maxRetries := 3
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ å°è¯• %d/%d", attempt, maxRetries)

		result, err := f.singleStepUpload(uploadCtx.ctx, data, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash)
		if err == nil {
			fs.Infof(f, "âœ… è·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ æˆåŠŸ: %s", uploadCtx.fileName)
			return result, nil
		}

		lastErr = err
		fs.Debugf(f, "ğŸŒ è·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ å°è¯• %d å¤±è´¥: %v", attempt, err)

		// æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯
		if attempt < maxRetries {
			// ä¸ºè·¨äº‘ä¼ è¾“å¢åŠ æ›´é•¿çš„ç­‰å¾…æ—¶é—´
			waitTime := time.Duration(attempt*3) * time.Second
			fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“ç­‰å¾… %v åé‡è¯•", waitTime)
			time.Sleep(waitTime)
		}
	}

	return nil, fmt.Errorf("è·¨äº‘ä¼ è¾“å•æ­¥ä¸Šä¼ å¤±è´¥ï¼ˆå°è¯•%dæ¬¡ï¼‰: %w", maxRetries, lastErr)
}

// executeChunkedUpload æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeChunkedUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è¿›å…¥executeChunkedUploadå‡½æ•°ï¼Œæ–‡ä»¶: %s", uploadCtx.fileName)
	fs.Debugf(f, "æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ ç­–ç•¥")

	// è·¨äº‘ä¼ è¾“æ£€æµ‹å’Œæ™ºèƒ½ç§’ä¼ ç­–ç•¥
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘å¼€å§‹è·¨äº‘ä¼ è¾“æ£€æµ‹...")
	fs.Debugf(f, " å¼€å§‹æ£€æµ‹æ˜¯å¦ä¸ºè·¨äº‘ä¼ è¾“...")
	isRemoteSource := f.isRemoteSource(uploadCtx.src)
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è·¨äº‘ä¼ è¾“æ£€æµ‹ç»“æœ: %v", isRemoteSource)
	fs.Debugf(f, " è·¨äº‘ä¼ è¾“æ£€æµ‹ç»“æœ: %v", isRemoteSource)

	// æ™ºèƒ½ç§’ä¼ æ£€æµ‹ï¼šæ— è®ºæœ¬åœ°è¿˜æ˜¯è·¨äº‘ä¼ è¾“ï¼Œéƒ½å…ˆå°è¯•è·å–MD5è¿›è¡Œç§’ä¼ æ£€æµ‹
	var md5Hash string

	var skipMD5Calculation bool // æ ‡å¿—æ˜¯å¦è·³è¿‡MD5è®¡ç®—ä»¥é¿å…é‡å¤ä¸‹è½½

	if isRemoteSource {
		fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“ï¼Œå…ˆå°è¯•è·å–æºæ–‡ä»¶MD5è¿›è¡Œç§’ä¼ æ£€æµ‹")

		// å°è¯•ä»æºå¯¹è±¡è·å–å·²çŸ¥MD5ï¼ˆæ— éœ€é¢å¤–è®¡ç®—ï¼‰
		if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
			md5Hash = hashValue
			fs.Infof(f, "ğŸš€ è·¨äº‘ä¼ è¾“è·å–åˆ°æºæ–‡ä»¶MD5: %sï¼Œå°†å°è¯•ç§’ä¼ ", md5Hash)
		} else {
			fs.Debugf(f, " è·¨äº‘ä¼ è¾“æºæ–‡ä»¶æ— å¯ç”¨MD5ï¼Œè·³è¿‡ç§’ä¼ æ£€æµ‹é¿å…é‡å¤ä¸‹è½½: %v", err)

			// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šè·¨äº‘ä¼ è¾“æ—¶ä¸é‡æ–°è®¡ç®—MD5
			// é¿å…ä¸ºäº†ç§’ä¼ è€Œé‡å¤ä¸‹è½½å¤§æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨æ­£å¸¸ä¸Šä¼ æµç¨‹
			fs.Infof(f, "ğŸ”§ 115â†’123è·¨äº‘ä¼ è¾“ï¼šè·³è¿‡MD5è®¡ç®—ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼Œä½¿ç”¨æ­£å¸¸ä¸Šä¼ æµç¨‹")
			md5Hash = ""              // ä¸ä½¿ç”¨ç§’ä¼ ï¼Œç›´æ¥ä¸Šä¼ 
			skipMD5Calculation = true // è®¾ç½®æ ‡å¿—ï¼Œé¿å…åç»­é‡å¤è®¡ç®—MD5
		}
	}

	// æœ¬åœ°æ–‡ä»¶æˆ–è·¨äº‘ä¼ è¾“æ— MD5æ—¶ï¼šè·å–æˆ–è®¡ç®—MD5ï¼ˆä½†è·³è¿‡å·²å†³å®šä¸è®¡ç®—çš„æƒ…å†µï¼‰
	if (!isRemoteSource || md5Hash == "") && !skipMD5Calculation {
		if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
			md5Hash = hashValue
			fs.Debugf(f, "ä½¿ç”¨å·²çŸ¥MD5: %s", md5Hash)
		} else if !isRemoteSource {
			// å¯¹äºåˆ†ç‰‡ä¸Šä¼ ï¼Œå¿…é¡»è®¡ç®—å‡†ç¡®çš„MD5
			fs.Debugf(f, "æºå¯¹è±¡æ— MD5å“ˆå¸Œï¼Œå°è¯•ä»è¾“å…¥æµè®¡ç®—MD5")

			// ä¼˜å…ˆå°è¯•ä»fs.Objectæ¥å£è®¡ç®—MD5
			if srcObj, ok := uploadCtx.src.(fs.Object); ok {
				fs.Debugf(f, "ä½¿ç”¨fs.Objectæ¥å£è®¡ç®—MD5")
				md5Reader, err := srcObj.Open(uploadCtx.ctx)
				if err != nil {
					fs.Debugf(f, "æ— æ³•æ‰“å¼€æºå¯¹è±¡ï¼Œå›é€€åˆ°è¾“å…¥æµè®¡ç®—: %v", err)
				} else {
					defer md5Reader.Close()
					hasher := md5.New()
					_, err = io.Copy(hasher, md5Reader)
					if err != nil {
						fs.Debugf(f, "ä»æºå¯¹è±¡è®¡ç®—MD5å¤±è´¥ï¼Œå›é€€åˆ°è¾“å…¥æµè®¡ç®—: %v", err)
					} else {
						md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
						fs.Debugf(f, "ä»æºå¯¹è±¡è®¡ç®—MD5å®Œæˆ: %s", md5Hash)
					}
				}
			}
		}

		// å¦‚æœä»æºå¯¹è±¡è®¡ç®—MD5å¤±è´¥ï¼Œå°è¯•é€šç”¨æ–¹æ³•è®¡ç®—MD5
		if md5Hash == "" {
			fs.Debugf(f, "ä½¿ç”¨é€šç”¨æ–¹æ³•è®¡ç®—MD5ï¼ˆè·¨äº‘ç›˜ä¼ è¾“å…¼å®¹æ¨¡å¼ï¼‰")

			// å°è¯•é€šè¿‡fs.ObjectInfoæ¥å£é‡æ–°æ‰“å¼€æºå¯¹è±¡
			opener := uploadCtx.src
			fs.Debugf(f, "é€šè¿‡fs.ObjectInfoæ¥å£è®¡ç®—MD5")

			// æ£€æŸ¥æ˜¯å¦æœ‰Openæ–¹æ³•ï¼ˆé€šè¿‡æ¥å£æ–­è¨€ï¼‰
			if openable, hasOpen := opener.(interface {
				Open(context.Context, ...fs.OpenOption) (io.ReadCloser, error)
			}); hasOpen {
				md5Reader, err := openable.Open(uploadCtx.ctx)
				if err != nil {
					fs.Debugf(f, "æ— æ³•é€šè¿‡Openæ–¹æ³•æ‰“å¼€æºå¯¹è±¡: %v", err)
				} else {
					defer md5Reader.Close()
					md5Hash, err = f.calculateMD5FromReader(uploadCtx.ctx, md5Reader, uploadCtx.src.Size())
					if err != nil {
						fs.Debugf(f, "é€šè¿‡Openæ–¹æ³•è®¡ç®—MD5å¤±è´¥: %v", err)
					} else {
						fs.Debugf(f, "é€šè¿‡Openæ–¹æ³•è®¡ç®—MD5æˆåŠŸ: %s", md5Hash)
					}
				}
			}

			// å¦‚æœä»ç„¶æ²¡æœ‰MD5ï¼Œä½¿ç”¨è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5ï¼ˆæ­¤æ—¶å·²ç¡®ä¿æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
			if md5Hash == "" {
				fs.Debugf(f, "ä½¿ç”¨è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5")
				var err error
				md5Hash, uploadCtx.in, err = f.calculateMD5WithStreamCache(uploadCtx.ctx, uploadCtx.in, uploadCtx.src.Size())
				if err != nil {
					return nil, fmt.Errorf("æ— æ³•è®¡ç®—æ–‡ä»¶MD5: %w", err)
				}
				fs.Debugf(f, "è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5å®Œæˆ: %s", md5Hash)
			}
		}
	}

	// è·¨äº‘ä¼ è¾“æ— MD5æ—¶çš„ç‰¹æ®Šå¤„ç†ï¼šä½¿ç”¨ç»Ÿä¸€çš„è·¨äº‘ä¼ è¾“å¤„ç†å™¨
	if isRemoteSource && md5Hash == "" {
		fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“æ— å¯ç”¨MD5ï¼Œä½¿ç”¨ç»Ÿä¸€è·¨äº‘ä¼ è¾“å¤„ç†å™¨")
		fs.Infof(f, "ğŸ”§ 115ç½‘ç›˜åªæ”¯æŒSHA1ï¼Œ123ç½‘ç›˜éœ€è¦MD5ï¼Œå°†ä¸‹è½½åè®¡ç®—MD5")

		// ç›´æ¥è°ƒç”¨handleCrossCloudTransferï¼Œå®ƒå·²ç»é›†æˆäº†CrossCloudCoordinator
		// å¯ä»¥é¿å…é‡å¤ä¸‹è½½ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¹¶æ­£ç¡®å¤„ç†MD5è®¡ç®—å’Œç§’ä¼ 
		return f.handleCrossCloudTransfer(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName)
	}

	// ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åˆ›å»ºä¸“ç”¨ä¼šè¯ï¼Œå¼ºåˆ¶ä½¿ç”¨v2 API
	createResp, err := f.createUploadV2(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash, uploadCtx.src.Size())
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ç›®å½•IDä¸å­˜åœ¨çš„é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡è¯•ä¸€æ¬¡
		if strings.Contains(err.Error(), "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•") {
			fs.Debugf(f, "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œé‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¹¶é‡è¯•")

			// é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•ID
			leaf, parentID, findErr := f.dirCache.FindPath(uploadCtx.ctx, uploadCtx.fileName, true)
			if findErr != nil {
				return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", findErr)
			}

			newParentFileID, parseErr := parseParentID(parentID)
			if parseErr != nil {
				return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", parseErr)
			}

			fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d (åŸID: %d)", newParentFileID, uploadCtx.parentFileID)
			uploadCtx.parentFileID = newParentFileID
			uploadCtx.fileName = leaf

			// é‡è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯
			createResp, err = f.createUploadV2(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash, uploadCtx.src.Size())
			if err != nil {
				return nil, fmt.Errorf("é‡è¯•åˆ›å»ºv2åˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
			}
		} else {
			return nil, fmt.Errorf("åˆ›å»ºv2åˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
		}
	}

	// å¦‚æœè§¦å‘ç§’ä¼ ï¼Œç›´æ¥è¿”å›
	if createResp.Data.Reuse {
		fs.Debugf(f, "è§¦å‘ç§’ä¼ åŠŸèƒ½")
		return f.createObject(uploadCtx.fileName, createResp.Data.FileID, uploadCtx.src.Size(), md5Hash, time.Now(), false), nil
	}

	// æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ 
	return f.executeChunkedUploadWithResume(uploadCtx, createResp)
}

// executeStreamingUpload æ‰§è¡Œæµå¼ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeStreamingUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	fs.Debugf(f, "æ‰§è¡Œæµå¼ä¸Šä¼ ç­–ç•¥")

	// ä½¿ç”¨ç°æœ‰çš„æµå¼ä¸Šä¼ é€»è¾‘ï¼Œä½†ç®€åŒ–å›é€€ç­–ç•¥
	return f.streamingPutSimplified(uploadCtx)
}

// executeChunkedUploadWithResume æ‰§è¡Œå¸¦æ–­ç‚¹ç»­ä¼ çš„åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) executeChunkedUploadWithResume(uploadCtx *UnifiedUploadContext, createResp *UploadCreateResp) (*Object, error) {
	// ä¼˜å…ˆå°è¯•v2å¤šçº¿ç¨‹ä¸Šä¼ ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹å¯¹è±¡
	fs.Debugf(f, "ğŸš€ å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ æµç¨‹ï¼Œæ–‡ä»¶å¤§å°: %d bytes", uploadCtx.src.Size())
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ1/3ã€‘åˆ›å»ºæ–‡ä»¶ - è°ƒç”¨åˆ›å»ºæ–‡ä»¶æ¥å£ï¼Œæ£€æŸ¥æ˜¯å¦ç§’ä¼ ")

	// éµå¾ªAPIè§„èŒƒï¼šä¸¥æ ¼ä½¿ç”¨APIè¿”å›çš„SliceSize
	// æ ¹æ®123ç½‘ç›˜APIæ–‡æ¡£ï¼Œåˆ†ç‰‡å¤§å°å¿…é¡»ä¸¥æ ¼æŒ‰ç…§APIè¿”å›çš„sliceSizeè¿›è¡Œä¸Šä¼ 
	apiSliceSize := createResp.Data.SliceSize

	if apiSliceSize <= 0 {
		// å¦‚æœAPIæ²¡æœ‰è¿”å›æœ‰æ•ˆçš„SliceSizeï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€
		fs.Debugf(f, "âš ï¸ APIæœªè¿”å›æœ‰æ•ˆSliceSize(%d)ï¼Œä½¿ç”¨64MBä½œä¸ºå›é€€", apiSliceSize)
		apiSliceSize = 64 * 1024 * 1024 // 64MBå›é€€å¤§å°
	} else {
		// ä½¿ç”¨APIè¿”å›çš„åˆ†ç‰‡å¤§å°ï¼Œç¡®ä¿ä¸æœåŠ¡å™¨è¦æ±‚ä¸€è‡´
		fs.Debugf(f, " ä½¿ç”¨APIè§„èŒƒåˆ†ç‰‡å¤§å°: %s (APIè¿”å›SliceSize=%d)", fs.SizeSuffix(apiSliceSize), apiSliceSize)
	}

	fs.Debugf(f, " ä½¿ç”¨APIè§„èŒƒåˆ†ç‰‡å¤§å°: %s (APIè¿”å›SliceSize=%d)", fs.SizeSuffix(apiSliceSize), createResp.Data.SliceSize)

	// ä½¿ç”¨æ–°çš„v2å¤šçº¿ç¨‹ä¸Šä¼ å®ç°ï¼Œä¸¥æ ¼éµå¾ªAPIè¿”å›çš„åˆ†ç‰‡å¤§å°
	result, err := f.v2MultiThreadUpload(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, createResp, apiSliceSize, uploadCtx.fileName)
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯å¯ä»¥é€šè¿‡fallbackè§£å†³çš„é”™è¯¯
		if f.shouldFallbackFromV2Upload(err) {
			fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ é‡åˆ°å¯æ¢å¤é”™è¯¯ï¼Œå°è¯•ä¼ ç»Ÿæ–¹å¼: %v", err)

			// å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼ï¼šå°è¯•è½¬æ¢ä¸ºfs.Object
			if srcObj, ok := uploadCtx.src.(fs.Object); ok {
				fs.Debugf(f, "å›é€€åˆ°ä¼ ç»Ÿåˆ†ç‰‡ä¸Šä¼ ")
				return f.uploadFileInChunksWithResume(uploadCtx.ctx, srcObj, createResp, uploadCtx.src.Size(), apiSliceSize, uploadCtx.fileName)
			}

			// æœ€åå›é€€åˆ°æµå¼ä¸Šä¼ 
			fs.Debugf(f, "æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ç­–ç•¥")
			return f.executeStreamingUpload(uploadCtx)
		} else {
			// å¯¹äºä¸å¯æ¢å¤çš„é”™è¯¯ï¼ˆå¦‚upload_completeå¤±è´¥ï¼‰ï¼Œç›´æ¥è¿”å›é”™è¯¯
			fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ é‡åˆ°ä¸å¯æ¢å¤é”™è¯¯ï¼Œç›´æ¥è¿”å›: %v", err)
			return nil, err
		}
	}

	return result, nil
}

// shouldFallbackFromV2Upload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä»v2ä¸Šä¼ fallbackåˆ°ä¼ ç»Ÿæ–¹å¼
func (f *Fs) shouldFallbackFromV2Upload(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// ä¸åº”è¯¥fallbackçš„é”™è¯¯ç±»å‹ï¼ˆè¿™äº›æ˜¯æœ€ç»ˆé”™è¯¯ï¼Œfallbackä¹Ÿæ— æ³•è§£å†³ï¼‰
	if strings.Contains(errStr, "upload completion failed") ||
		strings.Contains(errStr, "æ–‡ä»¶æ ¡éªŒè¶…æ—¶") ||
		strings.Contains(errStr, "å®Œæˆä¸Šä¼ å¤±è´¥") {
		return false
	}

	// åº”è¯¥fallbackçš„é”™è¯¯ç±»å‹ï¼ˆè¿™äº›å¯èƒ½é€šè¿‡ä¼ ç»Ÿæ–¹å¼è§£å†³ï¼‰
	if strings.Contains(errStr, "ç½‘ç»œè¿æ¥") ||
		strings.Contains(errStr, "è¶…æ—¶") ||
		strings.Contains(errStr, "åˆ†ç‰‡ä¸Šä¼ å¤±è´¥") ||
		strings.Contains(errStr, "å¹¶å‘") {
		return true
	}

	// é»˜è®¤ä¸fallbackï¼Œé¿å…ä¸å¿…è¦çš„é‡è¯•
	return false
}

// v2MultiThreadUpload åŸºäºv2 APIçš„å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®ç°
// è§£å†³"æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é™åˆ¶ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹å¯¹è±¡çš„çœŸæ­£å¹¶å‘ä¸Šä¼ 
func (f *Fs) v2MultiThreadUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, chunkSize int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ2/3ã€‘ä¸Šä¼ åˆ†ç‰‡ - å¼€å§‹å¤šçº¿ç¨‹å¹¶å‘ä¸Šä¼ ï¼Œåˆ†ç‰‡å¤§å°=%s", fs.SizeSuffix(chunkSize))

	// æ³¨æ„ï¼šè·¨äº‘ä¼ è¾“æ£€æµ‹å·²åœ¨æ›´æ—©é˜¶æ®µå¤„ç†ï¼Œæ­¤å‡½æ•°åªå¤„ç†æœ¬åœ°æ–‡ä»¶

	preuploadID := createResp.Data.PreuploadID
	fileSize := src.Size()

	// å…³é”®éªŒè¯ï¼šç¡®ä¿ä½¿ç”¨çš„åˆ†ç‰‡å¤§å°ä¸APIè¿”å›çš„SliceSizeä¸€è‡´
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize > 0 && chunkSize != apiSliceSize {
		fs.Debugf(f, "âš ï¸ åˆ†ç‰‡å¤§å°ä¸åŒ¹é…è­¦å‘Š: ä¼ å…¥å¤§å°=%s, APIè¦æ±‚å¤§å°=%s, å¼ºåˆ¶ä½¿ç”¨APIè¦æ±‚å¤§å°",
			fs.SizeSuffix(chunkSize), fs.SizeSuffix(apiSliceSize))
		chunkSize = apiSliceSize
	}

	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	fs.Debugf(f, "å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ : æ–‡ä»¶å¤§å°=%s, åˆ†ç‰‡å¤§å°=%s, æ€»åˆ†ç‰‡æ•°=%d, APIè¦æ±‚SliceSize=%d",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(chunkSize), totalChunks, apiSliceSize)

	// å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
	progress, err := f.prepareUploadProgress(preuploadID, totalChunks, chunkSize, fileSize, fileName, "", src.Remote())
	if err != nil {
		return nil, fmt.Errorf("å‡†å¤‡ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)

	// æ™ºèƒ½å¤šçº¿ç¨‹å¯ç”¨ç­–ç•¥ - æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´æœ€å°å¹¶å‘æ•°
	minConcurrency := 1
	if fileSize > 2*1024*1024*1024 { // >2GB - å¼ºåˆ¶è‡³å°‘4ä¸ªå¹¶å‘
		minConcurrency = 4
	} else if fileSize > 1*1024*1024*1024 { // >1GB - å¼ºåˆ¶è‡³å°‘3ä¸ªå¹¶å‘
		minConcurrency = 3
	} else if fileSize > 500*1024*1024 { // >500MB - å¼ºåˆ¶è‡³å°‘2ä¸ªå¹¶å‘
		minConcurrency = 2
	} else if fileSize > 100*1024*1024 { // >100MB - å¼ºåˆ¶è‡³å°‘2ä¸ªå¹¶å‘
		minConcurrency = 2
	}

	if concurrencyParams.actual < minConcurrency {
		concurrencyParams.actual = minConcurrency
		fs.Debugf(f, " æ™ºèƒ½å¤šçº¿ç¨‹è°ƒæ•´: æ–‡ä»¶å¤§å°=%sï¼Œæœ€å°å¹¶å‘æ•°=%dï¼Œè°ƒæ•´åå¹¶å‘æ•°=%d",
			fs.SizeSuffix(fileSize), minConcurrency, concurrencyParams.actual)
	}

	fs.Debugf(f, "ğŸš€ v2å¤šçº¿ç¨‹å‚æ•° - ç½‘ç»œé€Ÿåº¦: %s/s, æœ€ä¼˜å¹¶å‘æ•°: %d, å®é™…å¹¶å‘æ•°: %d",
		fs.SizeSuffix(concurrencyParams.networkSpeed), concurrencyParams.optimal, concurrencyParams.actual)

	// æ³¨æ„ï¼šè·¨äº‘ä¼ è¾“æµç¼“å­˜ç­–ç•¥å·²åœ¨v2UploadChunksWithConcurrencyå‡½æ•°ä¸­å®ç°

	// é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šå¤šçº¿ç¨‹æˆ–å•çº¿ç¨‹
	if totalChunks > 1 && concurrencyParams.actual > 1 {
		fs.Debugf(f, "ä½¿ç”¨v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶å‘æ•°: %d", concurrencyParams.actual)
		return f.v2UploadChunksWithConcurrency(ctx, in, src, createResp, progress, fileName, concurrencyParams.actual)
	}

	// å•çº¿ç¨‹ä¸Šä¼ ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
	fs.Debugf(f, "ä½¿ç”¨v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")
	return f.v2UploadChunksSingleThreaded(ctx, in, src, createResp, progress, fileName)
}

// downloadThenUpload è·¨äº‘ä¼ è¾“ç­–ç•¥ï¼šå…ˆå®Œæ•´ä¸‹è½½åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ï¼Œå†è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ 
// ç¡®ä¿è·¨äº‘ä¼ è¾“çš„å¯é æ€§ï¼Œé¿å…å¤æ‚çš„æµç®¡ç†é—®é¢˜
func (f *Fs) downloadThenUpload(ctx context.Context, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, maxConcurrency int) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ“¥ è·¨äº‘ä¼ è¾“ç­–ç•¥ï¼šå¼€å§‹ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ (%s)", fs.SizeSuffix(fileSize))

	// é‡æ–°æ‰“å¼€æºå¯¹è±¡è·å–å…¨æ–°çš„è¾“å…¥æµ
	srcObj, ok := src.(fs.Object)
	if !ok {
		return nil, fmt.Errorf("æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€ï¼Œæ— æ³•è¿›è¡Œè·¨äº‘ä¼ è¾“")
	}

	// åˆ›å»ºæœ¬åœ°ä¸´æ—¶æ–‡ä»¶ç”¨äºå®Œæ•´ä¸‹è½½
	tempFile, err := os.CreateTemp("", "cross_cloud_download_*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// å®Œæ•´ä¸‹è½½æ–‡ä»¶å†…å®¹åˆ°æœ¬åœ°ï¼Œæ˜¾ç¤ºè¯¦ç»†è¿›åº¦
	fs.Infof(f, "ğŸ“¥ æ­£åœ¨ä»æºäº‘ç›˜ä¸‹è½½æ–‡ä»¶å†…å®¹ï¼Œé¢„è®¡å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ™ºèƒ½é€‰æ‹©ä¸‹è½½ç­–ç•¥ï¼šå¤§æ–‡ä»¶ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼Œå°æ–‡ä»¶ä½¿ç”¨å•çº¿ç¨‹
	startTime := time.Now()
	var written int64

	// å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å¯ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½å™¨ç”¨äºè·¨äº‘ä¼ è¾“
	// è·¨äº‘ä¼ è¾“åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ä½¿ç”¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
	// ä¼˜åŒ–ï¼šé™ä½é˜ˆå€¼åˆ°20MBï¼Œè®©æ›´å¤šæ–‡ä»¶äº«å—å¹¶å‘ä¸‹è½½ä¼˜åŒ–
	if fileSize > 20*1024*1024 && f.concurrentDownloader != nil {
		fs.Infof(f, "ğŸš€ 123ç½‘ç›˜å¯ç”¨ç»Ÿä¸€å¹¶å‘ä¸‹è½½ä¼˜åŒ– (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))

		// å®Œå…¨å‚è€ƒ115ç½‘ç›˜ï¼šåˆ›å»ºTransferå¯¹è±¡ç”¨äºè¿›åº¦æ˜¾ç¤º
		var downloadTransfer *accounting.Transfer
		if stats := accounting.GlobalStats(); stats != nil {
			// è·å–æºæ–‡ä»¶ç³»ç»Ÿ
			var srcFs fs.Fs
			if srcFsInfo := srcObj.Fs(); srcFsInfo != nil {
				if srcFsTyped, ok := srcFsInfo.(fs.Fs); ok {
					srcFs = srcFsTyped
				}
			}

			downloadTransfer = stats.NewTransferRemoteSize(
				fmt.Sprintf("ğŸ“¥ %s (è·¨äº‘ä¸‹è½½)", fileName),
				fileSize,
				srcFs, // æºæ–‡ä»¶ç³»ç»Ÿ
				f,     // ç›®æ ‡æ˜¯123ç½‘ç›˜
			)
			defer downloadTransfer.Done(ctx, nil)
			// è°ƒè¯•æ—¥å¿—å·²ä¼˜åŒ–ï¼šå¯¹è±¡åˆ›å»ºè¯¦æƒ…ä»…åœ¨è¯¦ç»†æ¨¡å¼ä¸‹è¾“å‡º
		}

		// ä½¿ç”¨æ”¯æŒAccountçš„å¹¶å‘ä¸‹è½½å™¨
		if downloadTransfer != nil {
			written, err = f.concurrentDownloader.DownloadToFileWithAccount(ctx, srcObj, tempFile, downloadTransfer)
		} else {
			// å›é€€åˆ°åŸæ–¹æ³•
			written, err = f.concurrentDownloader.DownloadToFile(ctx, srcObj, tempFile)
		}
	} else {
		fs.Infof(f, "ğŸ“¥ 123ç½‘ç›˜ä½¿ç”¨å•çº¿ç¨‹ä¸‹è½½ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•é‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
		}
		defer srcReader.Close()
		written, err = f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, true, fileName)
	}
	downloadDuration := time.Since(startTime)

	// Note: err is set in both branches above and checked here
	if err != nil {
		return nil, fmt.Errorf("ä»æºäº‘ç›˜ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}

	// è®¡ç®—ä¸‹è½½é€Ÿåº¦
	downloadSpeed := float64(written) / downloadDuration.Seconds() / (1024 * 1024) // MB/s
	fs.Infof(f, "âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: %sï¼Œè€—æ—¶: %vï¼Œå¹³å‡é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), downloadDuration.Round(time.Second), downloadSpeed)

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´å‡†å¤‡ä¸Šä¼ 
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(f, "ğŸš€ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜: %s", fs.SizeSuffix(written))

	// ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ ï¼ˆé€’å½’è°ƒç”¨ï¼Œä½†æ­¤æ—¶isRemoteSourceä¸ºfalseï¼‰
	uploadStartTime := time.Now()
	result, err := f.v2UploadChunksWithConcurrency(ctx, tempFile, &localFileInfo{
		name:    fileName,
		size:    written,
		modTime: src.ModTime(ctx),
	}, createResp, progress, fileName, maxConcurrency)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(written) / uploadDuration.Seconds() / (1024 * 1024) // MB/s
	totalDuration := time.Since(startTime)

	fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“å®Œæˆ: %sï¼Œæ€»è€—æ—¶: %v (ä¸‹è½½: %v + ä¸Šä¼ : %v)ï¼Œä¸Šä¼ é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), totalDuration.Round(time.Second),
		downloadDuration.Round(time.Second), uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// localFileInfo æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ç»“æ„ï¼Œç”¨äºdownloadThenUploadç­–ç•¥
type localFileInfo struct {
	name    string
	size    int64
	modTime time.Time
}

func (lfi *localFileInfo) Fs() fs.Info                           { return nil }
func (lfi *localFileInfo) String() string                        { return lfi.name }
func (lfi *localFileInfo) Remote() string                        { return lfi.name }
func (lfi *localFileInfo) ModTime(ctx context.Context) time.Time { return lfi.modTime }
func (lfi *localFileInfo) Size() int64                           { return lfi.size }
func (lfi *localFileInfo) Storable() bool                        { return true }
func (lfi *localFileInfo) Hash(ctx context.Context, ty fshash.Type) (string, error) {
	return "", fshash.ErrUnsupported
}

// streamingPutSimplified ç®€åŒ–çš„æµå¼ä¸Šä¼ å®ç°
func (f *Fs) streamingPutSimplified(uploadCtx *UnifiedUploadContext) (*Object, error) {
	// æ£€æŸ¥æ˜¯å¦æœ‰å·²çŸ¥MD5
	if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
		fs.Debugf(f, "ä½¿ç”¨å·²çŸ¥MD5è¿›è¡Œæµå¼ä¸Šä¼ : %s", hashValue)
		return f.putWithKnownMD5(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName, hashValue)
	}

	// å¯¹äºä¸­ç­‰å¤§å°æ–‡ä»¶ï¼Œä½¿ç”¨å†…å­˜ç¼“å†²ç­–ç•¥
	if uploadCtx.src.Size() > 0 && uploadCtx.src.Size() <= 50*1024*1024 { // 50MBä»¥ä¸‹
		fs.Debugf(f, "ä¸­ç­‰æ–‡ä»¶ä½¿ç”¨å†…å­˜ç¼“å†²ç­–ç•¥")
		return f.putSmallFileWithMD5(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName)
	}

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“
	fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“")
	return f.streamingPutWithChunkedResume(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName)
}

// v2ChunkResult v2åˆ†ç‰‡ä¸Šä¼ ç»“æœ
type v2ChunkResult struct {
	chunkIndex int64
	err        error
	chunkHash  string
	duration   time.Duration // ä¸Šä¼ è€—æ—¶
	size       int64         // åˆ†ç‰‡å¤§å°
}

// v2UploadChunksWithConcurrency åŸºäºv2 APIçš„å¤šçº¿ç¨‹å¹¶å‘åˆ†ç‰‡ä¸Šä¼ 
// ä½¿ç”¨io.SectionReaderå®ç°çœŸæ­£çš„å¹¶å‘ä¸Šä¼ ï¼Œä¸ä¾èµ–æºå¯¹è±¡ç±»å‹
func (f *Fs) v2UploadChunksWithConcurrency(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, maxConcurrency int) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	// è°ƒè¯•preuploadID
	fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ å‚æ•°æ£€æŸ¥: preuploadID='%s', totalChunks=%d, maxConcurrency=%d", preuploadID, totalChunks, maxConcurrency)
	if preuploadID == "" {
		return nil, fmt.Errorf("é¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	// è·¨äº‘ä¼ è¾“æ£€æµ‹ï¼šè°ƒæ•´å•æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ å¹¶å‘ï¼ˆä¸å½±å“æ–‡ä»¶çº§å¹¶å‘ï¼‰
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		// ä¿®å¤ï¼šä»…é™åˆ¶å•æ–‡ä»¶å†…çš„åˆ†ç‰‡ä¸Šä¼ å¹¶å‘ï¼Œä¸å½±å“å¤šæ–‡ä»¶ä¼ è¾“å¹¶å‘
		// 115ç½‘ç›˜æ”¯æŒ10ä¸ªæ–‡ä»¶åŒæ—¶ä¸‹è½½ï¼Œæ¯ä¸ªæ–‡ä»¶2çº¿ç¨‹ï¼Œè¿™é‡Œåªé™åˆ¶å•æ–‡ä»¶å†…çš„åˆ†ç‰‡å¹¶å‘
		fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“ï¼Œè°ƒæ•´å•æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ å¹¶å‘ä»¥é€‚é…115ç½‘ç›˜é™åˆ¶")
		if maxConcurrency > 2 {
			maxConcurrency = 2 // é™åˆ¶å•æ–‡ä»¶åˆ†ç‰‡å¹¶å‘ä¸º2ï¼ŒåŒ¹é…115ç½‘ç›˜å•æ–‡ä»¶2çº¿ç¨‹é™åˆ¶
			fs.Debugf(f, " è·¨äº‘ä¼ è¾“ï¼šå•æ–‡ä»¶åˆ†ç‰‡å¹¶å‘è°ƒæ•´ä¸º %d", maxConcurrency)
		}
	}

	fs.Debugf(f, "å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡ï¼Œæœ€å¤§å¹¶å‘æ•°ï¼š%d", totalChunks, maxConcurrency)

	// æ£€æŸ¥å½“å‰uploadPacerçš„é…ç½®ï¼Œç»™å‡ºQPSé™åˆ¶æé†’
	uploadQPS := 5.0 // v2åˆ†ç‰‡ä¸Šä¼ APIçš„ä¿å®ˆä¼°è®¡QPS
	if maxConcurrency > int(uploadQPS) {
		fs.Infof(f, "âš ï¸  QPSæé†’: å½“å‰å¹¶å‘æ•°ä¸º%dï¼Œv2åˆ†ç‰‡ä¸Šä¼ APIé™åˆ¶çº¦%.1f QPSï¼Œå¦‚é‡429é”™è¯¯è¯·è€ƒè™‘é™ä½å¹¶å‘æ•°", maxConcurrency, uploadQPS)
	} else {
		fs.Infof(f, "âœ… QPSé…ç½®: å¹¶å‘æ•°%dé€‚é…v2åˆ†ç‰‡ä¸Šä¼ APIé™åˆ¶%.1f QPS", maxConcurrency, uploadQPS)
	}

	// è·å–é¢„æœŸçš„æ•´ä½“æ–‡ä»¶MD5ç”¨äºæœ€ç»ˆéªŒè¯
	var expectedMD5 string
	if hashValue, err := src.Hash(ctx, fshash.MD5); err == nil && hashValue != "" {
		expectedMD5 = strings.ToLower(hashValue)
		fs.Debugf(f, "é¢„æœŸæ•´ä½“æ–‡ä»¶MD5: %s", expectedMD5)
	}

	// å°†è¾“å…¥æµè¯»å–åˆ°å†…å­˜æˆ–ä¸´æ—¶æ–‡ä»¶ï¼Œä»¥æ”¯æŒå¹¶å‘è®¿é—®ï¼ˆæ­¤æ—¶å·²ç¡®ä¿æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
	var dataSource io.ReaderAt
	var cleanup func()

	if fileSize <= maxMemoryBufferSize {
		// å°æ–‡ä»¶ï¼šè¯»å–åˆ°å†…å­˜è¿›è¡Œå¹¶å‘ä¸Šä¼ 
		fs.Debugf(f, "ğŸ“ å°æ–‡ä»¶(%s)ï¼Œè¯»å–åˆ°å†…å­˜è¿›è¡Œå¹¶å‘ä¸Šä¼ ", fs.SizeSuffix(fileSize))
		data, err := f.readToMemoryWithRetry(ctx, in, fileSize, false)
		if err != nil {
			return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜å¤±è´¥: %w", err)
		}
		dataSource = bytes.NewReader(data)
		cleanup = func() {} // å†…å­˜æ•°æ®æ— éœ€æ¸…ç†
		fs.Debugf(f, " å°æ–‡ä»¶å†…å­˜ç¼“å­˜å®Œæˆï¼Œå®é™…å¤§å°: %s", fs.SizeSuffix(int64(len(data))))
	} else {
		// å¤§æ–‡ä»¶ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¹¶å‘ä¸Šä¼ 
		fs.Debugf(f, "ğŸ—‚ï¸  å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¹¶å‘ä¸Šä¼ ", fs.SizeSuffix(fileSize))
		tempFile, written, err := f.createTempFileWithRetry(ctx, in, fileSize, false, fileName)
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}

		dataSource = tempFile
		cleanup = func() {
		}

		fs.Debugf(f, " å¤§æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå®Œæˆï¼ŒæœŸæœ›å¤§å°: %sï¼Œå®é™…å¤§å°: %s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}
	defer cleanup()

	// éªŒè¯æ•°æ®æºå®Œæ•´æ€§
	fs.Debugf(f, " éªŒè¯æ•°æ®æºå®Œæ•´æ€§...")
	if dataSource == nil {
		return nil, fmt.Errorf("æ•°æ®æºåˆ›å»ºå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¤šçº¿ç¨‹ä¸Šä¼ ")
	}

	// æµ‹è¯•æ•°æ®æºçš„éšæœºè®¿é—®èƒ½åŠ›
	testBuffer := make([]byte, 1024)
	if _, err := dataSource.ReadAt(testBuffer, 0); err != nil && err != io.EOF {
		fs.Debugf(f, "âš ï¸  æ•°æ®æºéšæœºè®¿é—®æµ‹è¯•å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ•°æ®æºä¸æ”¯æŒéšæœºè®¿é—®ï¼Œæ— æ³•è¿›è¡Œå¤šçº¿ç¨‹ä¸Šä¼ : %w", err)
	}
	fs.Debugf(f, " æ•°æ®æºéªŒè¯é€šè¿‡ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘è®¿é—®")

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
	uploadTimeout := time.Duration(totalChunks) * 10 * time.Minute
	uploadTimeout = min(uploadTimeout, 2*time.Hour)
	workerCtx, cancelWorkers := context.WithTimeout(ctx, uploadTimeout)
	defer cancelWorkers()

	// åˆ›å»ºå·¥ä½œé˜Ÿåˆ—å’Œç»“æœé€šé“
	queueSize := maxConcurrency * 2
	queueSize = min(queueSize, int(totalChunks))
	chunkJobs := make(chan int64, queueSize)
	results := make(chan v2ChunkResult, maxConcurrency)

	// å¯åŠ¨å·¥ä½œåç¨‹
	fs.Debugf(f, "ğŸš€ å¯åŠ¨ %d ä¸ªå·¥ä½œåç¨‹è¿›è¡Œå¹¶å‘ä¸Šä¼ ", maxConcurrency)
	for i := 0; i < maxConcurrency; i++ {
		workerID := i + 1
		fs.Debugf(f, "ğŸ“‹ å¯åŠ¨å·¥ä½œåç¨‹ #%d", workerID)
		// ä¿®å¤ï¼šä¼ é€’è¿œç¨‹è·¯å¾„å‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskIDï¼Œä¼˜å…ˆä½¿ç”¨src.Remote()
		remotePath := src.Remote()
		if remotePath == "" && progress.FilePath != "" {
			remotePath = progress.FilePath
		}
		fs.Debugf(f, " Worker-%d ä½¿ç”¨è¿œç¨‹è·¯å¾„: '%s' (src.Remote: '%s', progress.FilePath: '%s')",
			workerID, remotePath, src.Remote(), progress.FilePath)
		go f.v2ChunkUploadWorker(workerCtx, dataSource, preuploadID, chunkSize, fileSize, totalChunks, remotePath, chunkJobs, results)
	}
	fs.Debugf(f, " æ‰€æœ‰å·¥ä½œåç¨‹å·²å¯åŠ¨ï¼Œå¼€å§‹åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡åˆ†å‘")

	// å‘é€éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ä»»åŠ¡
	pendingChunks := int64(0)
	go func() {
		defer close(chunkJobs)
		for chunkIndex := range totalChunks {
			partNumber := chunkIndex + 1
			if !progress.IsUploaded(partNumber) {
				select {
				case chunkJobs <- chunkIndex:
					// ä»»åŠ¡å‘é€æˆåŠŸ
				case <-workerCtx.Done():
					fs.Debugf(f, "ä¸Šä¸‹æ–‡å–æ¶ˆï¼Œåœæ­¢å‘é€åˆ†ç‰‡ä»»åŠ¡")
					return
				}
			}
		}
	}()

	// è®¡ç®—éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡æ•°é‡
	for chunkIndex := range totalChunks {
		partNumber := chunkIndex + 1
		if !progress.IsUploaded(partNumber) {
			pendingChunks++
		}
	}

	fs.Debugf(f, "éœ€è¦ä¸Šä¼  %d ä¸ªåˆ†ç‰‡", pendingChunks)

	// æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
	if pendingChunks > 0 {
		completedChunks := int64(0)
		totalUploadedBytes := int64(0)
		uploadStartTime := time.Now()

		fs.Infof(f, "ğŸš€ å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ : %dä¸ªåˆ†ç‰‡, æ€»å¤§å°: %s, å¹¶å‘æ•°: %d",
			pendingChunks, fs.SizeSuffix(fileSize), maxConcurrency)

		// ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæŸ¥æ‰¾Accountå¯¹è±¡è¿›è¡Œé›†æˆ
		var currentAccount *accounting.Account
		if stats := accounting.GlobalStats(); stats != nil {
			// å°è¯•ç²¾ç¡®åŒ¹é…
			currentAccount = stats.GetInProgressAccount(fileName)
			if currentAccount == nil {
				// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
				allAccounts := stats.ListInProgressAccounts()
				for _, accountName := range allAccounts {
					if strings.Contains(accountName, fileName) || strings.Contains(fileName, accountName) {
						currentAccount = stats.GetInProgressAccount(accountName)
						if currentAccount != nil {
							break
						}
					}
				}
			}
		}

		for completedChunks < pendingChunks {
			select {
			case result := <-results:
				if result.err != nil {
					fs.Errorf(f, "âŒ åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %v", result.chunkIndex+1, result.err)

					// æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ™ºèƒ½å›é€€
					shouldFallback := f.shouldFallbackToSingleThread(result.err, src)
					if shouldFallback {
						fs.Infof(f, "ğŸ”„ æ£€æµ‹åˆ°å¤šçº¿ç¨‹ä¸Šä¼ é—®é¢˜ï¼Œå¯åŠ¨æ™ºèƒ½å›é€€æœºåˆ¶...")

						// ä¿å­˜å½“å‰è¿›åº¦åˆ°ç»Ÿä¸€ç®¡ç†å™¨
						saveErr := f.saveUploadProgress(progress)
						if saveErr != nil {
							fs.Debugf(f, "å›é€€å‰ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
						}

						// å°è¯•å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
						return f.fallbackToSingleThreadUpload(ctx, in, src, createResp, progress, fileName, result.err)
					}

					// å¦‚æœä¸éœ€è¦å›é€€ï¼Œä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
					saveErr := f.saveUploadProgress(progress)
					if saveErr != nil {
						fs.Debugf(f, "é”™è¯¯å¤„ç†æ—¶ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
					}
					return nil, fmt.Errorf("åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %w", result.chunkIndex+1, result.err)
				}

				// æ ‡è®°åˆ†ç‰‡å®Œæˆ
				partNumber := result.chunkIndex + 1
				progress.SetUploaded(partNumber)
				completedChunks++

				// è®¡ç®—å·²ä¸Šä¼ çš„å­—èŠ‚æ•°
				chunkStart := (result.chunkIndex) * chunkSize
				actualChunkSize := chunkSize
				if chunkStart+chunkSize > fileSize {
					actualChunkSize = fileSize - chunkStart
				}
				totalUploadedBytes += actualChunkSize

				// ä¿å­˜è¿›åº¦åˆ°ç»Ÿä¸€ç®¡ç†å™¨ï¼ˆä¸»åç¨‹å¤‡ä»½ä¿å­˜ï¼Œå·¥ä½œåç¨‹å·²ç»ä¿å­˜è¿‡ï¼‰
				// è¿™é‡Œä¸»è¦æ˜¯ä¸ºäº†åŒæ­¥UploadProgressç»“æ„ä½“çš„çŠ¶æ€åˆ°ç»Ÿä¸€ç®¡ç†å™¨
				err := f.saveUploadProgress(progress)
				if err != nil {
					fs.Debugf(f, "ä¸»åç¨‹ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
				}

				// è®¡ç®—æ•´ä½“è¿›åº¦ç»Ÿè®¡
				elapsed := time.Since(uploadStartTime)
				percentage := float64(completedChunks) / float64(pendingChunks) * 100
				avgSpeed := float64(totalUploadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s

				// ä¼°ç®—å‰©ä½™æ—¶é—´
				var eta string
				if avgSpeed > 0 {
					remainingBytes := fileSize - totalUploadedBytes
					etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
					eta = fmt.Sprintf("ETA: %v", time.Duration(etaSeconds)*time.Second)
				} else {
					eta = "ETA: è®¡ç®—ä¸­..."
				}

				fs.Infof(f, "ğŸ“Š ä¸Šä¼ è¿›åº¦: %d/%dåˆ†ç‰‡ (%.1f%%) | å·²ä¼ è¾“: %s/%s | é€Ÿåº¦: %.2f MB/s | %s",
					completedChunks, pendingChunks, percentage,
					fs.SizeSuffix(totalUploadedBytes), fs.SizeSuffix(fileSize), avgSpeed, eta)

				// ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯
				if currentAccount != nil {
					extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completedChunks, pendingChunks)
					currentAccount.SetExtraInfo(extraInfo)
				}

			case <-workerCtx.Done():
				return nil, fmt.Errorf("ä¸Šä¼ è¶…æ—¶æˆ–è¢«å–æ¶ˆ")
			}
		}

		// æœ€ç»ˆç»Ÿè®¡
		totalElapsed := time.Since(uploadStartTime)
		finalSpeed := float64(fileSize) / totalElapsed.Seconds() / 1024 / 1024
		fs.Infof(f, "ğŸ‰ å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼æ–‡ä»¶å¤§å°: %s | æ€»è€—æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s",
			fs.SizeSuffix(fileSize), totalElapsed.Round(time.Second), finalSpeed)
	}

	// å®Œæˆä¸Šä¼ ï¼Œä¼ é€’é¢„æœŸMD5ç”¨äºéªŒè¯
	return f.completeV2Upload(ctx, preuploadID, fileName, fileSize, expectedMD5)
}

// v2ChunkUploadWorker v2åˆ†ç‰‡ä¸Šä¼ å·¥ä½œåç¨‹ï¼Œå¸¦è¶…æ—¶å’Œæ­»é”æ£€æµ‹
// ä¿®å¤ï¼šå¢åŠ remotePathå‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskID
func (f *Fs) v2ChunkUploadWorker(ctx context.Context, dataSource io.ReaderAt, preuploadID string, chunkSize, fileSize, totalChunks int64, remotePath string, jobs <-chan int64, results chan<- v2ChunkResult) {
	workerID := fmt.Sprintf("Worker-%d", time.Now().UnixNano()%1000) // ç®€å•çš„å·¥ä½œåç¨‹ID
	fs.Debugf(f, " %s å¯åŠ¨ï¼Œç­‰å¾…åˆ†ç‰‡ä»»åŠ¡...", workerID)

	processedCount := 0
	lastActivityTime := time.Now()

	// å¯åŠ¨æ­»é”æ£€æµ‹åç¨‹
	deadlockCtx, cancelDeadlock := context.WithCancel(ctx)
	defer cancelDeadlock()

	go f.workerDeadlockDetector(deadlockCtx, workerID, &lastActivityTime, 5*time.Minute)

	for chunkIndex := range jobs {
		select {
		case <-ctx.Done():
			fs.Debugf(f, "âš ï¸  %s æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œå·²å¤„ç† %d ä¸ªåˆ†ç‰‡", workerID, processedCount)
			return
		default:
			// æ›´æ–°æ´»åŠ¨æ—¶é—´
			lastActivityTime = time.Now()
			processedCount++
			partNumber := chunkIndex + 1
			fs.Debugf(f, "ğŸ“‹ %s å¼€å§‹å¤„ç†åˆ†ç‰‡ %d/%d (ç¬¬%dä¸ªä»»åŠ¡)", workerID, partNumber, totalChunks, processedCount)

			// ä¸ºå•ä¸ªåˆ†ç‰‡å¤„ç†è®¾ç½®è¶…æ—¶
			chunkCtx, cancelChunk := context.WithTimeout(ctx, 10*time.Minute)
			// ä¿®å¤ï¼šä¼ é€’remotePathå‚æ•°
			result := f.processChunkWithTimeout(chunkCtx, dataSource, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks, remotePath, workerID)
			cancelChunk()

			// å‘é€ç»“æœæ—¶ä¹Ÿè¦æ£€æµ‹è¶…æ—¶
			select {
			case results <- result:
				lastActivityTime = time.Now() // æ›´æ–°æ´»åŠ¨æ—¶é—´
			case <-ctx.Done():
				fs.Debugf(f, "âš ï¸  %s å‘é€ç»“æœæ—¶æ”¶åˆ°å–æ¶ˆä¿¡å·", workerID)
				return
			case <-time.After(30 * time.Second):
				fs.Errorf(f, "âš ï¸  %s å‘é€ç»“æœè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ­»é”", workerID)
				// å°è¯•å‘é€è¶…æ—¶é”™è¯¯ç»“æœ
				timeoutResult := v2ChunkResult{
					chunkIndex: chunkIndex,
					err:        fmt.Errorf("å·¥ä½œåç¨‹å‘é€ç»“æœè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ­»é”"),
				}
				select {
				case results <- timeoutResult:
				default:
					fs.Errorf(f, "âŒ %s æ— æ³•å‘é€è¶…æ—¶é”™è¯¯ç»“æœï¼Œé€šé“å¯èƒ½å·²æ»¡", workerID)
				}
				return
			}
		}
	}

	fs.Debugf(f, "ğŸ %s å®Œæˆå·¥ä½œï¼Œå…±å¤„ç† %d ä¸ªåˆ†ç‰‡", workerID, processedCount)
}

// workerDeadlockDetector å·¥ä½œåç¨‹æ­»é”æ£€æµ‹å™¨
func (f *Fs) workerDeadlockDetector(ctx context.Context, workerID string, lastActivityTime *time.Time, timeout time.Duration) {
	ticker := time.NewTicker(30 * time.Second) // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			if time.Since(*lastActivityTime) > timeout {
				fs.Errorf(f, "ğŸš¨ %s å¯èƒ½å‘ç”Ÿæ­»é”ï¼è¶…è¿‡ %v æ— æ´»åŠ¨", workerID, timeout)
				// è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ­»é”å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚å‘é€å‘Šè­¦ç­‰
			}
		}
	}
}

// processChunkWithTimeout å¸¦è¶…æ—¶çš„åˆ†ç‰‡å¤„ç†å‡½æ•°
// ä¿®å¤ï¼šå¢åŠ remotePathå‚æ•°ä»¥æ”¯æŒç»Ÿä¸€TaskID
func (f *Fs) processChunkWithTimeout(ctx context.Context, dataSource io.ReaderAt, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64, remotePath string, workerID string) v2ChunkResult {
	partNumber := chunkIndex + 1

	// è®¡ç®—åˆ†ç‰‡çš„èµ·å§‹ä½ç½®å’Œå¤§å°
	start := chunkIndex * chunkSize
	actualChunkSize := chunkSize
	if start+chunkSize > fileSize {
		actualChunkSize = fileSize - start
	}

	// åˆ›å»ºåˆ†ç‰‡è¯»å–å™¨
	chunkReader := io.NewSectionReader(dataSource, start, actualChunkSize)

	// è¯»å–åˆ†ç‰‡æ•°æ®å¹¶éªŒè¯
	chunkData := make([]byte, actualChunkSize)
	bytesRead, err := io.ReadFull(chunkReader, chunkData)
	if err != nil {
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err),
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®é•¿åº¦
	if int64(bytesRead) != actualChunkSize {
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("åˆ†ç‰‡æ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", actualChunkSize, bytesRead),
		}
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	hasher := md5.New()
	hasher.Write(chunkData)
	chunkHash := fmt.Sprintf("%x", hasher.Sum(nil))

	// ä¸Šä¼ åˆ†ç‰‡ï¼ˆè®°å½•å¼€å§‹æ—¶é—´ç”¨äºé€Ÿåº¦è®¡ç®—ï¼‰
	startTime := time.Now()

	fs.Debugf(f, "ğŸš€ %s å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %d/%d (å¤§å°: %s, MD5: %s)",
		workerID, partNumber, totalChunks, fs.SizeSuffix(actualChunkSize), chunkHash[:8]+"...")

	// ä¼˜åŒ–ï¼šé›†æˆUnifiedErrorHandlerçš„æ™ºèƒ½é‡è¯•æœºåˆ¶
	var uploadErr error
	maxRetries := 3
	for retry := range maxRetries {
		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
		select {
		case <-ctx.Done():
			return v2ChunkResult{
				chunkIndex: chunkIndex,
				err:        fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ è¢«å–æ¶ˆ: %w", ctx.Err()),
			}
		default:
		}

		uploadErr = f.uploadChunkV2(ctx, preuploadID, partNumber, chunkData, chunkHash)
		if uploadErr == nil {
			break // ä¸Šä¼ æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
		}

		// ä¼˜åŒ–ï¼šä½¿ç”¨UnifiedErrorHandleråˆ¤æ–­é‡è¯•ç­–ç•¥
		if f.errorHandler != nil {
			shouldRetry, retryDelay := f.errorHandler.HandleErrorWithRetry(ctx, uploadErr,
				fmt.Sprintf("åˆ†ç‰‡%dä¸Šä¼ ", partNumber), retry, maxRetries)
			if !shouldRetry {
				fs.Debugf(f, "%s UnifiedErrorHandleråˆ¤æ–­åˆ†ç‰‡ %d ä¸å¯é‡è¯•: %v",
					workerID, partNumber, uploadErr)
				break
			}
			if retryDelay > 0 && retry < maxRetries {
				fs.Debugf(f, "âš ï¸  %s åˆ†ç‰‡ %d/%d ä¸Šä¼ å¤±è´¥ï¼ŒUnifiedErrorHandlerå»ºè®®å»¶è¿Ÿ %v åé‡è¯• (%d/%d): %v",
					workerID, partNumber, totalChunks, retryDelay, retry+1, maxRetries, uploadErr)
				select {
				case <-ctx.Done():
					return v2ChunkResult{
						chunkIndex: chunkIndex,
						err:        fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ é‡è¯•æ—¶è¢«å–æ¶ˆ: %w", ctx.Err()),
					}
				case <-time.After(retryDelay):
				}
				continue
			}
		}

		if retry < maxRetries {
			retryDelay := time.Duration(retry+1) * 2 * time.Second // å›é€€åˆ°é»˜è®¤å»¶è¿Ÿ
			fs.Debugf(f, "âš ï¸  %s åˆ†ç‰‡ %d/%d ä¸Šä¼ å¤±è´¥ï¼Œ%våé‡è¯• (%d/%d): %v",
				workerID, partNumber, totalChunks, retryDelay, retry+1, maxRetries, uploadErr)

			select {
			case <-ctx.Done():
				return v2ChunkResult{
					chunkIndex: chunkIndex,
					err:        fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ é‡è¯•æ—¶è¢«å–æ¶ˆ: %w", ctx.Err()),
				}
			case <-time.After(retryDelay):
				// ç»§ç»­é‡è¯•
			}
		}
	}

	if uploadErr != nil {
		duration := time.Since(startTime)
		fs.Errorf(f, "âŒ %s åˆ†ç‰‡ %d/%d ä¸Šä¼ å¤±è´¥ (è€—æ—¶: %v, é‡è¯•: %dæ¬¡): %v",
			workerID, partNumber, totalChunks, duration, maxRetries, uploadErr)
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡å¤±è´¥(é‡è¯•%dæ¬¡): %w", maxRetries, uploadErr),
		}
	}

	// è®¡ç®—ä¸Šä¼ é€Ÿåº¦å’Œç»Ÿè®¡ä¿¡æ¯
	duration := time.Since(startTime)
	speed := float64(actualChunkSize) / duration.Seconds() / 1024 / 1024 // MB/s

	fs.Infof(f, "âœ… %s åˆ†ç‰‡ %d/%d ä¸Šä¼ æˆåŠŸ (å¤§å°: %s, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s)",
		workerID, partNumber, totalChunks, fs.SizeSuffix(actualChunkSize), duration, speed)

	// ç®€åŒ–ï¼šä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯åˆ°ç»Ÿä¸€ç®¡ç†å™¨
	if f.resumeManager != nil {
		// ç®€åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€çš„TaskIDè·å–å’Œè¿ç§»å‡½æ•°
		taskID := f.getTaskIDWithMigration(preuploadID, remotePath, fileSize)

		// ä½¿ç”¨ç»Ÿä¸€ç®¡ç†å™¨æ ‡è®°åˆ†ç‰‡å®Œæˆï¼Œå¢åŠ é‡è¯•æœºåˆ¶
		maxRetries := 3
		var saveErr error
		for retry := range maxRetries {
			saveErr = f.resumeManager.MarkChunkCompleted(taskID, chunkIndex)
			if saveErr == nil {
				fs.Debugf(f, "ğŸ’¾ %s æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å·²ä¿å­˜: åˆ†ç‰‡%d/%d (TaskID: %s)",
					workerID, partNumber, totalChunks, taskID)
				break
			}
			if retry < maxRetries-1 {
				fs.Debugf(f, "âš ï¸ %s ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥ï¼Œé‡è¯• %d/%d: %v",
					workerID, retry+1, maxRetries, saveErr)
				time.Sleep(time.Duration(retry+1) * 100 * time.Millisecond) // é€’å¢å»¶è¿Ÿ
			}
		}
		if saveErr != nil {
			fs.Errorf(f, "âŒ %s ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æœ€ç»ˆå¤±è´¥: %v", workerID, saveErr)
			// æ³¨æ„ï¼šè¿™é‡Œä¸è¿”å›é”™è¯¯ï¼Œå› ä¸ºåˆ†ç‰‡ä¸Šä¼ å·²ç»æˆåŠŸï¼Œåªæ˜¯ä¿å­˜è¿›åº¦å¤±è´¥
		}
	}

	// æˆåŠŸ
	return v2ChunkResult{
		chunkIndex: chunkIndex,
		err:        nil,
		chunkHash:  chunkHash,
		duration:   duration,
		size:       actualChunkSize,
	}
}

// shouldFallbackToSingleThread åˆ¤æ–­æ˜¯å¦åº”è¯¥å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
func (f *Fs) shouldFallbackToSingleThread(err error, src fs.ObjectInfo) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())

	// æ£€æŸ¥è·¨äº‘ä¼ è¾“ç›¸å…³é”™è¯¯
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		// è·¨äº‘ä¼ è¾“æ—¶ï¼Œä»¥ä¸‹é”™è¯¯è§¦å‘å›é€€
		fallbackKeywords := []string{
			"æ•°æ®ä¸å®Œæ•´",
			"æ•°æ®ä¸åŒ¹é…",
			"è¯»å–å¤±è´¥",
			"è¿æ¥é‡ç½®",
			"è¶…æ—¶",
			"ç½‘ç»œé”™è¯¯",
			"æºå¯¹è±¡",
			"é‡æ–°æ‰“å¼€",
			"æ­»é”",
			"å·¥ä½œåç¨‹",
		}

		for _, keyword := range fallbackKeywords {
			if strings.Contains(errStr, keyword) {
				fs.Debugf(f, " æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“é”™è¯¯å…³é”®è¯: %s", keyword)
				return true
			}
		}
	}

	// æ£€æŸ¥å¤šçº¿ç¨‹ç›¸å…³é”™è¯¯
	multiThreadKeywords := []string{
		"å¹¶å‘",
		"åç¨‹",
		"goroutine",
		"concurrent",
		"deadlock",
		"timeout",
		"context canceled",
		"context deadline exceeded",
	}

	for _, keyword := range multiThreadKeywords {
		if strings.Contains(errStr, keyword) {
			fs.Debugf(f, " æ£€æµ‹åˆ°å¤šçº¿ç¨‹é”™è¯¯å…³é”®è¯: %s", keyword)
			return true
		}
	}

	return false
}

// fallbackToSingleThreadUpload å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
func (f *Fs) fallbackToSingleThreadUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, originalErr error) (*Object, error) {
	fs.Infof(f, "ğŸ”„ å¤šçº¿ç¨‹ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ æ¨¡å¼")
	fs.Debugf(f, "åŸå§‹é”™è¯¯: %v", originalErr)

	// æ£€æŸ¥æ˜¯å¦ä¸ºè·¨äº‘ä¼ è¾“
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“åœºæ™¯ï¼Œä½¿ç”¨å¢å¼ºå•çº¿ç¨‹æ¨¡å¼")
	}

	// å°è¯•å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
	fs.Debugf(f, " å°è¯•v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ...")
	result, err := f.v2UploadChunksSingleThreaded(ctx, in, src, createResp, progress, fileName)
	if err == nil {
		fs.Infof(f, "âœ… å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ æˆåŠŸ")
		return result, nil
	}

	fs.Debugf(f, "å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ä¹Ÿå¤±è´¥: %v", err)

	// å¦‚æœå•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ä¹Ÿå¤±è´¥ï¼Œå°è¯•æµå¼ä¸Šä¼ 
	if isRemoteSource {
		fs.Infof(f, "ğŸ”„ å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•æµå¼ä¸Šä¼ ...")

		// åˆ›å»ºæµå¼ä¸Šä¼ ä¸Šä¸‹æ–‡
		// æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–parentFileIDï¼Œæš‚æ—¶ä½¿ç”¨0ä½œä¸ºå ä½ç¬¦
		uploadCtx := &UnifiedUploadContext{
			ctx:          ctx,
			in:           in,
			src:          src,
			parentFileID: 0, // ä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºé»˜è®¤çˆ¶ç›®å½•
			fileName:     fileName,
		}

		streamResult, streamErr := f.streamingPutSimplified(uploadCtx)
		if streamErr == nil {
			fs.Infof(f, "âœ… æµå¼ä¸Šä¼ æˆåŠŸ")
			return streamResult, nil
		}

		fs.Debugf(f, "æµå¼ä¸Šä¼ ä¹Ÿå¤±è´¥: %v", streamErr)

		// è¿”å›æœ€è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
		return nil, fmt.Errorf("æ‰€æœ‰ä¸Šä¼ æ–¹å¼éƒ½å¤±è´¥ - åŸå§‹å¤šçº¿ç¨‹é”™è¯¯: %w, å•çº¿ç¨‹é”™è¯¯: %v, æµå¼ä¸Šä¼ é”™è¯¯: %v", originalErr, err, streamErr)
	}

	// éè·¨äº‘ä¼ è¾“åœºæ™¯ï¼Œè¿”å›å•çº¿ç¨‹ä¸Šä¼ é”™è¯¯
	return nil, fmt.Errorf("å¤šçº¿ç¨‹ä¸Šä¼ å¤±è´¥åå•çº¿ç¨‹ä¸Šä¼ ä¹Ÿå¤±è´¥ - åŸå§‹é”™è¯¯: %w, å•çº¿ç¨‹é”™è¯¯: %v", originalErr, err)
}

// uploadChunkV2 ä½¿ç”¨v2 APIä¸Šä¼ å•ä¸ªåˆ†ç‰‡
func (f *Fs) uploadChunkV2(ctx context.Context, preuploadID string, partNumber int64, chunkData []byte, chunkHash string) error {
	// æ£€æŸ¥preuploadIDæ˜¯å¦ä¸ºç©º
	if preuploadID == "" {
		fs.Errorf(f, "âŒ è‡´å‘½é”™è¯¯ï¼šé¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•ä¸Šä¼ åˆ†ç‰‡ %d", partNumber)
		return fmt.Errorf("é¢„ä¸Šä¼ IDä¸èƒ½ä¸ºç©º")
	}

	// æ£€æŸ¥åˆ†ç‰‡æ•°æ®æ˜¯å¦ä¸ºç©º
	if len(chunkData) == 0 {
		fs.Errorf(f, "âŒ è‡´å‘½é”™è¯¯ï¼šåˆ†ç‰‡ %d æ•°æ®ä¸ºç©º", partNumber)
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(chunkData))

	// ä½¿ç”¨uploadPacerè¿›è¡ŒQPSé™åˆ¶æ§åˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		// ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„ä¸Šä¼ åŸŸå
		uploadDomain, err := f.getCachedUploadDomain(ctx)
		if err != nil {
			return false, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}

		// åˆ›å»ºmultipartè¡¨å•æ•°æ®
		var buf bytes.Buffer
		writer := multipart.NewWriter(&buf)

		// æ·»åŠ è¡¨å•å­—æ®µ
		writer.WriteField("preuploadID", preuploadID)
		writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
		writer.WriteField("sliceMD5", chunkHash)

		// æ·»åŠ æ–‡ä»¶æ•°æ®
		part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
		if err != nil {
			return false, fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
		}

		_, err = part.Write(chunkData)
		if err != nil {
			return false, fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		err = writer.Close()
		if err != nil {
			return false, fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
		}

		// ä½¿ç”¨é€šç”¨çš„APIè°ƒç”¨æ–¹æ³•ï¼Œä½†éœ€è¦ç‰¹æ®Šå¤„ç†multipartæ•°æ®
		var response struct {
			Code    int    `json:"code"`
			Message string `json:"message"`
		}

		// ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜çš„ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
		err = f.makeAPICallWithRestMultipartToDomain(ctx, uploadDomain, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯QPSé™åˆ¶é”™è¯¯
			if strings.Contains(err.Error(), "429") || strings.Contains(err.Error(), "é¢‘ç‡") {
				fs.Debugf(f, "åˆ†ç‰‡%dé‡åˆ°QPSé™åˆ¶ï¼Œå°†é‡è¯•: %v", partNumber, err)
				return true, err
			}
			return false, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		if response.Code != 0 {
			// æ£€æŸ¥æ˜¯å¦æ˜¯QPSé™åˆ¶ç›¸å…³é”™è¯¯
			if response.Code == 429 || response.Code == 20003 || strings.Contains(response.Message, "é¢‘ç‡") {
				fs.Debugf(f, "åˆ†ç‰‡%dé‡åˆ°APIé¢‘ç‡é™åˆ¶(code=%d)ï¼Œå°†é‡è¯•", partNumber, response.Code)
				return true, fmt.Errorf("APIé¢‘ç‡é™åˆ¶: code=%d, message=%s", response.Code, response.Message)
			}
			return false, fmt.Errorf("APIè¿”å›é”™è¯¯: code=%d, message=%s", response.Code, response.Message)
		}

		return false, nil
	})
}

// v2UploadChunksSingleThreaded v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
func (f *Fs) v2UploadChunksSingleThreaded(ctx context.Context, in io.Reader, _ fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	fs.Debugf(f, "å¼€å§‹v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡", totalChunks)

	// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆå¯¹äºå•çº¿ç¨‹ï¼Œè¿™æ ·æ›´ç®€å•ï¼‰
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯è¯»å–çš„æ•°æ®é•¿åº¦ä¸é¢„æœŸæ–‡ä»¶å¤§å°åŒ¹é…
	actualDataSize := int64(len(data))
	if actualDataSize != fileSize {
		return nil, fmt.Errorf("æ•°æ®é•¿åº¦ä¸åŒ¹é…: å®é™…è¯»å– %d å­—èŠ‚ï¼Œé¢„æœŸ %d å­—èŠ‚", actualDataSize, fileSize)
	}

	if actualDataSize == 0 {
		return nil, fmt.Errorf("è¯»å–çš„æ–‡ä»¶æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	fs.Debugf(f, "æ•°æ®éªŒè¯é€šè¿‡: è¯»å– %d å­—èŠ‚ï¼Œåˆ†ç‰‡å¤§å° %dï¼Œæ€»åˆ†ç‰‡æ•° %d", actualDataSize, chunkSize, totalChunks)

	// é€ä¸ªä¸Šä¼ åˆ†ç‰‡
	for chunkIndex := range totalChunks {
		partNumber := chunkIndex + 1

		// æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
		if progress.IsUploaded(partNumber) {
			fs.Debugf(f, "åˆ†ç‰‡ %d å·²ä¸Šä¼ ï¼Œè·³è¿‡", partNumber)
			continue
		}

		// è®¡ç®—åˆ†ç‰‡æ•°æ®ï¼Œæ·»åŠ è¾¹ç•Œæ£€æŸ¥
		start := chunkIndex * chunkSize
		end := start + chunkSize
		end = min(end, fileSize)

		// é¢å¤–çš„è¾¹ç•Œæ£€æŸ¥ï¼Œé˜²æ­¢slice panic
		if start >= actualDataSize {
			return nil, fmt.Errorf("åˆ†ç‰‡èµ·å§‹ä½ç½®è¶…å‡ºæ•°æ®èŒƒå›´: start=%d, dataSize=%d", start, actualDataSize)
		}
		end = min(end, actualDataSize)

		chunkData := data[start:end]

		// è®¡ç®—åˆ†ç‰‡MD5
		hasher := md5.New()
		hasher.Write(chunkData)
		chunkHash := fmt.Sprintf("%x", hasher.Sum(nil))

		// æ·»åŠ è¯¦ç»†çš„MD5è°ƒè¯•ä¿¡æ¯
		fs.Debugf(f, "åˆ†ç‰‡ %d MD5è®¡ç®—: æ•°æ®é•¿åº¦=%d, MD5=%s",
			partNumber, len(chunkData), chunkHash)

		// ä¸Šä¼ åˆ†ç‰‡
		err = f.uploadChunkV2(ctx, preuploadID, partNumber, chunkData, chunkHash)
		if err != nil {
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		// æ ‡è®°åˆ†ç‰‡å®Œæˆ
		progress.SetUploaded(partNumber)

		// ä¿å­˜è¿›åº¦
		err = f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
		}

		fs.Debugf(f, " åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ ï¼ˆå•çº¿ç¨‹ç‰ˆæœ¬ï¼Œæ²¡æœ‰é¢„æœŸMD5ï¼‰
	return f.completeV2Upload(ctx, preuploadID, fileName, fileSize, "")
}

// completeV2Upload å®Œæˆv2ä¸Šä¼ å¹¶è¿”å›Object
func (f *Fs) completeV2Upload(ctx context.Context, preuploadID, fileName string, fileSize int64, expectedMD5 string) (*Object, error) {
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ3/3ã€‘ä¸Šä¼ å®Œæ¯• - å¼€å§‹è½®è¯¢æ ¡éªŒï¼Œç­‰å¾…æœåŠ¡ç«¯æ–‡ä»¶æ ¡éªŒå®Œæˆ")
	// è°ƒç”¨å¢å¼ºçš„å®Œæˆä¸Šä¼ é€»è¾‘ï¼ŒåŒ…å«MD5éªŒè¯å’ŒåŠ¨æ€è½®è¯¢
	result, err := f.completeUploadWithMD5VerificationAndSize(ctx, preuploadID, expectedMD5, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆä¸Šä¼ å¤±è´¥: %w", err)
	}

	// ä¿®å¤ï¼šæ¸…ç†ä¸Šä¼ è¿›åº¦æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºè·¯å¾„ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
	f.removeUploadProgress(preuploadID, fileName, fileSize)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// åˆ›å»ºå¹¶è¿”å›Objectï¼Œä½¿ç”¨ä»APIè·å–çš„å®é™…æ•°æ®
	obj := &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,                                 // æˆ‘ä»¬æœ‰å®Œæ•´çš„å…ƒæ•°æ®
		id:          strconv.FormatInt(result.FileID, 10), // ä½¿ç”¨å®é™…çš„æ–‡ä»¶ID
		size:        fileSize,
		md5sum:      result.Etag, // ä½¿ç”¨ä»APIè·å–çš„MD5
		modTime:     time.Now(),
		isDir:       false,
	}

	fs.Infof(f, "ğŸ‰ æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼æ–‡ä»¶å: %s | å¤§å°: %s | æ–‡ä»¶ID: %d | MD5: %s", fileName, fs.SizeSuffix(fileSize), result.FileID, result.Etag)
	return obj, nil
}

// completeUploadWithMD5VerificationAndSize å®Œæˆä¸Šä¼ å¹¶è¿›è¡ŒMD5éªŒè¯ï¼Œæ”¯æŒåŠ¨æ€è½®è¯¢
func (f *Fs) completeUploadWithMD5VerificationAndSize(ctx context.Context, preuploadID, expectedMD5 string, fileSize int64) (*UploadCompleteResult, error) {
	// é¦–å…ˆè°ƒç”¨æ”¯æŒåŠ¨æ€è½®è¯¢çš„å®Œæˆä¸Šä¼ é€»è¾‘å¹¶è·å–ç»“æœ
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, err
	}

	// å¦‚æœæä¾›äº†é¢„æœŸMD5ï¼Œè¿›è¡Œé¢å¤–çš„éªŒè¯
	if expectedMD5 != "" && result.Etag != "" {
		fs.Debugf(f, "å¼€å§‹MD5éªŒè¯ï¼Œé¢„æœŸå€¼: %s, å®é™…å€¼: %s", expectedMD5, result.Etag)

		if result.Etag != expectedMD5 {
			fs.Debugf(f, "MD5ä¸åŒ¹é…ï¼Œä½†æ–‡ä»¶å·²ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: %d", result.FileID)
			// ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»ä¸Šä¼ æˆåŠŸï¼Œåªæ˜¯MD5éªŒè¯æœ‰é—®é¢˜
		} else {
			fs.Debugf(f, "MD5éªŒè¯æˆåŠŸ")
		}
	} else if expectedMD5 != "" {
		fs.Debugf(f, "æœåŠ¡å™¨æœªè¿”å›MD5å€¼ï¼Œè·³è¿‡éªŒè¯")
	}

	return result, nil
}

// calculateMD5WithStreamCache ä»è¾“å…¥æµè®¡ç®—MD5å¹¶ç¼“å­˜æµå†…å®¹ï¼Œè¿”å›æ–°çš„å¯è¯»æµ
func (f *Fs) calculateMD5WithStreamCache(ctx context.Context, in io.Reader, expectedSize int64) (string, io.Reader, error) {
	fs.Debugf(f, "å¼€å§‹ä»è¾“å…¥æµè®¡ç®—MD5å¹¶ç¼“å­˜æµå†…å®¹ï¼Œé¢„æœŸå¤§å°: %s", fs.SizeSuffix(expectedSize))

	hasher := md5.New()
	startTime := time.Now()

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜
	if expectedSize > maxMemoryBufferSize {
		fs.Infof(f, "ğŸ“‹ å‡†å¤‡ä¸Šä¼ å¤§æ–‡ä»¶ (%s) - æ­£åœ¨è®¡ç®—MD5å“ˆå¸Œä»¥å¯ç”¨ç§’ä¼ åŠŸèƒ½...", fs.SizeSuffix(expectedSize))
		fs.Infof(f, "ğŸ’¡ æç¤ºï¼šMD5è®¡ç®—å®Œæˆåå°†æ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ ï¼Œå¤§æ–‡ä»¶è®¡ç®—éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
		fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜æµå†…å®¹ï¼Œé‡‡ç”¨åˆ†å—è¯»å–ç­–ç•¥")
		tempFile, err := os.CreateTemp("", "stream_cache_*.tmp")
		if err != nil {
			return "", nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}

		// ä½¿ç”¨åˆ†å—è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ªå¤§æ–‡ä»¶å¯¼è‡´è¶…æ—¶
		written, err := f.copyWithChunksAndTimeout(ctx, tempFile, hasher, in, expectedSize)
		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			return "", nil, fmt.Errorf("ç¼“å­˜æµå†…å®¹æ—¶å¤åˆ¶æ•°æ®å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
		_, err = tempFile.Seek(0, io.SeekStart)
		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			return "", nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
		}

		md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
		elapsed := time.Since(startTime)
		speed := float64(written) / elapsed.Seconds() / 1024 / 1024 // MB/s
		fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | å¤„ç†: %s | è€—æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s",
			md5Hash, fs.SizeSuffix(written), elapsed.Round(time.Second), speed)
		fs.Debugf(f, "å¤§æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %sï¼Œç¼“å­˜äº† %s æ•°æ®", md5Hash, fs.SizeSuffix(written))

		// è¿”å›ä¸€ä¸ªè‡ªåŠ¨æ¸…ç†çš„è¯»å–å™¨
		return md5Hash, &tempFileReader{file: tempFile}, nil
	} else {
		// ä¿®å¤æ•°æ®æµè¯»å–é—®é¢˜ï¼šå°æ–‡ä»¶åœ¨å†…å­˜ä¸­ç¼“å­˜
		fs.Infof(f, "ğŸ“‹ æ­£åœ¨è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ (%s)...", fs.SizeSuffix(expectedSize))
		fs.Debugf(f, "å°æ–‡ä»¶åœ¨å†…å­˜ä¸­ç¼“å­˜æµå†…å®¹")

		// å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´å®‰å…¨çš„æ•°æ®è¯»å–æ–¹å¼
		data := make([]byte, expectedSize)
		totalRead := int64(0)

		// å¾ªç¯è¯»å–ç›´åˆ°è¯»å®Œæ‰€æœ‰æ•°æ®
		for totalRead < expectedSize {
			n, err := in.Read(data[totalRead:])
			totalRead += int64(n)

			if err == io.EOF {
				break // æ­£å¸¸ç»“æŸ
			}
			if err != nil {
				return "", nil, fmt.Errorf("è¯»å–è¾“å…¥æµå¤±è´¥: %w", err)
			}
		}

		// è°ƒæ•´æ•°æ®å¤§å°ä»¥åŒ¹é…å®é™…è¯»å–çš„å†…å®¹
		if totalRead != expectedSize {
			fs.Debugf(f, "ğŸš¨ æ•°æ®æµå¤§å°è°ƒæ•´ï¼šæœŸæœ›%då­—èŠ‚ï¼Œå®é™…è¯»å–%då­—èŠ‚", expectedSize, totalRead)
			data = data[:totalRead]
		}

		// è®¡ç®—MD5
		hasher.Write(data)
		written := totalRead

		md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
		elapsed := time.Since(startTime)
		fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | å¤„ç†: %s | è€—æ—¶: %v",
			md5Hash, fs.SizeSuffix(written), elapsed.Round(time.Millisecond))
		fs.Debugf(f, "å°æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %sï¼Œç¼“å­˜äº† %s æ•°æ®", md5Hash, fs.SizeSuffix(written))

		// ä¿®å¤ï¼šä½¿ç”¨å®é™…è¯»å–çš„æ•°æ®åˆ›å»ºReader
		return md5Hash, bytes.NewReader(data), nil
	}
}

// copyWithChunksAndTimeout åˆ†å—å¤åˆ¶æ•°æ®ï¼Œæ”¯æŒè¶…æ—¶æ§åˆ¶å’Œè¿›åº¦ç›‘æ§
// åŒæ—¶å†™å…¥åˆ°æ–‡ä»¶å’ŒMD5è®¡ç®—å™¨ï¼Œé¿å…å¤§æ–‡ä»¶ä¸€æ¬¡æ€§è¯»å–å¯¼è‡´çš„è¶…æ—¶é—®é¢˜
// ä¿®å¤ç‰ˆæœ¬ï¼šè§£å†³è·¨äº‘ä¼ è¾“å¡æ­»é—®é¢˜ï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œè¶…æ—¶æœºåˆ¶
func (f *Fs) copyWithChunksAndTimeout(ctx context.Context, file *os.File, hasher hash.Hash, in io.Reader, expectedSize int64) (int64, error) {
	const (
		chunkSize       = 4 * 1024 * 1024  // 4MB åˆ†å—å¤§å°ï¼ˆå‡å°ä»¥æé«˜å“åº”æ€§ï¼‰
		timeoutPerChunk = 60 * time.Second // æ¯ä¸ªåˆ†å—çš„è¶…æ—¶æ—¶é—´ï¼ˆå¢åŠ åˆ°60ç§’ï¼‰
		maxRetries      = 3                // æœ€å¤§é‡è¯•æ¬¡æ•°
	)

	multiWriter := io.MultiWriter(file, hasher)
	buffer := make([]byte, chunkSize)
	var totalWritten int64

	fs.Debugf(f, "ğŸš€ å¼€å§‹åˆ†å—å¤åˆ¶ï¼Œåˆ†å—å¤§å°: %sï¼Œé¢„æœŸæ€»å¤§å°: %s", fs.SizeSuffix(chunkSize), fs.SizeSuffix(expectedSize))

	// ä¸ºå¤§æ–‡ä»¶æ·»åŠ è¿›åº¦æç¤º
	var lastProgressTime time.Time
	startTime := time.Now()
	const progressInterval = 3 * time.Second // æ¯3ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆæ›´é¢‘ç¹çš„åé¦ˆï¼‰

	// æ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æ•´ä¸ªæ“ä½œæ— é™æœŸå¡ä½
	var overallTimeout time.Duration
	if expectedSize > 0 {
		// åŸºäºæ–‡ä»¶å¤§å°è®¡ç®—åˆç†çš„æ€»è¶…æ—¶æ—¶é—´ï¼šæ¯MBæœ€å¤š30ç§’ï¼Œæœ€å°‘5åˆ†é’Ÿï¼Œæœ€å¤š2å°æ—¶
		timeoutPerMB := 30 * time.Second
		overallTimeout = time.Duration(expectedSize/(1024*1024)) * timeoutPerMB
		overallTimeout = max(overallTimeout, 5*time.Minute)
		overallTimeout = min(overallTimeout, 2*time.Hour)
	} else {
		overallTimeout = 30 * time.Minute // æœªçŸ¥å¤§å°æ–‡ä»¶é»˜è®¤30åˆ†é’Ÿè¶…æ—¶
	}

	overallCtx, overallCancel := context.WithTimeout(ctx, overallTimeout)
	defer overallCancel()

	fs.Debugf(f, "â° è®¾ç½®æ€»ä½“è¶…æ—¶æ—¶é—´: %v", overallTimeout)

	chunkNumber := 0
	maxChunks := 100000 // é˜²æ­¢æ— é™å¾ªç¯ï¼šæœ€å¤š100000ä¸ªåˆ†ç‰‡
	for {
		chunkNumber++
		if chunkNumber > maxChunks {
			return totalWritten, fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°é‡è¶…è¿‡æœ€å¤§é™åˆ¶ %dï¼Œå¯èƒ½å­˜åœ¨å¾ªç¯", maxChunks)
		}

		// æ£€æŸ¥æ€»ä½“è¶…æ—¶
		select {
		case <-overallCtx.Done():
			return totalWritten, fmt.Errorf("æ•´ä½“æ“ä½œè¶…æ—¶ (%v): %w", overallTimeout, overallCtx.Err())
		default:
		}

		var n int
		var readErr error

		// ä½¿ç”¨é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œä¸ç¨³å®šé—®é¢˜
	retryLoop:
		for retry := range maxRetries {
			// ä¸ºæ¯ä¸ªåˆ†å—è®¾ç½®è¶…æ—¶
			chunkCtx, cancel := context.WithTimeout(overallCtx, timeoutPerChunk)

			// åˆ›å»ºä¸€ä¸ªå¸¦è¶…æ—¶çš„è¯»å–å™¨
			readDone := make(chan struct {
				n   int
				err error
			}, 1)

			go func() {
				n, err := in.Read(buffer)
				readDone <- struct {
					n   int
					err error
				}{n, err}
			}()

			select {
			case result := <-readDone:
				n, readErr = result.n, result.err
				cancel()
				break retryLoop // æˆåŠŸè¯»å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
			case <-chunkCtx.Done():
				cancel()
				if retry < maxRetries-1 {
					fs.Debugf(f, "âš ï¸ åˆ†å— %d è¯»å–è¶…æ—¶ï¼Œé‡è¯• %d/%d", chunkNumber, retry+1, maxRetries)
					time.Sleep(time.Duration(retry+1) * time.Second) // é€’å¢å»¶è¿Ÿ
					continue
				} else {
					return totalWritten, fmt.Errorf("åˆ†å— %d è¯»å–è¶…æ—¶ï¼Œå·²é‡è¯• %d æ¬¡: %w", chunkNumber, maxRetries, chunkCtx.Err())
				}
			}
		}

		if readErr != nil {
			if readErr == io.EOF {
				break
			}
			return totalWritten, fmt.Errorf("è¯»å–åˆ†å— %d æ•°æ®å¤±è´¥: %w", chunkNumber, readErr)
		}

		if n == 0 {
			break
		}

		// å†™å…¥æ•°æ®
		written, writeErr := multiWriter.Write(buffer[:n])
		if writeErr != nil {
			return totalWritten, fmt.Errorf("å†™å…¥åˆ†å— %d æ•°æ®å¤±è´¥: %w", chunkNumber, writeErr)
		}

		if written != n {
			return totalWritten, fmt.Errorf("å†™å…¥åˆ†å— %d æ•°æ®ä¸å®Œæ•´: æœŸæœ› %dï¼Œå®é™… %d", chunkNumber, n, written)
		}

		totalWritten += int64(written)

		// ä¼˜åŒ–çš„è¿›åº¦æ˜¾ç¤ºï¼šåŸºäºæ—¶é—´é—´éš”ï¼ŒåŒ…å«é€Ÿåº¦å’ŒETAä¿¡æ¯
		now := time.Now()
		if now.Sub(lastProgressTime) >= progressInterval || (expectedSize > 0 && totalWritten >= expectedSize) {
			elapsed := now.Sub(startTime)
			if expectedSize > 0 {
				progress := float64(totalWritten) / float64(expectedSize) * 100
				speed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s

				// è®¡ç®—ETA
				var etaStr string
				if speed > 0 && progress < 100 {
					remainingBytes := expectedSize - totalWritten
					etaSeconds := float64(remainingBytes) / (speed * 1024 * 1024)
					eta := time.Duration(etaSeconds) * time.Second
					etaStr = fmt.Sprintf(" | ETA: %v", eta.Round(time.Second))
				} else {
					etaStr = ""
				}

				fs.Infof(f, "ğŸ”„ MD5è®¡ç®—è¿›åº¦: %s / %s (%.1f%%) | é€Ÿåº¦: %.2f MB/s%s | åˆ†å—: %d",
					fs.SizeSuffix(totalWritten), fs.SizeSuffix(expectedSize), progress, speed, etaStr, chunkNumber)
			} else {
				// æœªçŸ¥å¤§å°çš„æ–‡ä»¶ï¼Œåªæ˜¾ç¤ºå·²ä¼ è¾“é‡å’Œé€Ÿåº¦
				speed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s
				fs.Infof(f, "ğŸ”„ MD5è®¡ç®—è¿›åº¦: %s | é€Ÿåº¦: %.2f MB/s | è€—æ—¶: %v | åˆ†å—: %d",
					fs.SizeSuffix(totalWritten), speed, elapsed.Round(time.Second), chunkNumber)
			}
			lastProgressTime = now
		}

		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¢«å–æ¶ˆ
		select {
		case <-ctx.Done():
			return totalWritten, fmt.Errorf("æ“ä½œè¢«å–æ¶ˆ: %w", ctx.Err())
		default:
		}
	}

	// æœ€ç»ˆè¿›åº¦æŠ¥å‘Š
	elapsed := time.Since(startTime)
	avgSpeed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s
	fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼æ€»å¤§å°: %s | å¹³å‡é€Ÿåº¦: %.2f MB/s | æ€»è€—æ—¶: %v | æ€»åˆ†å—æ•°: %d",
		fs.SizeSuffix(totalWritten), avgSpeed, elapsed.Round(time.Second), chunkNumber-1)

	return totalWritten, nil
}

// readToMemoryWithRetry å¢å¼ºçš„å†…å­˜è¯»å–å‡½æ•°ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“é‡è¯•
func (f *Fs) readToMemoryWithRetry(_ context.Context, in io.Reader, expectedSize int64, isRemoteSource bool) ([]byte, error) {
	maxRetries := 1
	if isRemoteSource {
		maxRetries = 3 // è·¨äº‘ä¼ è¾“å¢åŠ é‡è¯•æ¬¡æ•°
	}

	var lastErr error
	for retry := 0; retry < maxRetries; retry++ {
		if retry > 0 {
			fs.Debugf(f, "ğŸ”„ å†…å­˜è¯»å–é‡è¯• %d/%d", retry, maxRetries)
			time.Sleep(time.Duration(retry) * 2 * time.Second) // é€’å¢å»¶è¿Ÿ
		}

		// ä½¿ç”¨é™åˆ¶è¯»å–å™¨é˜²æ­¢è¯»å–è¿‡å¤šæ•°æ®
		var limitedReader io.Reader
		if expectedSize > 0 {
			limitedReader = io.LimitReader(in, expectedSize+1024) // é¢å¤–1KBç”¨äºæ£€æµ‹å¤§å°å¼‚å¸¸
		} else {
			limitedReader = io.LimitReader(in, maxMemoryBufferSize) // ä½¿ç”¨æœ€å¤§å†…å­˜é™åˆ¶
		}

		data, err := io.ReadAll(limitedReader)
		if err != nil {
			lastErr = fmt.Errorf("è¯»å–æ•°æ®å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, lastErr
		}

		actualSize := int64(len(data))

		// éªŒè¯æ•°æ®å¤§å°
		if expectedSize > 0 {
			if actualSize > expectedSize+1024 {
				lastErr = fmt.Errorf("è¯»å–æ•°æ®è¿‡å¤š: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
				if retry < maxRetries {
					continue
				}
				return nil, lastErr
			}

			if actualSize < expectedSize {
				fs.Debugf(f, "âš ï¸  æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
				if isRemoteSource && retry < maxRetries {
					lastErr = fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
					continue
				}
			}
		}

		fs.Debugf(f, " å†…å­˜è¯»å–æˆåŠŸ: %då­—èŠ‚", actualSize)
		return data, nil
	}

	return nil, lastErr
}

// createTempFileWithRetry å¢å¼ºçš„ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå‡½æ•°ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“é‡è¯•
func (f *Fs) createTempFileWithRetry(ctx context.Context, in io.Reader, expectedSize int64, isRemoteSource bool, fileName ...string) (*os.File, int64, error) {
	maxRetries := 1
	if isRemoteSource {
		maxRetries = 3 // è·¨äº‘ä¼ è¾“å¢åŠ é‡è¯•æ¬¡æ•°
	}

	var lastErr error
	for retry := 0; retry < maxRetries; retry++ {
		if retry > 0 {
			fs.Debugf(f, "ğŸ”„ ä¸´æ—¶æ–‡ä»¶åˆ›å»ºé‡è¯• %d/%d", retry, maxRetries)
			time.Sleep(time.Duration(retry) * 2 * time.Second) // é€’å¢å»¶è¿Ÿ
		}

		tempFile, err := os.CreateTemp("", "v2upload_*.tmp")
		if err != nil {
			lastErr = fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// ä½¿ç”¨ç¼“å†²å†™å…¥æå‡æ€§èƒ½
		bufWriter := bufio.NewWriterSize(tempFile, 1024*1024) // 1MBç¼“å†²åŒº
		written, err := f.copyWithProgressAndValidation(ctx, bufWriter, in, expectedSize, isRemoteSource, fileName...)

		// åˆ·æ–°ç¼“å†²åŒº
		flushErr := bufWriter.Flush()
		if flushErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("åˆ·æ–°ç¼“å†²åŒºå¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, flushErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("å¤åˆ¶æ•°æ®å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// è·¨äº‘ä¼ è¾“é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å®é™…å¤§å°
		if isRemoteSource && expectedSize > 0 {
			fileInfo, statErr := tempFile.Stat()
			if statErr != nil {
				tempFile.Close()
				os.Remove(tempFile.Name())
				lastErr = fmt.Errorf("è·å–ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, statErr)
				if retry < maxRetries {
					continue
				}
				return nil, 0, lastErr
			}

			actualFileSize := fileInfo.Size()
			if actualFileSize != written {
				tempFile.Close()
				os.Remove(tempFile.Name())
				lastErr = fmt.Errorf("æ–‡ä»¶å¤§å°éªŒè¯å¤±è´¥(é‡è¯•%d/%d): å†™å…¥%då­—èŠ‚ï¼Œæ–‡ä»¶å®é™…%då­—èŠ‚", retry, maxRetries, written, actualFileSize)
				if retry < maxRetries {
					continue
				}
				return nil, 0, lastErr
			}

			if actualFileSize != expectedSize {
				fs.Debugf(f, "âš ï¸  è·¨äº‘ä¼ è¾“å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualFileSize)
				if actualFileSize < expectedSize && retry < maxRetries {
					tempFile.Close()
					os.Remove(tempFile.Name())
					lastErr = fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´(é‡è¯•%d/%d): æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", retry, maxRetries, expectedSize, actualFileSize)
					continue
				}
			}
		}

		// å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
		if syncErr := tempFile.Sync(); syncErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("åŒæ­¥ç£ç›˜å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, syncErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
		if _, seekErr := tempFile.Seek(0, 0); seekErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, seekErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		fs.Debugf(f, " ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ: %då­—èŠ‚", written)
		return tempFile, written, nil
	}

	return nil, 0, lastErr
}

// copyWithProgressAndValidation å¸¦è¿›åº¦å’ŒéªŒè¯çš„æ•°æ®å¤åˆ¶
func (f *Fs) copyWithProgressAndValidation(ctx context.Context, dst io.Writer, src io.Reader, expectedSize int64, isRemoteSource bool, fileName ...string) (int64, error) {
	// ä¼˜åŒ–ç¼“å†²åŒºå¤§å°ä»¥æå‡ä¼ è¾“é€Ÿåº¦
	bufferSize := 1024 * 1024 // 1MB - å¤§å¹…æå‡é€Ÿåº¦
	if isRemoteSource {
		// è·¨äº‘ä¼ è¾“ä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒºä»¥æå‡é€Ÿåº¦
		if expectedSize > 1024*1024*1024 { // å¤§äº1GBçš„æ–‡ä»¶
			bufferSize = 2 * 1024 * 1024 // 2MBç¼“å†²åŒº
		} else {
			bufferSize = 1024 * 1024 // 1MBç¼“å†²åŒº
		}
	}

	buffer := make([]byte, bufferSize)
	var totalWritten int64
	startTime := time.Now()
	lastProgressTime := startTime

	for {
		select {
		case <-ctx.Done():
			return totalWritten, ctx.Err()
		default:
		}

		// è®¾ç½®è¯»å–è¶…æ—¶ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
		readTimeout := 30 * time.Second
		if isRemoteSource {
			readTimeout = 60 * time.Second // è·¨äº‘ä¼ è¾“å¢åŠ è¶…æ—¶æ—¶é—´
		}
		_ = readTimeout // æš‚æ—¶æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥è¶…æ—¶æ§åˆ¶

		// è¯»å–æ•°æ®
		n, readErr := src.Read(buffer)
		if n > 0 {
			// å†™å…¥æ•°æ®
			written, writeErr := dst.Write(buffer[:n])
			if writeErr != nil {
				return totalWritten, fmt.Errorf("å†™å…¥å¤±è´¥: %w", writeErr)
			}
			totalWritten += int64(written)

			// å®šæœŸè¾“å‡ºè¿›åº¦ - æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
			progressInterval := 2 * time.Second                  // æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
			if isRemoteSource && expectedSize > 1024*1024*1024 { // å¤§æ–‡ä»¶è·¨äº‘ä¼ è¾“
				progressInterval = 1 * time.Second // æ¯1ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
			}

			if time.Since(lastProgressTime) > progressInterval {
				progress := float64(totalWritten) / float64(expectedSize) * 100
				elapsed := time.Since(startTime).Seconds()
				currentSpeed := float64(totalWritten) / elapsed / (1024 * 1024) // MB/s

				// è·å–æ–‡ä»¶åç”¨äºæ˜¾ç¤º
				displayFileName := "æ–‡ä»¶"
				if len(fileName) > 0 && fileName[0] != "" {
					displayFileName = fileName[0]
					// å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªå–æ˜¾ç¤º
					if len(displayFileName) > 30 {
						displayFileName = displayFileName[:27] + "..."
					}
				}

				if expectedSize > 0 {
					remainingBytes := expectedSize - totalWritten
					eta := time.Duration(float64(remainingBytes) / (currentSpeed * 1024 * 1024) * float64(time.Second))
					// ä½¿ç”¨INFOæ—¥å¿—è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ
					fs.Infof(f, "ğŸ“Š [%s] 123ç½‘ç›˜æ•°æ®å¤åˆ¶è¿›åº¦: %s/%s (%.1f%%) - é€Ÿåº¦: %.2f MB/s - é¢„è®¡å‰©ä½™: %v",
						displayFileName, fs.SizeSuffix(totalWritten), fs.SizeSuffix(expectedSize), progress, currentSpeed, eta.Round(time.Second))
				} else {
					fs.Infof(f, "ğŸ“Š [%s] 123ç½‘ç›˜æ•°æ®å¤åˆ¶è¿›åº¦: %s - é€Ÿåº¦: %.2f MB/s",
						displayFileName, fs.SizeSuffix(totalWritten), currentSpeed)
				}
				lastProgressTime = time.Now()
			}
		}

		if readErr != nil {
			if readErr == io.EOF {
				break // æ­£å¸¸ç»“æŸ
			}
			return totalWritten, fmt.Errorf("è¯»å–å¤±è´¥: %w", readErr)
		}
	}

	// éªŒè¯æœ€ç»ˆå¤§å°å’Œæ•°æ®å®Œæ•´æ€§
	if expectedSize > 0 && totalWritten != expectedSize {
		if isRemoteSource && totalWritten < expectedSize {
			// è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´ï¼Œå°è¯•æ¢å¤
			fs.Debugf(f, "ğŸ”„ è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´ï¼Œå°è¯•æ•°æ®æ¢å¤...")
			return totalWritten, fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚ï¼Œå»ºè®®é‡è¯•", expectedSize, totalWritten)
		}
		fs.Debugf(f, "âš ï¸  æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, totalWritten)
	}

	// å¯¹äºè·¨äº‘ä¼ è¾“ï¼Œæ·»åŠ é¢å¤–çš„å®Œæ•´æ€§æ£€æŸ¥
	if isRemoteSource && expectedSize > 0 {
		fs.Debugf(f, " è·¨äº‘ä¼ è¾“æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡: %då­—èŠ‚", totalWritten)
	}

	return totalWritten, nil
}

// tempFileReader åŒ…è£…ä¸´æ—¶æ–‡ä»¶ï¼Œå®ç°è‡ªåŠ¨æ¸…ç†
type tempFileReader struct {
	file *os.File
}

func (r *tempFileReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

func (r *tempFileReader) Close() error {
	if r.file != nil {
		fileName := r.file.Name()
		r.file.Close()
		os.Remove(fileName)
		r.file = nil
	}
	return nil
}

// calculateMD5FromReader ä»è¾“å…¥æµè®¡ç®—MD5å“ˆå¸Œï¼Œæ”¯æŒè·¨äº‘ç›˜ä¼ è¾“
// ä½¿ç”¨åˆ†å—è¯»å–å’Œè¶…æ—¶æ§åˆ¶ï¼Œé¿å…å¤§æ–‡ä»¶å¤„ç†æ—¶çš„ç½‘ç»œè¶…æ—¶é—®é¢˜
func (f *Fs) calculateMD5FromReader(ctx context.Context, in io.Reader, expectedSize int64) (string, error) {
	fs.Debugf(f, "å¼€å§‹ä»è¾“å…¥æµè®¡ç®—MD5ï¼Œé¢„æœŸå¤§å°: %s", fs.SizeSuffix(expectedSize))

	hasher := md5.New()
	startTime := time.Now()

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†å—è¯»å–é¿å…è¶…æ—¶
	if expectedSize > maxMemoryBufferSize {
		fs.Infof(f, "ğŸ“‹ æ­£åœ¨è®¡ç®—å¤§æ–‡ä»¶MD5å“ˆå¸Œ (%s) - å¯ç”¨ç§’ä¼ æ£€æµ‹...", fs.SizeSuffix(expectedSize))
		fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨åˆ†å—è¯»å–è®¡ç®—MD5")
		tempFile, err := os.CreateTemp("", "md5calc_*.tmp")
		if err != nil {
			return "", fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer func() {
			tempFile.Close()
			os.Remove(tempFile.Name())
		}()

		// ä½¿ç”¨åˆ†å—è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ªå¤§æ–‡ä»¶å¯¼è‡´è¶…æ—¶
		written, err := f.copyWithChunksAndTimeout(ctx, tempFile, hasher, in, expectedSize)
		if err != nil {
			return "", fmt.Errorf("è®¡ç®—MD5æ—¶å¤åˆ¶æ•°æ®å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		fs.Debugf(f, "MD5è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† %s æ•°æ®", fs.SizeSuffix(written))
	} else {
		// å°æ–‡ä»¶ç›´æ¥åœ¨å†…å­˜ä¸­è®¡ç®—
		fs.Debugf(f, "å°æ–‡ä»¶åœ¨å†…å­˜ä¸­è®¡ç®—MD5")
		written, err := io.Copy(hasher, in)
		if err != nil {
			return "", fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		fs.Debugf(f, "MD5è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† %s æ•°æ®", fs.SizeSuffix(written))
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	elapsed := time.Since(startTime)
	fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | è€—æ—¶: %v", md5Hash, elapsed.Round(time.Millisecond))
	fs.Debugf(f, "MD5è®¡ç®—ç»“æœ: %s", md5Hash)
	return md5Hash, nil
}

// singleStepUploadMultipart ä½¿ç”¨multipart formä¸Šä¼ æ–‡ä»¶çš„ä¸“ç”¨æ–¹æ³•
func (f *Fs) singleStepUploadMultipart(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string, result any) error {
	// åˆ›å»ºmultipart formæ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	if err := writer.WriteField("parentFileID", strconv.FormatInt(parentFileID, 10)); err != nil {
		return fmt.Errorf("å†™å…¥parentFileIDå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("filename", fileName); err != nil {
		return fmt.Errorf("å†™å…¥filenameå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("etag", md5Hash); err != nil {
		return fmt.Errorf("å†™å…¥etagå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("size", strconv.Itoa(len(data))); err != nil {
		return fmt.Errorf("å†™å…¥sizeå¤±è´¥: %w", err)
	}

	// æ·»åŠ duplicateå­—æ®µï¼ˆä¿ç•™ä¸¤è€…ï¼Œé¿å…è¦†ç›–ï¼‰
	if err := writer.WriteField("duplicate", "1"); err != nil {
		return fmt.Errorf("å†™å…¥duplicateå¤±è´¥: %w", err)
	}

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	fileWriter, err := writer.CreateFormFile("file", fileName)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºæ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
	}

	if _, err := fileWriter.Write(data); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	if err := writer.Close(); err != nil {
		return fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è¿›è¡Œå•æ­¥ä¸Šä¼ multipartè°ƒç”¨
	return f.makeAPICallWithRestMultipart(ctx, "/upload/v2/file/single/create", "POST", &buf, writer.FormDataContentType(), result)
}

// attemptResumeDownload å°è¯•æ–­ç‚¹ç»­ä¼ æ¢å¤ä¸‹è½½
// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šæ™ºèƒ½æ¢å¤éƒ¨åˆ†ä¸‹è½½çš„æ•°æ®
func (f *Fs) attemptResumeDownload(ctx context.Context, srcObj fs.Object, tempFile *os.File, chunkSize, numChunks, maxConcurrency int64) error {
	fs.Debugf(f, "ğŸ”„ å¼€å§‹æ–­ç‚¹ç»­ä¼ æ¢å¤ï¼Œåˆ†æå·²ä¸‹è½½çš„åˆ†ç‰‡...")

	// åˆ†æå·²ä¸‹è½½çš„æ•°æ®ï¼Œç¡®å®šå“ªäº›åˆ†ç‰‡éœ€è¦é‡æ–°ä¸‹è½½
	fileSize := srcObj.Size()
	missingChunks := make([]int64, 0)

	// æ£€æŸ¥æ¯ä¸ªåˆ†ç‰‡çš„å®Œæ•´æ€§
	for chunkIndex := range numChunks {
		start := chunkIndex * chunkSize
		end := start + chunkSize - 1
		if end >= fileSize {
			end = fileSize - 1
		}

		// æ£€æŸ¥è¿™ä¸ªåˆ†ç‰‡æ˜¯å¦å®Œæ•´ä¸‹è½½
		if !f.isChunkComplete(tempFile, start, end) {
			missingChunks = append(missingChunks, chunkIndex)
		}
	}

	if len(missingChunks) == 0 {
		fs.Infof(f, "âœ… æ‰€æœ‰åˆ†ç‰‡éƒ½å·²å®Œæ•´ä¸‹è½½")
		return nil
	}

	fs.Infof(f, "ğŸ”„ éœ€è¦é‡æ–°ä¸‹è½½ %d ä¸ªåˆ†ç‰‡", len(missingChunks))

	// åªä¸‹è½½ç¼ºå¤±çš„åˆ†ç‰‡
	return f.downloadMissingChunks(ctx, srcObj, tempFile, chunkSize, missingChunks, maxConcurrency)
}

// isChunkComplete æ£€æŸ¥æŒ‡å®šåˆ†ç‰‡æ˜¯å¦å®Œæ•´ä¸‹è½½
// ä¿®å¤ï¼šå¢å¼ºåˆ†ç‰‡å®Œæ•´æ€§éªŒè¯ï¼Œæä¾›æ›´å¯é çš„æ£€æŸ¥
func (f *Fs) isChunkComplete(tempFile *os.File, start, end int64) bool {
	expectedSize := end - start + 1

	// æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶³å¤Ÿ
	stat, err := tempFile.Stat()
	if err != nil {
		fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šæ— æ³•è·å–æ–‡ä»¶çŠ¶æ€: %v", err)
		return false
	}

	if stat.Size() <= start {
		fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šæ–‡ä»¶å¤§å°(%d)ä¸è¶³ï¼Œèµ·å§‹ä½ç½®: %d", stat.Size(), start)
		return false
	}

	// æ”¹è¿›1ï¼šç²¾ç¡®æ£€æŸ¥åˆ†ç‰‡å¤§å°
	actualAvailableSize := stat.Size() - start
	if actualAvailableSize < expectedSize {
		fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šå¯ç”¨å¤§å°(%d) < æœŸæœ›å¤§å°(%d)", actualAvailableSize, expectedSize)
		return false
	}

	// æ”¹è¿›2ï¼šæ£€æŸ¥æ›´å¤šæ•°æ®ç‚¹ï¼Œè€Œä¸ä»…ä»…æ˜¯å‰1KB
	checkSize := min(expectedSize, 4096) // æ£€æŸ¥å‰4KBæˆ–æ•´ä¸ªåˆ†ç‰‡ï¼ˆå¦‚æœæ›´å°ï¼‰
	buffer := make([]byte, checkSize)
	n, err := tempFile.ReadAt(buffer, start)
	if err != nil {
		fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šè¯»å–æ•°æ®å¤±è´¥: %v", err)
		return false
	}

	if int64(n) < checkSize {
		fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šè¯»å–å¤§å°(%d) < æ£€æŸ¥å¤§å°(%d)", n, checkSize)
		return false
	}

	// æ”¹è¿›3ï¼šå¤šç‚¹é‡‡æ ·æ£€æŸ¥ï¼Œé¿å…è¯¯åˆ¤
	// æ£€æŸ¥å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾ä¸‰ä¸ªä½ç½®
	samplePoints := []int64{0, checkSize / 2, checkSize - 256}
	for _, point := range samplePoints {
		if point >= checkSize {
			continue
		}

		// æ£€æŸ¥256å­—èŠ‚çš„æ ·æœ¬
		sampleSize := min(256, checkSize-point)
		allZero := true
		for i := point; i < point+sampleSize; i++ {
			if buffer[i] != 0 {
				allZero = false
				break
			}
		}

		if allZero {
			fs.Debugf(f, " åˆ†ç‰‡éªŒè¯å¤±è´¥ï¼šæ£€æµ‹åˆ°é›¶å­—èŠ‚åŒºåŸŸåœ¨ä½ç½® %d", point)
			return false
		}
	}

	fs.Debugf(f, " åˆ†ç‰‡éªŒè¯é€šè¿‡ï¼šèŒƒå›´[%d-%d], å¤§å°=%d", start, end, expectedSize)
	return true
}

// downloadMissingChunks ä¸‹è½½ç¼ºå¤±çš„åˆ†ç‰‡
func (f *Fs) downloadMissingChunks(ctx context.Context, srcObj fs.Object, tempFile *os.File, chunkSize int64, missingChunks []int64, maxConcurrency int64) error {
	if len(missingChunks) == 0 {
		return nil
	}

	fs.Infof(f, "ğŸš€ å¼€å§‹ä¸‹è½½ %d ä¸ªç¼ºå¤±åˆ†ç‰‡", len(missingChunks))

	// åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
	semaphore := make(chan struct{}, maxConcurrency)
	var wg sync.WaitGroup
	errChan := make(chan error, len(missingChunks))

	for _, chunkIndex := range missingChunks {
		wg.Add(1)
		go func(idx int64) {
			defer wg.Done()

			// æ·»åŠ panicæ¢å¤æœºåˆ¶
			defer func() {
				if r := recover(); r != nil {
					fs.Errorf(f, "ä¸‹è½½ç¼ºå¤±åˆ†ç‰‡ %d å‘ç”Ÿpanic: %v", idx, r)
					errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d panic: %v", idx, r)
				}
			}()

			// ä¿®å¤ä¿¡å·é‡æ³„æ¼ï¼šä½¿ç”¨å¸¦è¶…æ—¶çš„ä¿¡å·é‡è·å–
			chunkTimeout := 10 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š10åˆ†é’Ÿè¶…æ—¶
			chunkCtx, cancelChunk := context.WithTimeout(ctx, chunkTimeout)
			defer cancelChunk()

			// è·å–ä¿¡å·é‡ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
			select {
			case semaphore <- struct{}{}:
				// æˆåŠŸè·å–ä¿¡å·é‡ï¼Œç«‹å³è®¾ç½®deferé‡Šæ”¾
				defer func() {
					select {
					case <-semaphore:
					default:
						// å¦‚æœä¿¡å·é‡å·²ç»è¢«é‡Šæ”¾ï¼Œé¿å…é˜»å¡
					}
				}()
			case <-chunkCtx.Done():
				errChan <- fmt.Errorf("é‡æ–°ä¸‹è½½åˆ†ç‰‡ %d è·å–ä¿¡å·é‡è¶…æ—¶: %w", idx, chunkCtx.Err())
				return
			}

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := idx * chunkSize
			end := start + chunkSize - 1
			if end >= srcObj.Size() {
				end = srcObj.Size() - 1
			}

			// ä¸‹è½½åˆ†ç‰‡ï¼Œä½¿ç”¨å¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
			err := f.downloadChunk(chunkCtx, srcObj, tempFile, start, end, idx)
			if err != nil {
				// æ£€æŸ¥æ˜¯å¦æ˜¯è¶…æ—¶é”™è¯¯
				if chunkCtx.Err() != nil {
					errChan <- fmt.Errorf("é‡æ–°ä¸‹è½½åˆ†ç‰‡ %d è¶…æ—¶: %w", idx, chunkCtx.Err())
				} else {
					errChan <- fmt.Errorf("é‡æ–°ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", idx, err)
				}
				return
			}

			fs.Debugf(f, " åˆ†ç‰‡ %d é‡æ–°ä¸‹è½½æˆåŠŸ", idx)
		}(chunkIndex)
	}

	// ä¿®å¤æ­»é”é£é™©ï¼šæ·»åŠ æ•´ä½“è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æ— é™ç­‰å¾…
	overallTimeout := time.Duration(len(missingChunks)) * 15 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š15åˆ†é’Ÿ
	overallTimeout = min(overallTimeout, 2*time.Hour)                      // æœ€å¤§2å°æ—¶è¶…æ—¶

	// ä½¿ç”¨å¸¦è¶…æ—¶çš„ç­‰å¾…æœºåˆ¶
	done := make(chan struct{})
	go func() {
		defer close(done)
		wg.Wait()
		close(errChan)
	}()

	// ç­‰å¾…å®Œæˆæˆ–è¶…æ—¶
	select {
	case <-done:
		// æ­£å¸¸å®Œæˆï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
		for err := range errChan {
			if err != nil {
				return err
			}
		}
		fs.Infof(f, "âœ… æ‰€æœ‰ç¼ºå¤±åˆ†ç‰‡ä¸‹è½½å®Œæˆ")
		return nil
	case <-time.After(overallTimeout):
		fs.Errorf(f, "ğŸš¨ ç¼ºå¤±åˆ†ç‰‡ä¸‹è½½æ•´ä½“è¶…æ—¶ (%v)ï¼Œå¯èƒ½å­˜åœ¨æ­»é”", overallTimeout)
		return fmt.Errorf("ç¼ºå¤±åˆ†ç‰‡ä¸‹è½½æ•´ä½“è¶…æ—¶: %v", overallTimeout)
	}
}

// CrossCloudTransferProgress è·¨äº‘ç›˜ä¼ è¾“ç»Ÿä¸€è¿›åº¦ç®¡ç†å™¨
// ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šåˆ›å»ºç»Ÿä¸€çš„è·¨äº‘ç›˜ä¼ è¾“è¿›åº¦ç®¡ç†å™¨ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒ
type CrossCloudTransferProgress struct {
	fileName  string
	totalSize int64
	startTime time.Time

	// ä¸‹è½½é˜¶æ®µ
	downloadStartTime time.Time
	downloadEndTime   time.Time
	downloadBytes     int64
	downloadChunks    int64
	totalChunks       int64

	// ä¸Šä¼ é˜¶æ®µ
	uploadStartTime time.Time
	uploadEndTime   time.Time
	uploadBytes     int64

	// å½“å‰é˜¶æ®µ
	currentPhase string // "download", "processing", "upload", "complete"

	// è¿›åº¦åŒæ­¥
	mu sync.RWMutex

	// rclone accountingé›†æˆ
	account *accounting.Account
}

// NewCrossCloudTransferProgress åˆ›å»ºæ–°çš„è·¨äº‘ç›˜ä¼ è¾“è¿›åº¦ç®¡ç†å™¨
func NewCrossCloudTransferProgress(fileName string, totalSize int64, account *accounting.Account) *CrossCloudTransferProgress {
	return &CrossCloudTransferProgress{
		fileName:     fileName,
		totalSize:    totalSize,
		startTime:    time.Now(),
		currentPhase: "download",
		account:      account,
	}
}

// StartDownloadPhase å¼€å§‹ä¸‹è½½é˜¶æ®µ
func (ctp *CrossCloudTransferProgress) StartDownloadPhase(totalChunks int64) {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.currentPhase = "download"
	ctp.downloadStartTime = time.Now()
	ctp.totalChunks = totalChunks
	ctp.downloadChunks = 0
	ctp.downloadBytes = 0

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		ctp.account.SetExtraInfo(fmt.Sprintf("[ä¸‹è½½é˜¶æ®µ] 0/%dåˆ†ç‰‡", totalChunks))
	}
}

// UpdateDownloadProgress æ›´æ–°ä¸‹è½½è¿›åº¦
func (ctp *CrossCloudTransferProgress) UpdateDownloadProgress(completedChunks, downloadedBytes int64) {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.downloadChunks = completedChunks
	ctp.downloadBytes = downloadedBytes

	// è®¡ç®—ä¸‹è½½è¿›åº¦ç™¾åˆ†æ¯”
	downloadPercent := float64(completedChunks) / float64(ctp.totalChunks) * 100

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		extraInfo := fmt.Sprintf("[ä¸‹è½½é˜¶æ®µ] %d/%dåˆ†ç‰‡ (%.1f%%)",
			completedChunks, ctp.totalChunks, downloadPercent)
		ctp.account.SetExtraInfo(extraInfo)
	}
}

// StartProcessingPhase å¼€å§‹å¤„ç†é˜¶æ®µï¼ˆMD5è®¡ç®—ç­‰ï¼‰
func (ctp *CrossCloudTransferProgress) StartProcessingPhase() {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.currentPhase = "processing"
	ctp.downloadEndTime = time.Now()

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		ctp.account.SetExtraInfo("[å¤„ç†é˜¶æ®µ] è®¡ç®—MD5...")
	}
}

// StartUploadPhase å¼€å§‹ä¸Šä¼ é˜¶æ®µ
func (ctp *CrossCloudTransferProgress) StartUploadPhase() {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.currentPhase = "upload"
	ctp.uploadStartTime = time.Now()
	ctp.uploadBytes = 0

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		ctp.account.SetExtraInfo("[ä¸Šä¼ é˜¶æ®µ] å¼€å§‹ä¸Šä¼ ...")
	}
}

// UpdateUploadProgress æ›´æ–°ä¸Šä¼ è¿›åº¦
func (ctp *CrossCloudTransferProgress) UpdateUploadProgress(uploadedBytes int64) {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.uploadBytes = uploadedBytes

	// è®¡ç®—ä¸Šä¼ è¿›åº¦ç™¾åˆ†æ¯”
	uploadPercent := float64(uploadedBytes) / float64(ctp.totalSize) * 100

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		extraInfo := fmt.Sprintf("[ä¸Šä¼ é˜¶æ®µ] %.1f%% (%s/%s)",
			uploadPercent, fs.SizeSuffix(uploadedBytes), fs.SizeSuffix(ctp.totalSize))
		ctp.account.SetExtraInfo(extraInfo)
	}
}

// Complete å®Œæˆä¼ è¾“
func (ctp *CrossCloudTransferProgress) Complete() {
	ctp.mu.Lock()
	defer ctp.mu.Unlock()

	ctp.currentPhase = "complete"
	ctp.uploadEndTime = time.Now()

	// è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯
	totalDuration := time.Since(ctp.startTime)
	downloadDuration := ctp.downloadEndTime.Sub(ctp.downloadStartTime)
	uploadDuration := ctp.uploadEndTime.Sub(ctp.uploadStartTime)

	// æ›´æ–°rcloneè¿›åº¦æ˜¾ç¤º
	if ctp.account != nil {
		ctp.account.SetExtraInfo(fmt.Sprintf("[å®Œæˆ] æ€»è€—æ—¶: %v", totalDuration.Round(time.Second)))
	}

	// è¾“å‡ºè¯¦ç»†çš„ä¼ è¾“ç»Ÿè®¡
	fs.Infof(nil, "ğŸ‰ è·¨äº‘ç›˜ä¼ è¾“å®Œæˆ: %s", ctp.fileName)
	fs.Infof(nil, "   ğŸ“Š ä¼ è¾“ç»Ÿè®¡: æ€»å¤§å°=%s, æ€»è€—æ—¶=%v",
		fs.SizeSuffix(ctp.totalSize), totalDuration.Round(time.Second))
	fs.Infof(nil, "   ğŸ“¥ ä¸‹è½½é˜¶æ®µ: è€—æ—¶=%v, é€Ÿåº¦=%.2f MB/s",
		downloadDuration.Round(time.Second),
		float64(ctp.downloadBytes)/downloadDuration.Seconds()/1024/1024)
	fs.Infof(nil, "   ğŸ“¤ ä¸Šä¼ é˜¶æ®µ: è€—æ—¶=%v, é€Ÿåº¦=%.2f MB/s",
		uploadDuration.Round(time.Second),
		float64(ctp.uploadBytes)/uploadDuration.Seconds()/1024/1024)
}

// GetCurrentStatus è·å–å½“å‰çŠ¶æ€ä¿¡æ¯
func (ctp *CrossCloudTransferProgress) GetCurrentStatus() (phase string, progress float64, eta time.Duration) {
	ctp.mu.RLock()
	defer ctp.mu.RUnlock()

	phase = ctp.currentPhase

	switch phase {
	case "download":
		if ctp.totalChunks > 0 {
			progress = float64(ctp.downloadChunks) / float64(ctp.totalChunks) * 50 // ä¸‹è½½å æ€»è¿›åº¦çš„50%
		}
	case "processing":
		progress = 50 // å¤„ç†é˜¶æ®µå›ºå®šä¸º50%
	case "upload":
		if ctp.totalSize > 0 {
			uploadProgress := float64(ctp.uploadBytes) / float64(ctp.totalSize) * 50 // ä¸Šä¼ å æ€»è¿›åº¦çš„50%
			progress = 50 + uploadProgress                                           // ä¸‹è½½50% + ä¸Šä¼ è¿›åº¦
		}
	case "complete":
		progress = 100
	}

	// ç®€å•çš„ETAä¼°ç®—
	if progress > 0 && progress < 100 {
		elapsed := time.Since(ctp.startTime)
		totalEstimated := time.Duration(float64(elapsed) / progress * 100)
		eta = totalEstimated - elapsed
	}

	return phase, progress, eta
}

// memoryBufferedCrossCloudTransferDirect å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“ï¼ˆç›´æ¥ç‰ˆæœ¬ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šç›´æ¥ä½¿ç”¨srcObjï¼Œé¿å…é‡å¤æ‰“å¼€æ–‡ä»¶
func (f *Fs) memoryBufferedCrossCloudTransferDirect(ctx context.Context, srcObj fs.Object, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸ“ å¼€å§‹å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“ï¼ˆç›´æ¥ç‰ˆæœ¬ï¼‰ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: æ‰“å¼€æºæ–‡ä»¶å¹¶ç›´æ¥è¯»å–åˆ°å†…å­˜
	startTime := time.Now()
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
	}
	defer srcReader.Close()

	data := make([]byte, fileSize)
	n, err := io.ReadFull(srcReader, data)
	downloadDuration := time.Since(startTime)

	if err != nil && err != io.ErrUnexpectedEOF {
		return nil, fmt.Errorf("å†…å­˜è¯»å–æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if int64(n) != fileSize {
		fs.Debugf(f, "å®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…ï¼Œä½¿ç”¨å®é™…å¤§å°", n, fileSize)
		data = data[:n]
		fileSize = int64(n)
	}

	fs.Infof(f, "ğŸ“¥ å†…å­˜ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// æ­¥éª¤2: è®¡ç®—MD5
	fs.Infof(f, "ğŸ” å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	md5StartTime := time.Now()
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration := time.Since(md5StartTime)

	fs.Infof(f, "ğŸ” MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v", md5Hash, md5Duration)

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        fileSize,
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ­¥éª¤4: ä¸Šä¼ æ–‡ä»¶
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ æ–‡ä»¶...")
	uploadStartTime := time.Now()
	reader := bytes.NewReader(data)
	err = f.uploadFile(ctx, reader, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	// ç¼“å­˜MD5ä»¥ä¾›åç»­ä½¿ç”¨
	f.cacheMD5(src, md5Hash)

	fs.Infof(f, "âœ… å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// optimizedDiskCrossCloudTransferDirect ä¼˜åŒ–çš„ç£ç›˜è·¨äº‘ä¼ è¾“ï¼ˆç›´æ¥ç‰ˆæœ¬ï¼Œé¿å…é‡å¤ä¸‹è½½ï¼‰
// ä¿®å¤é‡å¤ä¸‹è½½é—®é¢˜ï¼šç›´æ¥ä½¿ç”¨srcObjï¼Œé¿å…é‡å¤æ‰“å¼€æ–‡ä»¶
func (f *Fs) optimizedDiskCrossCloudTransferDirect(ctx context.Context, srcObj fs.Object, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	startTime := time.Now()
	fs.Debugf(f, "ğŸ’¾ å¼€å§‹ä¼˜åŒ–ç£ç›˜è·¨äº‘ä¼ è¾“ï¼ˆç›´æ¥ç‰ˆæœ¬ï¼‰ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: æ£€æŸ¥MD5ç¼“å­˜
	var md5Hash string
	var downloadDuration, md5Duration time.Duration

	if cachedMD5, found := f.getCachedMD5(src); found {
		fs.Infof(f, "ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„MD5: %s", cachedMD5)
		md5Hash = cachedMD5
		md5Duration = 0 // ç¼“å­˜å‘½ä¸­ï¼Œæ— éœ€è®¡ç®—æ—¶é—´

		// ä»éœ€è¦ä¸‹è½½æ–‡ä»¶ç”¨äºä¸Šä¼ ï¼Œä½†å¯ä»¥è·³è¿‡MD5è®¡ç®—
		tempFile, err := os.CreateTemp("", "cross_cloud_cached_*.tmp")
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}

		// ä¿®å¤é‡å¤ä¸‹è½½ï¼šç›´æ¥ä½¿ç”¨srcObjæ‰“å¼€æ–‡ä»¶
		downloadStartTime := time.Now()
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
		}
		defer srcReader.Close()

		written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, false, fileName) // ä¸éœ€è¦è®¡ç®—MD5
		downloadDuration = time.Since(downloadStartTime)

		if err != nil {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
		}

		if written != fileSize {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
		}

		fs.Infof(f, "ğŸ“¥ ä¸‹è½½å®Œæˆ (ä½¿ç”¨ç¼“å­˜MD5): %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
			fs.SizeSuffix(fileSize), downloadDuration,
			fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

		// ç›´æ¥è·³è½¬åˆ°ä¸Šä¼ æ­¥éª¤
		return f.continueWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash, downloadDuration, md5Duration, startTime)
	}

	// æ­¥éª¤2: ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸ä¸‹è½½å¹¶è®¡ç®—MD5
	fs.Infof(f, "ğŸ’¾ MD5ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¸‹è½½å¹¶è®¡ç®—MD5")

	// åˆ›å»ºæœ¬åœ°ä¸´æ—¶æ–‡ä»¶
	tempFile, err := os.CreateTemp("", "cross_cloud_optimized_*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä¿®å¤é‡å¤ä¸‹è½½ï¼šç›´æ¥ä½¿ç”¨srcObjæ‰“å¼€æ–‡ä»¶
	downloadStartTime := time.Now()
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
	}
	defer srcReader.Close()

	written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, true, fileName) // éœ€è¦è®¡ç®—MD5
	downloadDuration = time.Since(downloadStartTime)

	if err != nil {
		return nil, fmt.Errorf("ä¸‹è½½å¹¶è®¡ç®—MD5å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
	}

	// MD5å·²åœ¨copyWithProgressAndValidationä¸­è®¡ç®—
	md5Duration = 0 // å·²åŒ…å«åœ¨ä¸‹è½½æ—¶é—´ä¸­

	fs.Infof(f, "ğŸ“¥ ä¸‹è½½å¹¶MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// è·å–è®¡ç®—çš„MD5ï¼ˆè¿™éœ€è¦ä»copyWithProgressAndValidationè¿”å›ï¼‰
	// ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬é‡æ–°è®¡ç®—MD5
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	md5StartTime := time.Now()
	hasher := md5.New()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
	}
	md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration = time.Since(md5StartTime)

	fs.Infof(f, "ğŸ” MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v", md5Hash, md5Duration)

	// ç»§ç»­ä¸Šä¼ æµç¨‹
	return f.continueWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash, downloadDuration, md5Duration, startTime)
}

// EndToEndIntegrityVerifier ç«¯åˆ°ç«¯å®Œæ•´æ€§éªŒè¯å™¨
// å¢å¼ºå®Œæ•´æ€§éªŒè¯ï¼šæ·»åŠ ç«¯åˆ°ç«¯MD5éªŒè¯å’Œæ–­ç‚¹ç»­ä¼ å®Œæ•´æ€§æ£€æŸ¥
type EndToEndIntegrityVerifier struct {
	fs *Fs
}

// NewEndToEndIntegrityVerifier åˆ›å»ºæ–°çš„ç«¯åˆ°ç«¯å®Œæ•´æ€§éªŒè¯å™¨
func NewEndToEndIntegrityVerifier(fs *Fs) *EndToEndIntegrityVerifier {
	return &EndToEndIntegrityVerifier{fs: fs}
}

// VerifyCrossCloudTransfer éªŒè¯è·¨äº‘ç›˜ä¼ è¾“çš„å®Œæ•´æ€§
func (eiv *EndToEndIntegrityVerifier) VerifyCrossCloudTransfer(ctx context.Context, sourceObj fs.Object, targetObj *Object, expectedMD5 string) (*CrossCloudIntegrityResult, error) {
	result := &CrossCloudIntegrityResult{
		SourcePath:  sourceObj.Remote(),
		TargetPath:  targetObj.remote,
		SourceSize:  sourceObj.Size(),
		TargetSize:  targetObj.size,
		ExpectedMD5: expectedMD5,
		StartTime:   time.Now(),
	}

	fs.Infof(eiv.fs, "ğŸ” å¼€å§‹ç«¯åˆ°ç«¯å®Œæ•´æ€§éªŒè¯: %s", sourceObj.Remote())

	// æ­¥éª¤1: éªŒè¯æ–‡ä»¶å¤§å°
	if result.SourceSize != result.TargetSize {
		result.Error = fmt.Sprintf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æºæ–‡ä»¶=%s, ç›®æ ‡æ–‡ä»¶=%s",
			fs.SizeSuffix(result.SourceSize), fs.SizeSuffix(result.TargetSize))
		result.SizeMatch = false
		fs.Errorf(eiv.fs, "âŒ å¤§å°éªŒè¯å¤±è´¥: %s", result.Error)
		return result, fmt.Errorf("æ–‡ä»¶å¤§å°éªŒè¯å¤±è´¥")
	}
	result.SizeMatch = true
	fs.Debugf(eiv.fs, " æ–‡ä»¶å¤§å°éªŒè¯é€šè¿‡: %s", fs.SizeSuffix(result.SourceSize))

	// æ­¥éª¤2: è·å–æºæ–‡ä»¶MD5ï¼ˆå¦‚æœå¯ç”¨ï¼‰
	sourceMD5, err := sourceObj.Hash(ctx, fshash.MD5)
	if err == nil && sourceMD5 != "" {
		result.SourceMD5 = sourceMD5
		fs.Debugf(eiv.fs, "ğŸ“‹ æºæ–‡ä»¶MD5: %s", sourceMD5)
	} else {
		fs.Debugf(eiv.fs, "âš ï¸ æ— æ³•è·å–æºæ–‡ä»¶MD5: %v", err)
	}

	// æ­¥éª¤3: è·å–ç›®æ ‡æ–‡ä»¶MD5
	if targetObj.md5sum != "" {
		result.TargetMD5 = targetObj.md5sum
		fs.Debugf(eiv.fs, "ğŸ“‹ ç›®æ ‡æ–‡ä»¶MD5: %s", targetObj.md5sum)
	} else {
		fs.Debugf(eiv.fs, "âš ï¸ ç›®æ ‡æ–‡ä»¶MD5ä¸å¯ç”¨")
	}

	// æ­¥éª¤4: éªŒè¯MD5ä¸€è‡´æ€§
	result.MD5Match = true
	if expectedMD5 != "" {
		// éªŒè¯ä¸æœŸæœ›MD5çš„ä¸€è‡´æ€§
		if result.TargetMD5 != "" && !strings.EqualFold(result.TargetMD5, expectedMD5) {
			result.MD5Match = false
			result.Error = fmt.Sprintf("ç›®æ ‡æ–‡ä»¶MD5ä¸æœŸæœ›ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedMD5, result.TargetMD5)
		}

		if result.SourceMD5 != "" && !strings.EqualFold(result.SourceMD5, expectedMD5) {
			result.MD5Match = false
			if result.Error != "" {
				result.Error += "; "
			}
			result.Error += fmt.Sprintf("æºæ–‡ä»¶MD5ä¸æœŸæœ›ä¸åŒ¹é…: æœŸæœ›=%s, å®é™…=%s", expectedMD5, result.SourceMD5)
		}
	}

	// éªŒè¯æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶MD5æ˜¯å¦ä¸€è‡´
	if result.SourceMD5 != "" && result.TargetMD5 != "" {
		if !strings.EqualFold(result.SourceMD5, result.TargetMD5) {
			result.MD5Match = false
			if result.Error != "" {
				result.Error += "; "
			}
			result.Error += fmt.Sprintf("æºæ–‡ä»¶ä¸ç›®æ ‡æ–‡ä»¶MD5ä¸åŒ¹é…: æº=%s, ç›®æ ‡=%s", result.SourceMD5, result.TargetMD5)
		}
	}

	result.EndTime = time.Now()
	result.Duration = result.EndTime.Sub(result.StartTime)
	result.IsValid = result.SizeMatch && result.MD5Match

	if result.IsValid {
		fs.Infof(eiv.fs, "âœ… ç«¯åˆ°ç«¯å®Œæ•´æ€§éªŒè¯é€šè¿‡: %s (è€—æ—¶: %v)", sourceObj.Remote(), result.Duration)
	} else {
		fs.Errorf(eiv.fs, "âŒ ç«¯åˆ°ç«¯å®Œæ•´æ€§éªŒè¯å¤±è´¥: %s - %s", sourceObj.Remote(), result.Error)
	}

	return result, nil
}

// CrossCloudIntegrityResult è·¨äº‘ç›˜ä¼ è¾“å®Œæ•´æ€§éªŒè¯ç»“æœ
type CrossCloudIntegrityResult struct {
	SourcePath  string        `json:"source_path"`
	TargetPath  string        `json:"target_path"`
	SourceSize  int64         `json:"source_size"`
	TargetSize  int64         `json:"target_size"`
	SourceMD5   string        `json:"source_md5"`
	TargetMD5   string        `json:"target_md5"`
	ExpectedMD5 string        `json:"expected_md5"`
	SizeMatch   bool          `json:"size_match"`
	MD5Match    bool          `json:"md5_match"`
	IsValid     bool          `json:"is_valid"`
	Error       string        `json:"error,omitempty"`
	StartTime   time.Time     `json:"start_time"`
	EndTime     time.Time     `json:"end_time"`
	Duration    time.Duration `json:"duration"`
}

// openSourceWithoutConcurrency æ‰“å¼€æºæ–‡ä»¶ä½†é¿å…è§¦å‘å¹¶å‘ä¸‹è½½
// ä¿®å¤å¤šé‡å¹¶å‘ä¸‹è½½ï¼šä¸“é—¨ç”¨äºè·¨äº‘ä¼ è¾“ï¼Œé¿å…æºå¯¹è±¡å¯åŠ¨è‡ªå·±çš„å¹¶å‘ä¸‹è½½
func (f *Fs) openSourceWithoutConcurrency(ctx context.Context, srcObj fs.Object) (io.ReadCloser, error) {
	// æ£€æŸ¥æºå¯¹è±¡ç±»å‹ï¼Œä½¿ç”¨ç‰¹å®šçš„æ–¹æ³•é¿å…å¹¶å‘ä¸‹è½½
	switch obj := srcObj.(type) {
	case interface {
		openNormal(context.Context, ...fs.OpenOption) (io.ReadCloser, error)
	}:
		// å¦‚æœæºå¯¹è±¡æœ‰openNormalæ–¹æ³•ï¼ˆå¦‚123ç½‘ç›˜ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
		fs.Debugf(f, " ä½¿ç”¨openNormalæ–¹æ³•é¿å…å¹¶å‘ä¸‹è½½")
		return obj.openNormal(ctx)
	case interface {
		open(context.Context, ...fs.OpenOption) (io.ReadCloser, error)
	}:
		// å¦‚æœæºå¯¹è±¡æœ‰openæ–¹æ³•ï¼ˆå¦‚115ç½‘ç›˜ï¼‰ï¼Œä½¿ç”¨å®ƒ
		fs.Debugf(f, " ä½¿ç”¨openæ–¹æ³•é¿å…å¹¶å‘ä¸‹è½½")
		return obj.open(ctx)
	default:
		// å¯¹äºå…¶ä»–ç±»å‹ï¼Œä½¿ç”¨æ ‡å‡†Openæ–¹æ³•ï¼Œä½†æ·»åŠ ç‰¹æ®Šé€‰é¡¹
		fs.Debugf(f, " ä½¿ç”¨æ ‡å‡†Openæ–¹æ³•ï¼Œæ·»åŠ ç¦ç”¨å¹¶å‘ä¸‹è½½é€‰é¡¹")
		// æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„é€‰é¡¹æ¥æŒ‡ç¤ºä¸è¦ä½¿ç”¨å¹¶å‘ä¸‹è½½
		options := []fs.OpenOption{&DisableConcurrentDownloadOption{}}
		return srcObj.Open(ctx, options...)
	}
}

// DisableConcurrentDownloadOption ç¦ç”¨å¹¶å‘ä¸‹è½½çš„é€‰é¡¹
// ä¿®å¤å¤šé‡å¹¶å‘ä¸‹è½½ï¼šç”¨äºæŒ‡ç¤ºæºå¯¹è±¡ä¸è¦å¯åŠ¨å¹¶å‘ä¸‹è½½
type DisableConcurrentDownloadOption struct{}

func (o *DisableConcurrentDownloadOption) String() string {
	return "DisableConcurrentDownload"
}

func (o *DisableConcurrentDownloadOption) Header() (key, value string) {
	return "X-Disable-Concurrent-Download", "true"
}

func (o *DisableConcurrentDownloadOption) Mandatory() bool {
	return false
}

// getGlobalStats è·å–å…¨å±€ç»Ÿè®¡å¯¹è±¡
// ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šé›†æˆåˆ°rclone accountingç³»ç»Ÿ
func (f *Fs) getGlobalStats(_ context.Context) *accounting.StatsInfo {
	// ä½¿ç”¨å…¨å±€statsï¼ˆè¿™æ˜¯rcloneçš„æ ‡å‡†åšæ³•ï¼‰
	return accounting.GlobalStats()
}

// Pan123DownloadAdapter 123ç½‘ç›˜ä¸‹è½½é€‚é…å™¨
type Pan123DownloadAdapter struct {
	fs *Fs
}

// NewPan123DownloadAdapter åˆ›å»º123ç½‘ç›˜ä¸‹è½½é€‚é…å™¨
func NewPan123DownloadAdapter(filesystem *Fs) *Pan123DownloadAdapter {
	return &Pan123DownloadAdapter{
		fs: filesystem,
	}
}

// GetDownloadURL è·å–123ç½‘ç›˜ä¸‹è½½URL
func (a *Pan123DownloadAdapter) GetDownloadURL(ctx context.Context, obj fs.Object, start, end int64) (string, error) {
	o, ok := obj.(*Object)
	if !ok {
		return "", fmt.Errorf("å¯¹è±¡ç±»å‹ä¸åŒ¹é…")
	}

	// è·å–ä¸‹è½½URL
	downloadURL, err := a.fs.getDownloadURL(ctx, o.id)
	if err != nil {
		return "", fmt.Errorf("è·å–123ç½‘ç›˜ä¸‹è½½URLå¤±è´¥: %w", err)
	}

	return downloadURL, nil
}

// DownloadChunk ä¸‹è½½123ç½‘ç›˜å•ä¸ªåˆ†ç‰‡ - ä¼˜åŒ–ç‰ˆæœ¬
func (a *Pan123DownloadAdapter) DownloadChunk(ctx context.Context, url string, tempFile *os.File, start, end int64, account *accounting.Account) error {
	// ä¼˜åŒ–ï¼šæ·»åŠ é‡è¯•æœºåˆ¶ï¼Œæå‡ä¸‹è½½ç¨³å®šæ€§
	var lastErr error
	for retry := range 3 {
		if retry > 0 {
			// é‡è¯•å‰ç­‰å¾…ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
			time.Sleep(time.Duration(retry) * time.Second)
			fs.Debugf(a.fs, "ğŸ”„ 123ç½‘ç›˜åˆ†ç‰‡ä¸‹è½½é‡è¯• %d/3: bytes=%d-%d", retry+1, start, end)
		}

		// åˆ›å»ºHTTPè¯·æ±‚
		req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
		if err != nil {
			lastErr = fmt.Errorf("åˆ›å»ºä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
			continue
		}

		// ä¼˜åŒ–ï¼šè®¾ç½®æ›´å®Œæ•´çš„HTTPå¤´ï¼Œæå‡å…¼å®¹æ€§
		req.Header.Set("Range", fmt.Sprintf("bytes=%d-%d", start, end))
		req.Header.Set("User-Agent", a.fs.opt.UserAgent)
		req.Header.Set("Accept", "*/*")
		req.Header.Set("Connection", "keep-alive")

		// æ‰§è¡Œè¯·æ±‚
		resp, err := a.fs.client.Do(req)
		if err != nil {
			lastErr = fmt.Errorf("æ‰§è¡Œä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err)
			continue
		}

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode != http.StatusPartialContent && resp.StatusCode != http.StatusOK {
			resp.Body.Close()
			lastErr = fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d", resp.StatusCode)
			continue
		}

		// æˆåŠŸè·å–å“åº”ï¼Œæ‰§è¡Œä¸‹è½½
		err = a.processDownloadResponse(resp, tempFile, start, end, account)
		resp.Body.Close()

		if err == nil {
			return nil // æˆåŠŸå®Œæˆ
		}

		lastErr = err
	}

	return fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯•3æ¬¡: %w", lastErr)
}

// processDownloadResponse å¤„ç†ä¸‹è½½å“åº” - æ–°å¢è¾…åŠ©æ–¹æ³•
func (a *Pan123DownloadAdapter) processDownloadResponse(resp *http.Response, tempFile *os.File, start, end int64, account *accounting.Account) error {

	// ä¼˜åŒ–ï¼šä½¿ç”¨æµå¼è¯»å–ï¼Œå‡å°‘å†…å­˜å ç”¨
	expectedSize := end - start + 1

	// ä¼˜åŒ–ï¼šé¢„åˆ†é…ç¼“å†²åŒºï¼Œæå‡æ€§èƒ½
	buffer := make([]byte, expectedSize)
	totalRead := int64(0)

	// æµå¼è¯»å–æ•°æ®
	for totalRead < expectedSize {
		n, err := resp.Body.Read(buffer[totalRead:])
		if n > 0 {
			totalRead += int64(n)
		}
		if err != nil {
			if err == io.EOF {
				break
			}
			return fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®å¤§å°
	if totalRead != expectedSize {
		return fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", expectedSize, totalRead)
	}

	// ä¼˜åŒ–ï¼šç›´æ¥å†™å…¥åˆ°æŒ‡å®šä½ç½®ï¼Œé¿å…é¢å¤–çš„å†…å­˜å¤åˆ¶
	writtenBytes, err := tempFile.WriteAt(buffer[:totalRead], start)
	if err != nil {
		return fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯å®é™…å†™å…¥çš„å­—èŠ‚æ•°
	if int64(writtenBytes) != totalRead {
		return fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡å†™å…¥ä¸å®Œæ•´: æœŸæœ›å†™å…¥%dï¼Œå®é™…å†™å…¥%d", totalRead, writtenBytes)
	}

	// ä¼˜åŒ–ï¼šæŠ¥å‘Šåˆ†ç‰‡è¿›åº¦åˆ°rclone accountingç³»ç»Ÿ
	if account != nil {
		if err := account.AccountRead(int(totalRead)); err != nil {
			fs.Debugf(a.fs, "âš ï¸ æŠ¥å‘Š123ç½‘ç›˜åˆ†ç‰‡è¿›åº¦å¤±è´¥: %v", err)
		} else {
			fs.Debugf(a.fs, " 123ç½‘ç›˜åˆ†ç‰‡ä¸‹è½½å®Œæˆï¼Œå·²æŠ¥å‘Šè¿›åº¦: %s (bytes=%d-%d)",
				fs.SizeSuffix(totalRead), start, end)
		}
	} else {
		fs.Debugf(a.fs, "âš ï¸ 123ç½‘ç›˜Accountå¯¹è±¡ä¸ºç©ºï¼Œæ— æ³•æŠ¥å‘Šåˆ†ç‰‡è¿›åº¦ (bytes=%d-%d)", start, end)
	}

	return nil
}

// VerifyDownload éªŒè¯123ç½‘ç›˜ä¸‹è½½å®Œæ•´æ€§
func (a *Pan123DownloadAdapter) VerifyDownload(ctx context.Context, obj fs.Object, tempFile *os.File) error {
	// éªŒè¯æ–‡ä»¶å¤§å°
	stat, err := tempFile.Stat()
	if err != nil {
		return fmt.Errorf("è·å–ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if stat.Size() != obj.Size() {
		return fmt.Errorf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", obj.Size(), stat.Size())
	}

	return nil
}

// GetConfig è·å–123ç½‘ç›˜ä¸‹è½½é…ç½® - ä¼˜åŒ–ç‰ˆæœ¬
func (a *Pan123DownloadAdapter) GetConfig() common.DownloadConfig {
	return common.DownloadConfig{
		MinFileSize:      10 * 1024 * 1024, // ä¼˜åŒ–ï¼šæå‡åˆ°10MBé—¨æ§›ï¼Œé¿å…å°æ–‡ä»¶å¹¶å‘å¼€é”€
		MaxConcurrency:   6,                // ä¼˜åŒ–ï¼šæå‡åˆ°6çº¿ç¨‹ï¼Œå……åˆ†åˆ©ç”¨123ç½‘ç›˜å¸¦å®½
		DefaultChunkSize: 50 * 1024 * 1024, // ä¼˜åŒ–ï¼šæå‡åˆ°50MBåˆ†ç‰‡ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
		TimeoutPerChunk:  60 * time.Second, // ä¼˜åŒ–ï¼šå»¶é•¿åˆ°60ç§’ï¼Œé€‚åº”å¤§åˆ†ç‰‡ä¸‹è½½
	}
}

// manualCacheCleanup æ‰‹åŠ¨è§¦å‘ç¼“å­˜æ¸…ç†
// ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç®¡ç†å‘½ä»¤æ¥å£
func (f *Fs) manualCacheCleanup(ctx context.Context, strategy string) (interface{}, error) {
	fs.Infof(f, "å¼€å§‹æ‰‹åŠ¨ç¼“å­˜æ¸…ç†ï¼Œç­–ç•¥: %s", strategy)

	result := map[string]interface{}{
		"backend":  "123",
		"strategy": strategy,
		"caches":   make(map[string]interface{}),
	}

	// æ¸…ç†å„ä¸ªç¼“å­˜å®ä¾‹
	caches := map[string]cache.PersistentCache{
		"parent_ids": f.parentIDCache,
		"dir_list":   f.dirListCache,
		"path_to_id": f.pathToIDCache,
	}

	for name, c := range caches {
		if c != nil {
			beforeStats := c.Stats()

			// æ ¹æ®ç­–ç•¥æ‰§è¡Œæ¸…ç†
			var err error
			switch strategy {
			case "size", "lru", "priority_lru", "time":
				if badgerCache, ok := c.(*cache.BadgerCache); ok {
					// ä½¿ç”¨é»˜è®¤ç›®æ ‡å¤§å°è¿›è¡Œæ™ºèƒ½æ¸…ç†
					targetSize := int64(f.cacheConfig.TargetCleanSize)
					err = badgerCache.SmartCleanupWithStrategy(targetSize, strategy)
				} else {
					err = fmt.Errorf("ç¼“å­˜ç±»å‹ä¸æ”¯æŒæ™ºèƒ½æ¸…ç†")
				}
			case "clear":
				err = c.Clear()
				// æ·»åŠ é¢å¤–çš„éªŒè¯æ­¥éª¤ï¼Œç¡®ä¿ç¼“å­˜ç¡®å®è¢«æ¸…é™¤äº†
				if err == nil {
					// éªŒè¯æ¸…ç†æ˜¯å¦æˆåŠŸ
					keys, listErr := c.ListAllKeys()
					if listErr != nil {
						fs.Debugf(f, "æ— æ³•éªŒè¯æ¸…ç†æ“ä½œ: %v", listErr)
					} else if len(keys) > 0 {
						// å¦‚æœè¿˜æœ‰é”®å­˜åœ¨ï¼Œè®°å½•å‰å‡ ä¸ªé”®ç”¨äºè°ƒè¯•
						fs.Debugf(f, "æ¸…é™¤åç¼“å­˜ä¸­ä»æœ‰%dä¸ªé”®", len(keys))
						maxKeys := len(keys)
						if maxKeys > 5 {
							maxKeys = 5
						}
						fs.Debugf(f, "å‰%dä¸ªé”®: %v", maxKeys, keys[:maxKeys])
						// è®¤ä¸ºæ¸…ç†ä¸å®Œå…¨ï¼Œè¿”å›é”™è¯¯
						err = fmt.Errorf("æ¸…ç†åä»æœ‰%dä¸ªé”®æœªè¢«æ¸…é™¤", len(keys))
					} else {
						fs.Debugf(f, "éªŒè¯æˆåŠŸï¼šç¼“å­˜å·²å®Œå…¨æ¸…é™¤")
					}
				}
			default:
				err = fmt.Errorf("ä¸æ”¯æŒçš„æ¸…ç†ç­–ç•¥: %s", strategy)
			}

			afterStats := c.Stats()

			result["caches"].(map[string]interface{})[name] = map[string]interface{}{
				"success": err == nil,
				"error": func() string {
					if err != nil {
						return err.Error()
					}
					return ""
				}(),
				"before_size": beforeStats["total_size"],
				"after_size":  afterStats["total_size"],
				"cleaned_mb":  float64(beforeStats["total_size"].(int64)-afterStats["total_size"].(int64)) / (1024 * 1024),
			}

			if err != nil {
				fs.Errorf(f, "æ¸…ç†%sç¼“å­˜å¤±è´¥: %v", name, err)
			} else {
				fs.Infof(f, "æ¸…ç†%sç¼“å­˜æˆåŠŸ", name)
			}
		}
	}

	fs.Infof(f, "æ‰‹åŠ¨ç¼“å­˜æ¸…ç†å®Œæˆ")
	return result, nil
}

// getCacheStatistics è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
// ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç®¡ç†å‘½ä»¤æ¥å£
func (f *Fs) getCacheStatistics(ctx context.Context) (interface{}, error) {
	result := map[string]interface{}{
		"backend": "123",
		"caches":  make(map[string]interface{}),
	}

	// è·å–å„ä¸ªç¼“å­˜å®ä¾‹çš„ç»Ÿè®¡
	caches := map[string]cache.PersistentCache{
		"parent_ids": f.parentIDCache,
		"dir_list":   f.dirListCache,
		"path_to_id": f.pathToIDCache,
	}

	for name, c := range caches {
		if c != nil {
			stats := c.Stats()
			result["caches"].(map[string]interface{})[name] = stats
		} else {
			result["caches"].(map[string]interface{})[name] = map[string]interface{}{
				"status": "not_initialized",
			}
		}
	}

	return result, nil
}

// getCacheConfiguration è·å–å½“å‰ç¼“å­˜é…ç½®
// ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç®¡ç†å‘½ä»¤æ¥å£
func (f *Fs) getCacheConfiguration(ctx context.Context) (interface{}, error) {
	return map[string]interface{}{
		"backend": "123",
		"config": map[string]interface{}{
			"max_cache_size":       f.cacheConfig.MaxCacheSize,
			"target_clean_size":    f.cacheConfig.TargetCleanSize,
			"mem_table_size":       f.cacheConfig.MemTableSize,
			"enable_smart_cleanup": f.cacheConfig.EnableSmartCleanup,
			"cleanup_strategy":     f.cacheConfig.CleanupStrategy,
		},
		"user_config": map[string]interface{}{
			"cache_max_size":       f.opt.CacheMaxSize,
			"cache_target_size":    f.opt.CacheTargetSize,
			"enable_smart_cleanup": f.opt.EnableSmartCleanup,
			"cleanup_strategy":     f.opt.CleanupStrategy,
		},
	}, nil
}

// resetCacheConfiguration é‡ç½®ç¼“å­˜é…ç½®ä¸ºé»˜è®¤å€¼
// ğŸ”§ æ–°å¢ï¼šç¼“å­˜ç®¡ç†å‘½ä»¤æ¥å£
func (f *Fs) resetCacheConfiguration(ctx context.Context) (interface{}, error) {
	fs.Infof(f, "é‡ç½®123ç¼“å­˜é…ç½®ä¸ºé»˜è®¤å€¼")

	// é‡ç½®ä¸ºé»˜è®¤é…ç½®
	defaultConfig := common.DefaultUnifiedCacheConfig("123")
	f.cacheConfig = defaultConfig

	return map[string]interface{}{
		"backend": "123",
		"message": "ç¼“å­˜é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼",
		"config": map[string]interface{}{
			"max_cache_size":       f.cacheConfig.MaxCacheSize,
			"target_clean_size":    f.cacheConfig.TargetCleanSize,
			"mem_table_size":       f.cacheConfig.MemTableSize,
			"enable_smart_cleanup": f.cacheConfig.EnableSmartCleanup,
			"cleanup_strategy":     f.cacheConfig.CleanupStrategy,
		},
	}, nil
}
