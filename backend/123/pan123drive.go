package _123

import (
	"bufio"
	"bytes"
	"context"
	"crypto/md5"
	"encoding/json"
	"fmt"
	"hash"
	"hash/crc32"
	"io"
	"math"
	"math/rand/v2"
	"mime/multipart"
	"net/http"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"
	"unicode/utf8"

	"github.com/rclone/rclone/lib/atexit"
	"github.com/rclone/rclone/lib/dircache"
	"github.com/rclone/rclone/lib/oauthutil"
	"github.com/rclone/rclone/lib/pacer"
	"github.com/rclone/rclone/lib/rest"
	"golang.org/x/oauth2"

	// 123ç½‘ç›˜SDK
	"github.com/rclone/rclone/fs"
	"github.com/rclone/rclone/fs/accounting"
	"github.com/rclone/rclone/fs/config/configmap"
	"github.com/rclone/rclone/fs/config/configstruct"
	"github.com/rclone/rclone/fs/fserrors"
	"github.com/rclone/rclone/fs/fshttp"
	fshash "github.com/rclone/rclone/fs/hash"
	"github.com/rclone/rclone/lib/cache"
)

// "Mozilla/5.0 AppleWebKit/600 Safari/600 Chrome/124.0.0.0 Edg/124.0.0.0"
const (
	openAPIRootURL     = "https://open-api.123pan.com"
	defaultUserAgent   = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36"
	tokenRefreshWindow = 10 * time.Minute // tokenRefreshWindow å®šä¹‰äº†åœ¨ä»¤ç‰Œè¿‡æœŸå‰å¤šé•¿æ—¶é—´å°è¯•åˆ·æ–°ä»¤ç‰Œ

	// åŸºäº123ç½‘ç›˜å®˜æ–¹é™åˆ¶çš„APIé€Ÿç‡é™åˆ¶ï¼ˆ2025å¹´1æœˆæ›´æ–°ï¼‰
	// å®˜æ–¹QPSé™åˆ¶æ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
	// æ›´æ–°æ—¶é—´ï¼š2025-07-04ï¼ŒåŸºäºæœ€æ–°å®˜æ–¹QPSé™åˆ¶è¡¨
	// æ‰€æœ‰APIé™åˆ¶ç°åœ¨é€šè¿‡getPacerForEndpointç»Ÿä¸€ç®¡ç†ï¼Œä»¥ä¸‹å¸¸é‡ä»…ä½œå‚è€ƒ

	// é«˜é¢‘ç‡API (15-20 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	listV2APIMinSleep = 200 * time.Millisecond // ~5 QPS ç”¨äº api/v2/file/list (å®˜æ–¹15 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	fileMoveMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/move (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileInfosMinSleep   = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/file/infos (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	userInfoMinSleep    = 250 * time.Millisecond // ~4 QPS ç”¨äº api/v1/user/info (å®˜æ–¹10 QPSï¼Œæä¿å®ˆè®¾ç½®)
	accessTokenMinSleep = 300 * time.Millisecond // ~3.3 QPS ç”¨äº api/v1/access_token (å®˜æ–¹8 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ä½é¢‘ç‡API (5 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	uploadCreateMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº upload/v1/file/create (å®˜æ–¹5 QPSï¼Œæä¿å®ˆè®¾ç½®)
	mkdirMinSleep        = 500 * time.Millisecond // ~2 QPS ç”¨äº upload/v1/file/mkdir (å®˜æ–¹5 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileTrashMinSleep    = 500 * time.Millisecond // ~2 QPS ç”¨äº api/v1/file/trash (å®˜æ–¹5 QPSï¼Œæä¿å®ˆè®¾ç½®)

	downloadInfoMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº api/v1/file/download_info (å®˜æ–¹5 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// æœ€ä½é¢‘ç‡API (1 QPS) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	fileDeleteMinSleep     = 2000 * time.Millisecond // ~0.5 QPS ç”¨äº api/v1/file/delete (å®˜æ–¹1 QPSï¼Œæä¿å®ˆè®¾ç½®)
	fileListMinSleep       = 2000 * time.Millisecond // ~0.5 QPS ç”¨äº api/v1/file/list (å®˜æ–¹1 QPSï¼Œæä¿å®ˆè®¾ç½®)
	videoTranscodeMinSleep = 2000 * time.Millisecond // ~0.5 QPS ç”¨äº api/v1/video/transcode/list (å®˜æ–¹1 QPSï¼Œæä¿å®ˆè®¾ç½®)

	// ç‰¹æ®ŠAPI (ä¿å®ˆä¼°è®¡) - è¿›ä¸€æ­¥é™ä½é¿å…429é”™è¯¯
	uploadV2SliceMinSleep = 500 * time.Millisecond // ~2 QPS ç”¨äº upload/v2/file/slice (æä¿å®ˆè®¾ç½®é¿å…429é”™è¯¯)

	maxSleep      = 30 * time.Second // 429é€€é¿çš„æœ€å¤§ç¡çœ æ—¶é—´
	decayConstant = 2

	// æ–‡ä»¶ä¸Šä¼ ç›¸å…³å¸¸é‡
	singleStepUploadLimit = 50 * 1024 * 1024  // 500MB - å•æ­¥ä¸Šä¼ APIçš„æ–‡ä»¶å¤§å°é™åˆ¶
	maxMemoryBufferSize   = 512 * 1024 * 1024 // 512MB - å†…å­˜ç¼“å†²çš„æœ€å¤§å¤§å°ï¼Œé˜²æ­¢å†…å­˜ä¸è¶³
	maxFileNameBytes      = 255               // æ–‡ä»¶åçš„æœ€å¤§å­—èŠ‚é•¿åº¦ï¼ˆUTF-8ç¼–ç ï¼‰

	// æ–‡ä»¶å†²çªå¤„ç†ç­–ç•¥å¸¸é‡å·²ç§»é™¤ï¼Œå½“å‰ä½¿ç”¨APIé»˜è®¤è¡Œä¸º

	// ä¸Šä¼ ç›¸å…³å¸¸é‡ - ä¼˜åŒ–å¤§æ–‡ä»¶ä¼ è¾“æ€§èƒ½
	defaultChunkSize    = 100 * fs.Mebi // å¢åŠ é»˜è®¤åˆ†ç‰‡å¤§å°åˆ°100MB
	minChunkSize        = 50 * fs.Mebi  // å¢åŠ æœ€å°åˆ†ç‰‡å¤§å°åˆ°50MB
	maxChunkSize        = 500 * fs.Mebi // è®¾ç½®æœ€å¤§åˆ†ç‰‡å¤§å°ä¸º500MB
	defaultUploadCutoff = 100 * fs.Mebi // é™ä½åˆ†ç‰‡ä¸Šä¼ é˜ˆå€¼
	maxUploadParts      = 10000

	// è¿æ¥å’Œè¶…æ—¶è®¾ç½® - é’ˆå¯¹å¤§æ–‡ä»¶ä¼ è¾“ä¼˜åŒ–çš„å‚æ•°
	defaultConnTimeout = 30 * time.Second  // å¢åŠ è¿æ¥è¶…æ—¶ï¼Œé€‚åº”ç½‘ç»œæ³¢åŠ¨
	defaultTimeout     = 600 * time.Second // å¢åŠ æ€»ä½“è¶…æ—¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶ä¼ è¾“

	// é‡è¯•å’Œé€€é¿è®¾ç½®
	maxRetries      = 10               // å¢åŠ æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæé«˜æˆåŠŸç‡
	baseRetryDelay  = 2 * time.Second  // å¢åŠ åŸºç¡€é‡è¯•å»¶è¿Ÿ
	maxRetryDelay   = 60 * time.Second // å¢åŠ æœ€å¤§é‡è¯•å»¶è¿Ÿ
	retryMultiplier = 2.0              // æŒ‡æ•°é€€é¿å€æ•°

	// æ–‡ä»¶åéªŒè¯ç›¸å…³å¸¸é‡
	maxFileNameLength = 256          // 123ç½‘ç›˜æ–‡ä»¶åæœ€å¤§é•¿åº¦ï¼ˆåŒ…æ‹¬æ‰©å±•åï¼‰
	invalidChars      = `"\/:*?|><\` // 123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦
	replacementChar   = "_"          // ç”¨äºæ›¿æ¢éæ³•å­—ç¬¦çš„å®‰å…¨å­—ç¬¦

	// æ—¥å¿—çº§åˆ«å¸¸é‡
	LogLevelNone    = 0 // æ— æ—¥å¿—
	LogLevelError   = 1 // ä»…é”™è¯¯
	LogLevelInfo    = 2 // ä¿¡æ¯çº§åˆ«
	LogLevelDebug   = 3 // è°ƒè¯•çº§åˆ«
	LogLevelVerbose = 4 // è¯¦ç»†çº§åˆ«

	// ç½‘ç»œé€Ÿåº¦ç¼“å­˜ç›¸å…³å¸¸é‡
	NetworkSpeedCacheTTL = 5 * time.Minute // ç½‘ç»œé€Ÿåº¦ç¼“å­˜æœ‰æ•ˆæœŸ

	// ç¼“å­˜å’Œæ¸…ç†ç›¸å…³å¸¸é‡
	DefaultCacheCleanupInterval = 5 * time.Minute // é»˜è®¤ç¼“å­˜æ¸…ç†é—´éš”
	DefaultCacheCleanupTimeout  = 2 * time.Minute // é»˜è®¤ç¼“å­˜æ¸…ç†è¶…æ—¶
	BufferCleanupThreshold      = 5 * time.Minute // ç¼“å†²åŒºæ¸…ç†é˜ˆå€¼

	// é…ç½®éªŒè¯ç›¸å…³å¸¸é‡
	MaxListChunk          = 10000 // æœ€å¤§åˆ—è¡¨å—å¤§å°
	MaxUploadPartsLimit   = 10000 // æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°é™åˆ¶
	MaxConcurrentLimit    = 100   // æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	DefaultListChunk      = 1000  // é»˜è®¤åˆ—è¡¨å—å¤§å°
	DefaultMaxUploadParts = 1000  // é»˜è®¤æœ€å¤§ä¸Šä¼ åˆ†ç‰‡æ•°

	// ç½‘ç»œè´¨é‡è¯„ä¼°å¸¸é‡
	LatencyThreshold          = 50              // å»¶è¿Ÿé˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
	LatencyScoreRange         = 1000            // å»¶è¿Ÿåˆ†æ•°è®¡ç®—èŒƒå›´
	QualityAdjustmentInterval = 5 * time.Minute // è´¨é‡è°ƒæ•´é—´éš”

	// ç¼“å­˜ç®¡ç†å¸¸é‡
	PreloadQueueCapacity    = 1000 // é¢„åŠ è½½é˜Ÿåˆ—å®¹é‡
	MaxHotFiles             = 100  // æœ€å¤§çƒ­ç‚¹æ–‡ä»¶æ•°
	HotFileCleanupBatchSize = 20   // çƒ­ç‚¹æ–‡ä»¶æ¸…ç†æ‰¹æ¬¡å¤§å°

	// ç½‘ç»œæ£€æµ‹å¸¸é‡
	NetworkTestSize = 512 * 1024 // ç½‘ç»œæµ‹è¯•æ–‡ä»¶å¤§å°ï¼ˆ512KBï¼‰
)

// Options å®šä¹‰æ­¤åç«¯çš„é…ç½®é€‰é¡¹
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ ‡æ³¨äº†å¯ä»¥è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®çš„é€‰é¡¹
type Options struct {
	// 123ç½‘ç›˜ç‰¹å®šçš„è®¤è¯é…ç½®
	ClientID     string `config:"client_id"`
	ClientSecret string `config:"client_secret"`
	Token        string `config:"token"`
	UserAgent    string `config:"user_agent"`
	RootFolderID string `config:"root_folder_id"`

	// æ–‡ä»¶æ“ä½œé…ç½® (éƒ¨åˆ†å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	ListChunk      int           `config:"list_chunk"`    // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Checkers
	ChunkSize      fs.SizeSuffix `config:"chunk_size"`    // å¯è€ƒè™‘ä½¿ç”¨fs.Config.ChunkSize
	UploadCutoff   fs.SizeSuffix `config:"upload_cutoff"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.MultiThreadCutoff
	MaxUploadParts int           `config:"max_upload_parts"`

	// ç½‘ç»œå’Œè¶…æ—¶é…ç½® (å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	PacerMinSleep     fs.Duration `config:"pacer_min_sleep"`
	ListPacerMinSleep fs.Duration `config:"list_pacer_min_sleep"`
	ConnTimeout       fs.Duration `config:"conn_timeout"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.ConnectTimeout
	Timeout           fs.Duration `config:"timeout"`      // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Timeout

	// å¹¶å‘æ§åˆ¶é…ç½® (å¯è€ƒè™‘ä½¿ç”¨rcloneæ ‡å‡†é…ç½®)
	MaxConcurrentUploads   int         `config:"max_concurrent_uploads"`   // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Transfers
	MaxConcurrentDownloads int         `config:"max_concurrent_downloads"` // å¯è€ƒè™‘ä½¿ç”¨fs.Config.Checkers
	ProgressUpdateInterval fs.Duration `config:"progress_update_interval"`
	EnableProgressDisplay  bool        `config:"enable_progress_display"`

	// 123ç½‘ç›˜ç‰¹å®šçš„QPSæ§åˆ¶é€‰é¡¹
	UploadPacerMinSleep   fs.Duration `config:"upload_pacer_min_sleep"`
	DownloadPacerMinSleep fs.Duration `config:"download_pacer_min_sleep"`
	StrictPacerMinSleep   fs.Duration `config:"strict_pacer_min_sleep"`

	// è°ƒè¯•é…ç½®
	DebugLevel             int         `config:"debug_level"`              // è°ƒè¯•çº§åˆ«ï¼š0=æ— ï¼Œ1=é”™è¯¯ï¼Œ2=ä¿¡æ¯ï¼Œ3=è°ƒè¯•ï¼Œ4=è¯¦ç»†
	EnablePerformanceLog   bool        `config:"enable_performance_log"`   // å¯ç”¨æ€§èƒ½ç›‘æ§æ—¥å¿—
	PerformanceLogInterval fs.Duration `config:"performance_log_interval"` // æ€§èƒ½æ—¥å¿—è¾“å‡ºé—´éš”
}

// Fs è¡¨ç¤ºè¿œç¨‹123ç½‘ç›˜é©±åŠ¨å™¨å®ä¾‹
type Fs struct {
	name         string       // æ­¤è¿œç¨‹å®ä¾‹çš„åç§°
	originalName string       // æœªä¿®æ”¹çš„åŸå§‹é…ç½®åç§°
	root         string       // å½“å‰æ“ä½œçš„æ ¹è·¯å¾„
	opt          Options      // è§£æåçš„é…ç½®é€‰é¡¹
	features     *fs.Features // å¯é€‰åŠŸèƒ½ç‰¹æ€§

	// APIå®¢æˆ·ç«¯å’Œèº«ä»½éªŒè¯ç›¸å…³
	client       *http.Client     // ç”¨äºAPIè°ƒç”¨çš„HTTPå®¢æˆ·ç«¯ï¼ˆä¿ç•™ç”¨äºç‰¹æ®Šæƒ…å†µï¼‰
	rst          *rest.Client     // rcloneæ ‡å‡†restå®¢æˆ·ç«¯ï¼ˆæ¨èä½¿ç”¨ï¼‰
	token        string           // ç¼“å­˜çš„è®¿é—®ä»¤ç‰Œ
	tokenExpiry  time.Time        // ç¼“å­˜è®¿é—®ä»¤ç‰Œçš„è¿‡æœŸæ—¶é—´
	tokenMu      sync.Mutex       // ä¿æŠ¤tokenå’ŒtokenExpiryçš„äº’æ–¥é”
	tokenRenewer *oauthutil.Renew // ä»¤ç‰Œè‡ªåŠ¨æ›´æ–°å™¨

	// ç¼“å­˜å¹¶å‘å®‰å…¨é”
	cacheMu sync.RWMutex // ä¿æŠ¤æ‰€æœ‰ç¼“å­˜æ“ä½œçš„è¯»å†™é”

	// é…ç½®å’ŒçŠ¶æ€ä¿¡æ¯
	m            configmap.Mapper // é…ç½®æ˜ å°„å™¨
	rootFolderID string           // æ ¹æ–‡ä»¶å¤¹çš„ID
	// userID       string           // æ¥è‡ªAPIçš„ç”¨æˆ·IDï¼ˆæš‚æœªä½¿ç”¨ï¼‰

	// æ€§èƒ½å’Œé€Ÿç‡é™åˆ¶ - é’ˆå¯¹ä¸åŒAPIé™åˆ¶çš„å¤šä¸ªè°ƒé€Ÿå™¨
	listPacer     *fs.Pacer // æ–‡ä»¶åˆ—è¡¨APIçš„è°ƒé€Ÿå™¨ï¼ˆçº¦2 QPSï¼‰
	strictPacer   *fs.Pacer // ä¸¥æ ¼APIï¼ˆå¦‚user/info, move, deleteï¼‰çš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	uploadPacer   *fs.Pacer // ä¸Šä¼ APIçš„è°ƒé€Ÿå™¨ï¼ˆ1 QPSï¼‰
	downloadPacer *fs.Pacer // ä¸‹è½½APIçš„è°ƒé€Ÿå™¨ï¼ˆ2 QPSï¼‰

	// ç›®å½•ç¼“å­˜
	dirCache *dircache.DirCache

	// BadgerDBæŒä¹…åŒ–ç¼“å­˜å®ä¾‹
	parentIDCache *cache.BadgerCache // parentFileIDéªŒè¯ç¼“å­˜
	dirListCache  *cache.BadgerCache // ç›®å½•åˆ—è¡¨ç¼“å­˜
	pathToIDCache *cache.BadgerCache // è·¯å¾„åˆ°FileIDæ˜ å°„ç¼“å­˜

	// æ€§èƒ½ä¼˜åŒ–ç›¸å…³ - ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨æ›´æ ‡å‡†åŒ–çš„å¹¶å‘æ§åˆ¶
	concurrencyManager *ConcurrencyManager         // ç»Ÿä¸€çš„å¹¶å‘æ§åˆ¶ç®¡ç†å™¨
	memoryManager      *MemoryManager              // å†…å­˜ç®¡ç†å™¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
	performanceMetrics *PerformanceMetrics         // æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
	enhancedMetrics    *EnhancedPerformanceMetrics // å¢å¼ºæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
	resourcePool       *ResourcePool               // èµ„æºæ± ç®¡ç†å™¨ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
	// ğŸ—‘ï¸ downloadURLCache å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤
	apiVersionManager *APIVersionManager        // APIç‰ˆæœ¬ç®¡ç†å™¨
	resumeManager     *ResumeManager            // æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
	dynamicAdjuster   *DynamicParameterAdjuster // åŠ¨æ€å‚æ•°è°ƒæ•´å™¨

	// ç¼“å­˜æ—¶é—´é…ç½®
	cacheConfig CacheConfig
}

// debugf æ ¹æ®è°ƒè¯•çº§åˆ«è¾“å‡ºè°ƒè¯•ä¿¡æ¯
func (f *Fs) debugf(level int, format string, args ...any) {
	if f.opt.DebugLevel >= level {
		fs.Debugf(f, format, args...)
	}
}

// CacheConfig ç¼“å­˜æ—¶é—´é…ç½®
type CacheConfig struct {
	ParentIDCacheTTL    time.Duration // parentFileIDéªŒè¯ç¼“å­˜TTLï¼Œé»˜è®¤5åˆ†é’Ÿ
	DirListCacheTTL     time.Duration // ç›®å½•åˆ—è¡¨ç¼“å­˜TTLï¼Œé»˜è®¤3åˆ†é’Ÿ
	DownloadURLCacheTTL time.Duration // ä¸‹è½½URLç¼“å­˜TTLï¼Œé»˜è®¤åŠ¨æ€ï¼ˆæ ¹æ®APIè¿”å›ï¼‰
	PathToIDCacheTTL    time.Duration // è·¯å¾„æ˜ å°„ç¼“å­˜TTLï¼Œé»˜è®¤12åˆ†é’Ÿ
}

// DefaultCacheConfig è¿”å›é»˜è®¤çš„ç¼“å­˜é…ç½®
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šç»Ÿä¸€TTLä¸º5åˆ†é’Ÿï¼Œé¿å…ç¼“å­˜ä¸ä¸€è‡´é—®é¢˜
func DefaultCacheConfig() CacheConfig {
	unifiedTTL := 5 * time.Minute // ç»Ÿä¸€TTLç­–ç•¥
	return CacheConfig{
		ParentIDCacheTTL:    unifiedTTL, // ç»Ÿä¸€ä¸º5åˆ†é’Ÿ
		DirListCacheTTL:     unifiedTTL, // ä»3åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œä¸å…¶ä»–ç¼“å­˜ä¿æŒä¸€è‡´
		DownloadURLCacheTTL: 0,          // åŠ¨æ€TTLï¼Œæ ¹æ®APIè¿”å›çš„è¿‡æœŸæ—¶é—´
		PathToIDCacheTTL:    unifiedTTL, // ä»12åˆ†é’Ÿæ”¹ä¸º5åˆ†é’Ÿï¼Œé¿å…æŒ‡å‘è¿‡æœŸç¼“å­˜
	}
}

// ParentIDCacheEntry çˆ¶ç›®å½•IDç¼“å­˜æ¡ç›®
type ParentIDCacheEntry struct {
	ParentID  int64     `json:"parent_id"`
	Valid     bool      `json:"valid"`
	ExpiresAt time.Time `json:"expires_at"`
	CreatedAt time.Time `json:"created_at"`
}

// ParentIDCacheFile ç¼“å­˜æ–‡ä»¶ç»“æ„ï¼ˆæ‰¹é‡å­˜å‚¨ï¼‰
type ParentIDCacheFile struct {
	Version   int                           `json:"version"`
	UpdatedAt time.Time                     `json:"updated_at"`
	Entries   map[string]ParentIDCacheEntry `json:"entries"`
}

// DirListCacheEntry ç›®å½•åˆ—è¡¨ç¼“å­˜æ¡ç›®
type DirListCacheEntry struct {
	FileList   []FileListInfoRespDataV2 `json:"file_list"`
	LastFileID int64                    `json:"last_file_id"`
	TotalCount int                      `json:"total_count"`
	CachedAt   time.Time                `json:"cached_at"`
	ParentID   int64                    `json:"parent_id"`
	Version    int64                    `json:"version"`  // ç¼“å­˜ç‰ˆæœ¬å·
	Checksum   string                   `json:"checksum"` // æ•°æ®æ ¡éªŒå’Œ
}

// ğŸ—‘ï¸ DownloadURLCacheEntry å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// PathToIDCacheEntry è·¯å¾„åˆ°FileIDæ˜ å°„ç¼“å­˜æ¡ç›®
type PathToIDCacheEntry struct {
	Path     string    `json:"path"`
	FileID   string    `json:"file_id"`
	IsDir    bool      `json:"is_dir"`
	ParentID string    `json:"parent_id"`
	CachedAt time.Time `json:"cached_at"`
}

// MemoryManager ç®¡ç†å°æ–‡ä»¶ç¼“å­˜çš„å†…å­˜ä½¿ç”¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
type MemoryManager struct {
	mu          sync.Mutex
	totalUsed   int64                  // å½“å‰ä½¿ç”¨çš„æ€»å†…å­˜
	maxTotal    int64                  // æœ€å¤§å…è®¸çš„æ€»å†…å­˜ï¼ˆé»˜è®¤200MBï¼‰
	fileBuffers map[string]*BufferInfo // æ–‡ä»¶ID -> ç¼“å†²åŒºä¿¡æ¯
	maxFileSize int64                  // å•ä¸ªæ–‡ä»¶æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆé»˜è®¤50MBï¼‰
}

// BufferInfo ç¼“å†²åŒºä¿¡æ¯ï¼Œç”¨äºæ™ºèƒ½æ¸…ç†
type BufferInfo struct {
	Size        int64     // ç¼“å†²åŒºå¤§å°
	LastAccess  time.Time // æœ€åè®¿é—®æ—¶é—´
	AccessCount int64     // è®¿é—®æ¬¡æ•°
}

// NewMemoryManager åˆ›å»ºæ–°çš„å†…å­˜ç®¡ç†å™¨
func NewMemoryManager(maxTotal, maxFileSize int64) *MemoryManager {
	if maxTotal <= 0 {
		maxTotal = 200 * 1024 * 1024 // é»˜è®¤200MBæ€»é™åˆ¶
	}
	if maxFileSize <= 0 {
		maxFileSize = 50 * 1024 * 1024 // é»˜è®¤50MBå•æ–‡ä»¶é™åˆ¶
	}

	return &MemoryManager{
		maxTotal:    maxTotal,
		maxFileSize: maxFileSize,
		fileBuffers: make(map[string]*BufferInfo),
	}
}

// CanAllocate æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ†é…æŒ‡å®šå¤§å°çš„å†…å­˜ï¼Œå¦‚æœç©ºé—´ä¸è¶³ä¼šå°è¯•æ¸…ç†
func (mm *MemoryManager) CanAllocate(size int64) bool {
	if size <= 0 {
		return false
	}

	// æ£€æŸ¥å•æ–‡ä»¶å¤§å°é™åˆ¶
	if size > mm.maxFileSize {
		return false
	}

	mm.mu.Lock()
	defer mm.mu.Unlock()

	// å¦‚æœç©ºé—´è¶³å¤Ÿï¼Œç›´æ¥è¿”å›
	if mm.totalUsed+size <= mm.maxTotal {
		return true
	}

	// ç©ºé—´ä¸è¶³ï¼Œå°è¯•æ¸…ç†æ—§ç¼“å­˜
	return mm.tryCleanupMemory(size)
}

// tryCleanupMemory å°è¯•æ¸…ç†å†…å­˜ä»¥è…¾å‡ºç©ºé—´
func (mm *MemoryManager) tryCleanupMemory(requiredSize int64) bool {
	// æ”¶é›†å¯æ¸…ç†çš„ç¼“å†²åŒºï¼ˆè¶…è¿‡5åˆ†é’Ÿæœªè®¿é—®çš„ï¼‰
	var candidates []string
	now := time.Now()

	for fileID, bufferInfo := range mm.fileBuffers {
		if now.Sub(bufferInfo.LastAccess) > BufferCleanupThreshold {
			candidates = append(candidates, fileID)
		}
	}

	// æŒ‰è®¿é—®æ¬¡æ•°æ’åºï¼Œä¼˜å…ˆæ¸…ç†è®¿é—®æ¬¡æ•°å°‘çš„
	sort.Slice(candidates, func(i, j int) bool {
		return mm.fileBuffers[candidates[i]].AccessCount < mm.fileBuffers[candidates[j]].AccessCount
	})

	// é€ä¸ªæ¸…ç†ç›´åˆ°æœ‰è¶³å¤Ÿç©ºé—´
	freedSize := int64(0)
	for _, fileID := range candidates {
		if bufferInfo, exists := mm.fileBuffers[fileID]; exists {
			freedSize += bufferInfo.Size
			delete(mm.fileBuffers, fileID)
			mm.totalUsed -= bufferInfo.Size

			// æ£€æŸ¥æ˜¯å¦å·²æœ‰è¶³å¤Ÿç©ºé—´
			if mm.totalUsed+requiredSize <= mm.maxTotal {
				return true
			}
		}
	}

	return false
}

// Allocate åˆ†é…å†…å­˜å¹¶è®°å½•ä½¿ç”¨æƒ…å†µ
func (mm *MemoryManager) Allocate(fileID string, size int64) bool {
	if size <= 0 || size > mm.maxFileSize {
		return false
	}

	mm.mu.Lock()
	defer mm.mu.Unlock()

	// æ£€æŸ¥æ˜¯å¦å·²ç»åˆ†é…è¿‡
	if _, exists := mm.fileBuffers[fileID]; exists {
		return false
	}

	// æ£€æŸ¥æ€»å†…å­˜é™åˆ¶
	if mm.totalUsed+size > mm.maxTotal {
		return false
	}

	// åˆ†é…å†…å­˜
	mm.fileBuffers[fileID] = &BufferInfo{
		Size:        size,
		LastAccess:  time.Now(),
		AccessCount: 1,
	}
	mm.totalUsed += size
	return true
}

// Release é‡Šæ”¾æŒ‡å®šæ–‡ä»¶çš„å†…å­˜
func (mm *MemoryManager) Release(fileID string) {
	mm.mu.Lock()
	defer mm.mu.Unlock()

	if bufferInfo, exists := mm.fileBuffers[fileID]; exists {
		delete(mm.fileBuffers, fileID)
		mm.totalUsed -= bufferInfo.Size
		if mm.totalUsed < 0 {
			mm.totalUsed = 0 // é˜²æ­¢è´Ÿæ•°
		}
	}
}

// GetStats è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡
func (mm *MemoryManager) GetStats() (totalUsed, maxTotal int64, activeFiles int) {
	mm.mu.Lock()
	defer mm.mu.Unlock()

	return mm.totalUsed, mm.maxTotal, len(mm.fileBuffers)
}

// AccessBuffer è®°å½•ç¼“å†²åŒºè®¿é—®ï¼Œç”¨äºLRUæ¸…ç†ç­–ç•¥
func (mm *MemoryManager) AccessBuffer(fileID string) {
	mm.mu.Lock()
	defer mm.mu.Unlock()

	if bufferInfo, exists := mm.fileBuffers[fileID]; exists {
		bufferInfo.LastAccess = time.Now()
		bufferInfo.AccessCount++
	}
}

// PerformanceMetrics æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
type PerformanceMetrics struct {
	mu                 sync.RWMutex
	startTime          time.Time
	totalUploads       int64
	totalDownloads     int64
	totalUploadBytes   int64
	totalDownloadBytes int64
	uploadErrors       int64
	downloadErrors     int64
	avgUploadSpeed     float64 // MB/s
	avgDownloadSpeed   float64 // MB/s
	peakUploadSpeed    float64 // MB/s
	peakDownloadSpeed  float64 // MB/s
	activeUploads      int64
	activeDownloads    int64
	memoryUsage        int64 // å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆå­—èŠ‚ï¼‰
	peakMemoryUsage    int64 // å³°å€¼å†…å­˜ä½¿ç”¨é‡ï¼ˆå­—èŠ‚ï¼‰
	apiCallCount       int64 // APIè°ƒç”¨æ€»æ•°
	apiErrorCount      int64 // APIé”™è¯¯æ€»æ•°
	retryCount         int64 // é‡è¯•æ€»æ•°
	partialFileCount   int64 // partialæ–‡ä»¶æ•°é‡
}

// NewPerformanceMetrics åˆ›å»ºæ–°çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
func NewPerformanceMetrics() *PerformanceMetrics {
	return &PerformanceMetrics{
		startTime: time.Now(),
	}
}

// RecordUploadStart è®°å½•ä¸Šä¼ å¼€å§‹
func (pm *PerformanceMetrics) RecordUploadStart(size int64) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.activeUploads++
	pm.totalUploads++
	pm.totalUploadBytes += size
}

// RecordUploadComplete è®°å½•ä¸Šä¼ å®Œæˆ
func (pm *PerformanceMetrics) RecordUploadComplete(size int64, duration time.Duration) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.activeUploads--

	if duration > 0 {
		speedMBps := float64(size) / (1024 * 1024) / duration.Seconds()
		pm.updateUploadSpeed(speedMBps)
	}
}

// RecordUploadError è®°å½•ä¸Šä¼ é”™è¯¯
func (pm *PerformanceMetrics) RecordUploadError() {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.uploadErrors++
	if pm.activeUploads > 0 {
		pm.activeUploads--
	}
}

// RecordDownloadStart è®°å½•ä¸‹è½½å¼€å§‹
func (pm *PerformanceMetrics) RecordDownloadStart(size int64) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.activeDownloads++
	pm.totalDownloads++
	pm.totalDownloadBytes += size
}

// RecordDownloadComplete è®°å½•ä¸‹è½½å®Œæˆ
func (pm *PerformanceMetrics) RecordDownloadComplete(size int64, duration time.Duration) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.activeDownloads--

	if duration > 0 {
		speedMBps := float64(size) / (1024 * 1024) / duration.Seconds()
		pm.updateDownloadSpeed(speedMBps)
	}
}

// RecordDownloadError è®°å½•ä¸‹è½½é”™è¯¯
func (pm *PerformanceMetrics) RecordDownloadError() {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.downloadErrors++
	if pm.activeDownloads > 0 {
		pm.activeDownloads--
	}
}

// RecordAPICall è®°å½•APIè°ƒç”¨
func (pm *PerformanceMetrics) RecordAPICall() {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.apiCallCount++
}

// RecordAPIError è®°å½•APIé”™è¯¯
func (pm *PerformanceMetrics) RecordAPIError() {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.apiErrorCount++
}

// RecordRetry è®°å½•é‡è¯•
func (pm *PerformanceMetrics) RecordRetry() {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.retryCount++
}

// UpdateMemoryUsage æ›´æ–°å†…å­˜ä½¿ç”¨æƒ…å†µ
func (pm *PerformanceMetrics) UpdateMemoryUsage(current int64) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	pm.memoryUsage = current
	if current > pm.peakMemoryUsage {
		pm.peakMemoryUsage = current
	}
}

// RecordPartialFile è®°å½•partialæ–‡ä»¶
func (pm *PerformanceMetrics) RecordPartialFile(increment bool) {
	pm.mu.Lock()
	defer pm.mu.Unlock()
	if increment {
		pm.partialFileCount++
	} else if pm.partialFileCount > 0 {
		pm.partialFileCount--
	}
}

// updateUploadSpeed æ›´æ–°ä¸Šä¼ é€Ÿåº¦ç»Ÿè®¡ - æ”¹è¿›çš„ç®—æ³•
func (pm *PerformanceMetrics) updateUploadSpeed(speedMBps float64) {
	if speedMBps > pm.peakUploadSpeed {
		pm.peakUploadSpeed = speedMBps
	}

	// æ”¹è¿›çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ç®—æ³•
	// ä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œæ–°æ•°æ®æƒé‡æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´
	alpha := pm.calculateDynamicAlpha(pm.totalUploads)
	if pm.avgUploadSpeed == 0 {
		pm.avgUploadSpeed = speedMBps
	} else {
		pm.avgUploadSpeed = pm.avgUploadSpeed*(1-alpha) + speedMBps*alpha
	}
}

// calculateDynamicAlpha è®¡ç®—åŠ¨æ€æƒé‡ç³»æ•°
func (pm *PerformanceMetrics) calculateDynamicAlpha(sampleCount int64) float64 {
	// æ ¹æ®æ ·æœ¬æ•°é‡åŠ¨æ€è°ƒæ•´æƒé‡
	// æ ·æœ¬è¶Šå¤šï¼Œæ–°æ•°æ®æƒé‡è¶Šå°ï¼Œç¡®ä¿ç¨³å®šæ€§
	// æ ·æœ¬è¶Šå°‘ï¼Œæ–°æ•°æ®æƒé‡è¶Šå¤§ï¼Œç¡®ä¿å¿«é€Ÿå“åº”
	if sampleCount <= 1 {
		return 1.0 // ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼Œå®Œå…¨é‡‡ç”¨æ–°å€¼
	} else if sampleCount <= 10 {
		return 0.3 // å‰10ä¸ªæ ·æœ¬ï¼Œè¾ƒé«˜æƒé‡
	} else if sampleCount <= 50 {
		return 0.15 // ä¸­ç­‰æ ·æœ¬æ•°ï¼Œä¸­ç­‰æƒé‡
	} else {
		return 0.05 // å¤§é‡æ ·æœ¬ï¼Œè¾ƒä½æƒé‡ï¼Œä¿æŒç¨³å®š
	}
}

// updateDownloadSpeed æ›´æ–°ä¸‹è½½é€Ÿåº¦ç»Ÿè®¡ - æ”¹è¿›çš„ç®—æ³•
func (pm *PerformanceMetrics) updateDownloadSpeed(speedMBps float64) {
	if speedMBps > pm.peakDownloadSpeed {
		pm.peakDownloadSpeed = speedMBps
	}

	// æ”¹è¿›çš„æŒ‡æ•°åŠ æƒç§»åŠ¨å¹³å‡ç®—æ³•
	// ä½¿ç”¨åŠ¨æ€æƒé‡ï¼Œæ–°æ•°æ®æƒé‡æ ¹æ®æ ·æœ¬æ•°é‡è°ƒæ•´
	alpha := pm.calculateDynamicAlpha(pm.totalDownloads)
	if pm.avgDownloadSpeed == 0 {
		pm.avgDownloadSpeed = speedMBps
	} else {
		pm.avgDownloadSpeed = pm.avgDownloadSpeed*(1-alpha) + speedMBps*alpha
	}
}

// GetStats è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯ - æ”¹è¿›çš„ç²¾åº¦å’Œè¯¦ç»†ç¨‹åº¦
func (pm *PerformanceMetrics) GetStats() map[string]any {
	pm.mu.RLock()
	defer pm.mu.RUnlock()

	uptime := time.Since(pm.startTime)

	// è®¡ç®—æ›´ç²¾ç¡®çš„é”™è¯¯ç‡
	var uploadErrorRate, downloadErrorRate, apiErrorRate float64
	if pm.totalUploads > 0 {
		uploadErrorRate = float64(pm.uploadErrors) / float64(pm.totalUploads)
	}
	if pm.totalDownloads > 0 {
		downloadErrorRate = float64(pm.downloadErrors) / float64(pm.totalDownloads)
	}
	if pm.apiCallCount > 0 {
		apiErrorRate = float64(pm.apiErrorCount) / float64(pm.apiCallCount)
	}

	// è®¡ç®—ååé‡æ•ˆç‡
	var uploadEfficiency, downloadEfficiency float64
	if pm.peakUploadSpeed > 0 {
		uploadEfficiency = pm.avgUploadSpeed / pm.peakUploadSpeed
	}
	if pm.peakDownloadSpeed > 0 {
		downloadEfficiency = pm.avgDownloadSpeed / pm.peakDownloadSpeed
	}

	stats := map[string]any{
		"uptime_seconds":           uptime.Seconds(),
		"total_uploads":            pm.totalUploads,
		"total_downloads":          pm.totalDownloads,
		"total_upload_bytes":       pm.totalUploadBytes,
		"total_download_bytes":     pm.totalDownloadBytes,
		"upload_errors":            pm.uploadErrors,
		"download_errors":          pm.downloadErrors,
		"active_uploads":           pm.activeUploads,
		"active_downloads":         pm.activeDownloads,
		"avg_upload_speed_mbps":    pm.avgUploadSpeed,
		"avg_download_speed_mbps":  pm.avgDownloadSpeed,
		"peak_upload_speed_mbps":   pm.peakUploadSpeed,
		"peak_download_speed_mbps": pm.peakDownloadSpeed,
		"memory_usage_bytes":       pm.memoryUsage,
		"peak_memory_usage_bytes":  pm.peakMemoryUsage,
		"api_call_count":           pm.apiCallCount,
		"api_error_count":          pm.apiErrorCount,
		"retry_count":              pm.retryCount,
		"partial_file_count":       pm.partialFileCount,
		// æ–°å¢çš„ç²¾ç¡®æŒ‡æ ‡
		"upload_error_rate":   uploadErrorRate,
		"download_error_rate": downloadErrorRate,
		"api_error_rate":      apiErrorRate,
		"upload_efficiency":   uploadEfficiency,
		"download_efficiency": downloadEfficiency,
	}

	return stats
}

// LogStats è®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—
func (pm *PerformanceMetrics) LogStats(f *Fs) {
	stats := pm.GetStats()

	// å®‰å…¨åœ°è·å–é”™è¯¯ç‡
	var apiErrorRate float64
	if rate, ok := stats["api_error_rate"].(float64); ok {
		apiErrorRate = rate * 100
	}

	fs.Infof(f, "æ€§èƒ½ç»Ÿè®¡: ä¸Šä¼ %dä¸ªæ–‡ä»¶(%.2fMB/så¹³å‡, %.2fMB/så³°å€¼), ä¸‹è½½%dä¸ªæ–‡ä»¶(%.2fMB/så¹³å‡, %.2fMB/så³°å€¼), APIè°ƒç”¨%dæ¬¡, é”™è¯¯ç‡%.2f%%",
		stats["total_uploads"],
		stats["avg_upload_speed_mbps"],
		stats["peak_upload_speed_mbps"],
		stats["total_downloads"],
		stats["avg_download_speed_mbps"],
		stats["peak_download_speed_mbps"],
		stats["api_call_count"],
		apiErrorRate,
	)
}

// ProgressReadCloser åŒ…è£…ReadCloserä»¥æä¾›è¿›åº¦è·Ÿè¸ªå’Œèµ„æºç®¡ç†
type ProgressReadCloser struct {
	io.ReadCloser
	fs              *Fs
	totalSize       int64
	transferredSize int64
	startTime       time.Time // ä¸‹è½½å¼€å§‹æ—¶é—´
}

// Read å®ç°io.Readeræ¥å£ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦
func (prc *ProgressReadCloser) Read(p []byte) (n int, err error) {
	n, err = prc.ReadCloser.Read(p)
	if n > 0 {
		prc.transferredSize += int64(n)
		// rcloneå†…ç½®çš„accountingä¼šè‡ªåŠ¨å¤„ç†è¿›åº¦è·Ÿè¸ª
	}
	return n, err
}

// Close å®ç°io.Closeræ¥å£ï¼ŒåŒæ—¶æ¸…ç†èµ„æº
func (prc *ProgressReadCloser) Close() error {
	// é‡Šæ”¾ä¸‹è½½ä¿¡å·é‡
	prc.fs.concurrencyManager.ReleaseDownload()

	// è®°å½•ä¸‹è½½å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(prc.startTime)
	success := prc.transferredSize == prc.totalSize
	if success {
		prc.fs.performanceMetrics.RecordDownloadComplete(prc.transferredSize, duration)
	} else {
		prc.fs.performanceMetrics.RecordDownloadError()
	}

	// å…³é—­åº•å±‚ReadCloser
	return prc.ReadCloser.Close()
}

// Object æè¿°123ç½‘ç›˜ä¸­çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å¯¹è±¡
type Object struct {
	fs          *Fs       // çˆ¶æ–‡ä»¶ç³»ç»Ÿå®ä¾‹
	remote      string    // è¿œç¨‹è·¯å¾„
	hasMetaData bool      // æ˜¯å¦å·²åŠ è½½å…ƒæ•°æ®
	id          string    // æ–‡ä»¶/æ–‡ä»¶å¤¹ID
	size        int64     // æ–‡ä»¶å¤§å°
	md5sum      string    // MD5å“ˆå¸Œå€¼
	modTime     time.Time // ä¿®æ”¹æ—¶é—´
	// createTime  time.Time // åˆ›å»ºæ—¶é—´ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
	isDir bool // æ˜¯å¦ä¸ºç›®å½•
}

// ConcurrentDownloadReader å¹¶å‘ä¸‹è½½çš„æ–‡ä»¶è¯»å–å™¨
type ConcurrentDownloadReader struct {
	file     *os.File
	tempPath string
}

// Read å®ç°io.Readeræ¥å£
func (r *ConcurrentDownloadReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

// Close å®ç°io.Closeræ¥å£ï¼Œå…³é—­æ–‡ä»¶å¹¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
func (r *ConcurrentDownloadReader) Close() error {
	var err error
	if r.file != nil {
		err = r.file.Close()
		r.file = nil
	}
	if r.tempPath != "" {
		if removeErr := os.Remove(r.tempPath); removeErr != nil {
			fs.Debugf(nil, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", r.tempPath, removeErr)
		} else {
			fs.Debugf(nil, "å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: %s", r.tempPath)
		}
		r.tempPath = ""
	}
	return err
}

// æ–‡ä»¶åéªŒè¯ç›¸å…³çš„å…¨å±€å˜é‡
var (
	// invalidCharsRegex ç”¨äºåŒ¹é…123ç½‘ç›˜ä¸å…è®¸çš„æ–‡ä»¶åå­—ç¬¦çš„æ­£åˆ™è¡¨è¾¾å¼
	invalidCharsRegex = regexp.MustCompile(`["\/:*?|><\\]`)
)

// validateFileName éªŒè¯æ–‡ä»¶åæ˜¯å¦ç¬¦åˆ123ç½‘ç›˜çš„é™åˆ¶è§„åˆ™ï¼ˆå…¨å±€å‡½æ•°ç‰ˆæœ¬ï¼‰
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†éªŒè¯åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè§„åˆ™
// æ³¨æ„ï¼šæ¨èä½¿ç”¨Fs.validateFileNameUnifiedæ–¹æ³•ä»¥è·å¾—æ›´å®Œæ•´çš„éªŒè¯
func validateFileName(name string) error {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„è·¯å¾„æ£€æŸ¥
	if strings.Contains(name, "/") || strings.Contains(name, "\\") {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½åŒ…å«è·¯å¾„åˆ†éš”ç¬¦")
	}

	// åŸºç¡€æ£€æŸ¥ï¼šç©ºå€¼å’Œç©ºæ ¼ - ä½¿ç”¨é€šç”¨å·¥å…·
	if StringUtil.IsEmpty(name) {
		return fmt.Errorf("æ–‡ä»¶åä¸èƒ½ä¸ºç©ºæˆ–å…¨éƒ¨æ˜¯ç©ºæ ¼")
	}

	// UTF-8æœ‰æ•ˆæ€§æ£€æŸ¥
	if !utf8.ValidString(name) {
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«æ— æ•ˆçš„UTF-8å­—ç¬¦")
	}

	// 123ç½‘ç›˜ç‰¹å®šçš„é•¿åº¦æ£€æŸ¥ï¼ˆå­—ç¬¦æ•°ï¼‰
	runeCount := utf8.RuneCountInString(name)
	if runeCount > maxFileNameLength {
		return fmt.Errorf("æ–‡ä»¶åé•¿åº¦è¶…è¿‡é™åˆ¶ï¼šå½“å‰%dä¸ªå­—ç¬¦ï¼Œæœ€å¤§å…è®¸%dä¸ªå­—ç¬¦",
			runeCount, maxFileNameLength)
	}

	// å­—èŠ‚é•¿åº¦æ£€æŸ¥
	byteLength := len([]byte(name))
	if byteLength > maxFileNameBytes {
		return fmt.Errorf("æ–‡ä»¶åå­—èŠ‚é•¿åº¦è¶…è¿‡%d: %d", maxFileNameBytes, byteLength)
	}

	// ç¦ç”¨å­—ç¬¦æ£€æŸ¥
	if invalidCharsRegex.MatchString(name) {
		invalidFound := invalidCharsRegex.FindAllString(name, -1)
		return fmt.Errorf("æ–‡ä»¶ååŒ…å«ä¸å…è®¸çš„å­—ç¬¦ï¼š%vï¼Œ123ç½‘ç›˜ä¸å…è®¸ä½¿ç”¨ä»¥ä¸‹å­—ç¬¦ï¼š%s",
			invalidFound, invalidChars)
	}

	return nil
}

// cleanFileName æ¸…ç†æ–‡ä»¶åï¼Œå°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºå®‰å…¨å­—ç¬¦
// åŒæ—¶ç¡®ä¿æ–‡ä»¶åé•¿åº¦ä¸è¶…è¿‡é™åˆ¶
func cleanFileName(name string) string {
	if name == "" {
		return "æœªå‘½åæ–‡ä»¶"
	}

	// æ›¿æ¢éæ³•å­—ç¬¦
	cleaned := invalidCharsRegex.ReplaceAllString(name, replacementChar)

	// å¦‚æœæ–‡ä»¶åè¿‡é•¿ï¼Œè¿›è¡Œæˆªæ–­å¤„ç†
	if utf8.RuneCountInString(cleaned) > maxFileNameLength {
		// ä¿ç•™æ–‡ä»¶æ‰©å±•å
		ext := filepath.Ext(cleaned)
		nameWithoutExt := strings.TrimSuffix(cleaned, ext)

		// è®¡ç®—å¯ç”¨çš„æ–‡ä»¶åé•¿åº¦ï¼ˆæ€»é•¿åº¦ - æ‰©å±•åé•¿åº¦ï¼‰
		maxNameLength := maxFileNameLength - utf8.RuneCountInString(ext)
		if maxNameLength < 1 {
			// å¦‚æœæ‰©å±•åå¤ªé•¿ï¼Œåªä¿ç•™éƒ¨åˆ†æ‰©å±•å
			maxNameLength = maxFileNameLength - 10 // ä¿ç•™è‡³å°‘10ä¸ªå­—ç¬¦ç»™æ–‡ä»¶å
			if maxNameLength < 1 {
				maxNameLength = 1
			}
			extRunes := []rune(ext)
			if len(extRunes) > 10 {
				ext = string(extRunes[:10])
			}
		}

		// æˆªæ–­æ–‡ä»¶åä¸»ä½“éƒ¨åˆ†
		nameRunes := []rune(nameWithoutExt)
		if len(nameRunes) > maxNameLength {
			nameWithoutExt = string(nameRunes[:maxNameLength])
		}

		cleaned = nameWithoutExt + ext
	}

	return cleaned
}

// validateAndCleanFileName éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶åçš„ä¾¿æ·å‡½æ•°
// å¦‚æœéªŒè¯å¤±è´¥ï¼Œè¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
func validateAndCleanFileName(name string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := validateFileName(name); err == nil {
		return name, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(name)

	// è¿”å›æ¸…ç†åçš„æ–‡ä»¶åå’Œè­¦å‘Šä¿¡æ¯
	warning = fmt.Errorf("æ–‡ä»¶åå·²è‡ªåŠ¨æ¸…ç†ï¼šåŸå§‹åç§° '%s' -> æ¸…ç†ååç§° '%s'", name, cleanedName)
	return cleanedName, warning
}

// generateUniqueFileName ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª
// å¦‚æœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œä¼šæ·»åŠ æ•°å­—åç¼€
func (f *Fs) generateUniqueFileName(ctx context.Context, parentFileID int64, baseName string) (string, error) {
	// é¦–å…ˆæ£€æŸ¥åŸå§‹æ–‡ä»¶åæ˜¯å¦å­˜åœ¨
	exists, err := f.fileExists(ctx, parentFileID, baseName)
	if err != nil {
		return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
	}

	if !exists {
		return baseName, nil
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
	ext := filepath.Ext(baseName)
	nameWithoutExt := strings.TrimSuffix(baseName, ext)

	for i := 1; i <= 999; i++ {
		newName := fmt.Sprintf("%s (%d)%s", nameWithoutExt, i, ext)

		// éªŒè¯æ–°æ–‡ä»¶åé•¿åº¦
		if len([]byte(newName)) > maxFileNameBytes {
			// å¦‚æœå¤ªé•¿ï¼Œæˆªæ–­åŸºç¡€åç§°
			maxBaseLength := maxFileNameBytes - len([]byte(fmt.Sprintf(" (%d)%s", i, ext)))
			if maxBaseLength > 0 {
				truncatedBase := string([]byte(nameWithoutExt)[:maxBaseLength])
				newName = fmt.Sprintf("%s (%d)%s", truncatedBase, i, ext)
			} else {
				// å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œä½¿ç”¨ç®€åŒ–åç§°
				newName = fmt.Sprintf("file_%d%s", i, ext)
			}
		}

		exists, err := f.fileExists(ctx, parentFileID, newName)
		if err != nil {
			return "", fmt.Errorf("æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %w", err)
		}

		if !exists {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
			return newName, nil
		}
	}

	// å¦‚æœå°è¯•äº†999æ¬¡éƒ½æ²¡æœ‰æ‰¾åˆ°å”¯ä¸€åç§°ï¼Œä½¿ç”¨æ—¶é—´æˆ³
	timestamp := time.Now().Unix()
	newName := fmt.Sprintf("%s_%d%s", nameWithoutExt, timestamp, ext)
	fs.Debugf(f, "ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å: %s -> %s", baseName, newName)
	return newName, nil
}

// fileExists æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExists(ctx context.Context, parentFileID int64, fileName string) (bool, error) {
	// ä½¿ç”¨ListFile APIæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
	response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", 0)
	if err != nil {
		return false, err
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			return true, nil
		}
	}

	return false, nil
}

// verifyParentFileID éªŒè¯çˆ¶ç›®å½•IDæ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨BadgerDBç¼“å­˜å‡å°‘APIè°ƒç”¨
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) verifyParentFileID(ctx context.Context, parentFileID int64) (bool, error) {
	// é¦–å…ˆæ£€æŸ¥BadgerDBç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
	cacheKey := fmt.Sprintf("parent_%d", parentFileID)
	if f.parentIDCache != nil {
		// ä½¿ç”¨è¯»é”ä¿æŠ¤ç¼“å­˜è¯»å–
		f.cacheMu.RLock()
		cached, found := f.parentIDCache.GetBool(cacheKey)
		f.cacheMu.RUnlock()

		if found {
			f.debugf(LogLevelVerbose, "çˆ¶ç›®å½•ID %d BadgerDBç¼“å­˜éªŒè¯é€šè¿‡: %v", parentFileID, cached)
			return cached, nil
		}
	}

	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID: %d", parentFileID)

	// æ‰§è¡Œå®é™…çš„APIéªŒè¯
	response, err := f.ListFile(ctx, int(parentFileID), 1, "", "", 0)
	if err != nil {
		fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %v", parentFileID, err)
		return false, err
	}

	isValid := response.Code == 0
	if !isValid {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼ŒAPIè¿”å›: code=%d, message=%s", parentFileID, response.Code, response.Message)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// ç¼“å­˜ç»“æœåˆ°BadgerDBï¼ˆæˆåŠŸå’Œå¤±è´¥éƒ½ç¼“å­˜ï¼Œé¿å…é‡å¤éªŒè¯æ— æ•ˆIDï¼‰
	if f.parentIDCache != nil {
		// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥
		f.cacheMu.Lock()
		err := f.parentIDCache.SetBool(cacheKey, isValid, f.cacheConfig.ParentIDCacheTTL)
		f.cacheMu.Unlock()

		if err != nil {
			fs.Debugf(f, "ä¿å­˜çˆ¶ç›®å½•ID %d ç¼“å­˜å¤±è´¥: %v", parentFileID, err)
		} else {
			f.debugf(LogLevelVerbose, "å·²ä¿å­˜çˆ¶ç›®å½•ID %d åˆ°BadgerDBç¼“å­˜: valid=%v, TTL=%v", parentFileID, isValid, f.cacheConfig.ParentIDCacheTTL)
		}
	}

	return isValid, nil
}

// clearParentIDCache æ¸…ç†parentFileIDç¼“å­˜ä¸­çš„ç‰¹å®šæ¡ç›®
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearParentIDCache(parentFileID ...int64) {
	if f.parentIDCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	if len(parentFileID) > 0 {
		// æ¸…ç†æŒ‡å®šçš„parentFileID
		for _, id := range parentFileID {
			cacheKey := fmt.Sprintf("parent_%d", id)
			if err := f.parentIDCache.Delete(cacheKey); err != nil {
				fs.Debugf(f, "æ¸…ç†çˆ¶ç›®å½•ID %d ç¼“å­˜å¤±è´¥: %v", id, err)
			} else {
				fs.Debugf(f, "å·²æ¸…ç†çˆ¶ç›®å½•ID %d çš„ç¼“å­˜", id)
			}
		}
	} else {
		// æ¸…ç†æ‰€æœ‰ç¼“å­˜
		if err := f.parentIDCache.Clear(); err != nil {
			fs.Debugf(f, "æ¸…ç†æ‰€æœ‰çˆ¶ç›®å½•IDç¼“å­˜å¤±è´¥: %v", err)
		} else {
			fs.Debugf(f, "å·²æ¸…ç†æ‰€æœ‰çˆ¶ç›®å½•IDç¼“å­˜")
		}
	}
}

// invalidateRelatedCaches æ™ºèƒ½ç¼“å­˜å¤±æ•ˆ - æ ¹æ®æ“ä½œç±»å‹æ¸…ç†ç›¸å…³ç¼“å­˜
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šç®€åŒ–ç¼“å­˜å¤±æ•ˆé€»è¾‘ï¼Œæé«˜å¯é æ€§
func (f *Fs) invalidateRelatedCaches(path string, operation string) {
	fs.Debugf(f, "è½»é‡çº§ç¼“å­˜å¤±æ•ˆ: æ“ä½œ=%s, è·¯å¾„=%s", operation, path)

	// ğŸš€ ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼šå¯¹æ‰€æœ‰æ–‡ä»¶æ“ä½œéƒ½è¿›è¡Œå…¨é¢ç¼“å­˜æ¸…ç†
	// è™½ç„¶ä¼šå½±å“ä¸€äº›æ€§èƒ½ï¼Œä½†ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œé¿å…å¤æ‚çš„æ¡ä»¶åˆ¤æ–­

	switch operation {
	case "upload", "put", "mkdir", "delete", "remove", "rmdir", "rename", "move":
		// ç»Ÿä¸€å¤„ç†ï¼šæ¸…ç†å½“å‰è·¯å¾„å’Œçˆ¶è·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
		f.clearAllRelatedCaches(path)

		// é¢å¤–æ¸…ç†çˆ¶ç›®å½•ç¼“å­˜
		parentPath := filepath.Dir(path)
		if parentPath != "." && parentPath != "/" {
			f.clearAllRelatedCaches(parentPath)
		}

		fs.Debugf(f, "å·²æ¸…ç†è·¯å¾„ %s å’Œçˆ¶è·¯å¾„ %s çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜", path, parentPath)
	}
}

// clearAllRelatedCaches æ¸…ç†æŒ‡å®šè·¯å¾„çš„æ‰€æœ‰ç›¸å…³ç¼“å­˜
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šæ–°å¢ç»Ÿä¸€ç¼“å­˜æ¸…ç†å‡½æ•°
func (f *Fs) clearAllRelatedCaches(path string) {
	// æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜
	f.clearPathToIDCache(path)

	// æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šæ¸…ç†æ‰€æœ‰ï¼‰
	if f.dirListCache != nil {
		f.cacheMu.Lock()
		err := f.dirListCache.Clear()
		f.cacheMu.Unlock()
		if err != nil {
			fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥: %v", err)
		}
	}

	// æ¸…ç†çˆ¶ç›®å½•IDç¼“å­˜ï¼ˆç®€å•ç­–ç•¥ï¼šæ¸…ç†æ‰€æœ‰ï¼‰
	if f.parentIDCache != nil {
		f.cacheMu.Lock()
		err := f.parentIDCache.Clear()
		f.cacheMu.Unlock()
		if err != nil {
			fs.Debugf(f, "æ¸…ç†çˆ¶ç›®å½•IDç¼“å­˜å¤±è´¥: %v", err)
		}
	}
}

// clearDirListCacheByPattern æ ¹æ®è·¯å¾„æ¨¡å¼æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜
// ğŸ”§ è½»é‡çº§ä¼˜åŒ–ï¼šå®ç°ç®€å•æœ‰æ•ˆçš„ç¼“å­˜æ¸…ç†ç­–ç•¥
func (f *Fs) clearDirListCacheByPattern(path string) {
	if f.dirListCache == nil {
		return
	}

	fs.Debugf(f, "è½»é‡çº§ç¼“å­˜æ¸…ç†: è·¯å¾„=%s", path)

	// ğŸš€ ç®€å•æœ‰æ•ˆçš„ç­–ç•¥ï¼šæ¸…ç†æ‰€æœ‰ç›®å½•åˆ—è¡¨ç¼“å­˜
	// è™½ç„¶ä¼šå½±å“ä¸€äº›æ€§èƒ½ï¼Œä½†ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼Œé¿å…å¤æ‚çš„æ¨¡å¼åŒ¹é…
	f.cacheMu.Lock()
	err := f.dirListCache.Clear()
	f.cacheMu.Unlock()

	if err != nil {
		fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†æ‰€æœ‰ç›®å½•åˆ—è¡¨ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
	}
}

// getDirListFromCache ä»ç¼“å­˜è·å–ç›®å½•åˆ—è¡¨
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) getDirListFromCache(parentFileID int64, lastFileID int64) (*DirListCacheEntry, bool) {
	if f.dirListCache == nil {
		return nil, false
	}

	// ä½¿ç”¨è¯»é”ä¿æŠ¤ç¼“å­˜è¯»å–æ“ä½œ
	f.cacheMu.RLock()
	defer f.cacheMu.RUnlock()

	cacheKey := fmt.Sprintf("dirlist_%d_%d", parentFileID, lastFileID)
	var entry DirListCacheEntry

	found, err := f.dirListCache.Get(cacheKey, &entry)
	if err != nil {
		fs.Debugf(f, "è·å–ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return nil, false
	}

	if found {
		// ä½¿ç”¨å¿«é€ŸéªŒè¯æé«˜æ€§èƒ½ï¼Œä»…åœ¨å¿…è¦æ—¶è¿›è¡Œå®Œæ•´æ ¡éªŒ
		if !fastValidateCacheEntry(entry.FileList) {
			f.debugf(LogLevelDebug, "ç›®å½•åˆ—è¡¨ç¼“å­˜å¿«é€Ÿæ ¡éªŒå¤±è´¥: %s", cacheKey)

			// éœ€è¦å†™æ“ä½œæ¥æ¸…ç†ç¼“å­˜ï¼Œå…ˆé‡Šæ”¾è¯»é”ï¼Œè·å–å†™é”
			f.cacheMu.RUnlock()
			f.cacheMu.Lock()
			// æ¸…ç†æŸåçš„ç¼“å­˜
			f.dirListCache.Delete(cacheKey)
			f.cacheMu.Unlock()
			f.cacheMu.RLock() // é‡æ–°è·å–è¯»é”ä»¥ä¿æŒdeferçš„ä¸€è‡´æ€§

			return nil, false
		}

		// å¯¹äºå…³é”®æ•°æ®ï¼Œä»è¿›è¡Œå®Œæ•´æ ¡éªŒï¼ˆä½†é¢‘ç‡è¾ƒä½ï¼‰
		if entry.Checksum != "" && len(entry.FileList) > 100 {
			if !validateCacheEntry(entry.FileList, entry.Checksum) {
				f.debugf(LogLevelDebug, "ç›®å½•åˆ—è¡¨ç¼“å­˜å®Œæ•´æ ¡éªŒå¤±è´¥: %s", cacheKey)

				// éœ€è¦å†™æ“ä½œæ¥æ¸…ç†ç¼“å­˜ï¼Œå…ˆé‡Šæ”¾è¯»é”ï¼Œè·å–å†™é”
				f.cacheMu.RUnlock()
				f.cacheMu.Lock()
				f.dirListCache.Delete(cacheKey)
				f.cacheMu.Unlock()
				f.cacheMu.RLock() // é‡æ–°è·å–è¯»é”ä»¥ä¿æŒdeferçš„ä¸€è‡´æ€§

				return nil, false
			}
		}

		f.debugf(LogLevelVerbose, "ç›®å½•åˆ—è¡¨ç¼“å­˜å‘½ä¸­: parentID=%d, lastFileID=%d, æ–‡ä»¶æ•°=%d, ç‰ˆæœ¬=%d",
			parentFileID, lastFileID, len(entry.FileList), entry.Version)
		return &entry, true
	}

	return nil, false
}

// saveDirListToCache ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) saveDirListToCache(parentFileID int64, lastFileID int64, fileList []FileListInfoRespDataV2, nextLastFileID int64) {
	if f.dirListCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	cacheKey := fmt.Sprintf("dirlist_%d_%d", parentFileID, lastFileID)
	entry := DirListCacheEntry{
		FileList:   fileList,
		LastFileID: nextLastFileID,
		TotalCount: len(fileList),
		CachedAt:   time.Now(),
		ParentID:   parentFileID,
		Version:    generateCacheVersion(),
		Checksum:   calculateChecksum(fileList),
	}

	// ä½¿ç”¨é…ç½®çš„ç›®å½•åˆ—è¡¨ç¼“å­˜TTL
	if err := f.dirListCache.Set(cacheKey, entry, f.cacheConfig.DirListCacheTTL); err != nil {
		fs.Debugf(f, "ä¿å­˜ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		f.debugf(LogLevelVerbose, "å·²ä¿å­˜ç›®å½•åˆ—è¡¨åˆ°ç¼“å­˜: parentID=%d, æ–‡ä»¶æ•°=%d, TTL=%v", parentFileID, len(fileList), f.cacheConfig.DirListCacheTTL)
	}
}

// clearDirListCache æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearDirListCache(parentFileID int64) {
	if f.dirListCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤æ“ä½œ
	f.cacheMu.Lock()
	defer f.cacheMu.Unlock()

	// æ¸…ç†æŒ‡å®šçˆ¶ç›®å½•çš„æ‰€æœ‰åˆ†é¡µç¼“å­˜
	prefix := fmt.Sprintf("dirlist_%d_", parentFileID)
	if err := f.dirListCache.DeletePrefix(prefix); err != nil {
		fs.Debugf(f, "æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜å¤±è´¥ %s: %v", prefix, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜: parentID=%d", parentFileID)
	}
}

// ğŸ—‘ï¸ getDownloadURLFromCache å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// ğŸ—‘ï¸ saveDownloadURLToCache å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// getPathToIDFromCache ä»ç¼“å­˜è·å–è·¯å¾„åˆ°FileIDçš„æ˜ å°„
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) getPathToIDFromCache(path string) (string, bool, bool) {
	if f.pathToIDCache == nil {
		return "", false, false
	}

	cacheKey := fmt.Sprintf("path_to_id_%s", path)
	var entry PathToIDCacheEntry

	// ä½¿ç”¨è¯»é”ä¿æŠ¤ç¼“å­˜è¯»å–
	f.cacheMu.RLock()
	found, err := f.pathToIDCache.Get(cacheKey, &entry)
	f.cacheMu.RUnlock()

	if err != nil {
		fs.Debugf(f, "è·å–è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
		return "", false, false
	}

	if found {
		fs.Debugf(f, "è·¯å¾„æ˜ å°„ç¼“å­˜å‘½ä¸­: %s -> %s (dir: %v)", path, entry.FileID, entry.IsDir)
		return entry.FileID, entry.IsDir, true
	}

	return "", false, false
}

// savePathToIDToCache ä¿å­˜è·¯å¾„åˆ°FileIDçš„æ˜ å°„åˆ°ç¼“å­˜
func (f *Fs) savePathToIDToCache(path, fileID, parentID string, isDir bool) {
	if f.pathToIDCache == nil {
		return
	}

	cacheKey := fmt.Sprintf("path_to_id_%s", path)
	entry := PathToIDCacheEntry{
		Path:     path,
		FileID:   fileID,
		IsDir:    isDir,
		ParentID: parentID,
		CachedAt: time.Now(),
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜å†™å…¥
	f.cacheMu.Lock()
	err := f.pathToIDCache.Set(cacheKey, entry, f.cacheConfig.PathToIDCacheTTL)
	f.cacheMu.Unlock()

	if err != nil {
		fs.Debugf(f, "ä¿å­˜è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²ä¿å­˜è·¯å¾„æ˜ å°„åˆ°ç¼“å­˜: %s -> %s (dir: %v), TTL=%v", path, fileID, isDir, f.cacheConfig.PathToIDCacheTTL)
	}
}

// clearPathToIDCache æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜
// å¢å¼ºç‰ˆæœ¬ï¼šæ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤
func (f *Fs) clearPathToIDCache(path string) {
	if f.pathToIDCache == nil {
		return
	}

	// ä½¿ç”¨å†™é”ä¿æŠ¤ç¼“å­˜åˆ é™¤
	f.cacheMu.Lock()
	cacheKey := fmt.Sprintf("path_to_id_%s", path)
	err := f.pathToIDCache.Delete(cacheKey)
	f.cacheMu.Unlock()

	if err != nil {
		fs.Debugf(f, "æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜å¤±è´¥ %s: %v", cacheKey, err)
	} else {
		fs.Debugf(f, "å·²æ¸…ç†è·¯å¾„æ˜ å°„ç¼“å­˜: %s", path)
	}
}

// verifyChunkIntegrity éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§
func (f *Fs) verifyChunkIntegrity(ctx context.Context, preuploadID string, partNumber int64) (bool, error) {
	fs.Debugf(f, "éªŒè¯åˆ†ç‰‡ %d çš„å®Œæ•´æ€§", partNumber)

	// è·å–åˆ†ç‰‡ä¿¡æ¯
	response, err := f.getUploadPartInfo(ctx, preuploadID, partNumber)
	if err != nil {
		fs.Debugf(f, "è·å–åˆ†ç‰‡ %d ä¿¡æ¯å¤±è´¥: %v", partNumber, err)
		return false, err
	}

	// æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å­˜åœ¨ä¸”çŠ¶æ€æ­£ç¡®
	if response.Code != 0 {
		fs.Debugf(f, "åˆ†ç‰‡ %d ä¸å­˜åœ¨æˆ–çŠ¶æ€å¼‚å¸¸: code=%d, message=%s", partNumber, response.Code, response.Message)
		return false, nil
	}

	// è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„å®Œæ•´æ€§æ£€æŸ¥ï¼Œæ¯”å¦‚ETagéªŒè¯
	// ç›®å‰ç®€å•æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "åˆ†ç‰‡ %d å®Œæ•´æ€§éªŒè¯é€šè¿‡", partNumber)
	return true, nil
}

// getUploadPartInfo è·å–ä¸Šä¼ åˆ†ç‰‡çš„ä¿¡æ¯
func (f *Fs) getUploadPartInfo(ctx context.Context, preuploadID string, partNumber int64) (*ListResponse, error) {
	// ç”±äº123ç½‘ç›˜APIé™åˆ¶ï¼Œæˆ‘ä»¬é€šè¿‡æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€
	if f.resumeManager != nil {
		info, err := f.resumeManager.LoadResumeInfo(preuploadID)
		if err == nil && info != nil {
			// æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼  (partNumberä»1å¼€å§‹ï¼ŒchunkIndexä»0å¼€å§‹)
			chunkIndex := partNumber - 1
			if etag, exists := info.UploadedChunks[chunkIndex]; exists {
				fs.Debugf(f, "åˆ†ç‰‡ %d å·²ä¸Šä¼ ï¼ŒETag: %s", partNumber, etag)
				return &ListResponse{
					Code:    0,
					Message: "åˆ†ç‰‡å·²å­˜åœ¨",
					Data:    GetFileListRespDataV2{},
				}, nil
			}
		}
	}

	// åˆ†ç‰‡ä¸å­˜åœ¨æˆ–æœªä¸Šä¼ 
	fs.Debugf(f, "åˆ†ç‰‡ %d éœ€è¦ä¸Šä¼ ", partNumber)
	return &ListResponse{
		Code:    1,
		Message: "åˆ†ç‰‡ä¸å­˜åœ¨",
		Data:    GetFileListRespDataV2{},
	}, nil
}

// resumeUploadWithIntegrityCheck æ¢å¤ä¸Šä¼ æ—¶è¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥
func (f *Fs) resumeUploadWithIntegrityCheck(ctx context.Context, progress *UploadProgress) error {
	fs.Debugf(f, "å¼€å§‹éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§")

	corruptedChunks := make([]int64, 0)

	// éªŒè¯æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
	uploadedParts := progress.GetUploadedParts()
	for partNumber := range uploadedParts {
		if uploadedParts[partNumber] {
			valid, err := f.verifyChunkIntegrity(ctx, progress.PreuploadID, partNumber)
			if err != nil {
				fs.Debugf(f, "éªŒè¯åˆ†ç‰‡ %d å®Œæ•´æ€§æ—¶å‡ºé”™: %v", partNumber, err)
				// å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œæ ‡è®°ä¸ºéœ€è¦é‡æ–°ä¸Šä¼ 
				corruptedChunks = append(corruptedChunks, partNumber)
			} else if !valid {
				fs.Debugf(f, "åˆ†ç‰‡ %d å®Œæ•´æ€§éªŒè¯å¤±è´¥ï¼Œéœ€è¦é‡æ–°ä¸Šä¼ ", partNumber)
				corruptedChunks = append(corruptedChunks, partNumber)
			}
		}
	}

	// æ¸…é™¤æŸåçš„åˆ†ç‰‡æ ‡è®° - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
	if len(corruptedChunks) > 0 {
		fs.Logf(f, "å‘ç° %d ä¸ªæŸåçš„åˆ†ç‰‡ï¼Œå°†é‡æ–°ä¸Šä¼ : %v", len(corruptedChunks), corruptedChunks)
		for _, partNumber := range corruptedChunks {
			progress.RemoveUploaded(partNumber)
		}

		// ä¿å­˜æ›´æ–°åçš„è¿›åº¦
		err := f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜æ›´æ–°åçš„è¿›åº¦å¤±è´¥: %v", err)
		}
	} else {
		fs.Debugf(f, "æ‰€æœ‰å·²ä¸Šä¼ åˆ†ç‰‡å®Œæ•´æ€§éªŒè¯é€šè¿‡")
	}

	return nil
}

// getParentID è·å–æ–‡ä»¶æˆ–ç›®å½•çš„çˆ¶ç›®å½•ID
func (f *Fs) getParentID(ctx context.Context, fileID int64) (int64, error) {
	f.debugf(LogLevelDebug, "å°è¯•è·å–æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID", fileID)

	// é¦–å…ˆå°è¯•ä»ç¼“å­˜æŸ¥æ‰¾
	if f.dirListCache != nil {
		// éå†ç¼“å­˜çš„ç›®å½•åˆ—è¡¨ï¼ŒæŸ¥æ‰¾åŒ…å«è¯¥æ–‡ä»¶çš„ç›®å½•
		// è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æ–¹æ³•ï¼Œé€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ
		parentID := f.searchParentIDInCache(fileID)
		if parentID > 0 {
			f.debugf(LogLevelVerbose, "ä»ç¼“å­˜æ‰¾åˆ°æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID: %d", fileID, parentID)
			return parentID, nil
		}
	}

	// å¦‚æœç¼“å­˜ä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€šè¿‡APIæŸ¥è¯¢
	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…APIè·å–æ–‡ä»¶ä¿¡æ¯
	var response struct {
		Code int `json:"code"`
		Data struct {
			ParentFileID int64 `json:"parentFileId"`
		} `json:"data"`
		Message string `json:"message"`
	}

	url := fmt.Sprintf("/api/v1/file/info?fileId=%d", fileID)
	err := f.makeAPICallWithRest(ctx, url, "GET", nil, &response)
	if err != nil {
		return 0, fmt.Errorf("è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return 0, fmt.Errorf("APIè¿”å›é”™è¯¯: %s", response.Message)
	}

	f.debugf(LogLevelDebug, "é€šè¿‡APIè·å–åˆ°æ–‡ä»¶ID %d çš„çˆ¶ç›®å½•ID: %d", fileID, response.Data.ParentFileID)
	return response.Data.ParentFileID, nil
}

// searchParentIDInCache åœ¨ç¼“å­˜ä¸­æœç´¢æ–‡ä»¶çš„çˆ¶ç›®å½•ID
func (f *Fs) searchParentIDInCache(_ int64) int64 {
	// è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æœç´¢ï¼Œéå†æœ€è¿‘çš„ç¼“å­˜æ¡ç›®
	// åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥è€ƒè™‘å»ºç«‹åå‘ç´¢å¼•æ¥ä¼˜åŒ–æŸ¥æ‰¾

	// ç”±äºBadgerDBçš„é™åˆ¶ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æœç´¢ç­–ç•¥
	// å®é™…é¡¹ç›®ä¸­å¯ä»¥è€ƒè™‘ç»´æŠ¤ä¸€ä¸ªæ–‡ä»¶IDåˆ°çˆ¶ç›®å½•IDçš„æ˜ å°„ç¼“å­˜

	return 0 // æš‚æ—¶è¿”å›0è¡¨ç¤ºæœªæ‰¾åˆ°
}

// fileExistsInDirectory æ£€æŸ¥æŒ‡å®šç›®å½•ä¸­æ˜¯å¦å­˜åœ¨æŒ‡å®šåç§°çš„æ–‡ä»¶
func (f *Fs) fileExistsInDirectory(ctx context.Context, parentID int64, fileName string) (bool, int64, error) {
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• %d ä¸­æ˜¯å¦å­˜åœ¨æ–‡ä»¶: %s", parentID, fileName)

	response, err := f.ListFile(ctx, int(parentID), 100, "", "", 0)
	if err != nil {
		return false, 0, err
	}

	if response.Code != 0 {
		return false, 0, fmt.Errorf("ListFile APIé”™è¯¯: code=%d, message=%s", response.Code, response.Message)
	}

	for _, file := range response.Data.FileList {
		if file.Filename == fileName {
			fs.Debugf(f, "æ‰¾åˆ°æ–‡ä»¶ %sï¼ŒID: %d", fileName, file.FileID)
			return true, int64(file.FileID), nil
		}
	}

	fs.Debugf(f, "ç›®å½• %d ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: %s", parentID, fileName)
	return false, 0, nil
}

// normalizePath è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†123ç½‘ç›˜è·¯å¾„çš„ç‰¹æ®Šè¦æ±‚
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†è·¯å¾„å¤„ç†åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè¦æ±‚
// dircacheè¦æ±‚ï¼šè·¯å¾„ä¸åº”è¯¥ä»¥/å¼€å¤´æˆ–ç»“å°¾
func normalizePath(path string) string {
	if path == "" {
		return ""
	}

	// ä½¿ç”¨filepath.Cleanè¿›è¡ŒåŸºç¡€è·¯å¾„æ¸…ç†
	path = filepath.Clean(path)

	// å¤„ç†filepath.Cleançš„ç‰¹æ®Šè¿”å›å€¼
	if path == "." {
		return ""
	}

	// ç§»é™¤å¼€å¤´çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimPrefix(path, "/")
	path = strings.TrimPrefix(path, "\\") // å¤„ç†Windowsè·¯å¾„åˆ†éš”ç¬¦

	// ç§»é™¤ç»“å°¾çš„æ–œæ ï¼ˆå¦‚æœæœ‰ï¼‰
	path = strings.TrimSuffix(path, "/")
	path = strings.TrimSuffix(path, "\\")

	// å°†åæ–œæ è½¬æ¢ä¸ºæ­£æ–œæ ï¼ˆæ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦ï¼‰
	path = strings.ReplaceAll(path, "\\", "/")

	// æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿ç»“æœä¸ä»¥/å¼€å¤´æˆ–ç»“å°¾
	path = strings.Trim(path, "/")

	return path
}

// isRemoteSource æ£€æŸ¥æºå¯¹è±¡æ˜¯å¦æ¥è‡ªè¿œç¨‹äº‘ç›˜ï¼ˆéæœ¬åœ°æ–‡ä»¶ï¼‰
func (f *Fs) isRemoteSource(src fs.ObjectInfo) bool {
	// æ£€æŸ¥æºå¯¹è±¡çš„ç±»å‹ï¼Œå¦‚æœä¸æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼Œåˆ™è®¤ä¸ºæ˜¯è¿œç¨‹æº
	// è¿™å¯ä»¥é€šè¿‡æ£€æŸ¥æºå¯¹è±¡çš„Fs()æ–¹æ³•è¿”å›çš„ç±»å‹æ¥åˆ¤æ–­
	srcFs := src.Fs()
	if srcFs == nil {
		fs.Debugf(f, "ğŸ” isRemoteSource: srcFsä¸ºnilï¼Œè¿”å›false")
		return false
	}

	// æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ
	fsType := srcFs.Name()
	isRemote := fsType != "local" && fsType != ""

	fs.Debugf(f, "ğŸ” isRemoteSourceæ£€æµ‹: fsType='%s', isRemote=%v", fsType, isRemote)

	// ç‰¹åˆ«æ£€æµ‹115ç½‘ç›˜å’Œå…¶ä»–äº‘ç›˜
	if strings.Contains(fsType, "115") || strings.Contains(fsType, "pan") {
		fs.Debugf(f, "âœ… æ˜ç¡®è¯†åˆ«ä¸ºäº‘ç›˜æº: %s", fsType)
		return true
	}

	return isRemote
}

// init å‡½æ•°ç”¨äºå‘Fsæ³¨å†Œ123ç½‘ç›˜åç«¯
func init() {
	fs.Register(&fs.RegInfo{
		Name:        "123",
		Description: "123ç½‘ç›˜é©±åŠ¨å™¨",
		NewFs:       newFs,
		CommandHelp: commandHelp,
		Options: []fs.Option{{
			Name:     "client_id",
			Help:     "123ç½‘ç›˜APIå®¢æˆ·ç«¯IDã€‚",
			Required: true,
		}, {
			Name:      "client_secret",
			Help:      "123ç½‘ç›˜APIå®¢æˆ·ç«¯å¯†é’¥ã€‚",
			Required:  true,
			Sensitive: true,
		}, {
			Name:      "token",
			Help:      "OAuthè®¿é—®ä»¤ç‰Œï¼ˆJSONæ ¼å¼ï¼‰ã€‚é€šå¸¸ç•™ç©ºã€‚",
			Sensitive: true,
		}, {
			Name:     "user_agent",
			Help:     "APIè¯·æ±‚çš„User-Agentå¤´ã€‚",
			Default:  defaultUserAgent,
			Advanced: true,
		}, {
			Name:     "root_folder_id",
			Help:     "æ ¹æ–‡ä»¶å¤¹çš„IDã€‚ç•™ç©ºè¡¨ç¤ºæ ¹ç›®å½•ã€‚",
			Default:  "0",
			Advanced: true,
		}, {
			Name:     "list_chunk",
			Help:     "æ¯ä¸ªåˆ—è¡¨è¯·æ±‚ä¸­è·å–çš„æ–‡ä»¶æ•°é‡ã€‚",
			Default:  100,
			Advanced: true,
		}, {
			Name:     "pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆuser/info, move, deleteï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  userInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "list_pacer_min_sleep",
			Help:     "æ–‡ä»¶åˆ—è¡¨APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  listV2APIMinSleep,
			Advanced: true,
		}, {
			Name:     "conn_timeout",
			Help:     "APIè¯·æ±‚çš„è¿æ¥è¶…æ—¶æ—¶é—´ã€‚",
			Default:  defaultConnTimeout,
			Advanced: true,
		}, {
			Name:     "timeout",
			Help:     "APIè¯·æ±‚çš„æ•´ä½“è¶…æ—¶æ—¶é—´ã€‚",
			Default:  defaultTimeout,
			Advanced: true,
		}, {
			Name:     "chunk_size",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ çš„å—å¤§å°ã€‚",
			Default:  defaultChunkSize,
			Advanced: true,
		}, {
			Name:     "upload_cutoff",
			Help:     "åˆ‡æ¢åˆ°å¤šéƒ¨åˆ†ä¸Šä¼ çš„é˜ˆå€¼ã€‚",
			Default:  defaultUploadCutoff,
			Advanced: true,
		}, {
			Name:     "max_upload_parts",
			Help:     "å¤šéƒ¨åˆ†ä¸Šä¼ ä¸­çš„æœ€å¤§åˆ†ç‰‡æ•°ã€‚",
			Default:  maxUploadParts,
			Advanced: true,
		}, {
			Name:     "max_concurrent_uploads",
			Help:     "æœ€å¤§å¹¶å‘ä¸Šä¼ æ•°é‡ã€‚å‡å°‘æ­¤å€¼å¯é™ä½å†…å­˜ä½¿ç”¨å’Œæé«˜ç¨³å®šæ€§ã€‚",
			Default:  1, // ä»2æ”¹ä¸º1ï¼Œå‡å°‘å¹¶å‘å‹åŠ›
			Advanced: true,
		}, {
			Name:     "max_concurrent_downloads",
			Help:     "æœ€å¤§å¹¶å‘ä¸‹è½½æ•°é‡ã€‚æé«˜æ­¤å€¼å¯æå‡ä¸‹è½½é€Ÿåº¦ï¼Œä½†ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ã€‚",
			Default:  4, // ä»1æ”¹ä¸º4ï¼Œæå‡ä¸‹è½½æ€§èƒ½
			Advanced: true,
		}, {
			Name:     "progress_update_interval",
			Help:     "è¿›åº¦æ›´æ–°é—´éš”ã€‚",
			Default:  fs.Duration(5 * time.Second),
			Advanced: true,
		}, {
			Name:     "enable_progress_display",
			Help:     "æ˜¯å¦å¯ç”¨è¿›åº¦æ˜¾ç¤ºã€‚",
			Default:  true,
			Advanced: true,
		}, {
			Name:     "upload_pacer_min_sleep",
			Help:     "ä¸Šä¼ APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚è®¾ç½®æ›´é«˜çš„å€¼å¯ä»¥é¿å…429é”™è¯¯ã€‚",
			Default:  uploadV2SliceMinSleep,
			Advanced: true,
		}, {
			Name:     "download_pacer_min_sleep",
			Help:     "ä¸‹è½½APIè°ƒç”¨ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  downloadInfoMinSleep,
			Advanced: true,
		}, {
			Name:     "strict_pacer_min_sleep",
			Help:     "ä¸¥æ ¼APIè°ƒç”¨ï¼ˆmove, deleteç­‰ï¼‰ä¹‹é—´çš„æœ€å°ç­‰å¾…æ—¶é—´ã€‚",
			Default:  fileMoveMinSleep,
			Advanced: true,
		}, {
			Name:     "debug_level",
			Help:     "è°ƒè¯•æ—¥å¿—çº§åˆ«ï¼š0=æ— ï¼Œ1=é”™è¯¯ï¼Œ2=ä¿¡æ¯ï¼Œ3=è°ƒè¯•ï¼Œ4=è¯¦ç»†ã€‚",
			Default:  LogLevelVerbose,
			Advanced: true,
		}},
	})
}

var commandHelp = []fs.CommandHelp{{
	Name:  "getdownloadurlua",
	Short: "é€šè¿‡æ–‡ä»¶è·¯å¾„è·å–ä¸‹è½½URL",
	Long: `æ­¤å‘½ä»¤ä½¿ç”¨æ–‡ä»¶è·¯å¾„æ£€ç´¢æ–‡ä»¶çš„ä¸‹è½½URLã€‚
ç”¨æ³•:
rclone backend getdownloadurlau 123:path/to/file VidHub/1.7.24
è¯¥å‘½ä»¤è¿”å›æŒ‡å®šæ–‡ä»¶çš„ä¸‹è½½URLã€‚è¯·ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®ã€‚`,
},
}

// Command æ‰§è¡Œåç«¯ç‰¹å®šçš„å‘½ä»¤ã€‚
func (f *Fs) Command(ctx context.Context, name string, arg []string, opt map[string]string) (out any, err error) {
	switch name {

	case "getdownloadurlua":
		if len(arg) < 2 {
			return nil, fmt.Errorf("éœ€è¦æä¾›æ–‡ä»¶è·¯å¾„å’ŒUser-Agentå‚æ•°")
		}
		path := arg[0]
		ua := arg[1]
		return f.getDownloadURLByUA(ctx, path, ua)

	case "stats":
		// è¿”å›æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
		return f.performanceMetrics.GetStats(), nil

	case "logstats":
		// è®°å½•æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—
		f.performanceMetrics.LogStats(f)
		return "æ€§èƒ½ç»Ÿè®¡å·²è®°å½•åˆ°æ—¥å¿—", nil

	default:
		return nil, fs.ErrorCommandNotFound
	}
}

type ListRequest struct {
	ParentFileID int    `json:"parentFileId"`
	Limit        int    `json:"limit"`
	SearchData   string `json:"searchData,omitempty"`
	SearchMode   string `json:"searchMode,omitempty"`
	LastFileID   int    `json:"lastFileID,omitempty"`
}

type ListResponse struct {
	Code    int                   `json:"code"`
	Message string                `json:"message"`
	Data    GetFileListRespDataV2 `json:"data"` // æ›´æ”¹ä¸ºç‰¹å®šç±»å‹
}

// GetFileListRespDataV2 è¡¨ç¤ºæ–‡ä»¶åˆ—è¡¨å“åº”çš„æ•°æ®ç»“æ„
type GetFileListRespDataV2 struct {
	// -1ä»£è¡¨æœ€åä¸€é¡µï¼ˆæ— éœ€å†ç¿»é¡µæŸ¥è¯¢ï¼‰, å…¶ä»–ä»£è¡¨ä¸‹ä¸€é¡µå¼€å§‹çš„æ–‡ä»¶idï¼Œæºå¸¦åˆ°è¯·æ±‚å‚æ•°ä¸­
	LastFileId int64 `json:"lastFileId"`
	// æ–‡ä»¶åˆ—è¡¨
	FileList []FileListInfoRespDataV2 `json:"fileList"`
}

// FileListInfoRespDataV2 è¡¨ç¤ºæ¥è‡ªAPIå“åº”çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹é¡¹ç›®
type FileListInfoRespDataV2 struct {
	// æ–‡ä»¶ID
	FileID int64 `json:"fileID"`
	// æ–‡ä»¶å
	Filename string `json:"filename"`
	// 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	Type int `json:"type"`
	// æ–‡ä»¶å¤§å°
	Size int64 `json:"size"`
	// md5
	Etag string `json:"etag"`
	// æ–‡ä»¶å®¡æ ¸çŠ¶æ€, å¤§äº 100 ä¸ºå®¡æ ¸é©³å›æ–‡ä»¶
	Status int `json:"status"`
	// ç›®å½•ID
	ParentFileID int64 `json:"parentFileID"`
	// æ–‡ä»¶åˆ†ç±», 0-æœªçŸ¥ 1-éŸ³é¢‘ 2-è§†é¢‘ 3-å›¾ç‰‡
	Category int `json:"category"`
}

func (f *Fs) ListFile(ctx context.Context, parentFileID, limit int, searchData, searchMode string, lastFileID int) (*ListResponse, error) {
	fs.Debugf(f, "è°ƒç”¨ListFileï¼Œå‚æ•°ï¼šparentFileID=%d, limit=%d, lastFileID=%d", parentFileID, limit, lastFileID)

	// åªæœ‰åœ¨æ²¡æœ‰æœç´¢æ¡ä»¶æ—¶æ‰ä½¿ç”¨ç¼“å­˜
	if searchData == "" && searchMode == "" {
		// å°è¯•ä»ç¼“å­˜è·å–
		if cached, found := f.getDirListFromCache(int64(parentFileID), int64(lastFileID)); found {
			// æ„é€ ç¼“å­˜å“åº”
			result := &ListResponse{
				Code:    0,
				Message: "success",
				Data: GetFileListRespDataV2{
					LastFileId: cached.LastFileID,
					FileList:   cached.FileList,
				},
			}
			fs.Debugf(f, "ç›®å½•åˆ—è¡¨ç¼“å­˜å‘½ä¸­: parentFileID=%d, fileCount=%d", parentFileID, len(cached.FileList))
			return result, nil
		}
	}

	// æ„é€ æŸ¥è¯¢å‚æ•°
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", limit))

	if searchData != "" {
		params.Add("searchData", searchData)
	}
	if searchMode != "" {
		params.Add("searchMode", searchMode)
	}
	if lastFileID != 0 {
		params.Add("lastFileID", fmt.Sprintf("%d", lastFileID))
	}

	// æ„é€ å¸¦æŸ¥è¯¢å‚æ•°çš„ç«¯ç‚¹
	endpoint := "/api/v2/file/list?" + params.Encode()
	fs.Debugf(f, "APIç«¯ç‚¹: %s%s", openAPIRootURL, endpoint)

	// ç›´æ¥ä½¿ç”¨v2 APIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var result ListResponse
	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &result)
	if err != nil {
		fs.Debugf(f, "ListFile APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "ListFile APIå“åº”: code=%d, message=%s, fileCount=%d", result.Code, result.Message, len(result.Data.FileList))

	// åªæœ‰åœ¨æ²¡æœ‰æœç´¢æ¡ä»¶ä¸”APIè°ƒç”¨æˆåŠŸæ—¶æ‰ç¼“å­˜ç»“æœ
	if searchData == "" && searchMode == "" && result.Code == 0 {
		f.saveDirListToCache(int64(parentFileID), int64(lastFileID), result.Data.FileList, result.Data.LastFileId)
	}

	return &result, nil
}

func (f *Fs) pathToFileID(ctx context.Context, filePath string) (string, error) {
	fs.Debugf(f, "ğŸ” pathToFileIDå¼€å§‹: filePath=%s", filePath)

	// æ ¹ç›®å½•
	if filePath == "/" {
		return "0", nil
	}
	if len(filePath) > 1 && strings.HasSuffix(filePath, "/") {
		filePath = filePath[:len(filePath)-1]
	}

	// å°è¯•ä»ç¼“å­˜è·å–è·¯å¾„æ˜ å°„
	if cachedFileID, _, found := f.getPathToIDFromCache(filePath); found {
		fs.Debugf(f, "è·¯å¾„æ˜ å°„ç¼“å­˜å‘½ä¸­: %s -> %s", filePath, cachedFileID)
		return cachedFileID, nil
	}
	currentID := "0"
	parts := strings.Split(filePath, "/")
	if len(parts) > 0 && parts[0] == "" { // å¦‚æœè·¯å¾„ä»¥/å¼€å¤´ï¼Œç§»é™¤ç©ºå­—ç¬¦ä¸²
		parts = parts[1:]
	}
	for _, part := range parts {
		if part == "" {
			continue
		}
		findPart := false
		next := "0" // æ ¹æ®APIä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºnext
		for {
			parentFileID, err := strconv.Atoi(currentID)
			if err != nil {
				return "", fmt.Errorf("invalid parentFileId: %s", currentID)
			}

			lastFileIDInt, err := strconv.Atoi(next)
			if err != nil {
				return "", fmt.Errorf("invalid next token: %s", next)
			}

			var response *ListResponse
			// ä½¿ç”¨åˆ—è¡¨è°ƒé€Ÿå™¨è¿›è¡Œé€Ÿç‡é™åˆ¶
			err = f.listPacer.Call(func() (bool, error) {
				response, err = f.ListFile(ctx, parentFileID, 100, "", "", lastFileIDInt)
				if err != nil {
					return shouldRetry(ctx, nil, err)
				}
				return false, nil
			})
			if err != nil {
				return "", fmt.Errorf("list file failed: %w", err)
			}
			if response.Code != 200 && response.Code != 0 {
				return "", fmt.Errorf("API returned error code %d: %s", response.Code, response.Message)
			}
			infoList := response.Data.FileList
			if len(infoList) == 0 {
				break
			}
			for _, item := range infoList {
				if item.Filename == part {
					currentID = strconv.FormatInt(item.FileID, 10)
					findPart = true

					// è®°å½•æ‰¾åˆ°çš„é¡¹ç›®ç±»å‹ä¿¡æ¯ï¼Œç”¨äºåç»­ç¼“å­˜
					isDir := (item.Type == 1) // Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
					fs.Debugf(f, "pathToFileIDæ‰¾åˆ°é¡¹ç›®: %s -> ID=%s, Type=%d, isDir=%v", part, currentID, item.Type, isDir)

					// å¦‚æœè¿™æ˜¯è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œç«‹å³ç¼“å­˜ç±»å‹ä¿¡æ¯
					if len(parts) > 0 && part == parts[len(parts)-1] {
						currentPath := "/" + strings.Join(parts, "/")
						f.savePathToIDToCache(currentPath, currentID, "0", isDir)
						fs.Debugf(f, "ç«‹å³ç¼“å­˜è·¯å¾„ç±»å‹: %s -> ID=%s, isDir=%v", currentPath, currentID, isDir)
					}

					break
				}
			}
			if findPart {
				break
			}

			nextRaw := strconv.FormatInt(response.Data.LastFileId, 10)
			if nextRaw == "-1" {
				break
			} else {
				next = nextRaw
				// æ— éœ€æ‰‹åŠ¨ç¡çœ  - è°ƒé€Ÿå™¨å¤„ç†é€Ÿç‡é™åˆ¶
			}
		}
		if !findPart {
			return "", fs.ErrorObjectNotFound
		}
	}

	if currentID == "0" && filePath != "/" {
		return "", fs.ErrorObjectNotFound
	}

	// ç¼“å­˜è·¯å¾„æ˜ å°„ç»“æœï¼ˆå¦‚æœè¿˜æ²¡æœ‰è¢«ç¼“å­˜ï¼‰
	// æ³¨é‡Šæ‰é”™è¯¯çš„ç¼“å­˜é€»è¾‘ï¼Œå› ä¸ºæ­£ç¡®çš„ç±»å‹ä¿¡æ¯å·²ç»åœ¨è·¯å¾„è§£æå¾ªç¯ä¸­ç¼“å­˜äº†
	// if currentID != "0" {
	// 	// æ£€æŸ¥æ˜¯å¦å·²ç»ç¼“å­˜è¿‡
	// 	if _, _, found := f.getPathToIDFromCache(filePath); !found {
	// 		// é»˜è®¤å‡è®¾æ˜¯ç›®å½•ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æŸ¥æ‰¾è·¯å¾„
	// 		isDir := true
	// 		f.savePathToIDToCache(filePath, currentID, "0", isDir)
	// 		fs.Debugf(f, "ç¼“å­˜è·¯å¾„æ˜ å°„: %s -> ID=%s, isDir=%v", filePath, currentID, isDir)
	// 	}
	// }

	fs.Debugf(f, "ğŸ” pathToFileIDç»“æœ: filePath=%s -> currentID=%s", filePath, currentID)
	return currentID, nil
}

type FileDetail struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

type FileInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		List []FileDetail `json:"list"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

type FileDetailResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID       int64  `json:"fileID"`
		Filename     string `json:"filename"`
		Type         int    `json:"type"`
		Size         int64  `json:"size"`
		ETag         string `json:"etag"`
		Status       int    `json:"status"`
		ParentFileID int64  `json:"parentFileID"`
		CreateAt     string `json:"createAt"`
		Trashed      int    `json:"trashed"`
	} `json:"data"`
	TraceID string `json:"x-traceID"`
}

// FileInfo è¡¨ç¤º'list'æ•°ç»„ä¸­å•ä¸ªæ–‡ä»¶çš„ä¿¡æ¯
type FileInfo struct {
	FileID       int64  `json:"fileId"`
	Filename     string `json:"filename"`
	ParentFileID int64  `json:"parentFileId"`
	Type         int    `json:"type"`
	ETag         string `json:"etag"`
	Size         int64  `json:"size"`
	Category     int    `json:"category"`
	Status       int    `json:"status"`
	PunishFlag   int    `json:"punishFlag"`
	S3KeyFlag    string `json:"s3KeyFlag"`
	StorageNode  string `json:"storageNode"`
	Trashed      int    `json:"trashed"`
	CreateAt     string `json:"createAt"`
	UpdateAt     string `json:"updateAt"`
}

// DataContainer ä¿å­˜'list'æ•°ç»„
type DataContainer struct {
	List []FileInfo `json:"list"`
}

// FileInfosResponse æ˜¯APIå“åº”çš„é¡¶çº§ç»“æ„

// å®šä¹‰Dataç»“æ„ä½“ï¼Œæ˜ å°„dataå­—æ®µçš„å†…å®¹
type Data struct {
	FileList []FileInfo `json:"fileList"`
}

// å®šä¹‰FileInfosResponseç»“æ„ä½“ï¼Œæ˜ å°„æ•´ä¸ªå“åº”JSON
type FileInfosResponse struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     Data   `json:"data"`
	XTraceID string `json:"x-traceID"`
}

// å®šä¹‰FileInfoRequestç»“æ„ä½“ï¼Œç”¨äºå‘é€è¯·æ±‚çš„payload
type FileInfoRequest struct {
	FileIDs []int64 `json:"fileIDs"`
}

type Payload struct {
	S3KeyFlag string `json:"s3KeyFlag"`
	FileName  string `json:"fileName"`
	Etag      string `json:"etag"`
	Size      int64  `json:"size"`
	FileID    int64  `json:"fileId"`
}
type DownloadInfoResponse struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		DownloadURL string `json:"downloadUrl"` // ç›´é“¾åœ°å€
		ExpireTime  string `json:"expireTime"`  // è¿‡æœŸæ—¶é—´
	} `json:"data"`
}

type UploadCreateResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		FileID      int64  `json:"fileID"`
		PreuploadID string `json:"preuploadID"`
		Reuse       bool   `json:"reuse"`
		SliceSize   int64  `json:"sliceSize"`
	} `json:"data"`
}

// UploadProgress è·Ÿè¸ªå¤šéƒ¨åˆ†ä¸Šä¼ çš„è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
type UploadProgress struct {
	PreuploadID   string         `json:"preuploadID"`
	TotalParts    int64          `json:"totalParts"`
	ChunkSize     int64          `json:"chunkSize"`
	FileSize      int64          `json:"fileSize"`
	UploadedParts map[int64]bool `json:"uploadedParts"` // partNumber -> uploaded
	FilePath      string         `json:"filePath"`      // local file path for resume
	MD5Hash       string         `json:"md5Hash"`       // file MD5 for verification
	CreatedAt     time.Time      `json:"createdAt"`
	mu            sync.RWMutex   `json:"-"` // ä¿æŠ¤UploadedPartsçš„å¹¶å‘è®¿é—®
}

// SetUploaded çº¿ç¨‹å®‰å…¨åœ°æ ‡è®°åˆ†ç‰‡å·²ä¸Šä¼ 
func (up *UploadProgress) SetUploaded(partNumber int64) {
	up.mu.Lock()
	defer up.mu.Unlock()
	up.UploadedParts[partNumber] = true
}

// IsUploaded çº¿ç¨‹å®‰å…¨åœ°æ£€æŸ¥åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼ 
func (up *UploadProgress) IsUploaded(partNumber int64) bool {
	up.mu.RLock()
	defer up.mu.RUnlock()
	return up.UploadedParts[partNumber]
}

// GetUploadedCount çº¿ç¨‹å®‰å…¨åœ°è·å–å·²ä¸Šä¼ åˆ†ç‰‡æ•°é‡
func (up *UploadProgress) GetUploadedCount() int {
	up.mu.RLock()
	defer up.mu.RUnlock()
	return len(up.UploadedParts)
}

// RemoveUploaded çº¿ç¨‹å®‰å…¨åœ°ç§»é™¤å·²ä¸Šä¼ æ ‡è®°ï¼ˆç”¨äºé‡æ–°ä¸Šä¼ æŸåçš„åˆ†ç‰‡ï¼‰
func (up *UploadProgress) RemoveUploaded(partNumber int64) {
	up.mu.Lock()
	defer up.mu.Unlock()
	delete(up.UploadedParts, partNumber)
}

// GetUploadedParts çº¿ç¨‹å®‰å…¨åœ°è·å–å·²ä¸Šä¼ åˆ†ç‰‡åˆ—è¡¨çš„å‰¯æœ¬
func (up *UploadProgress) GetUploadedParts() map[int64]bool {
	up.mu.RLock()
	defer up.mu.RUnlock()

	// è¿”å›å‰¯æœ¬ä»¥é¿å…å¤–éƒ¨ä¿®æ”¹
	result := make(map[int64]bool)
	for k, v := range up.UploadedParts {
		result[k] = v
	}
	return result
}

type UserInfoResp struct {
	Code    int    `json:"code"`
	Message string `json:"message"`
	Data    struct {
		UID            int64  `json:"uid"`
		Username       string `json:"username"`
		DisplayName    string `json:"displayName"`
		HeadImage      string `json:"headImage"`
		Passport       string `json:"passport"`
		Mail           string `json:"mail"`
		SpaceUsed      int64  `json:"spaceUsed"`
		SpacePermanent int64  `json:"spacePermanent"`
		SpaceTemp      int64  `json:"spaceTemp"`
		SpaceTempExpr  int64  `json:"spaceTempExpr"` // ä¿®å¤ï¼šAPIè¿”å›æ•°å­—è€Œéå­—ç¬¦ä¸²
		Vip            bool   `json:"vip"`
		DirectTraffic  int64  `json:"directTraffic"`
		IsHideUID      bool   `json:"isHideUID"`
	} `json:"data"`
}

func (f *Fs) getDownloadURLByUA(ctx context.Context, filePath string, UA string) (string, error) {
	// Ensure token is valid before making API calls
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return "", err
	}

	if UA == "" {
		UA = f.opt.UserAgent
	}

	if filePath == "" {
		filePath = f.root
	}

	fileID, err := f.pathToFileID(ctx, filePath)
	if err != nil {
		return "", fmt.Errorf("failed to get file ID for path %q: %w", filePath, err)
	}

	// Use the standard getDownloadURL method
	return f.getDownloadURL(ctx, fileID)
}

// isUnrecoverableError æ£€æŸ¥é”™è¯¯æ˜¯å¦ä¸ºä¸å¯æ¢å¤çš„é”™è¯¯
func isUnrecoverableError(err error) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())
	unrecoverableErrors := []string{
		"invalid credentials",
		"authentication failed",
		"file not found",
		"permission denied",
		"quota exceeded",
		"invalid request",
		"bad request",
		"forbidden",
		"æ–‡ä»¶ä¸å­˜åœ¨",
		"æƒé™ä¸è¶³",
		"é…é¢å·²æ»¡",
		"æ— æ•ˆè¯·æ±‚",
	}

	for _, unrecoverable := range unrecoverableErrors {
		if strings.Contains(errStr, unrecoverable) {
			return true
		}
	}
	return false
}

// ConcurrencyManager ç»Ÿä¸€çš„å¹¶å‘æ§åˆ¶ç®¡ç†å™¨
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šæä¾›æ›´æ ‡å‡†åŒ–çš„å¹¶å‘æ§åˆ¶æ¥å£
type ConcurrencyManager struct {
	uploadSemaphore   chan struct{} // ä¸Šä¼ å¹¶å‘æ§åˆ¶ä¿¡å·é‡
	downloadSemaphore chan struct{} // ä¸‹è½½å¹¶å‘æ§åˆ¶ä¿¡å·é‡
	maxUploads        int           // æœ€å¤§å¹¶å‘ä¸Šä¼ æ•°
	maxDownloads      int           // æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
}

// NewConcurrencyManager åˆ›å»ºæ–°çš„å¹¶å‘æ§åˆ¶ç®¡ç†å™¨
func NewConcurrencyManager(maxUploads, maxDownloads int) *ConcurrencyManager {
	return &ConcurrencyManager{
		uploadSemaphore:   make(chan struct{}, maxUploads),
		downloadSemaphore: make(chan struct{}, maxDownloads),
		maxUploads:        maxUploads,
		maxDownloads:      maxDownloads,
	}
}

// AcquireUpload è·å–ä¸Šä¼ ä¿¡å·é‡
func (cm *ConcurrencyManager) AcquireUpload(ctx context.Context) error {
	select {
	case cm.uploadSemaphore <- struct{}{}:
		return nil
	case <-ctx.Done():
		return ctx.Err()
	}
}

// ReleaseUpload é‡Šæ”¾ä¸Šä¼ ä¿¡å·é‡
func (cm *ConcurrencyManager) ReleaseUpload() {
	select {
	case <-cm.uploadSemaphore:
		// ä¿¡å·é‡é‡Šæ”¾æˆåŠŸ
	default:
		// ä¿¡å·é‡é‡Šæ”¾å¤±è´¥ï¼Œè®°å½•è­¦å‘Š
		fs.Debugf(nil, "ä¸Šä¼ ä¿¡å·é‡é‡Šæ”¾å¤±è´¥")
	}
}

// AcquireDownload è·å–ä¸‹è½½ä¿¡å·é‡
func (cm *ConcurrencyManager) AcquireDownload(ctx context.Context) error {
	select {
	case cm.downloadSemaphore <- struct{}{}:
		return nil
	case <-ctx.Done():
		return ctx.Err()
	}
}

// ReleaseDownload é‡Šæ”¾ä¸‹è½½ä¿¡å·é‡
func (cm *ConcurrencyManager) ReleaseDownload() {
	select {
	case <-cm.downloadSemaphore:
		// ä¿¡å·é‡é‡Šæ”¾æˆåŠŸ
	default:
		// ä¿¡å·é‡é‡Šæ”¾å¤±è´¥ï¼Œè®°å½•è­¦å‘Š
		fs.Debugf(nil, "ä¸‹è½½ä¿¡å·é‡é‡Šæ”¾å¤±è´¥")
	}
}

// GetStats è·å–å¹¶å‘æ§åˆ¶ç»Ÿè®¡ä¿¡æ¯
func (cm *ConcurrencyManager) GetStats() (uploadActive, downloadActive int) {
	return cm.maxUploads - len(cm.uploadSemaphore), cm.maxDownloads - len(cm.downloadSemaphore)
}

// validateRequired éªŒè¯å¿…éœ€å­—æ®µä¸ä¸ºç©º
func validateRequired(fieldName, value string) error {
	if value == "" {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºç©º", fieldName)
	}
	return nil
}

// validateRange éªŒè¯æ•°å€¼åœ¨æŒ‡å®šèŒƒå›´å†…
func validateRange(fieldName string, value, min, max int) error {
	if value <= 0 || value < min || value > max {
		return fmt.Errorf("%s å¿…é¡»åœ¨ %d-%d èŒƒå›´å†…", fieldName, min, max)
	}
	return nil
}

// validateDuration éªŒè¯æ—¶é—´é…ç½®ä¸ä¸ºè´Ÿæ•°
func validateDuration(fieldName string, value time.Duration) error {
	if value < 0 {
		return fmt.Errorf("%s ä¸èƒ½ä¸ºè´Ÿæ•°", fieldName)
	}
	return nil
}

// validateSize éªŒè¯å¤§å°é…ç½®
func validateSize(fieldName string, value int64, min, max int64) error {
	if value < min {
		return fmt.Errorf("%s ä¸èƒ½å°äº %d", fieldName, min)
	}
	if max > 0 && value > max {
		return fmt.Errorf("%s ä¸èƒ½è¶…è¿‡ %d", fieldName, max)
	}
	return nil
}

// validateOptions éªŒè¯é…ç½®é€‰é¡¹çš„æœ‰æ•ˆæ€§
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»“æ„åŒ–éªŒè¯æ–¹æ³•ï¼Œå‡å°‘é‡å¤ä»£ç 
func validateOptions(opt *Options) error {
	var errors []string

	// éªŒè¯å¿…éœ€çš„è®¤è¯ä¿¡æ¯
	if err := validateRequired("client_id", opt.ClientID); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRequired("client_secret", opt.ClientSecret); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯æ•°å€¼èŒƒå›´ - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateRange("list_chunk", opt.ListChunk, 1, MaxListChunk); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_upload_parts", opt.MaxUploadParts, 1, MaxUploadPartsLimit); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_concurrent_uploads", opt.MaxConcurrentUploads, 1, MaxConcurrentLimit); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateRange("max_concurrent_downloads", opt.MaxConcurrentDownloads, 1, MaxConcurrentLimit); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯æ—¶é—´é…ç½® - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateDuration("pacer_min_sleep", time.Duration(opt.PacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("list_pacer_min_sleep", time.Duration(opt.ListPacerMinSleep)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("conn_timeout", time.Duration(opt.ConnTimeout)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("timeout", time.Duration(opt.Timeout)); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateDuration("progress_update_interval", time.Duration(opt.ProgressUpdateInterval)); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯å¤§å°é…ç½® - ä½¿ç”¨é€šç”¨éªŒè¯å‡½æ•°
	if err := validateSize("chunk_size", int64(opt.ChunkSize), 1, 5*1024*1024*1024); err != nil {
		errors = append(errors, err.Error())
	}
	if err := validateSize("upload_cutoff", int64(opt.UploadCutoff), 0, -1); err != nil {
		errors = append(errors, err.Error())
	}

	// éªŒè¯UserAgenté•¿åº¦
	if opt.UserAgent != "" && len(opt.UserAgent) > 200 {
		errors = append(errors, "user_agent é•¿åº¦ä¸èƒ½è¶…è¿‡ 200 å­—ç¬¦")
	}

	// éªŒè¯RootFolderIDæ ¼å¼ï¼ˆå¦‚æœæä¾›ï¼‰
	if opt.RootFolderID != "" {
		if _, err := parseFileID(opt.RootFolderID); err != nil {
			errors = append(errors, "root_folder_id å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ•°å­—")
		}
	}

	// è¿”å›èšåˆçš„é”™è¯¯ä¿¡æ¯
	if len(errors) > 0 {
		return fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %s", strings.Join(errors, "; "))
	}

	return nil
}

// createPacer åˆ›å»ºpacerçš„å·¥å‚å‡½æ•°ï¼Œå‡å°‘é‡å¤é…ç½®ä»£ç 
func createPacer(ctx context.Context, minSleep, maxSleep time.Duration, decayConstant float64) *fs.Pacer {
	return fs.NewPacer(ctx, pacer.NewDefault(
		pacer.MinSleep(minSleep),
		pacer.MaxSleep(maxSleep),
		pacer.DecayConstant(decayConstant)))
}

// applyStandardConfigDefaults åº”ç”¨rcloneæ ‡å‡†é…ç½®çš„é»˜è®¤å€¼
// è¿™ä¸ªå‡½æ•°å±•ç¤ºäº†å¦‚ä½•åœ¨æœªæ¥å¯ä»¥ä½¿ç”¨rcloneçš„æ ‡å‡†é…ç½®é€‰é¡¹
func applyStandardConfigDefaults(opt *Options) {
	// ç¤ºä¾‹ï¼šå¦‚æœæœªæ¥è¦ä½¿ç”¨rcloneæ ‡å‡†é…ç½®ï¼Œå¯ä»¥è¿™æ ·åšï¼š
	// if opt.MaxConcurrentUploads <= 0 {
	//     opt.MaxConcurrentUploads = fs.Config.Transfers
	// }
	// if opt.MaxConcurrentDownloads <= 0 {
	//     opt.MaxConcurrentDownloads = fs.Config.Checkers
	// }
	// if opt.ConnTimeout <= 0 {
	//     opt.ConnTimeout = fs.Duration(fs.Config.ConnectTimeout)
	// }
	// if opt.Timeout <= 0 {
	//     opt.Timeout = fs.Duration(fs.Config.Timeout)
	// }

	// ç›®å‰ä¿æŒç°æœ‰çš„123ç½‘ç›˜ç‰¹å®šé…ç½®
	_ = opt // é¿å…æœªä½¿ç”¨å˜é‡è­¦å‘Š
}

// normalizeOptions æ ‡å‡†åŒ–å’Œä¿®æ­£é…ç½®é€‰é¡¹
func normalizeOptions(opt *Options) {
	// è®¾ç½®é»˜è®¤å€¼
	if opt.ListChunk <= 0 {
		opt.ListChunk = DefaultListChunk
	}
	if opt.MaxUploadParts <= 0 {
		opt.MaxUploadParts = DefaultMaxUploadParts
	}
	if opt.MaxConcurrentUploads <= 0 {
		opt.MaxConcurrentUploads = 4
	}
	if opt.MaxConcurrentDownloads <= 0 {
		opt.MaxConcurrentDownloads = 4
	}

	// è®¾ç½®åˆç†çš„é»˜è®¤è¶…æ—¶æ—¶é—´
	if time.Duration(opt.PacerMinSleep) <= 0 {
		opt.PacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.ListPacerMinSleep) <= 0 {
		opt.ListPacerMinSleep = fs.Duration(100 * time.Millisecond)
	}
	if time.Duration(opt.ConnTimeout) <= 0 {
		opt.ConnTimeout = fs.Duration(60 * time.Second)
	}
	if time.Duration(opt.Timeout) <= 0 {
		opt.Timeout = fs.Duration(300 * time.Second)
	}
	if time.Duration(opt.ProgressUpdateInterval) <= 0 {
		opt.ProgressUpdateInterval = fs.Duration(1 * time.Second)
	}

	// ç¡®ä¿è¿›åº¦æ˜¾ç¤ºé»˜è®¤å¯ç”¨ï¼ˆGoä¸­boolé›¶å€¼æ˜¯falseï¼Œéœ€è¦æ˜¾å¼è®¾ç½®ï¼‰
	// æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ç®€å•ç”¨ if !opt.EnableProgressDisplayï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½æ˜¾å¼è®¾ç½®ä¸ºfalse
	// æˆ‘ä»¬éœ€è¦æ£€æŸ¥æ˜¯å¦æ˜¯ä»é…ç½®æ–‡ä»¶åŠ è½½çš„å€¼è¿˜æ˜¯é»˜è®¤çš„é›¶å€¼
	// ç”±äºconfigstruct.Setä¼šå¤„ç†é»˜è®¤å€¼ï¼Œè¿™é‡Œåº”è¯¥å·²ç»æ˜¯æ­£ç¡®çš„å€¼äº†
	// ä½†ä¸ºäº†ç¡®ä¿å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¿æŒç°æœ‰é€»è¾‘

	// è®¾ç½®åˆç†çš„é»˜è®¤å¤§å°
	if int64(opt.ChunkSize) <= 0 {
		opt.ChunkSize = fs.SizeSuffix(100 * 1024 * 1024) // 100MB
	}
	if int64(opt.UploadCutoff) <= 0 {
		opt.UploadCutoff = fs.SizeSuffix(200 * 1024 * 1024) // 200MB
	}

	// è®¾ç½®é»˜è®¤UserAgent
	if opt.UserAgent == "" {
		opt.UserAgent = "rclone/" + fs.Version
	}
}

// calculateChecksum è®¡ç®—æ•°æ®çš„è½»é‡æ ¡éªŒå’Œï¼ˆä½¿ç”¨CRC32æ›¿ä»£SHA256ï¼‰
func calculateChecksum(data interface{}) string {
	jsonData, err := json.Marshal(data)
	if err != nil {
		return ""
	}

	// ä½¿ç”¨CRC32æ›¿ä»£SHA256ï¼Œå‡å°‘CPUå¼€é”€
	hash := crc32.ChecksumIEEE(jsonData)
	return fmt.Sprintf("%08x", hash)
}

// generateCacheVersion ç”Ÿæˆç¼“å­˜ç‰ˆæœ¬å·
func generateCacheVersion() int64 {
	return time.Now().UnixNano()
}

// validateCacheEntry éªŒè¯ç¼“å­˜æ¡ç›®çš„å®Œæ•´æ€§ï¼ˆè½»é‡çº§éªŒè¯ï¼‰
func validateCacheEntry(data interface{}, expectedChecksum string) bool {
	if expectedChecksum == "" {
		return true // å¦‚æœæ²¡æœ‰æ ¡éªŒå’Œï¼Œè·³è¿‡éªŒè¯
	}

	// å¯¹äºæ€§èƒ½è€ƒè™‘ï¼Œåªå¯¹å°æ•°æ®è¿›è¡Œæ ¡éªŒ
	jsonData, err := json.Marshal(data)
	if err != nil {
		return false
	}

	// å¦‚æœæ•°æ®è¿‡å¤§ï¼ˆ>10KBï¼‰ï¼Œè·³è¿‡æ ¡éªŒä»¥æé«˜æ€§èƒ½
	if len(jsonData) > 10*1024 {
		return true
	}

	actualChecksum := calculateChecksum(data)
	return actualChecksum == expectedChecksum
}

// fastValidateCacheEntry å¿«é€ŸéªŒè¯ç¼“å­˜æ¡ç›®ï¼ˆä»…æ£€æŸ¥åŸºæœ¬ç»“æ„ï¼‰
func fastValidateCacheEntry(data interface{}) bool {
	if data == nil {
		return false
	}

	// ç®€å•çš„ç»“æ„æ£€æŸ¥ï¼Œé¿å…å¤æ‚çš„æ ¡éªŒå’Œè®¡ç®—
	switch v := data.(type) {
	case []interface{}:
		return len(v) >= 0 // æ•°ç»„ç±»å‹ï¼Œæ£€æŸ¥é•¿åº¦
	case map[string]any:
		return len(v) >= 0 // å¯¹è±¡ç±»å‹ï¼Œæ£€æŸ¥é”®æ•°é‡
	case string:
		return len(v) > 0 // å­—ç¬¦ä¸²ç±»å‹ï¼Œæ£€æŸ¥éç©º
	default:
		return true // å…¶ä»–ç±»å‹é»˜è®¤æœ‰æ•ˆ
	}
}

// APIVersionManager ç®¡ç†APIç‰ˆæœ¬å…¼å®¹æ€§
type APIVersionManager struct {
	mu                sync.RWMutex
	preferredVersions map[string]string // endpoint -> preferred version
	failedVersions    map[string]bool   // endpoint:version -> failed
	lastChecked       time.Time
}

// NewAPIVersionManager åˆ›å»ºAPIç‰ˆæœ¬ç®¡ç†å™¨
func NewAPIVersionManager() *APIVersionManager {
	return &APIVersionManager{
		preferredVersions: make(map[string]string),
		failedVersions:    make(map[string]bool),
		lastChecked:       time.Now(),
	}
}

// GetPreferredVersion è·å–ç«¯ç‚¹çš„é¦–é€‰ç‰ˆæœ¬
func (avm *APIVersionManager) GetPreferredVersion(endpoint string) string {
	avm.mu.RLock()
	defer avm.mu.RUnlock()

	// æå–ç«¯ç‚¹çš„åŸºç¡€è·¯å¾„
	basePath := avm.extractBasePath(endpoint)

	if version, exists := avm.preferredVersions[basePath]; exists {
		return version
	}

	// é»˜è®¤ä¼˜å…ˆä½¿ç”¨v2
	return "v2"
}

// MarkVersionFailed æ ‡è®°æŸä¸ªç‰ˆæœ¬å¤±è´¥
func (avm *APIVersionManager) MarkVersionFailed(endpoint, version string) {
	avm.mu.Lock()
	defer avm.mu.Unlock()

	basePath := avm.extractBasePath(endpoint)
	key := fmt.Sprintf("%s:%s", basePath, version)
	avm.failedVersions[key] = true

	// å¦‚æœv2å¤±è´¥ï¼Œåˆ‡æ¢åˆ°v1
	if version == "v2" {
		avm.preferredVersions[basePath] = "v1"
	}
}

// IsVersionFailed æ£€æŸ¥ç‰ˆæœ¬æ˜¯å¦å·²å¤±è´¥
func (avm *APIVersionManager) IsVersionFailed(endpoint, version string) bool {
	avm.mu.RLock()
	defer avm.mu.RUnlock()

	basePath := avm.extractBasePath(endpoint)
	key := fmt.Sprintf("%s:%s", basePath, version)
	return avm.failedVersions[key]
}

// extractBasePath æå–APIç«¯ç‚¹çš„åŸºç¡€è·¯å¾„
func (avm *APIVersionManager) extractBasePath(endpoint string) string {
	// ç§»é™¤ç‰ˆæœ¬å·ï¼Œæå–åŸºç¡€è·¯å¾„
	// ä¾‹å¦‚: "/upload/v2/file/create" -> "/upload/file/create"
	parts := strings.Split(endpoint, "/")
	var baseParts []string

	for _, part := range parts {
		if part != "" && !strings.HasPrefix(part, "v") {
			baseParts = append(baseParts, part)
		}
	}

	return "/" + strings.Join(baseParts, "/")
}

// shouldRetryCrossCloudTransfer æ™ºèƒ½é‡è¯•ç­–ç•¥ï¼Œä¸“é—¨é’ˆå¯¹è·¨äº‘ç›˜ä¼ è¾“
func (f *Fs) shouldRetryCrossCloudTransfer(err error, attempt int, transferredBytes int64, fileSize int64) (bool, time.Duration) {
	maxRetries := 5 // é»˜è®¤æœ€å¤šé‡è¯•5æ¬¡

	// å¦‚æœå·²ä¼ è¾“äº†å¤§é‡æ•°æ®ï¼Œç»™æ›´å¤šé‡è¯•æœºä¼š
	if transferredBytes > 100*1024*1024 { // è¶…è¿‡100MB
		maxRetries = 8
	}
	if transferredBytes > 500*1024*1024 { // è¶…è¿‡500MB
		maxRetries = 10
	}

	if attempt >= maxRetries {
		return false, 0
	}

	// æ£€æŸ¥é”™è¯¯ç±»å‹
	errStr := strings.ToLower(err.Error())
	isRetryableError := false

	retryableErrors := []string{
		"context deadline exceeded",
		"client.timeout",
		"request canceled",
		"connection reset",
		"temporary failure",
		"network timeout",
		"read timeout",
		"write timeout",
		"i/o timeout",
		"connection refused",
		"no such host",
		"network unreachable",
	}

	for _, retryable := range retryableErrors {
		if strings.Contains(errStr, retryable) {
			isRetryableError = true
			break
		}
	}

	if !isRetryableError {
		return false, 0
	}

	// è®¡ç®—é€€é¿å»¶è¿Ÿ
	baseDelay := time.Second

	// æ ¹æ®ä¼ è¾“è¿›åº¦è°ƒæ•´å»¶è¿Ÿ
	if transferredBytes > 0 && fileSize > 0 {
		progressRatio := float64(transferredBytes) / float64(fileSize)
		if progressRatio > 0.8 { // æ¥è¿‘å®Œæˆï¼Œå¿«é€Ÿé‡è¯•
			baseDelay = 2 * time.Second
		} else if progressRatio > 0.5 { // ä¼ è¾“è¿‡åŠï¼Œä¸­ç­‰å»¶è¿Ÿ
			baseDelay = 3 * time.Second
		} else { // åˆšå¼€å§‹ï¼Œè¾ƒé•¿å»¶è¿Ÿ
			baseDelay = 5 * time.Second
		}
	}

	// æŒ‡æ•°é€€é¿ï¼Œä½†æœ‰ä¸Šé™
	delay := time.Duration(attempt*attempt) * baseDelay
	maxDelay := 30 * time.Second
	if delay > maxDelay {
		delay = maxDelay
	}

	return true, delay
}

// buildVersionedEndpoint æ„å»ºå¸¦ç‰ˆæœ¬çš„ç«¯ç‚¹URL
func (f *Fs) buildVersionedEndpoint(baseEndpoint, version string) string {
	// å¦‚æœç«¯ç‚¹å·²ç»åŒ…å«ç‰ˆæœ¬ï¼Œç›´æ¥è¿”å›
	if strings.Contains(baseEndpoint, "/v1/") || strings.Contains(baseEndpoint, "/v2/") {
		return baseEndpoint
	}

	// æ’å…¥ç‰ˆæœ¬å·
	// ä¾‹å¦‚: "/upload/file/create" -> "/upload/v2/file/create"
	parts := strings.Split(baseEndpoint, "/")
	if len(parts) >= 3 {
		// åœ¨ç¬¬äºŒä¸ªéƒ¨åˆ†åæ’å…¥ç‰ˆæœ¬
		newParts := make([]string, 0, len(parts)+1)
		newParts = append(newParts, parts[0], parts[1], version)
		newParts = append(newParts, parts[2:]...)
		return strings.Join(newParts, "/")
	}

	return baseEndpoint
}

// UnifiedErrorClassifier123 123ç½‘ç›˜ç»Ÿä¸€é”™è¯¯åˆ†ç±»å™¨
type UnifiedErrorClassifier123 struct {
	// é”™è¯¯åˆ†ç±»ç»Ÿè®¡
	ServerOverloadCount int64
	URLExpiredCount     int64
	NetworkTimeoutCount int64
	RateLimitCount      int64
	UnknownErrorCount   int64
}

// ClassifyError ç»Ÿä¸€é”™è¯¯åˆ†ç±»æ–¹æ³•ï¼ˆ123ç½‘ç›˜ç‰ˆæœ¬ï¼‰
func (c *UnifiedErrorClassifier123) ClassifyError(err error) string {
	if err == nil {
		return "no_error"
	}

	errStr := strings.ToLower(err.Error())

	// æœåŠ¡å™¨è¿‡è½½é”™è¯¯
	if strings.Contains(errStr, "502") || strings.Contains(errStr, "bad gateway") ||
		strings.Contains(errStr, "503") || strings.Contains(errStr, "service unavailable") ||
		strings.Contains(errStr, "504") || strings.Contains(errStr, "gateway timeout") ||
		strings.Contains(errStr, "internal server error") {
		c.ServerOverloadCount++
		return "server_overload"
	}

	// URLè¿‡æœŸé”™è¯¯
	if strings.Contains(errStr, "download url is invalid") || strings.Contains(errStr, "expired") ||
		strings.Contains(errStr, "urlè¿‡æœŸ") || strings.Contains(errStr, "é“¾æ¥å¤±æ•ˆ") ||
		strings.Contains(errStr, "invalid download url") {
		c.URLExpiredCount++
		return "url_expired"
	}

	// ç½‘ç»œè¶…æ—¶é”™è¯¯
	if strings.Contains(errStr, "timeout") || strings.Contains(errStr, "deadline exceeded") ||
		strings.Contains(errStr, "context canceled") || strings.Contains(errStr, "connection reset") ||
		strings.Contains(errStr, "client.timeout") || strings.Contains(errStr, "i/o timeout") {
		c.NetworkTimeoutCount++
		return "network_timeout"
	}

	// é™æµé”™è¯¯
	if strings.Contains(errStr, "429") || strings.Contains(errStr, "too many requests") ||
		strings.Contains(errStr, "rate limit") || strings.Contains(errStr, "é¢‘ç‡è¿‡é«˜") {
		c.RateLimitCount++
		return "rate_limit"
	}

	// è®¤è¯é”™è¯¯
	if strings.Contains(errStr, "401") || strings.Contains(errStr, "unauthorized") ||
		strings.Contains(errStr, "token") || strings.Contains(errStr, "è®¤è¯å¤±è´¥") {
		return "auth_error"
	}

	// æƒé™é”™è¯¯
	if strings.Contains(errStr, "403") || strings.Contains(errStr, "forbidden") ||
		strings.Contains(errStr, "permission denied") {
		return "permission_error"
	}

	// èµ„æºä¸å­˜åœ¨é”™è¯¯
	if strings.Contains(errStr, "404") || strings.Contains(errStr, "not found") ||
		strings.Contains(errStr, "file not found") {
		return "not_found"
	}

	c.UnknownErrorCount++
	return "unknown"
}

// shouldRetry æ ¹æ®å“åº”å’Œé”™è¯¯ç¡®å®šæ˜¯å¦é‡è¯•APIè°ƒç”¨
// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€é”™è¯¯åˆ†ç±»å’Œé‡è¯•ç­–ç•¥ï¼Œä¸115ç½‘ç›˜ä¿æŒä¸€è‡´
func shouldRetry(ctx context.Context, resp *http.Response, err error) (bool, error) {
	// ä½¿ç”¨rcloneæ ‡å‡†çš„ä¸Šä¸‹æ–‡é”™è¯¯æ£€æŸ¥
	if fserrors.ContextError(ctx, &err) {
		return false, err
	}

	// ğŸ”§ ä¼˜åŒ–ï¼šä½¿ç”¨ç»Ÿä¸€é”™è¯¯åˆ†ç±»å™¨ï¼Œä¸115ç½‘ç›˜ä¿æŒä¸€è‡´
	classifier := &UnifiedErrorClassifier123{}
	errorType := classifier.ClassifyError(err)

	// åŸºäºé”™è¯¯ç±»å‹å†³å®šæ˜¯å¦é‡è¯•
	switch errorType {
	case "server_overload", "network_timeout":
		// æœåŠ¡å™¨è¿‡è½½å’Œç½‘ç»œè¶…æ—¶é”™è¯¯åº”è¯¥é‡è¯•
		fs.Debugf(nil, "123ç½‘ç›˜æ£€æµ‹åˆ°%sé”™è¯¯ï¼Œå°†é‡è¯•", errorType)
		return true, err

	case "rate_limit":
		// é™æµé”™è¯¯åº”è¯¥é‡è¯•ï¼Œä½†ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿ
		fs.Debugf(nil, "123ç½‘ç›˜æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œä½¿ç”¨30ç§’å»¶è¿Ÿé‡è¯•")
		return true, fserrors.NewErrorRetryAfter(30 * time.Second)

	case "url_expired":
		// URLè¿‡æœŸé”™è¯¯åº”è¯¥é‡è¯•ï¼ˆä¼šè§¦å‘URLåˆ·æ–°ï¼‰
		fs.Debugf(nil, "123ç½‘ç›˜æ£€æµ‹åˆ°URLè¿‡æœŸé”™è¯¯ï¼Œå°†é‡è¯•")
		return true, err

	case "auth_error":
		// è®¤è¯é”™è¯¯éœ€è¦åˆ·æ–°tokenåé‡è¯•
		fs.Debugf(nil, "123ç½‘ç›˜æ£€æµ‹åˆ°è®¤è¯é”™è¯¯ï¼Œéœ€è¦åˆ·æ–°token")
		return true, fserrors.NewErrorRetryAfter(1 * time.Second)

	case "permission_error", "not_found":
		// æƒé™é”™è¯¯å’Œèµ„æºä¸å­˜åœ¨é”™è¯¯ä¸é‡è¯•
		fs.Debugf(nil, "123ç½‘ç›˜æ£€æµ‹åˆ°%sé”™è¯¯ï¼Œä¸é‡è¯•", errorType)
		return false, err

	default:
		// ç½‘ç»œé”™è¯¯æ—¶é‡è¯•
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦ä¸ºä¸å¯æ¢å¤çš„é”™è¯¯ï¼ˆä¿ç•™123ç½‘ç›˜ç‰¹å®šçš„é”™è¯¯æ£€æŸ¥ï¼‰
			if isUnrecoverableError(err) {
				return false, err
			}

			// ä½¿ç”¨rcloneæ ‡å‡†çš„é‡è¯•åˆ¤æ–­
			return fserrors.ShouldRetry(err), err
		}

		// æ£€æŸ¥HTTPçŠ¶æ€ç  - ä½¿ç”¨rcloneæ ‡å‡†å¤„ç†åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šå¤„ç†
		if resp != nil {
			switch resp.StatusCode {
			case http.StatusTooManyRequests:
				// é€Ÿç‡å—é™ - ä½¿ç”¨è¾ƒé•¿çš„é€€é¿æ—¶é—´
				fs.Debugf(nil, "é€Ÿç‡å—é™ï¼ˆAPIé”™è¯¯429ï¼‰ï¼Œå°†ä½¿ç”¨æ›´é•¿é€€é¿æ—¶é—´é‡è¯•")
				return true, fserrors.NewErrorRetryAfter(calculateRetryDelay(3))
			case http.StatusInternalServerError, http.StatusBadGateway, http.StatusServiceUnavailable, http.StatusGatewayTimeout:
				// æœåŠ¡å™¨é”™è¯¯ - ä½¿ç”¨æ ‡å‡†é‡è¯•
				fs.Debugf(nil, "æœåŠ¡å™¨é”™è¯¯ %dï¼Œå°†é‡è¯•", resp.StatusCode)
				return true, fserrors.NewErrorRetryAfter(baseRetryDelay)
			case http.StatusUnauthorized:
				// ä»¤ç‰Œå¯èƒ½å·²è¿‡æœŸ - ä¸é‡è¯•ï¼Œè®©è°ƒç”¨è€…å¤„ç†ä»¤ç‰Œåˆ·æ–°
				return false, fserrors.NewErrorRetryAfter(baseRetryDelay)
			}
		}

		return false, err
	}
}

// calculateRetryDelay è®¡ç®—æŒ‡æ•°é€€é¿çš„é‡è¯•å»¶è¿Ÿ
func calculateRetryDelay(attempt int) time.Duration {
	if attempt <= 0 {
		return baseRetryDelay
	}

	delay := time.Duration(float64(baseRetryDelay) * math.Pow(retryMultiplier, float64(attempt-1)))
	if delay > maxRetryDelay {
		delay = maxRetryDelay
	}

	// æ·»åŠ éšæœºæŠ–åŠ¨ä»¥é¿å…é›·ç¾¤æ•ˆåº”
	jitter := time.Duration(rand.Float64() * float64(delay) * 0.1) // 10%çš„æŠ–åŠ¨
	return delay + jitter
}

// makeAPICallWithRest ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡ŒAPIè°ƒç”¨ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶
// è¿™æ˜¯æ¨èçš„APIè°ƒç”¨æ–¹æ³•ï¼Œæ›¿ä»£ç›´æ¥ä½¿ç”¨HTTPå®¢æˆ·ç«¯
func (f *Fs) makeAPICallWithRest(ctx context.Context, endpoint string, method string, reqBody any, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API: %s %s", method, endpoint)

	// å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿restå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
	if f.rst == nil {
		fs.Errorf(f, "âš ï¸  restå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œå°è¯•é‡æ–°åˆ›å»º")
		if f.client != nil {
			f.rst = rest.NewClient(f.client).SetRoot(openAPIRootURL)
			fs.Debugf(f, "âœ… restå®¢æˆ·ç«¯é‡æ–°åˆ›å»ºæˆåŠŸ")
		} else {
			return fmt.Errorf("HTTPå®¢æˆ·ç«¯å’Œrestå®¢æˆ·ç«¯éƒ½æœªåˆå§‹åŒ–ï¼Œæ— æ³•è¿›è¡ŒAPIè°ƒç”¨")
		}
	}

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - æ ¹æ®å®˜æ–¹æ–‡æ¡£åªæœ‰ä¸¤ä¸ªAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		// åªæœ‰åˆ†ç‰‡ä¸Šä¼ å’Œå•æ­¥ä¸Šä¼ ä½¿ç”¨ä¸Šä¼ åŸŸåï¼ˆå®˜æ–¹æ–‡æ¡£æ˜ç¡®è¯´æ˜ï¼‰
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		// å…¶ä»–APIï¼ˆåŒ…æ‹¬createã€upload_completeï¼‰ä½¿ç”¨æ ‡å‡†APIåŸŸå
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.CallJSON(ctx, &opts, reqBody, respBody)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		return shouldRetry(ctx, resp, err)
	})

	if err != nil {
		return fmt.Errorf("APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// makeAPICallWithRestMultipartToDomain ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨åˆ°æŒ‡å®šåŸŸå
func (f *Fs) makeAPICallWithRestMultipartToDomain(ctx context.Context, domain string, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart APIåˆ°åŸŸå: %s %s %s", method, domain, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ä½¿ç”¨è°ƒé€Ÿå™¨è¿›è¡ŒAPIè°ƒç”¨
	return pacer.Call(func() (bool, error) {
		opts := rest.Opts{
			Method:  method,
			RootURL: domain,
			Path:    endpoint,
			Body:    body,
			ExtraHeaders: map[string]string{
				"Content-Type": contentType,
			},
		}

		// æ·»åŠ å¿…è¦çš„è®¤è¯å¤´
		opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
		opts.ExtraHeaders["Platform"] = "open_platform"
		opts.ExtraHeaders["User-Agent"] = f.opt.UserAgent

		var resp *http.Response
		var err error
		resp, err = f.rst.Call(ctx, &opts)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
			if fserrors.ShouldRetry(err) {
				fs.Debugf(f, "APIè°ƒç”¨å¤±è´¥ï¼Œå°†é‡è¯•: %v", err)
				return true, err
			}
			return false, fmt.Errorf("APIè°ƒç”¨å¤±è´¥: %w", err)
		}
		defer resp.Body.Close()

		// æ£€æŸ¥å“åº”çŠ¶æ€
		if resp.StatusCode >= 400 {
			body, _ := io.ReadAll(resp.Body)

			// ç‰¹æ®Šå¤„ç†401é”™è¯¯ - tokenè¿‡æœŸ
			if resp.StatusCode == http.StatusUnauthorized {
				fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
				// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
				refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
				if refreshErr != nil {
					fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
					return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
				}
				// æ›´æ–°Authorizationå¤´
				opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
				fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
				return true, nil // é‡è¯•
			}

			return false, fmt.Errorf("APIè¿”å›é”™è¯¯çŠ¶æ€ %d: %s", resp.StatusCode, string(body))
		}

		// è§£æå“åº”JSON
		if respBody != nil {
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”JSONå¤±è´¥: %w", err)
			}
		}

		return false, nil
	})
}

// makeAPICallWithRestMultipart ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯è¿›è¡Œmultipart APIè°ƒç”¨
func (f *Fs) makeAPICallWithRestMultipart(ctx context.Context, endpoint string, method string, body io.Reader, contentType string, respBody any) error {
	fs.Debugf(f, "ğŸ”„ ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨multipart API: %s %s", method, endpoint)

	// ç¡®ä¿ä»¤ç‰Œåœ¨è¿›è¡ŒAPIè°ƒç”¨å‰æœ‰æ•ˆ
	err := f.refreshTokenIfNecessary(ctx, false, false)
	if err != nil {
		return fmt.Errorf("åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %w", err)
	}

	// æ ¹æ®ç«¯ç‚¹è·å–é€‚å½“çš„è°ƒé€Ÿå™¨
	pacer := f.getPacerForEndpoint(endpoint)

	// ç¡®å®šåŸºç¡€URL - ç‰¹å®šAPIä½¿ç”¨ä¸Šä¼ åŸŸå
	var baseURL string
	if strings.Contains(endpoint, "/upload/v2/file/slice") ||
		strings.Contains(endpoint, "/upload/v2/file/single/create") {
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}
		baseURL = uploadDomain
	} else {
		baseURL = openAPIRootURL
	}

	// æ„é€ resté€‰é¡¹ï¼ŒåŒ…å«å¿…è¦çš„HTTPå¤´
	opts := rest.Opts{
		Method:  method,
		Path:    endpoint,
		RootURL: baseURL,
		Body:    body,
		ExtraHeaders: map[string]string{
			"Authorization": "Bearer " + f.token,
			"Platform":      "open_platform",
			"User-Agent":    f.opt.UserAgent,
			"Content-Type":  contentType,
		},
	}

	// ä½¿ç”¨paceråŒ…è£…è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶å’Œé‡è¯•
	var resp *http.Response
	err = pacer.Call(func() (bool, error) {
		var err error
		resp, err = f.rst.Call(ctx, &opts)

		// æ£€æŸ¥æ˜¯å¦æ˜¯401é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•åˆ·æ–°token
		if resp != nil && resp.StatusCode == http.StatusUnauthorized {
			fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå¼ºåˆ¶åˆ·æ–°token")
			// å¼ºåˆ¶åˆ·æ–°tokenï¼Œå¿½ç•¥æ—¶é—´æ£€æŸ¥
			refreshErr := f.refreshTokenIfNecessary(ctx, true, true)
			if refreshErr != nil {
				fs.Errorf(f, "åˆ·æ–°tokenå¤±è´¥: %v", refreshErr)
				return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", refreshErr)
			}
			// æ›´æ–°Authorizationå¤´
			opts.ExtraHeaders["Authorization"] = "Bearer " + f.token
			fs.Debugf(f, "tokenå·²å¼ºåˆ¶åˆ·æ–°ï¼Œå°†é‡è¯•APIè°ƒç”¨")
			return true, nil // é‡è¯•
		}

		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// è§£æå“åº”
		if respBody != nil {
			defer resp.Body.Close()
			err = json.NewDecoder(resp.Body).Decode(respBody)
			if err != nil {
				return false, fmt.Errorf("è§£æå“åº”å¤±è´¥: %w", err)
			}
		}

		return false, nil
	})

	if err != nil {
		return fmt.Errorf("multipart APIè°ƒç”¨å¤±è´¥ [%s %s]: %w", method, endpoint, err)
	}

	return nil
}

// getPacerForEndpoint æ ¹æ®APIç«¯ç‚¹è¿”å›é€‚å½“çš„è°ƒé€Ÿå™¨
// åŸºäºå®˜æ–¹APIé™æµæ–‡æ¡£ï¼šhttps://123yunpan.yuque.com/org-wiki-123yunpan-muaork/cr6ced/
// æ›´æ–°æ—¶é—´ï¼š2025-07-04ï¼Œä½¿ç”¨æœ€æ–°å®˜æ–¹QPSé™åˆ¶è¡¨
func (f *Fs) getPacerForEndpoint(endpoint string) *fs.Pacer {
	switch {
	// é«˜é¢‘ç‡API (15-20 QPS)
	case strings.Contains(endpoint, "/api/v2/file/list"):
		return f.listPacer // ~14 QPS (å®˜æ–¹15 QPS)
	case strings.Contains(endpoint, "/upload/v2/file/get_upload_url"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"):
		return f.downloadPacer // ~16 QPS (å®˜æ–¹20 QPS)

	// ä¸­ç­‰é¢‘ç‡API (8-10 QPS)
	case strings.Contains(endpoint, "/api/v1/file/move"),
		strings.Contains(endpoint, "/api/v1/file/infos"),
		strings.Contains(endpoint, "/api/v1/user/info"):
		return f.listPacer // ~8 QPS (å®˜æ–¹10 QPS)
	case strings.Contains(endpoint, "/api/v1/access_token"):
		return f.downloadPacer // ~6 QPS (å®˜æ–¹8 QPS)

	// ä½é¢‘ç‡API (5 QPS)
	case strings.Contains(endpoint, "/upload/v1/file/create"),
		strings.Contains(endpoint, "/upload/v2/file/create"),
		strings.Contains(endpoint, "/upload/v1/file/mkdir"),
		strings.Contains(endpoint, "/api/v1/file/trash"),
		strings.Contains(endpoint, "/upload/v2/file/upload_complete"),
		strings.Contains(endpoint, "/api/v1/file/download_info"):
		return f.strictPacer // ~4 QPS (å®˜æ–¹5 QPS)

	// æœ€ä½é¢‘ç‡API (1 QPS)
	case strings.Contains(endpoint, "/api/v1/file/delete"),
		strings.Contains(endpoint, "/api/v1/file/list"),
		strings.Contains(endpoint, "/api/v1/video/transcode/list"):
		return f.strictPacer // ~0.8 QPS (å®˜æ–¹1 QPS) - ä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶

	// v2åˆ†ç‰‡ä¸Šä¼ API (ç‰¹æ®Šå¤„ç†ï¼Œä½¿ç”¨ä¸“ç”¨çš„uploadPacer)
	case strings.Contains(endpoint, "/upload/v2/file/slice"):
		return f.uploadPacer // ~5 QPS (åŸºäºå®˜æ–¹æ–‡æ¡£å’Œå®é™…æµ‹è¯•)

	// ä¸Šä¼ åŸŸåAPI (ç‰¹æ®Šå¤„ç†)
	case strings.Contains(endpoint, "/upload/v2/file/domain"):
		return f.strictPacer // ä¿å®ˆå¤„ç†ï¼Œé¿å…é¢‘ç¹è°ƒç”¨

	default:
		// ä¸ºäº†å®‰å…¨èµ·è§ï¼ŒæœªçŸ¥ç«¯ç‚¹é»˜è®¤ä½¿ç”¨æœ€ä¸¥æ ¼çš„è°ƒé€Ÿå™¨
		fs.Debugf(f, "âš ï¸  æœªçŸ¥APIç«¯ç‚¹ï¼Œä½¿ç”¨æœ€ä¸¥æ ¼é™åˆ¶: %s", endpoint)
		return f.strictPacer // æœ€ä¸¥æ ¼çš„é™åˆ¶
	}
}

// ErrorContext é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨äºæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
type ErrorContext struct {
	Operation string            // æ“ä½œç±»å‹
	Method    string            // HTTPæ–¹æ³•
	Endpoint  string            // APIç«¯ç‚¹
	FileID    string            // æ–‡ä»¶IDï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	FileName  string            // æ–‡ä»¶åï¼ˆå¦‚æœé€‚ç”¨ï¼‰
	Extra     map[string]string // é¢å¤–ä¿¡æ¯
}

// WrapError åŒ…è£…é”™è¯¯å¹¶æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
func (f *Fs) WrapError(err error, ctx ErrorContext) error {
	if err == nil {
		return nil
	}

	var details []string
	if ctx.Operation != "" {
		details = append(details, fmt.Sprintf("æ“ä½œ=%s", ctx.Operation))
	}
	if ctx.Method != "" && ctx.Endpoint != "" {
		details = append(details, fmt.Sprintf("API=%s %s", ctx.Method, ctx.Endpoint))
	}
	if ctx.FileID != "" {
		details = append(details, fmt.Sprintf("æ–‡ä»¶ID=%s", ctx.FileID))
	}
	if ctx.FileName != "" {
		details = append(details, fmt.Sprintf("æ–‡ä»¶å=%s", ctx.FileName))
	}
	for key, value := range ctx.Extra {
		details = append(details, fmt.Sprintf("%s=%s", key, value))
	}

	if len(details) > 0 {
		return fmt.Errorf("%s [%s]: %w", ctx.Operation, strings.Join(details, ", "), err)
	}
	return fmt.Errorf("%s: %w", ctx.Operation, err)
}

// calculateAdaptiveTimeout æ ¹æ®æ“ä½œç±»å‹è®¡ç®—è¶…æ—¶æ—¶é—´
// ç®€åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨rcloneæ ‡å‡†è¶…æ—¶é…ç½®åŠ ä¸Š123ç½‘ç›˜ç‰¹å®šè°ƒæ•´
func (f *Fs) calculateAdaptiveTimeout(method, endpoint string) time.Duration {
	// ä½¿ç”¨rcloneçš„æ ‡å‡†è¶…æ—¶é…ç½®
	baseTimeout := time.Duration(f.opt.Timeout)
	if baseTimeout <= 0 {
		baseTimeout = defaultTimeout
	}

	// ç®€åŒ–çš„æ“ä½œç±»å‹è°ƒæ•´
	switch {
	case strings.Contains(endpoint, "/upload/"):
		return baseTimeout * 3 // ä¸Šä¼ æ“ä½œéœ€è¦æ›´é•¿æ—¶é—´
	case strings.Contains(endpoint, "/download"):
		return baseTimeout * 2 // ä¸‹è½½æ“ä½œéœ€è¦æ›´é•¿æ—¶é—´
	case method == "POST":
		return baseTimeout * 2 // POSTæ“ä½œé€šå¸¸éœ€è¦æ›´å¤šæ—¶é—´
	default:
		return baseTimeout // ä½¿ç”¨æ ‡å‡†è¶…æ—¶
	}
}

// calculateCrossCloudTimeout é’ˆå¯¹è·¨äº‘ç›˜ä¼ è¾“çš„ç‰¹æ®Šè¶…æ—¶è®¡ç®—
func (f *Fs) calculateCrossCloudTimeout(fileSize int64, transferredBytes int64) time.Duration {
	baseTimeout := time.Duration(f.opt.Timeout)
	if baseTimeout <= 0 {
		baseTimeout = 300 * time.Second // 5åˆ†é’ŸåŸºç¡€è¶…æ—¶
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´
	if fileSize > 1024*1024*1024 { // å¤§äº1GB
		baseTimeout *= 4 // 20åˆ†é’Ÿ
	} else if fileSize > 500*1024*1024 { // å¤§äº500MB
		baseTimeout *= 3 // 15åˆ†é’Ÿ
	} else if fileSize > 100*1024*1024 { // å¤§äº100MB
		baseTimeout *= 2 // 10åˆ†é’Ÿ
	}

	// æ ¹æ®å·²ä¼ è¾“è¿›åº¦è°ƒæ•´ï¼ˆé¿å…é‡æ–°å¼€å§‹çš„æŸå¤±ï¼‰
	if transferredBytes > 0 && fileSize > 0 {
		progressRatio := float64(transferredBytes) / float64(fileSize)
		if progressRatio > 0.5 { // å·²ä¼ è¾“è¶…è¿‡50%
			baseTimeout = time.Duration(float64(baseTimeout) * 1.5) // ç»™æ›´å¤šæ—¶é—´å®Œæˆ
		} else if progressRatio > 0.8 { // å·²ä¼ è¾“è¶…è¿‡80%
			baseTimeout *= 2 // æ¥è¿‘å®Œæˆï¼Œç»™è¶³å¤Ÿæ—¶é—´
		}
	}

	// æ ¹æ®ç½‘ç»œè´¨é‡è°ƒæ•´ï¼ˆä½¿ç”¨åŸºç¡€æ€§èƒ½æŒ‡æ ‡ï¼‰
	if f.performanceMetrics != nil {
		// ä½¿ç”¨åŸºç¡€æ€§èƒ½æŒ‡æ ‡è¿›è¡Œç½‘ç»œè´¨é‡è¯„ä¼°
		stats := f.performanceMetrics.GetStats()
		if apiErrorRate, ok := stats["api_error_rate"].(float64); ok {
			if apiErrorRate > 0.5 { // é”™è¯¯ç‡è¾ƒé«˜ï¼Œç½‘ç»œè´¨é‡è¾ƒå·®
				baseTimeout = time.Duration(float64(baseTimeout) * 1.5)
			} else if apiErrorRate < 0.1 { // é”™è¯¯ç‡è¾ƒä½ï¼Œç½‘ç»œè´¨é‡è‰¯å¥½
				baseTimeout = time.Duration(float64(baseTimeout) * 0.8)
			}
		}
	}

	// è®¾ç½®æœ€å°å’Œæœ€å¤§é™åˆ¶
	minTimeout := 60 * time.Second // æœ€å°‘1åˆ†é’Ÿ
	maxTimeout := 30 * time.Minute // æœ€å¤š30åˆ†é’Ÿ

	if baseTimeout < minTimeout {
		baseTimeout = minTimeout
	} else if baseTimeout > maxTimeout {
		baseTimeout = maxTimeout
	}

	return baseTimeout
}

// ResumeInfo æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
type ResumeInfo struct {
	PreuploadID    string           `json:"preupload_id"`
	FileName       string           `json:"file_name"`
	FileSize       int64            `json:"file_size"`
	ChunkSize      int64            `json:"chunk_size"`
	UploadedChunks map[int64]string `json:"uploaded_chunks"` // chunk_index -> etag
	LastChunkIndex int64            `json:"last_chunk_index"`
	CreatedAt      time.Time        `json:"created_at"`
	LastUpdated    time.Time        `json:"last_updated"`
	MD5Hash        string           `json:"md5_hash"`
	ParentFileID   int64            `json:"parent_file_id"`
	TotalChunks    int64            `json:"total_chunks"`
	UploadedBytes  int64            `json:"uploaded_bytes"`
}

// ResumeManager æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
type ResumeManager struct {
	mu           sync.RWMutex
	resumeCache  *cache.BadgerCache
	fs           *Fs
	cleanupTimer *time.Timer
}

// NewResumeManager åˆ›å»ºæ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
func NewResumeManager(fs *Fs, cacheDir string) (*ResumeManager, error) {
	resumeCache, err := cache.NewBadgerCache("resume", cacheDir)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæ–­ç‚¹ç»­ä¼ ç¼“å­˜å¤±è´¥: %w", err)
	}

	rm := &ResumeManager{
		resumeCache: resumeCache,
		fs:          fs,
	}

	// å¯åŠ¨å®šæœŸæ¸…ç†è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	rm.startCleanupTimer()

	return rm, nil
}

// SaveResumeInfo ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *ResumeManager) SaveResumeInfo(info *ResumeInfo) error {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	info.LastUpdated = time.Now()
	cacheKey := fmt.Sprintf("resume_%s", info.PreuploadID)

	err := rm.resumeCache.Set(cacheKey, info, 24*time.Hour)
	if err != nil {
		return fmt.Errorf("ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	fs.Debugf(rm.fs, "ä¿å­˜æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s, å·²ä¸Šä¼ åˆ†ç‰‡: %d/%d",
		info.FileName, len(info.UploadedChunks), info.TotalChunks)

	return nil
}

// LoadResumeInfo åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *ResumeManager) LoadResumeInfo(preuploadID string) (*ResumeInfo, error) {
	rm.mu.RLock()
	defer rm.mu.RUnlock()

	cacheKey := fmt.Sprintf("resume_%s", preuploadID)
	var info ResumeInfo

	found, err := rm.resumeCache.Get(cacheKey, &info)
	if err != nil {
		return nil, fmt.Errorf("åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if !found {
		return nil, nil // æ²¡æœ‰æ‰¾åˆ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
	}

	// æ£€æŸ¥ä¿¡æ¯æ˜¯å¦è¿‡æœŸï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
	if time.Since(info.CreatedAt) > 24*time.Hour {
		rm.DeleteResumeInfo(preuploadID)
		return nil, nil
	}

	fs.Debugf(rm.fs, "åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s, å·²ä¸Šä¼ åˆ†ç‰‡: %d/%d",
		info.FileName, len(info.UploadedChunks), info.TotalChunks)

	return &info, nil
}

// UpdateChunkInfo æ›´æ–°åˆ†ç‰‡ä¸Šä¼ ä¿¡æ¯
func (rm *ResumeManager) UpdateChunkInfo(preuploadID string, chunkIndex int64, etag string, chunkSize int64) error {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	cacheKey := fmt.Sprintf("resume_%s", preuploadID)
	var info ResumeInfo

	found, err := rm.resumeCache.Get(cacheKey, &info)
	if err != nil {
		return fmt.Errorf("è·å–æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if !found {
		return fmt.Errorf("æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ä¸å­˜åœ¨: %s", preuploadID)
	}

	// æ›´æ–°åˆ†ç‰‡ä¿¡æ¯
	if info.UploadedChunks == nil {
		info.UploadedChunks = make(map[int64]string)
	}

	info.UploadedChunks[chunkIndex] = etag
	info.LastChunkIndex = chunkIndex
	info.LastUpdated = time.Now()
	info.UploadedBytes += chunkSize

	// ä¿å­˜æ›´æ–°åçš„ä¿¡æ¯
	err = rm.resumeCache.Set(cacheKey, &info, 24*time.Hour)
	if err != nil {
		return fmt.Errorf("æ›´æ–°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	fs.Debugf(rm.fs, "æ›´æ–°åˆ†ç‰‡ä¿¡æ¯: %s, åˆ†ç‰‡ %d, è¿›åº¦: %d/%d",
		info.FileName, chunkIndex, len(info.UploadedChunks), info.TotalChunks)

	return nil
}

// DeleteResumeInfo åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
func (rm *ResumeManager) DeleteResumeInfo(preuploadID string) error {
	rm.mu.Lock()
	defer rm.mu.Unlock()

	cacheKey := fmt.Sprintf("resume_%s", preuploadID)
	err := rm.resumeCache.Delete(cacheKey)
	if err != nil {
		return fmt.Errorf("åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯å¤±è´¥: %w", err)
	}

	fs.Debugf(rm.fs, "åˆ é™¤æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s", preuploadID)
	return nil
}

// GetResumeProgress è·å–æ–­ç‚¹ç»­ä¼ è¿›åº¦
func (rm *ResumeManager) GetResumeProgress(preuploadID string) (uploaded, total int64, percentage float64, err error) {
	info, err := rm.LoadResumeInfo(preuploadID)
	if err != nil {
		return 0, 0, 0, err
	}

	if info == nil {
		return 0, 0, 0, nil
	}

	uploaded = int64(len(info.UploadedChunks))
	total = info.TotalChunks

	if total > 0 {
		percentage = float64(uploaded) / float64(total) * 100.0
	}

	return uploaded, total, percentage, nil
}

// startCleanupTimer å¯åŠ¨å®šæœŸæ¸…ç†å®šæ—¶å™¨
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´çµæ´»çš„æ¸…ç†é…ç½®å’Œå¼‚æ­¥æ‰§è¡Œ
func (rm *ResumeManager) startCleanupTimer() {
	// æ¸…ç†é—´éš”é…ç½®ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
	cleanupInterval := 2 * time.Hour // æ¯2å°æ—¶æ¸…ç†ä¸€æ¬¡

	fs.Debugf(rm.fs, "ğŸ• å¯åŠ¨æ–­ç‚¹ç»­ä¼ å®šæœŸæ¸…ç†å®šæ—¶å™¨ï¼Œé—´éš”: %v", cleanupInterval)

	rm.cleanupTimer = time.AfterFunc(cleanupInterval, func() {
		// å¼‚æ­¥æ‰§è¡Œæ¸…ç†ï¼Œé¿å…é˜»å¡
		go func() {
			defer func() {
				if r := recover(); r != nil {
					fs.Debugf(rm.fs, "âš ï¸  æ–­ç‚¹ç»­ä¼ æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿpanic: %v", r)
				}
			}()

			fs.Debugf(rm.fs, "ğŸ§¹ å¼€å§‹æ‰§è¡Œå®šæœŸæ–­ç‚¹ç»­ä¼ æ¸…ç†...")
			startTime := time.Now()

			rm.cleanupExpiredResumeInfo()

			duration := time.Since(startTime)
			fs.Debugf(rm.fs, "âœ… æ–­ç‚¹ç»­ä¼ å®šæœŸæ¸…ç†å®Œæˆï¼Œè€—æ—¶: %v", duration)
		}()

		// é‡æ–°å¯åŠ¨å®šæ—¶å™¨
		rm.startCleanupTimer()
	})
}

// cleanupExpiredResumeInfo æ¸…ç†è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å…¨é¢çš„æ¸…ç†ç­–ç•¥å’Œé”™è¯¯å¤„ç†
func (rm *ResumeManager) cleanupExpiredResumeInfo() {
	fs.Debugf(rm.fs, "ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯")

	// è·å–æ‰€æœ‰æ–­ç‚¹ç»­ä¼ ä¿¡æ¯çš„é”®
	keys, err := rm.getAllResumeKeys()
	if err != nil {
		fs.Debugf(rm.fs, "âŒ è·å–æ–­ç‚¹ç»­ä¼ é”®åˆ—è¡¨å¤±è´¥: %v", err)
		return
	}

	if len(keys) == 0 {
		fs.Debugf(rm.fs, "ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯ï¼Œè·³è¿‡æ¸…ç†")
		return
	}

	cleanedCount := 0
	corruptedCount := 0
	totalCount := len(keys)

	// å¤šå±‚è¿‡æœŸç­–ç•¥
	now := time.Now()
	expiredThresholds := map[string]time.Time{
		"è¶…é•¿æœŸ": now.Add(-7 * 24 * time.Hour), // 7å¤©
		"é•¿æœŸ":  now.Add(-3 * 24 * time.Hour), // 3å¤©
		"ä¸­æœŸ":  now.Add(-24 * time.Hour),     // 1å¤©
		"çŸ­æœŸ":  now.Add(-6 * time.Hour),      // 6å°æ—¶ï¼ˆç”¨äºå¤±è´¥çš„ä¸Šä¼ ï¼‰
	}

	fs.Debugf(rm.fs, "ğŸ“Š å¼€å§‹å¤„ç† %d ä¸ªæ–­ç‚¹ç»­ä¼ æ¡ç›®", totalCount)

	for i, key := range keys {
		// æ˜¾ç¤ºè¿›åº¦
		if i%10 == 0 && i > 0 {
			fs.Debugf(rm.fs, "ğŸ”„ æ¸…ç†è¿›åº¦: %d/%d", i, totalCount)
		}

		// å°è¯•åŠ è½½æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
		info, err := rm.LoadResumeInfo(key)
		if err != nil {
			// å¦‚æœåŠ è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯æŸåçš„æ•°æ®ï¼Œç›´æ¥åˆ é™¤
			fs.Debugf(rm.fs, "ğŸ—‘ï¸  åˆ é™¤æŸåçš„æ–­ç‚¹ç»­ä¼ æ•°æ®: %s, é”™è¯¯: %v", key, err)
			rm.DeleteResumeInfo(key)
			corruptedCount++
			cleanedCount++
			continue
		}

		// æ£€æŸ¥ä¸åŒçº§åˆ«çš„è¿‡æœŸ
		shouldDelete := false
		deleteReason := ""

		if info.CreatedAt.Before(expiredThresholds["è¶…é•¿æœŸ"]) {
			shouldDelete = true
			deleteReason = "è¶…é•¿æœŸè¿‡æœŸ(>7å¤©)"
		} else if info.CreatedAt.Before(expiredThresholds["é•¿æœŸ"]) {
			shouldDelete = true
			deleteReason = "é•¿æœŸè¿‡æœŸ(>3å¤©)"
		} else if info.CreatedAt.Before(expiredThresholds["ä¸­æœŸ"]) {
			shouldDelete = true
			deleteReason = "ä¸­æœŸè¿‡æœŸ(>1å¤©)"
		} else if info.CreatedAt.Before(expiredThresholds["çŸ­æœŸ"]) && info.UploadedBytes == 0 {
			// æ²¡æœ‰ä»»ä½•ä¸Šä¼ è¿›åº¦çš„ä¼šè¯6å°æ—¶åæ¸…ç†ï¼ˆå¯èƒ½æ˜¯å¤±è´¥çš„ä¸Šä¼ ï¼‰
			shouldDelete = true
			deleteReason = "æ— è¿›åº¦ä¸Šä¼ çŸ­æœŸè¿‡æœŸ(>6å°æ—¶)"
		}

		if shouldDelete {
			rm.DeleteResumeInfo(key)
			cleanedCount++
			fs.Debugf(rm.fs, "ğŸ—‘ï¸  åˆ é™¤è¿‡æœŸçš„æ–­ç‚¹ç»­ä¼ ä¿¡æ¯: %s [%s]", key, deleteReason)
		}
	}

	// æ¸…ç†ç»Ÿè®¡æŠ¥å‘Š
	fs.Debugf(rm.fs, "âœ… æ–­ç‚¹ç»­ä¼ ä¿¡æ¯æ¸…ç†å®Œæˆ")
	fs.Debugf(rm.fs, "ğŸ“Š æ¸…ç†ç»Ÿè®¡: æ€»è®¡=%d, æ¸…ç†=%d, æŸå=%d, ä¿ç•™=%d",
		totalCount, cleanedCount, corruptedCount, totalCount-cleanedCount)

	// å¦‚æœæ¸…ç†äº†å¤§é‡æ•°æ®ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
	if cleanedCount > 10 {
		fs.Debugf(rm.fs, "âš ï¸  æœ¬æ¬¡æ¸…ç†äº†è¾ƒå¤šæ•°æ®(%dä¸ª)ï¼Œå»ºè®®æ£€æŸ¥ä¸Šä¼ æµç¨‹æ˜¯å¦æ­£å¸¸", cleanedCount)
	}
}

// getAllResumeKeys è·å–æ‰€æœ‰æ–­ç‚¹ç»­ä¼ ä¿¡æ¯çš„é”®
// å¢å¼ºç‰ˆæœ¬ï¼šå®ç°çœŸæ­£çš„å‰ç¼€æ‰«æåŠŸèƒ½
func (rm *ResumeManager) getAllResumeKeys() ([]string, error) {
	if rm.resumeCache == nil {
		return nil, fmt.Errorf("æ–­ç‚¹ç»­ä¼ ç¼“å­˜æœªåˆå§‹åŒ–")
	}

	// ä½¿ç”¨å‰ç¼€æ‰«æè·å–æ‰€æœ‰resumeç›¸å…³çš„é”®
	var keys []string

	// å°è¯•æ‰«æå¸¸è§çš„æ–­ç‚¹ç»­ä¼ é”®æ¨¡å¼
	// æ–­ç‚¹ç»­ä¼ é”®é€šå¸¸ä»¥æ–‡ä»¶è·¯å¾„æˆ–preuploadIDä¸ºåŸºç¡€
	prefixes := []string{
		"resume_",    // é€šç”¨æ–­ç‚¹ç»­ä¼ å‰ç¼€
		"upload_",    // ä¸Šä¼ ç›¸å…³å‰ç¼€
		"progress_",  // è¿›åº¦ç›¸å…³å‰ç¼€
		"preupload_", // é¢„ä¸Šä¼ ç›¸å…³å‰ç¼€
	}

	for _, prefix := range prefixes {
		// å°è¯•è·å–ä»¥è¯¥å‰ç¼€å¼€å¤´çš„é”®
		// ç”±äºcacheæ¥å£é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§å˜é€šæ–¹æ³•
		// ç”Ÿæˆå¯èƒ½çš„é”®åå¹¶æ£€æŸ¥æ˜¯å¦å­˜åœ¨
		for i := 0; i < 1000; i++ { // é™åˆ¶æ‰«æèŒƒå›´é¿å…æ— é™å¾ªç¯
			testKey := fmt.Sprintf("%s%d", prefix, i)

			// å°è¯•è·å–è¯¥é”®çš„å€¼æ¥æ£€æŸ¥æ˜¯å¦å­˜åœ¨
			var testValue interface{}
			found, err := rm.resumeCache.Get(testKey, &testValue)
			if err == nil && found {
				// é”®å­˜åœ¨ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
				keys = append(keys, testKey)
			}
			// å¦‚æœè¿ç»­å¤šæ¬¡æœªæ‰¾åˆ°ï¼Œè·³å‡ºå¾ªç¯
			if (!found || err != nil) && i > 10 {
				break
			}
		}
	}

	// å¦ä¸€ç§æ–¹æ³•ï¼šåŸºäºå·²çŸ¥çš„ä¸Šä¼ ä¼šè¯æ‰«æ
	// è¿™éœ€è¦ç»´æŠ¤ä¸€ä¸ªæ´»è·ƒä¼šè¯åˆ—è¡¨ï¼Œä½†ç›®å‰ç®€åŒ–å®ç°

	fs.Debugf(rm.fs, "ğŸ“‹ æ‰«æåˆ° %d ä¸ªæ–­ç‚¹ç»­ä¼ ç›¸å…³é”®", len(keys))
	return keys, nil
}

// Close å…³é—­æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
func (rm *ResumeManager) Close() error {
	if rm.cleanupTimer != nil {
		rm.cleanupTimer.Stop()
	}

	if rm.resumeCache != nil {
		return rm.resumeCache.Close()
	}

	return nil
}

// ğŸ—‘ï¸ DownloadURLCacheManager å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// AccessInfo è®¿é—®ä¿¡æ¯
type AccessInfo struct {
	FileID        string    `json:"file_id"`
	AccessCount   int       `json:"access_count"`
	LastAccess    time.Time `json:"last_access"`
	FirstAccess   time.Time `json:"first_access"`
	AccessPattern string    `json:"access_pattern"` // "frequent", "recent", "rare"
}

// EnhancedDownloadURLEntry å¢å¼ºçš„ä¸‹è½½URLç¼“å­˜æ¡ç›®
type EnhancedDownloadURLEntry struct {
	URL           string    `json:"url"`
	ExpireTime    time.Time `json:"expire_time"`
	CachedAt      time.Time `json:"cached_at"`
	AccessCount   int       `json:"access_count"`
	LastAccess    time.Time `json:"last_access"`
	FileSize      int64     `json:"file_size"`
	Priority      int       `json:"priority"`       // ç¼“å­˜ä¼˜å…ˆçº§ 1-10
	PreloadReason string    `json:"preload_reason"` // é¢„åŠ è½½åŸå› 
}

// ğŸ—‘ï¸ NewDownloadURLCacheManager å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// ğŸ—‘ï¸ æ‰€æœ‰DownloadURLCacheManageræ–¹æ³•å·²åˆ é™¤ - ä¸‹è½½URLç¼“å­˜å·²ç§»é™¤

// parseFileID é€šç”¨çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œç»Ÿä¸€é”™è¯¯å¤„ç†
func parseFileID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("æ–‡ä»¶IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseFileIDWithContext å¸¦ä¸Šä¸‹æ–‡çš„æ–‡ä»¶IDè½¬æ¢å‡½æ•°ï¼Œæä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
func parseFileIDWithContext(idStr, context string) (int64, error) {
	id, err := parseFileID(idStr)
	if err != nil {
		return 0, fmt.Errorf("%s: %w", context, err)
	}
	return id, nil
}

// parseParentID è§£æçˆ¶ç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºçˆ¶ç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseParentID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„çˆ¶ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// parseDirID è§£æç›®å½•IDå­—ç¬¦ä¸²ä¸ºint64
// ä¸“é—¨ç”¨äºç›®å½•IDè½¬æ¢ï¼Œæä¾›æ›´æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯
func parseDirID(idStr string) (int64, error) {
	if idStr == "" {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	id, err := strconv.ParseInt(idStr, 10, 64)
	if err != nil {
		return 0, fmt.Errorf("æ— æ•ˆçš„ç›®å½•IDæ ¼å¼: %s", idStr)
	}

	if id < 0 {
		return 0, fmt.Errorf("ç›®å½•IDä¸èƒ½ä¸ºè´Ÿæ•°: %d", id)
	}

	return id, nil
}

// handleHTTPError å¤„ç†HTTPçº§åˆ«çš„é”™è¯¯å¹¶ç¡®å®šé‡è¯•è¡Œä¸º
func (f *Fs) handleHTTPError(ctx context.Context, resp *http.Response) (bool, error) {
	switch resp.StatusCode {
	case http.StatusUnauthorized:
		// ä»¤ç‰Œå¯èƒ½å·²è¿‡æœŸ - å°è¯•åˆ·æ–°ä¸€æ¬¡
		fs.Debugf(f, "æ”¶åˆ°401é”™è¯¯ï¼Œå°è¯•åˆ·æ–°ä»¤ç‰Œ")
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			return false, fmt.Errorf("èº«ä»½éªŒè¯å¤±è´¥: %w", err)
		}
		return true, nil // ä½¿ç”¨æ–°ä»¤ç‰Œé‡è¯•
	case http.StatusTooManyRequests:
		// é€Ÿç‡å—é™ - åº”è¯¥ä½¿ç”¨æ›´é•¿é€€é¿æ—¶é—´é‡è¯•
		fs.Debugf(f, "é€Ÿç‡å—é™ï¼ˆHTTP 429ï¼‰ï¼Œå°†ä½¿ç”¨æ›´é•¿é€€é¿æ—¶é—´é‡è¯•")
		return true, fserrors.NewErrorRetryAfter(30 * time.Second)
	case http.StatusInternalServerError, http.StatusBadGateway, http.StatusServiceUnavailable, http.StatusGatewayTimeout:
		// æœåŠ¡å™¨é”™è¯¯ - åº”è¯¥é‡è¯•
		fs.Debugf(f, "æœåŠ¡å™¨é”™è¯¯ %dï¼Œå°†é‡è¯•", resp.StatusCode)
		return true, fmt.Errorf("server error: HTTP %d", resp.StatusCode)
	case http.StatusNotFound:
		return false, fs.ErrorObjectNotFound
	case http.StatusForbidden:
		return false, fmt.Errorf("permission denied: HTTP %d", resp.StatusCode)
	default:
		// è¯»å–é”™è¯¯å“åº”ä½“ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
		body, _ := io.ReadAll(resp.Body)
		return false, fmt.Errorf("HTTP error %d: %s", resp.StatusCode, string(body))
	}
}

// getFileInfo æ ¹æ®IDè·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
func (f *Fs) getFileInfo(ctx context.Context, fileID string) (*FileListInfoRespDataV2, error) {
	fs.Debugf(f, "ğŸ” getFileInfoå¼€å§‹: fileID=%s", fileID)

	// éªŒè¯æ–‡ä»¶ID
	_, err := parseFileIDWithContext(fileID, "è·å–æ–‡ä»¶ä¿¡æ¯")
	if err != nil {
		fs.Debugf(f, "ğŸ” getFileInfoæ–‡ä»¶IDéªŒè¯å¤±è´¥: %v", err)
		return nil, err
	}

	// ä½¿ç”¨æ–‡ä»¶è¯¦æƒ…API - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response FileDetailResponse
	endpoint := fmt.Sprintf("/api/v1/file/detail?fileID=%s", fileID)

	fs.Debugf(f, "ğŸ” getFileInfoè°ƒç”¨API: %s", endpoint)
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		fs.Debugf(f, "ğŸ” getFileInfo APIè°ƒç”¨å¤±è´¥: %v", err)
		return nil, err
	}

	fs.Debugf(f, "ğŸ” getFileInfo APIå“åº”: code=%d, message=%s", response.Code, response.Message)
	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ‡å‡†æ ¼å¼
	fileInfo := &FileListInfoRespDataV2{
		FileID:       response.Data.FileID,
		Filename:     response.Data.Filename,
		Type:         response.Data.Type,
		Size:         response.Data.Size,
		Etag:         response.Data.ETag,
		Status:       response.Data.Status,
		ParentFileID: response.Data.ParentFileID,
		Category:     0, // è¯¦æƒ…å“åº”ä¸­æœªæä¾›
	}

	fs.Debugf(f, "ğŸ” getFileInfoæˆåŠŸ: fileID=%s, filename=%s, type=%d, size=%d", fileID, fileInfo.Filename, fileInfo.Type, fileInfo.Size)
	return fileInfo, nil
}

// getDownloadURL è·å–æ–‡ä»¶çš„ä¸‹è½½URL
func (f *Fs) getDownloadURL(ctx context.Context, fileID string) (string, error) {
	// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜å·²åˆ é™¤ï¼Œç›´æ¥è°ƒç”¨APIè·å–
	fs.Debugf(f, "123ç½‘ç›˜è·å–ä¸‹è½½URL: fileID=%s", fileID)

	var response DownloadInfoResponse
	endpoint := fmt.Sprintf("/api/v1/file/download_info?fileID=%s", fileID)

	err := f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜å·²åˆ é™¤ï¼Œä¸å†ä¿å­˜åˆ°ç¼“å­˜
	fs.Debugf(f, "123ç½‘ç›˜ä¸‹è½½URLè·å–æˆåŠŸï¼Œä¸ä½¿ç”¨ç¼“å­˜: fileID=%s", fileID)

	return response.Data.DownloadURL, nil
}

// createDirectory åˆ›å»ºæ–°ç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™è¿”å›ç°æœ‰ç›®å½•çš„ID
func (f *Fs) createDirectory(ctx context.Context, parentID, name string) (string, error) {
	// éªŒè¯å‚æ•°
	if name == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if parentID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯ç›®å½•å
	if err := validateFileName(name); err != nil {
		return "", fmt.Errorf("ç›®å½•åéªŒè¯å¤±è´¥: %w", err)
	}

	// å°†çˆ¶ç›®å½•IDè½¬æ¢ä¸ºint64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return "", err
	}

	// é¦–å…ˆæ£€æŸ¥ç›®å½•æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…ä¸å¿…è¦çš„APIè°ƒç”¨
	fs.Debugf(f, "æ£€æŸ¥ç›®å½• '%s' æ˜¯å¦å·²å­˜åœ¨äºçˆ¶ç›®å½• %s", name, parentID)
	existingID, found, err := f.FindLeaf(ctx, parentID, name)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç°æœ‰ç›®å½•æ—¶å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
	} else if found {
		fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼ŒID: %s", name, existingID)
		return existingID, nil
	}

	// å¦‚æœå¸¸è§„æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼ˆé˜²æ­¢ç¼“å­˜é—®é¢˜ï¼‰
	if !found && err == nil {
		fs.Debugf(f, "å¸¸è§„æŸ¥æ‰¾æœªæ‰¾åˆ°ç›®å½• '%s'ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾", name)
		existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
		if err != nil {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å‡ºé”™: %vï¼Œç»§ç»­å°è¯•åˆ›å»º", err)
		} else if found {
			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°ç›®å½• '%s'ï¼ŒID: %s", name, existingID)
			return existingID, nil
		}
	}

	// ç›®å½•ä¸å­˜åœ¨ï¼Œå‡†å¤‡åˆ›å»º
	fs.Debugf(f, "ç›®å½• '%s' ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º", name)
	reqBody := map[string]any{
		"parentID": strconv.FormatInt(parentFileID, 10),
		"name":     name,
	}

	// è¿›è¡ŒAPIè°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	var response struct {
		Code    int         `json:"code"`
		Message string      `json:"message"`
		Data    interface{} `json:"data"` // ä½¿ç”¨interface{}å› ä¸ºä¸ç¡®å®šå…·ä½“ç»“æ„
	}

	err = f.makeAPICallWithRest(ctx, "/upload/v1/file/mkdir", "POST", reqBody, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•å·²å­˜åœ¨çš„é”™è¯¯
		if response.Code == 1 && strings.Contains(response.Message, "å·²ç»æœ‰åŒåæ–‡ä»¶å¤¹") {
			fs.Debugf(f, "ç›®å½• '%s' å·²å­˜åœ¨ï¼Œå¼ºåˆ¶æ¸…ç†ç¼“å­˜åæŸ¥æ‰¾ç°æœ‰ç›®å½•ID", name)

			// å¼ºåˆ¶æ¸…ç†ç›®å½•åˆ—è¡¨ç¼“å­˜ï¼Œç¡®ä¿è·å–æœ€æ–°çŠ¶æ€
			f.clearDirListCache(parentFileID)

			// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç¼“å­˜æ¸…ç†å®Œæˆ
			time.Sleep(100 * time.Millisecond)

			// æŸ¥æ‰¾ç°æœ‰ç›®å½•çš„ID - ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼
			existingID, found, err := f.findLeafWithForceRefresh(ctx, parentID, name)
			if err != nil {
				return "", fmt.Errorf("æŸ¥æ‰¾ç°æœ‰ç›®å½•å¤±è´¥: %w", err)
			}
			if !found {
				// å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œå°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼ˆå¯èƒ½æ˜¯APIå»¶è¿Ÿï¼‰
				fs.Debugf(f, "ç¬¬ä¸€æ¬¡æŸ¥æ‰¾å¤±è´¥ï¼Œå°è¯•é‡è¯•æŸ¥æ‰¾ç›®å½• '%s'", name)
				for retry := 0; retry < 3; retry++ {
					time.Sleep(time.Duration(retry+1) * 200 * time.Millisecond)
					existingID, found, err = f.findLeafWithForceRefresh(ctx, parentID, name)
					if err == nil && found {
						break
					}
					fs.Debugf(f, "é‡è¯•ç¬¬%dæ¬¡æŸ¥æ‰¾ç›®å½• '%s': found=%v, err=%v", retry+1, name, found, err)
				}

				if !found {
					return "", fmt.Errorf("ç›®å½•å·²å­˜åœ¨ä½†æ— æ³•æ‰¾åˆ°å…¶IDï¼Œå¯èƒ½æ˜¯APIåŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜")
				}
			}

			fs.Debugf(f, "æ‰¾åˆ°ç°æœ‰ç›®å½•ID: %s", existingID)
			return existingID, nil
		}
		return "", fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// åˆ›å»ºæˆåŠŸï¼Œä½†APIå¯èƒ½ä¸è¿”å›ç›®å½•IDï¼Œéœ€è¦é€šè¿‡æŸ¥æ‰¾è·å–
	fs.Debugf(f, "ç›®å½• '%s' åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹æŸ¥æ‰¾æ–°ç›®å½•ID", name)

	// æ¸…ç†ç¼“å­˜ä»¥ç¡®ä¿èƒ½æ‰¾åˆ°æ–°åˆ›å»ºçš„ç›®å½•
	f.clearDirListCache(parentFileID)

	// ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨ç«¯åŒæ­¥å®Œæˆ
	time.Sleep(200 * time.Millisecond)

	// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æ¨¡å¼æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•çš„IDï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
	var newDirID string
	var foundNew bool
	var errNew error

	// å°è¯•å¤šæ¬¡æŸ¥æ‰¾ï¼Œå¤„ç†æœåŠ¡å™¨ç«¯åŒæ­¥å»¶è¿Ÿ
	maxRetries := 5
	for attempt := 0; attempt < maxRetries; attempt++ {
		if attempt > 0 {
			// é€’å¢ç­‰å¾…æ—¶é—´ï¼š200ms, 400ms, 600ms, 800ms
			waitTime := time.Duration(attempt*200) * time.Millisecond
			fs.Debugf(f, "ç¬¬%dæ¬¡é‡è¯•æŸ¥æ‰¾æ–°ç›®å½• '%s'ï¼Œç­‰å¾… %v", attempt+1, name, waitTime)
			time.Sleep(waitTime)
		}

		// ä½¿ç”¨å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾ï¼Œè·³è¿‡ç¼“å­˜ç›´æ¥ä»APIè·å–æœ€æ–°æ•°æ®
		newDirID, foundNew, errNew = f.findLeafWithForceRefresh(ctx, parentID, name)
		if errNew != nil {
			fs.Debugf(f, "ç¬¬%dæ¬¡æŸ¥æ‰¾æ–°ç›®å½•å¤±è´¥: %v", attempt+1, errNew)
			continue
		}

		if foundNew {
			fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æˆåŠŸæ‰¾åˆ°æ–°åˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", attempt+1, name, newDirID)
			break
		}

		fs.Debugf(f, "ç¬¬%dæ¬¡å°è¯•æœªæ‰¾åˆ°æ–°ç›®å½• '%s'", attempt+1, name)
	}

	// æœ€ç»ˆæ£€æŸ¥ç»“æœ
	if errNew != nil {
		return "", fmt.Errorf("æŸ¥æ‰¾æ–°åˆ›å»ºç›®å½•å¤±è´¥ï¼Œå·²é‡è¯•%dæ¬¡: %w", maxRetries, errNew)
	}
	if !foundNew {
		return "", fmt.Errorf("æ–°åˆ›å»ºçš„ç›®å½•æœªæ‰¾åˆ°ï¼Œå·²é‡è¯•%dæ¬¡ï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨åŒæ­¥å»¶è¿Ÿæˆ–æƒé™é—®é¢˜", maxRetries)
	}

	fs.Debugf(f, "æˆåŠŸåˆ›å»ºç›®å½• '%s'ï¼ŒID: %s", name, newDirID)
	return newDirID, nil
}

// createUpload åˆ›å»ºä¸Šä¼ ä¼šè¯
func (f *Fs) createUpload(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// é¦–å…ˆéªŒè¯parentFileIDæ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "createUpload: éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		return nil, fmt.Errorf("çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®æ­£ï¼šAPIæ–‡æ¡£è¦æ±‚parentFileIDè€Œä¸æ˜¯parentFileId
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸º123 APIåŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API - createä½¿ç”¨æ ‡å‡†APIåŸŸå
	fs.Debugf(f, "ğŸ” DEBUG: ä½¿ç”¨v2 APIç«¯ç‚¹ /upload/v2/file/create å’Œæ ‡å‡†APIåŸŸå")
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, "åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	return &response, nil
}

// createUploadV2 ä¸“é—¨ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åˆ›å»ºä¼šè¯ï¼Œå¼ºåˆ¶ä½¿ç”¨v2 API
func (f *Fs) createUploadV2(ctx context.Context, parentFileID int64, filename, etag string, size int64) (*UploadCreateResp, error) {
	// éªŒè¯æ–‡ä»¶åç¬¦åˆAPIè¦æ±‚
	if err := f.validateFileName(filename); err != nil {
		return nil, fmt.Errorf("æ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}

	// é¦–å…ˆéªŒè¯parentFileIDæ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "createUploadV2: éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		return nil, fmt.Errorf("çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨", parentFileID)
	}

	reqBody := map[string]any{
		"parentFileID": parentFileID, // ä¿®æ­£ï¼šAPIæ–‡æ¡£è¦æ±‚parentFileIDè€Œä¸æ˜¯parentFileId
		"filename":     filename,
		"size":         size,
		"duplicate":    1, // 1: ä¿ç•™ä¸¤è€…ï¼Œæ–°æ–‡ä»¶åå°†è‡ªåŠ¨æ·»åŠ åç¼€; 2: è¦†ç›–åŸæ–‡ä»¶
		"containDir":   false,
	}

	// 123 APIéœ€è¦etag
	if etag == "" {
		return nil, fmt.Errorf("etag (MD5 hash) is required for 123 API but not provided")
	}
	reqBody["etag"] = strings.ToLower(etag)
	fs.Debugf(f, "ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åŒ…å«etag: %s", etag)

	var response UploadCreateResp

	// å¼ºåˆ¶ä½¿ç”¨v2 APIï¼Œä¸ä½¿ç”¨ç‰ˆæœ¬å›é€€ - ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•
	bodyBytes, _ := json.Marshal(reqBody)
	fs.Debugf(f, "è°ƒç”¨v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯: %s", string(bodyBytes))
	// æ ¹æ®å®˜æ–¹æ–‡æ¡£ä½¿ç”¨v2ç«¯ç‚¹å’Œæ ‡å‡†APIåŸŸå
	fs.Debugf(f, "ğŸ” DEBUG: ä½¿ç”¨v2 APIç«¯ç‚¹ /upload/v2/file/create å’Œæ ‡å‡†APIåŸŸå")
	err = f.makeAPICallWithRest(ctx, "/upload/v2/file/create", "POST", reqBody, &response)
	if err != nil {
		fs.Errorf(f, "v2 APIè°ƒç”¨å¤±è´¥: %v", err)

		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if strings.Contains(err.Error(), "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸  çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå¯èƒ½ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦æ¸…ç†ç¼“å­˜é‡è¯•", parentFileID)
			// æ¸…ç†ç›¸å…³ç¼“å­˜
			f.clearParentIDCache(parentFileID)
			f.clearDirListCache(parentFileID)
			// é‡ç½®ç›®å½•ç¼“å­˜
			if f.dirCache != nil {
				fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", parentFileID)
				f.dirCache.ResetRoot()
			}
			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: %w", err)
		}

		return nil, fmt.Errorf("v2 APIåˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// è¯¦ç»†è®°å½•APIå“åº”
	fs.Debugf(f, "v2 APIå“åº”: Code=%d, Message='%s', FileID=%d, PreuploadID='%s', Reuse=%v, SliceSize=%d",
		response.Code, response.Message, response.Data.FileID, response.Data.PreuploadID, response.Data.Reuse, response.Data.SliceSize)

	if response.Code != 0 {
		fs.Errorf(f, "v2 APIè¿”å›é”™è¯¯: Code=%d, Message='%s'", response.Code, response.Message)
		return nil, fmt.Errorf("v2 API error %d: %s", response.Code, response.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸ºç§’ä¼ 
	if response.Data.Reuse {
		fs.Debugf(f, "ğŸš€ v2 APIç§’ä¼ æˆåŠŸ: FileID=%d, æ–‡ä»¶å·²å­˜åœ¨äºæœåŠ¡å™¨", response.Data.FileID)
		return &response, nil
	}

	// éç§’ä¼ æƒ…å†µï¼ŒéªŒè¯PreuploadIDä¸ä¸ºç©º
	if response.Data.PreuploadID == "" {
		fs.Errorf(f, "âŒ v2 APIå“åº”å¼‚å¸¸: éç§’ä¼ æƒ…å†µä¸‹PreuploadIDä¸ºç©ºï¼Œå®Œæ•´å“åº”: %+v", response)
		return nil, fmt.Errorf("v2 APIè¿”å›çš„é¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	// è°ƒè¯•å“åº”æ•°æ®
	fs.Debugf(f, "âœ… v2åˆ›å»ºä¸Šä¼ ä¼šè¯æˆåŠŸ: FileID=%d, PreuploadID='%s', SliceSize=%d",
		response.Data.FileID, response.Data.PreuploadID, response.Data.SliceSize)

	return &response, nil
}

// uploadFile ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹
func (f *Fs) uploadFile(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64) error {
	return f.uploadFileWithPath(ctx, in, createResp, size, "")
}

// uploadFileWithPath ä½¿ç”¨ä¸Šä¼ ä¼šè¯ä¸Šä¼ æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileWithPath(ctx context.Context, in io.Reader, createResp *UploadCreateResp, size int64, filePath string) error {
	// ç¡®å®šå—å¤§å°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		chunkSize = int64(f.opt.ChunkSize)
	}

	// ç¡®ä¿å—å¤§å°åœ¨é™åˆ¶èŒƒå›´å†…
	if chunkSize < int64(minChunkSize) {
		chunkSize = int64(minChunkSize)
	}
	if chunkSize > int64(maxChunkSize) {
		chunkSize = int64(maxChunkSize)
	}

	// å¯¹äºå°æ–‡ä»¶ï¼Œå…¨éƒ¨è¯»å…¥å†…å­˜ï¼ˆæ— éœ€æ–­ç‚¹ç»­ä¼ ï¼‰
	if size <= chunkSize {
		data, err := io.ReadAll(in)
		if err != nil {
			return fmt.Errorf("failed to read file data: %w", err)
		}

		if int64(len(data)) != size {
			return fmt.Errorf("size mismatch: expected %d, got %d", size, len(data))
		}

		// å•éƒ¨åˆ†ä¸Šä¼ 
		return f.uploadSinglePart(ctx, createResp.Data.PreuploadID, data)
	}

	// å¤§æ–‡ä»¶çš„å¤šéƒ¨åˆ†ä¸Šä¼ ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
	return f.uploadMultiPartWithResume(ctx, in, createResp.Data.PreuploadID, size, chunkSize, filePath)
}

// uploadSinglePart å•éƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶
func (f *Fs) uploadSinglePart(ctx context.Context, preuploadID string, data []byte) error {
	// è®¡ç®—åˆ†ç‰‡MD5
	chunkHash := fmt.Sprintf("%x", md5.Sum(data))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err := f.uploadPartWithMultipart(ctx, preuploadID, 1, chunkHash, data)
	if err != nil {
		return fmt.Errorf("failed to upload part: %w", err)
	}

	// å®Œæˆä¸Šä¼ 
	return f.completeUpload(ctx, preuploadID)
}

// uploadMultiPartWithResume å¤šéƒ¨åˆ†ä¸Šä¼ æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadMultiPartWithResume(ctx context.Context, in io.Reader, preuploadID string, size, chunkSize int64, filePath string) error {
	// é˜²æ­¢é™¤é›¶é”™è¯¯
	if chunkSize <= 0 {
		return fmt.Errorf("æ— æ•ˆçš„åˆ†ç‰‡å¤§å°: %d", chunkSize)
	}

	uploadNums := (size + chunkSize - 1) / chunkSize

	// é™åˆ¶åˆ†ç‰‡æ•°é‡
	if uploadNums > int64(f.opt.MaxUploadParts) {
		return fmt.Errorf("file too large: would require %d parts, max allowed is %d", uploadNums, f.opt.MaxUploadParts)
	}

	// å°è¯•åŠ è½½ç°æœ‰è¿›åº¦
	progress, err := f.loadUploadProgress(preuploadID)
	if err != nil {
		fs.Debugf(f, "åŠ è½½ä¸Šä¼ è¿›åº¦å¤±è´¥: %v", err)
		progress = nil
	}

	// å¦‚æœä¸å­˜åœ¨è¿›åº¦æˆ–å‚æ•°ä¸åŒ¹é…ï¼Œåˆ›å»ºæ–°è¿›åº¦
	if progress == nil || progress.TotalParts != uploadNums || progress.ChunkSize != chunkSize || progress.FileSize != size {
		fs.Debugf(f, "ä¸º%såˆ›å»ºæ–°çš„ä¸Šä¼ è¿›åº¦", preuploadID)
		progress = &UploadProgress{
			PreuploadID:   preuploadID,
			TotalParts:    uploadNums,
			ChunkSize:     chunkSize,
			FileSize:      size,
			UploadedParts: make(map[int64]bool),
			FilePath:      filePath,
			CreatedAt:     time.Now(),
		}
	} else {
		fs.Debugf(f, "æ¢å¤ä¸Šä¼ %s: %d/%dä¸ªåˆ†ç‰‡å·²ä¸Šä¼ ",
			preuploadID, progress.GetUploadedCount(), progress.TotalParts)
	}

	// æ£€æŸ¥æ˜¯å¦å¯ä»¥ä»æ–‡ä»¶æ¢å¤ï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶è·¯å¾„ä¸”æ–‡ä»¶å­˜åœ¨ï¼‰
	var fileReader *os.File
	canResumeFromFile := filePath != "" && progress.GetUploadedCount() > 0

	if canResumeFromFile {
		fileReader, err = os.Open(filePath)
		if err != nil {
			fs.Debugf(f, "æ— æ³•æ‰“å¼€æ–‡ä»¶è¿›è¡Œæ–­ç‚¹ç»­ä¼ ï¼Œå›é€€åˆ°æµå¼ä¼ è¾“: %v", err)
			canResumeFromFile = false
		} else {
			defer fileReader.Close()
			// ä½¿ç”¨æ–‡ä»¶è¯»å–å™¨è€Œä¸æ˜¯æµä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ 
			in = fileReader
		}
	}

	buffer := make([]byte, chunkSize)

	for partIndex := int64(0); partIndex < uploadNums; partIndex++ {
		partNumber := partIndex + 1

		// å¦‚æœæ­¤åˆ†ç‰‡å·²ä¸Šä¼ åˆ™è·³è¿‡ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		if progress.IsUploaded(partNumber) {
			f.debugf(LogLevelVerbose, "è·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡ %d/%d", partNumber, uploadNums)

			// å¦‚æœå¯ä»¥å®šä½ï¼ˆæ–‡ä»¶è¯»å–å™¨ï¼‰ï¼Œå®šä½åˆ°ä¸‹ä¸€åˆ†ç‰‡
			if canResumeFromFile && fileReader != nil {
				nextOffset := (partIndex + 1) * chunkSize
				if nextOffset < size {
					_, err := fileReader.Seek(nextOffset, io.SeekStart)
					if err != nil {
						return fmt.Errorf("failed to seek to part %d: %w", partNumber+1, err)
					}
				}
			} else {
				// ä»éœ€è¦è¯»å–å¹¶ä¸¢å¼ƒæ•°æ®ä»¥æ¨è¿›è¯»å–å™¨
				if partIndex == uploadNums-1 {
					remaining := size - partIndex*chunkSize
					_, err := io.ReadFull(in, make([]byte, remaining))
					if err != nil {
						return fmt.Errorf("failed to skip part %d data: %w", partNumber, err)
					}
				} else {
					_, err := io.ReadFull(in, buffer)
					if err != nil {
						return fmt.Errorf("failed to skip part %d data: %w", partNumber, err)
					}
				}
			}
			continue
		}

		// å¦‚æœå¯ä»¥å®šä½ä¸”è¿™ä¸æ˜¯ç¬¬ä¸€ä¸ªæœªä¸Šä¼ çš„åˆ†ç‰‡ï¼Œå®šä½åˆ°æ­£ç¡®ä½ç½®
		if canResumeFromFile && fileReader != nil {
			currentOffset := partIndex * chunkSize
			_, err := fileReader.Seek(currentOffset, io.SeekStart)
			if err != nil {
				return fmt.Errorf("failed to seek to part %d: %w", partNumber, err)
			}
		}

		// è¯»å–å—æ•°æ®
		var partData []byte
		if partIndex == uploadNums-1 {
			// æœ€åä¸€åˆ†ç‰‡ - è¯»å–å‰©ä½™æ•°æ®
			remaining := size - partIndex*chunkSize
			partData = make([]byte, remaining)
			_, err := io.ReadFull(in, partData)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
		} else {
			// å¸¸è§„åˆ†ç‰‡
			_, err := io.ReadFull(in, buffer)
			if err != nil {
				return fmt.Errorf("failed to read part %d: %w", partNumber, err)
			}
			partData = buffer
		}

		// è®¡ç®—åˆ†ç‰‡MD5
		chunkHash := fmt.Sprintf("%x", md5.Sum(partData))

		// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•ï¼ŒåŒ…å«é‡è¯•é€»è¾‘
		err = func() error {
			// å¯¹äºè·¨äº‘ç›˜ä¼ è¾“ï¼Œä½¿ç”¨ä¸“é—¨çš„é‡è¯•ç­–ç•¥
			return f.uploadPacer.Call(func() (bool, error) {
				err := f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkHash, partData)

				if err != nil {
					// è®¡ç®—å·²ä¼ è¾“å­—èŠ‚æ•°
					transferredBytes := partIndex * chunkSize
					if shouldRetryTransfer, retryDelay := f.shouldRetryCrossCloudTransfer(err, 0, transferredBytes, size); shouldRetryTransfer {
						return true, fserrors.NewErrorRetryAfter(retryDelay)
					}
				}

				return shouldRetry(ctx, nil, err)
			})
		}()
		if err != nil {
			// è¿”å›é”™è¯¯å‰ä¿å­˜è¿›åº¦
			saveErr := f.saveUploadProgress(progress)
			if saveErr != nil {
				fs.Debugf(f, "ä¸Šä¼ é”™è¯¯åä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
			}
			return fmt.Errorf("failed to upload part %d: %w", partNumber, err)
		}

		// æ ‡è®°æ­¤åˆ†ç‰‡å·²ä¸Šä¼ å¹¶ä¿å­˜è¿›åº¦ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		progress.SetUploaded(partNumber)
		err = f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜ä¸Šä¼ è¿›åº¦å¤±è´¥: %v", err)
		}

		f.debugf(LogLevelVerbose, "å·²ä¸Šä¼ åˆ†ç‰‡ %d/%d", partNumber, uploadNums)
	}

	// å®Œæˆä¸Šä¼ 
	err = f.completeUpload(ctx, preuploadID)
	if err != nil {
		// è¿”å›é”™è¯¯å‰ä¿å­˜è¿›åº¦
		saveErr := f.saveUploadProgress(progress)
		if saveErr != nil {
			fs.Debugf(f, "å®Œæˆé”™è¯¯åä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
		}
		return err
	}

	// ä¸Šä¼ æˆåŠŸå®Œæˆï¼Œåˆ é™¤è¿›åº¦æ–‡ä»¶
	f.removeUploadProgress(preuploadID)
	return nil
}

// uploadPartWithMultipart ä½¿ç”¨æ­£ç¡®çš„multipart/form-dataæ ¼å¼ä¸Šä¼ åˆ†ç‰‡
func (f *Fs) uploadPartWithMultipart(ctx context.Context, preuploadID string, partNumber int64, chunkHash string, data []byte) error {
	if len(data) == 0 {
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(data))

	// ä½¿ç”¨uploadPacerè¿›è¡ŒQPSé™åˆ¶æ§åˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		// è·å–ä¸Šä¼ åŸŸå
		uploadDomain, err := f.getUploadDomain(ctx)
		if err != nil {
			return false, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥: %w", err)
		}

		// åˆ›å»ºmultipartè¡¨å•æ•°æ®
		var buf bytes.Buffer
		writer := multipart.NewWriter(&buf)

		// æ·»åŠ è¡¨å•å­—æ®µ
		writer.WriteField("preuploadID", preuploadID)
		writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
		writer.WriteField("sliceMD5", chunkHash)

		// æ·»åŠ æ–‡ä»¶æ•°æ®
		part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
		if err != nil {
			return false, fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
		}

		_, err = part.Write(data)
		if err != nil {
			return false, fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		err = writer.Close()
		if err != nil {
			return false, fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
		}

		// ä½¿ç”¨ä¸Šä¼ åŸŸåè¿›è¡Œmultipartä¸Šä¼ 
		var response struct {
			Code    int    `json:"code"`
			Message string `json:"message"`
		}

		// ä½¿ç”¨ä¸Šä¼ åŸŸåè°ƒç”¨åˆ†ç‰‡ä¸Šä¼ API
		err = f.makeAPICallWithRestMultipartToDomain(ctx, uploadDomain, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯QPSé™åˆ¶é”™è¯¯
			if strings.Contains(err.Error(), "429") || strings.Contains(err.Error(), "é¢‘ç‡") {
				fs.Debugf(f, "åˆ†ç‰‡%dé‡åˆ°QPSé™åˆ¶ï¼Œå°†é‡è¯•: %v", partNumber, err)
				return true, err
			}
			return false, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		if response.Code != 0 {
			return false, fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ APIé”™è¯¯ %d: %s", response.Code, response.Message)
		}

		fs.Debugf(f, "åˆ†ç‰‡ %d ä¸Šä¼ æˆåŠŸ", partNumber)
		return false, nil
	})
}

// UploadCompleteResult ä¸Šä¼ å®Œæˆç»“æœ
type UploadCompleteResult struct {
	FileID int64  `json:"fileID"`
	Etag   string `json:"etag"`
}

// completeUpload å®Œæˆå¤šéƒ¨åˆ†ä¸Šä¼ 
func (f *Fs) completeUpload(ctx context.Context, preuploadID string) error {
	_, err := f.completeUploadWithResult(ctx, preuploadID)
	return err
}

// completeUploadWithResult å®Œæˆå¤šéƒ¨åˆ†ä¸Šä¼ å¹¶è¿”å›ç»“æœ
func (f *Fs) completeUploadWithResult(ctx context.Context, preuploadID string) (*UploadCompleteResult, error) {
	return f.completeUploadWithResultAndSize(ctx, preuploadID, 0)
}

// completeUploadWithResultAndSize å®Œæˆå¤šéƒ¨åˆ†ä¸Šä¼ å¹¶è¿”å›ç»“æœï¼Œæ”¯æŒæ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´è½®è¯¢æ¬¡æ•°
func (f *Fs) completeUploadWithResultAndSize(ctx context.Context, preuploadID string, fileSize int64) (*UploadCompleteResult, error) {
	reqBody := map[string]any{
		"preuploadID": preuploadID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			Async     bool   `json:"async"`
			Completed bool   `json:"completed"`
			FileID    int64  `json:"fileID"`
			Etag      string `json:"etag"`
		} `json:"data"`
	}

	// ğŸ“Š æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è½®è¯¢æ¬¡æ•°ï¼ˆåŸºäºå®˜æ–¹æµç¨‹å›¾ä¼˜åŒ–ï¼‰
	// åŸºç¡€è½®è¯¢æ¬¡æ•°ï¼š20æ¬¡ï¼ˆ20ç§’ï¼‰
	// å¤§æ–‡ä»¶é¢å¤–è½®è¯¢ï¼šæ¯100MBå¢åŠ 60æ¬¡è½®è¯¢ï¼ˆå……è¶³çš„æ ¡éªŒæ—¶é—´ï¼‰
	// æœ€å¤§è½®è¯¢æ¬¡æ•°ï¼š600æ¬¡ï¼ˆ10åˆ†é’Ÿï¼‰- ä¸ºå¤§æ–‡ä»¶æä¾›å……è¶³æ—¶é—´
	maxAttempts := 20
	if fileSize > 0 {
		// è®¡ç®—æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰
		fileSizeMB := fileSize / (1024 * 1024)
		// æ¯100MBå¢åŠ 60æ¬¡è½®è¯¢ï¼Œä¸ºå¤§æ–‡ä»¶æä¾›æ›´å……è¶³çš„æ ¡éªŒæ—¶é—´
		extraAttempts := int(fileSizeMB/100) * 60
		if extraAttempts > 580 {
			extraAttempts = 580 // æœ€å¤š600æ¬¡è½®è¯¢ï¼ˆ10åˆ†é’Ÿï¼‰
		}
		maxAttempts += extraAttempts
		fs.Debugf(f, "ğŸ“Š æ ¹æ®æ–‡ä»¶å¤§å°(%dMB)è°ƒæ•´è½®è¯¢ç­–ç•¥: åŸºç¡€20æ¬¡ + é¢å¤–%dæ¬¡ = æ€»è®¡%dæ¬¡è½®è¯¢ (é¢„è®¡%dåˆ†é’Ÿ)", fileSizeMB, extraAttempts, maxAttempts, maxAttempts/60+1)
	}

	fs.Debugf(f, "ğŸ” DEBUG: completeUploadWithResultAndSize å¼€å§‹è½®è¯¢ï¼Œæ–‡ä»¶å¤§å°=%då­—èŠ‚ï¼Œæœ€å¤§è½®è¯¢æ¬¡æ•°=%d", fileSize, maxAttempts)

	// å¢å¼ºçš„è½®è¯¢é€»è¾‘ï¼ŒåŒ…å«ç½‘ç»œé”™è¯¯å®¹é”™å¤„ç†
	networkErrorCount := 0
	maxNetworkErrors := 10 // å…è®¸æœ€å¤š10æ¬¡ç½‘ç»œé”™è¯¯

	for attempt := 0; attempt < maxAttempts; attempt++ {
		// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API
		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			// ç½‘ç»œé”™è¯¯å¤„ç†ï¼šè®°å½•é”™è¯¯ä½†ç»§ç»­é‡è¯•ï¼Œè€Œä¸æ˜¯ç›´æ¥é€€å‡º
			networkErrorCount++
			fs.Debugf(f, "âš ï¸ è½®è¯¢ç¬¬%dæ¬¡é‡åˆ°ç½‘ç»œé”™è¯¯ (ç¬¬%dä¸ªç½‘ç»œé”™è¯¯): %v", attempt+1, networkErrorCount, err)

			if networkErrorCount >= maxNetworkErrors {
				return nil, fmt.Errorf("è½®è¯¢è¿‡ç¨‹ä¸­ç½‘ç»œé”™è¯¯è¿‡å¤š(%dæ¬¡)ï¼Œæœ€åé”™è¯¯: %w", networkErrorCount, err)
			}

			// ç½‘ç»œé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
			waitTime := time.Duration(networkErrorCount) * time.Second
			if waitTime > 5*time.Second {
				waitTime = 5 * time.Second
			}

			fs.Debugf(f, "ğŸ”„ ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…%våé‡è¯•...", waitTime)
			select {
			case <-ctx.Done():
				return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆè½®è¯¢è¢«å–æ¶ˆ: %w", ctx.Err())
			case <-time.After(waitTime):
				continue
			}
		}

		// é‡ç½®ç½‘ç»œé”™è¯¯è®¡æ•°å™¨ï¼ˆæˆåŠŸè°ƒç”¨APIï¼‰
		if networkErrorCount > 0 {
			fs.Debugf(f, "âœ… ç½‘ç»œè¿æ¥æ¢å¤ï¼Œé‡ç½®é”™è¯¯è®¡æ•°å™¨")
			networkErrorCount = 0
		}

		// æ£€æŸ¥APIå“åº”ç 
		if response.Code == 0 {
			// æˆåŠŸå“åº”ï¼Œç»§ç»­å¤„ç†
			fs.Debugf(f, "ğŸ‰ è½®è¯¢æˆåŠŸï¼æ–‡ä»¶æ ¡éªŒå®Œæˆï¼Œç¬¬%dæ¬¡å°è¯•æˆåŠŸ", attempt+1)
			break
		} else if response.Code == 20103 {
			// æ–‡ä»¶æ­£åœ¨æ ¡éªŒä¸­ï¼Œéœ€è¦è½®è¯¢ç­‰å¾…
			progressPercent := float64(attempt+1) / float64(maxAttempts) * 100
			fs.Debugf(f, "ğŸ“‹ æ–‡ä»¶æ­£åœ¨æ ¡éªŒä¸­ï¼Œç¬¬%dæ¬¡è½®è¯¢ç­‰å¾… (æœ€å¤š%dæ¬¡ï¼Œè¿›åº¦%.1f%%): %s",
				attempt+1, maxAttempts, progressPercent, response.Message)

			if attempt < maxAttempts-1 {
				// ç­‰å¾…1ç§’åé‡è¯•ï¼ˆAPIæ–‡æ¡£è¦æ±‚ï¼‰
				select {
				case <-ctx.Done():
					return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆè½®è¯¢è¢«å–æ¶ˆ: %w", ctx.Err())
				case <-time.After(1 * time.Second):
					continue
				}
			} else {
				// è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
				return nil, fmt.Errorf("æ–‡ä»¶æ ¡éªŒè¶…æ—¶ï¼Œå·²è½®è¯¢%dæ¬¡: API error %d: %s", maxAttempts, response.Code, response.Message)
			}
		} else {
			// å…¶ä»–é”™è¯¯ï¼Œç›´æ¥è¿”å›
			return nil, fmt.Errorf("upload completion failed: API error %d: %s", response.Code, response.Message)
		}
	}

	// å¦‚æœæ˜¯å¼‚æ­¥å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦ç­‰å¾…å®Œæˆ
	if response.Data.Async && !response.Data.Completed {
		fs.Debugf(f, "ä¸Šä¼ ä¸ºå¼‚æ­¥æ¨¡å¼ï¼Œè½®è¯¢å®ŒæˆçŠ¶æ€ï¼Œæ–‡ä»¶ID: %d", response.Data.FileID)

		// è½®è¯¢å¼‚æ­¥ä¸Šä¼ ç»“æœï¼Œè·å–æœ€ç»ˆçš„æ–‡ä»¶IDå’ŒMD5
		finalResult, err := f.pollAsyncUploadWithResult(ctx, preuploadID)
		if err != nil {
			return nil, fmt.Errorf("failed to wait for async upload completion: %w", err)
		}

		fs.Debugf(f, "å¼‚æ­¥ä¸Šä¼ æˆåŠŸå®Œæˆï¼Œæ–‡ä»¶ID: %d, MD5: %s", finalResult.FileID, finalResult.Etag)
		return finalResult, nil
	} else {
		fs.Debugf(f, "âœ… ä¸Šä¼ å®Œæˆç¡®è®¤æˆåŠŸï¼Œæ–‡ä»¶ID: %d, MD5: %s", response.Data.FileID, response.Data.Etag)
		return &UploadCompleteResult{
			FileID: response.Data.FileID,
			Etag:   response.Data.Etag,
		}, nil
	}
}

// uploadPartStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ åˆ†ç‰‡ï¼Œé¿å…å¤§å†…å­˜å ç”¨
func (f *Fs) uploadPartStream(ctx context.Context, uploadURL string, reader io.Reader, size int64) error {
	req, err := http.NewRequestWithContext(ctx, "PUT", uploadURL, reader)
	if err != nil {
		return fmt.Errorf("failed to create stream upload request: %w", err)
	}

	req.ContentLength = size
	req.Header.Set("Content-Type", "application/octet-stream")

	// ä½¿ç”¨è‡ªé€‚åº”è¶…æ—¶æœºåˆ¶
	adaptiveTimeout := f.getAdaptiveTimeout(size, "chunked_upload")
	var cancel context.CancelFunc
	ctx, cancel = context.WithTimeout(ctx, adaptiveTimeout)
	defer cancel()
	req = req.WithContext(ctx)

	resp, err := f.client.Do(req)
	if err != nil {
		return fmt.Errorf("stream upload request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusCreated {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("stream upload failed with status %d: %s", resp.StatusCode, string(body))
	}

	return nil
}

// pollAsyncUploadWithResult è½®è¯¢å¼‚æ­¥ä¸Šä¼ å®ŒæˆçŠ¶æ€å¹¶è¿”å›ç»“æœ
func (f *Fs) pollAsyncUploadWithResult(ctx context.Context, preuploadID string) (*UploadCompleteResult, error) {
	fs.Debugf(f, "è½®è¯¢å¼‚æ­¥ä¸Šä¼ å®ŒæˆçŠ¶æ€å¹¶è·å–ç»“æœï¼ŒpreuploadID: %s", preuploadID)

	// ä½¿ç”¨æŒ‡æ•°é€€é¿è½®è¯¢ï¼Œå¢åŠ åˆ°30æ¬¡å°è¯•ï¼Œæœ€å¤§5åˆ†é’Ÿ
	// å¯¹äºå¤§æ–‡ä»¶ï¼Œ123ç½‘ç›˜éœ€è¦æ›´é•¿çš„å¤„ç†æ—¶é—´
	maxAttempts := 30
	for attempt := 0; attempt < maxAttempts; attempt++ {
		reqBody := map[string]any{
			"preuploadID": preuploadID,
		}

		var response struct {
			Code    int    `json:"code"`
			Message string `json:"message"`
			Data    struct {
				Completed bool   `json:"completed"`
				FileID    int64  `json:"fileID"`
				Etag      string `json:"etag"`
			} `json:"data"`
		}

		// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è°ƒç”¨API - ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„upload_completeç«¯ç‚¹
		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			fs.Debugf(f, "å¼‚æ­¥è½®è¯¢ç¬¬%dæ¬¡å°è¯•å¤±è´¥: %v", attempt+1, err)
			// å³ä½¿å•ä¸ªè¯·æ±‚å¤±è´¥ä¹Ÿç»§ç»­è½®è¯¢
		} else if response.Code != 0 {
			fs.Debugf(f, "å¼‚æ­¥è½®è¯¢ç¬¬%dæ¬¡å°è¯•è¿”å›é”™è¯¯: APIé”™è¯¯ %d: %s", attempt+1, response.Code, response.Message)
		} else if response.Data.Completed {
			fs.Debugf(f, "å¼‚æ­¥ä¸Šä¼ åœ¨ç¬¬%dæ¬¡å°è¯•åæˆåŠŸå®Œæˆï¼Œæ–‡ä»¶ID: %d, MD5: %s", attempt+1, response.Data.FileID, response.Data.Etag)
			return &UploadCompleteResult{
				FileID: response.Data.FileID,
				Etag:   response.Data.Etag,
			}, nil
		}

		// ä¸‹æ¬¡å°è¯•å‰ç­‰å¾…ï¼ˆä¼˜åŒ–çš„æŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰
		// å‰å‡ æ¬¡å°è¯•ä½¿ç”¨è¾ƒçŸ­é—´éš”ï¼Œåç»­å¢åŠ åˆ°æœ€å¤§15ç§’
		var waitTime time.Duration
		if attempt < 5 {
			// å‰5æ¬¡ï¼š1s, 2s, 3s, 4s, 5s
			waitTime = time.Duration(attempt+1) * time.Second
		} else if attempt < 10 {
			// 6-10æ¬¡ï¼š5ç§’å›ºå®šé—´éš”
			waitTime = 5 * time.Second
		} else {
			// 11æ¬¡ä»¥åï¼š10-15ç§’é—´éš”
			waitTime = time.Duration(10+attempt-10) * time.Second
			if waitTime > 15*time.Second {
				waitTime = 15 * time.Second // æœ€å¤§15ç§’
			}
		}

		fs.Debugf(f, "å¼‚æ­¥ä¸Šä¼ å°šæœªå®Œæˆï¼Œç­‰å¾…%våè¿›è¡Œä¸‹æ¬¡è½®è¯¢ï¼ˆç¬¬%d/%dæ¬¡å°è¯•ï¼‰", waitTime, attempt+1, maxAttempts)

		select {
		case <-ctx.Done():
			return nil, ctx.Err()
		case <-time.After(waitTime):
			// ç»§ç»­ä¸‹æ¬¡å°è¯•
		}
	}

	// å¦‚æœè½®è¯¢å¤±è´¥ï¼Œè¿”å›é”™è¯¯
	return nil, fmt.Errorf("å¼‚æ­¥ä¸Šä¼ è½®è¯¢è¶…æ—¶ï¼Œæ— æ³•ç¡®è®¤æ–‡ä»¶çŠ¶æ€")
}

// saveUploadProgress å°†ä¸Šä¼ è¿›åº¦ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ä»¥æ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) saveUploadProgress(progress *UploadProgress) error {
	progressDir := filepath.Join(os.TempDir(), "rclone-123pan-progress")
	err := os.MkdirAll(progressDir, 0755)
	if err != nil {
		return fmt.Errorf("failed to create progress directory: %w", err)
	}

	// ä½¿ç”¨preuploadIDçš„MD5å“ˆå¸Œä»¥é¿å…æ–‡ä»¶åé•¿åº¦é—®é¢˜
	hasher := md5.New()
	hasher.Write([]byte(progress.PreuploadID))
	shortID := fmt.Sprintf("%x", hasher.Sum(nil))

	progressFile := filepath.Join(progressDir, shortID+".json")
	data, err := json.Marshal(progress)
	if err != nil {
		return fmt.Errorf("failed to marshal progress: %w", err)
	}

	err = os.WriteFile(progressFile, data, 0644)
	if err != nil {
		return fmt.Errorf("failed to save progress file: %w", err)
	}

	// å®‰å…¨åœ°æˆªå–PreuploadIDç”¨äºæ—¥å¿—æ˜¾ç¤º
	preuploadIDDisplay := progress.PreuploadID
	if len(preuploadIDDisplay) > 20 {
		preuploadIDDisplay = preuploadIDDisplay[:20] + "..."
	} else if len(preuploadIDDisplay) == 0 {
		preuploadIDDisplay = "<ç©º>"
	}
	fs.Debugf(f, "å·²ä¿å­˜ä¸Šä¼ è¿›åº¦%s: %d/%dä¸ªåˆ†ç‰‡å®Œæˆ",
		preuploadIDDisplay, progress.GetUploadedCount(), progress.TotalParts)
	return nil
}

// loadUploadProgress ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½ä¸Šä¼ è¿›åº¦
func (f *Fs) loadUploadProgress(preuploadID string) (*UploadProgress, error) {
	progressDir := filepath.Join(os.TempDir(), "rclone-123pan-progress")

	// ä½¿ç”¨preuploadIDçš„MD5å“ˆå¸Œä»¥é¿å…æ–‡ä»¶åé•¿åº¦é—®é¢˜
	hasher := md5.New()
	hasher.Write([]byte(preuploadID))
	shortID := fmt.Sprintf("%x", hasher.Sum(nil))

	progressFile := filepath.Join(progressDir, shortID+".json")

	data, err := os.ReadFile(progressFile)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, nil // ä¸å­˜åœ¨è¿›åº¦æ–‡ä»¶
		}
		return nil, fmt.Errorf("failed to read progress file: %w", err)
	}

	var progress UploadProgress
	err = json.Unmarshal(data, &progress)
	if err != nil {
		return nil, fmt.Errorf("failed to unmarshal progress: %w", err)
	}

	fs.Debugf(f, "å·²åŠ è½½ä¸Šä¼ è¿›åº¦%s: %d/%dä¸ªåˆ†ç‰‡å®Œæˆ",
		preuploadID, progress.GetUploadedCount(), progress.TotalParts)
	return &progress, nil
}

// removeUploadProgress åœ¨ä¸Šä¼ æˆåŠŸååˆ é™¤è¿›åº¦æ–‡ä»¶
func (f *Fs) removeUploadProgress(preuploadID string) {
	progressDir := filepath.Join(os.TempDir(), "rclone-123pan-progress")

	// ä½¿ç”¨preuploadIDçš„MD5å“ˆå¸Œä»¥é¿å…æ–‡ä»¶åé•¿åº¦é—®é¢˜
	hasher := md5.New()
	hasher.Write([]byte(preuploadID))
	shortID := fmt.Sprintf("%x", hasher.Sum(nil))

	progressFile := filepath.Join(progressDir, shortID+".json")

	err := os.Remove(progressFile)
	if err != nil && !os.IsNotExist(err) {
		fs.Debugf(f, "åˆ é™¤è¿›åº¦æ–‡ä»¶å¤±è´¥ %s: %v", progressFile, err)
	} else {
		fs.Debugf(f, "å·²åˆ é™¤ä¸Šä¼ è¿›åº¦æ–‡ä»¶ %s", preuploadID)
	}
}

// streamingPut å®ç°ä¼˜åŒ–çš„æµå¼ä¸Šä¼ ï¼Œç°åœ¨ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å…¥å£
// è¿™æ¶ˆé™¤äº†è·¨ç½‘ç›˜ä¼ è¾“ä¸­çš„åŒé‡ä¸‹è½½é—®é¢˜ï¼Œæé«˜ä¼ è¾“æ•ˆç‡
// ä¿®å¤äº†è·¨äº‘ç›˜ä¼ è¾“ä¸­"æºæ–‡ä»¶ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é—®é¢˜
func (f *Fs) streamingPut(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è¿›å…¥streamingPutå‡½æ•°ï¼Œæ–‡ä»¶: %s", fileName)

	// ğŸŒ æœ€æ—©æœŸè·¨äº‘ä¼ è¾“æ£€æµ‹ï¼šåœ¨ä»»ä½•å¤„ç†ä¹‹å‰å°±æ£€æµ‹å¹¶å¤„ç†è·¨äº‘ä¼ è¾“
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘streamingPutä¸­å¼€å§‹è·¨äº‘ä¼ è¾“æ£€æµ‹...")
	isRemoteSource := f.isRemoteSource(src)
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘streamingPutè·¨äº‘ä¼ è¾“æ£€æµ‹ç»“æœ: %v", isRemoteSource)

	if isRemoteSource {
		fs.Infof(f, "ğŸŒ ã€streamingPutã€‘æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“ï¼Œç›´æ¥ä½¿ç”¨ç®€åŒ–çš„ä¸‹è½½åä¸Šä¼ ç­–ç•¥")
		return f.handleCrossCloudTransferSimplified(ctx, src, parentFileID, fileName)
	}

	// æœ¬åœ°æ–‡ä»¶ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å…¥å£
	fs.Debugf(f, "æœ¬åœ°æ–‡ä»¶ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ å…¥å£è¿›è¡Œæµå¼ä¸Šä¼ ")
	return f.unifiedUpload(ctx, in, src, parentFileID, fileName)
}

// handleCrossCloudTransferSimplified ç®€åŒ–çš„è·¨äº‘ä¼ è¾“å¤„ç†å‡½æ•°
// é‡‡ç”¨æœ€ç›´æ¥çš„ç­–ç•¥ï¼šå®Œæ•´ä¸‹è½½ â†’ è®¡ç®—MD5 â†’ æ£€æŸ¥ç§’ä¼  â†’ ä¸Šä¼ 
func (f *Fs) handleCrossCloudTransferSimplified(ctx context.Context, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸŒ å¼€å§‹ç®€åŒ–è·¨äº‘ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: å®Œæ•´ä¸‹è½½åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
	fs.Infof(f, "ğŸ“¥ æ­¥éª¤1/4: å¼€å§‹ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶...")

	// å°è¯•è·å–åº•å±‚çš„fs.Object
	var srcObj fs.Object

	// é¦–å…ˆå°è¯•ç›´æ¥è½¬æ¢ä¸ºfs.Object
	if obj, ok := src.(fs.Object); ok {
		srcObj = obj
		fs.Debugf(f, "æºå¯¹è±¡æ˜¯fs.Objectç±»å‹ï¼Œç›´æ¥ä½¿ç”¨")
	} else if overrideRemote, ok := src.(*fs.OverrideRemote); ok {
		// å¤„ç†fs.OverrideRemoteåŒ…è£…ç±»å‹
		fs.Debugf(f, "æºå¯¹è±¡æ˜¯fs.OverrideRemoteç±»å‹ï¼Œå°è¯•è§£åŒ…")
		srcObj = overrideRemote.UnWrap()
		if srcObj == nil {
			return nil, fmt.Errorf("æ— æ³•ä»fs.OverrideRemoteä¸­è§£åŒ…å‡ºfs.Object")
		}
		fs.Debugf(f, "æˆåŠŸä»fs.OverrideRemoteè§£åŒ…å‡ºfs.Object")
	} else {
		// å°è¯•ä½¿ç”¨rcloneæ ‡å‡†çš„UnWrapObjectInfoå‡½æ•°
		fs.Debugf(f, "æºå¯¹è±¡ç±»å‹ %Tï¼Œå°è¯•ä½¿ç”¨æ ‡å‡†è§£åŒ…æ–¹æ³•", src)
		srcObj = fs.UnWrapObjectInfo(src)
		if srcObj == nil {
			return nil, fmt.Errorf("æºå¯¹è±¡ç±»å‹ %T æ— æ³•è§£åŒ…ä¸ºfs.Objectï¼Œæ— æ³•è¿›è¡Œè·¨äº‘ä¼ è¾“", src)
		}
		fs.Debugf(f, "ä½¿ç”¨æ ‡å‡†è§£åŒ…æ–¹æ³•æˆåŠŸè·å–fs.Object")
	}

	fs.Debugf(f, "ä½¿ç”¨fs.Objectæ¥å£æ‰“å¼€æºæ–‡ä»¶")
	srcReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
	}
	defer srcReader.Close()

	// ç»§ç»­å¤„ç†ä¸‹è½½...
	return f.handleDownloadThenUpload(ctx, srcReader, src, parentFileID, fileName)
}

// handleDownloadThenUpload å¤„ç†å®é™…çš„ä¸‹è½½ç„¶åä¸Šä¼ é€»è¾‘
func (f *Fs) handleDownloadThenUpload(ctx context.Context, srcReader io.ReadCloser, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸŒ å¼€å§‹ä¼˜åŒ–è·¨äº‘ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©æœ€ä¼˜ä¼ è¾“ç­–ç•¥
	switch {
	case fileSize <= 512*1024*1024: // 512MBä»¥ä¸‹ - å†…å­˜ç¼“å†²ç­–ç•¥
		fs.Infof(f, "ğŸ“ ä½¿ç”¨å†…å­˜ç¼“å†²ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.memoryBufferedCrossCloudTransfer(ctx, srcReader, src, parentFileID, fileName)
	case fileSize <= 2*1024*1024*1024: // 2GBä»¥ä¸‹ - æ··åˆç­–ç•¥
		fs.Infof(f, "ğŸ”„ ä½¿ç”¨æ··åˆç¼“å†²ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.hybridCrossCloudTransfer(ctx, srcReader, src, parentFileID, fileName)
	default: // å¤§æ–‡ä»¶ - ä¼˜åŒ–çš„ç£ç›˜ç­–ç•¥
		fs.Infof(f, "ğŸ’¾ ä½¿ç”¨ä¼˜åŒ–ç£ç›˜ç­–ç•¥ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		return f.optimizedDiskCrossCloudTransfer(ctx, srcReader, src, parentFileID, fileName)
	}
}

// memoryBufferedCrossCloudTransfer å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“ï¼ˆ512MBä»¥ä¸‹æ–‡ä»¶ï¼‰
func (f *Fs) memoryBufferedCrossCloudTransfer(ctx context.Context, srcReader io.ReadCloser, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸ“ å¼€å§‹å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ­¥éª¤1: ç›´æ¥è¯»å–åˆ°å†…å­˜
	startTime := time.Now()
	data := make([]byte, fileSize)
	n, err := io.ReadFull(srcReader, data)
	downloadDuration := time.Since(startTime)

	if err != nil && err != io.ErrUnexpectedEOF {
		return nil, fmt.Errorf("å†…å­˜è¯»å–æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if int64(n) != fileSize {
		fs.Debugf(f, "å®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…ï¼Œä½¿ç”¨å®é™…å¤§å°", n, fileSize)
		data = data[:n]
		fileSize = int64(n)
	}

	fs.Infof(f, "ğŸ“¥ å†…å­˜ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// æ­¥éª¤2: è®¡ç®—MD5
	fs.Infof(f, "ğŸ” å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	md5StartTime := time.Now()
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration := time.Since(md5StartTime)

	fs.Infof(f, "ğŸ” MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v", md5Hash, md5Duration)

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        fileSize,
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ­¥éª¤4: å†…å­˜ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä»å†…å­˜ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	// ä½¿ç”¨å†…å­˜æ•°æ®åˆ›å»ºreaderè¿›è¡Œä¸Šä¼ 
	dataReader := bytes.NewReader(data)
	err = f.uploadFile(ctx, dataReader, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å†…å­˜ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	fs.Infof(f, "âœ… å†…å­˜ç¼“å†²è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// hybridCrossCloudTransfer æ··åˆç¼“å†²è·¨äº‘ä¼ è¾“ï¼ˆ512MB-2GBæ–‡ä»¶ï¼‰
func (f *Fs) hybridCrossCloudTransfer(ctx context.Context, srcReader io.ReadCloser, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ğŸ”„ å¼€å§‹æ··åˆç¼“å†²è·¨äº‘ä¼ è¾“ï¼Œæ–‡ä»¶å¤§å°: %s", fs.SizeSuffix(fileSize))

	// å¯¹äºä¸­ç­‰å¤§å°æ–‡ä»¶ï¼Œå°è¯•å¹¶å‘ä¸‹è½½ä¼˜åŒ–
	if srcObj, ok := src.(fs.Object); ok && fileSize > 50*1024*1024 { // 50MBä»¥ä¸Šæ‰ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼Œé™ä½è§¦å‘é˜ˆå€¼
		fs.Infof(f, "ğŸš€ å°è¯•å¹¶å‘ä¸‹è½½ä¼˜åŒ–ç­–ç•¥")
		return f.concurrentDownloadCrossCloudTransfer(ctx, srcObj, parentFileID, fileName)
	}

	// å›é€€åˆ°ä¼˜åŒ–çš„ç£ç›˜ç­–ç•¥
	return f.optimizedDiskCrossCloudTransfer(ctx, srcReader, src, parentFileID, fileName)
}

// concurrentDownloadCrossCloudTransfer å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“
func (f *Fs) concurrentDownloadCrossCloudTransfer(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fileSize := srcObj.Size()
	fs.Infof(f, "ğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// è®¡ç®—åˆ†ç‰‡å‚æ•°
	chunkSize := int64(32 * 1024 * 1024) // 32MB per chunk
	if fileSize < chunkSize*2 {
		// æ–‡ä»¶å¤ªå°ï¼Œä¸å€¼å¾—å¹¶å‘ä¸‹è½½
		fs.Debugf(f, "æ–‡ä»¶å¤ªå°ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½")
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer srcReader.Close()
		return f.optimizedDiskCrossCloudTransfer(ctx, srcReader, srcObj, parentFileID, fileName)
	}

	numChunks := (fileSize + chunkSize - 1) / chunkSize
	maxConcurrency := int64(4) // æœ€å¤š4ä¸ªå¹¶å‘ä¸‹è½½
	if numChunks < maxConcurrency {
		maxConcurrency = numChunks
	}

	fs.Infof(f, "ğŸ“Š å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºç»„è£…
	tempFile, err := f.resourcePool.GetOptimizedTempFile("concurrent_download_", fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.resourcePool.PutTempFile(tempFile)

	// å¹¶å‘ä¸‹è½½å„ä¸ªåˆ†ç‰‡
	startTime := time.Now()
	err = f.downloadChunksConcurrently(ctx, srcObj, tempFile, chunkSize, numChunks, maxConcurrency)
	downloadDuration := time.Since(startTime)

	if err != nil {
		fs.Debugf(f, "å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		// å›é€€åˆ°æ™®é€šä¸‹è½½
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("å›é€€æ‰“å¼€æºæ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer srcReader.Close()
		return f.optimizedDiskCrossCloudTransfer(ctx, srcReader, srcObj, parentFileID, fileName)
	}

	fs.Infof(f, "ğŸ“¥ å¹¶å‘ä¸‹è½½å®Œæˆ: %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
		fs.SizeSuffix(fileSize), downloadDuration,
		fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¹¶ç»§ç»­åç»­å¤„ç†
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	// ç»§ç»­MD5è®¡ç®—å’Œä¸Šä¼ æµç¨‹
	return f.continueAfterDownload(ctx, tempFile, srcObj, parentFileID, fileName, downloadDuration)
}

// downloadChunksConcurrently å¹¶å‘ä¸‹è½½æ–‡ä»¶åˆ†ç‰‡ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) downloadChunksConcurrently(ctx context.Context, srcObj fs.Object, tempFile *os.File, chunkSize, numChunks, maxConcurrency int64) error {
	// åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
	progress := NewDownloadProgress(numChunks, srcObj.Size())

	// ğŸ”§ ä¼˜åŒ–ï¼šé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤º
	var currentAccount *accounting.Account
	if stats := accounting.GlobalStats(); stats != nil {
		if acc := stats.GetInProgressAccount(srcObj.Remote()); acc != nil {
			currentAccount = acc
		} else {
			// æ·»åŠ æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
			allAccounts := stats.ListInProgressAccounts()
			for _, accountName := range allAccounts {
				// å¦‚æœAccountåç§°åŒ…å«æˆ‘ä»¬çš„æ–‡ä»¶åï¼Œæˆ–è€…æˆ‘ä»¬çš„æ–‡ä»¶ååŒ…å«Accountåç§°
				if strings.Contains(accountName, srcObj.Remote()) || strings.Contains(srcObj.Remote(), accountName) {
					if acc := stats.GetInProgressAccount(accountName); acc != nil {
						currentAccount = acc
						break
					}
				}
			}
		}
	}

	// å¯åŠ¨è¿›åº¦æ›´æ–°åç¨‹ï¼ˆæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯ï¼‰
	progressCtx, cancelProgress := context.WithCancel(ctx)
	defer cancelProgress()

	go f.updateAccountExtraInfo(progressCtx, progress, srcObj.Remote(), currentAccount)

	// åˆ›å»ºå·¥ä½œæ± 
	semaphore := make(chan struct{}, maxConcurrency)
	errChan := make(chan error, numChunks)
	var wg sync.WaitGroup

	for i := int64(0); i < numChunks; i++ {
		wg.Add(1)
		go func(chunkIndex int64) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// è®¡ç®—åˆ†ç‰‡èŒƒå›´
			start := chunkIndex * chunkSize
			end := start + chunkSize - 1
			if end >= srcObj.Size() {
				end = srcObj.Size() - 1
			}
			actualChunkSize := end - start + 1

			// ä¸‹è½½åˆ†ç‰‡ï¼ˆå¸¦è¿›åº¦è·Ÿè¸ªï¼‰
			chunkStartTime := time.Now()
			err := f.downloadChunk(ctx, srcObj, tempFile, start, end, chunkIndex)
			chunkDuration := time.Since(chunkStartTime)

			if err != nil {
				errChan <- fmt.Errorf("ä¸‹è½½åˆ†ç‰‡ %d å¤±è´¥: %w", chunkIndex, err)
				return
			}

			// æ›´æ–°è¿›åº¦
			progress.UpdateChunkProgress(chunkIndex, actualChunkSize, chunkDuration)

			// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šè°ƒç”¨å…±äº«çš„è¿›åº¦æ˜¾ç¤ºå‡½æ•°
			f.displayUnifiedDownloadProgress(progress, "123ç½‘ç›˜", srcObj.Remote())

			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d ä¸‹è½½æˆåŠŸ: %d-%d (%s) | ç”¨æ—¶: %v | é€Ÿåº¦: %.2f MB/s",
				chunkIndex, start, end, fs.SizeSuffix(actualChunkSize), chunkDuration,
				float64(actualChunkSize)/chunkDuration.Seconds()/1024/1024)
		}(i)
	}

	// ç­‰å¾…æ‰€æœ‰ä¸‹è½½å®Œæˆ
	wg.Wait()
	close(errChan)

	// æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
	for err := range errChan {
		if err != nil {
			return err
		}
	}

	return nil
}

// downloadChunk ä¸‹è½½å•ä¸ªæ–‡ä»¶åˆ†ç‰‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
func (f *Fs) downloadChunk(ctx context.Context, srcObj fs.Object, tempFile *os.File, start, end, chunkIndex int64) error {
	const maxRetries = 4
	var lastErr error

	for retry := 0; retry < maxRetries; retry++ {
		// åœ¨æ¯æ¬¡é‡è¯•å‰å¼ºåˆ¶æ¸…é™¤ä¸‹è½½URLç¼“å­˜ï¼ˆå‚è€ƒ115ç½‘ç›˜ä¿®å¤æ–¹æ¡ˆï¼‰
		if retry > 0 {
			// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜å·²åˆ é™¤ï¼Œä¸éœ€è¦æ¸…é™¤ç¼“å­˜
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡é‡è¯•ï¼Œç›´æ¥è·å–æ–°ä¸‹è½½URL")

			// æ·»åŠ é‡è¯•å»¶è¿Ÿ
			retryDelay := time.Duration(retry) * time.Second
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d é‡è¯• %d/%d: ç­‰å¾… %v åé‡è¯•", chunkIndex, retry, maxRetries-1, retryDelay)
			time.Sleep(retryDelay)
		}

		// ä½¿ç”¨Rangeé€‰é¡¹æ‰“å¼€æ–‡ä»¶åˆ†ç‰‡
		rangeOption := &fs.RangeOption{Start: start, End: end}
		chunkReader, err := srcObj.Open(ctx, rangeOption)
		if err != nil {
			lastErr = fmt.Errorf("æ‰“å¼€åˆ†ç‰‡å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d æ‰“å¼€å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// è¯»å–åˆ†ç‰‡æ•°æ®
		chunkData, err := io.ReadAll(chunkReader)
		chunkReader.Close()
		if err != nil {
			lastErr = fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d è¯»å–å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// å†™å…¥ä¸´æ—¶æ–‡ä»¶çš„æ­£ç¡®ä½ç½®
		_, err = tempFile.WriteAt(chunkData, start)
		if err != nil {
			lastErr = fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d å†™å…¥å¤±è´¥ (é‡è¯• %d/%d): %v", chunkIndex, retry, maxRetries-1, err)
			continue
		}

		// æˆåŠŸå®Œæˆ
		if retry > 0 {
			fs.Debugf(f, "123ç½‘ç›˜åˆ†ç‰‡ %d é‡è¯•æˆåŠŸ (ç¬¬ %d æ¬¡å°è¯•)", chunkIndex, retry+1)
		}
		return nil
	}

	// æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
	return fmt.Errorf("123ç½‘ç›˜åˆ†ç‰‡ %d ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯• %d æ¬¡: %w", chunkIndex, maxRetries, lastErr)
}

// continueAfterDownload ä¸‹è½½å®Œæˆåç»§ç»­MD5è®¡ç®—å’Œä¸Šä¼ æµç¨‹
func (f *Fs) continueAfterDownload(ctx context.Context, tempFile *os.File, src fs.ObjectInfo, parentFileID int64, fileName string, downloadDuration time.Duration) (*Object, error) {
	fileSize := src.Size()
	startTime := time.Now().Add(-downloadDuration) // è°ƒæ•´å¼€å§‹æ—¶é—´ä»¥åŒ…å«ä¸‹è½½æ—¶é—´

	// æ­¥éª¤2: è®¡ç®—MD5
	fs.Infof(f, "ğŸ” å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	md5StartTime := time.Now()

	_, err := tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	hasher := md5.New()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration := time.Since(md5StartTime)

	fs.Infof(f, "ğŸ” MD5è®¡ç®—å®Œæˆ: %s, ç”¨æ—¶: %v", md5Hash, md5Duration)

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        fileSize,
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ­¥éª¤4: ä¸Šä¼ æ–‡ä»¶
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	err = f.uploadFile(ctx, tempFile, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	fs.Infof(f, "âœ… å¹¶å‘ä¸‹è½½è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// MD5ç¼“å­˜ç®¡ç†å™¨
type CrossCloudMD5Cache struct {
	cache map[string]CrossCloudMD5Entry
	mutex sync.RWMutex
}

type CrossCloudMD5Entry struct {
	MD5Hash    string
	FileSize   int64
	ModTime    time.Time
	CachedTime time.Time
}

var globalMD5Cache = &CrossCloudMD5Cache{
	cache: make(map[string]CrossCloudMD5Entry),
}

// getCachedMD5 å°è¯•ä»ç¼“å­˜è·å–MD5
func (f *Fs) getCachedMD5(src fs.ObjectInfo) (string, bool) {
	// ç”Ÿæˆç¼“å­˜é”®ï¼šæºæ–‡ä»¶ç³»ç»Ÿç±»å‹ + è·¯å¾„ + å¤§å° + ä¿®æ”¹æ—¶é—´
	srcFs := src.Fs()
	if srcFs == nil {
		return "", false
	}

	cacheKey := fmt.Sprintf("%s:%s:%d:%d",
		srcFs.Name(), src.Remote(), src.Size(), src.ModTime(context.Background()).Unix())

	globalMD5Cache.mutex.RLock()
	defer globalMD5Cache.mutex.RUnlock()

	entry, exists := globalMD5Cache.cache[cacheKey]
	if !exists {
		return "", false
	}

	// æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
	if time.Since(entry.CachedTime) > 24*time.Hour {
		// å¼‚æ­¥æ¸…ç†è¿‡æœŸç¼“å­˜
		go func() {
			globalMD5Cache.mutex.Lock()
			delete(globalMD5Cache.cache, cacheKey)
			globalMD5Cache.mutex.Unlock()
		}()
		return "", false
	}

	// éªŒè¯æ–‡ä»¶ä¿¡æ¯æ˜¯å¦åŒ¹é…
	if entry.FileSize == src.Size() && entry.ModTime.Equal(src.ModTime(context.Background())) {
		fs.Debugf(f, "ğŸ¯ MD5ç¼“å­˜å‘½ä¸­: %s -> %s", cacheKey, entry.MD5Hash)
		return entry.MD5Hash, true
	}

	return "", false
}

// cacheMD5 å°†MD5å­˜å…¥ç¼“å­˜
func (f *Fs) cacheMD5(src fs.ObjectInfo, md5Hash string) {
	srcFs := src.Fs()
	if srcFs == nil {
		return
	}

	cacheKey := fmt.Sprintf("%s:%s:%d:%d",
		srcFs.Name(), src.Remote(), src.Size(), src.ModTime(context.Background()).Unix())

	entry := CrossCloudMD5Entry{
		MD5Hash:    md5Hash,
		FileSize:   src.Size(),
		ModTime:    src.ModTime(context.Background()),
		CachedTime: time.Now(),
	}

	globalMD5Cache.mutex.Lock()
	globalMD5Cache.cache[cacheKey] = entry
	globalMD5Cache.mutex.Unlock()

	fs.Debugf(f, "ğŸ’¾ MD5å·²ç¼“å­˜: %s -> %s", cacheKey, md5Hash)
}

// continueWithKnownMD5 ä½¿ç”¨å·²çŸ¥MD5ç»§ç»­ä¸Šä¼ æµç¨‹
func (f *Fs) continueWithKnownMD5(ctx context.Context, tempFile *os.File, src fs.ObjectInfo, parentFileID int64, fileName string, md5Hash string, downloadDuration, md5Duration time.Duration, startTime time.Time) (*Object, error) {
	fileSize := src.Size()

	// æ­¥éª¤3: æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "âš¡ æ£€æŸ¥123ç½‘ç›˜ç§’ä¼ åŠŸèƒ½...")
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ ç§’ä¼ æˆåŠŸï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration, downloadDuration, md5Duration)
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        fileSize,
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ­¥éª¤4: ä¸Šä¼ æ–‡ä»¶
	fs.Infof(f, "ğŸ“¤ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜...")
	uploadStartTime := time.Now()

	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	err = f.uploadFile(ctx, tempFile, createResp, fileSize)
	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	totalDuration := time.Since(startTime)

	// ç¼“å­˜MD5ä»¥ä¾›åç»­ä½¿ç”¨
	f.cacheMD5(src, md5Hash)

	fs.Infof(f, "âœ… ä¼˜åŒ–è·¨äº‘ä¼ è¾“å®Œæˆï¼æ€»ç”¨æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)",
		totalDuration, downloadDuration, md5Duration, uploadDuration)

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        fileSize,
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// optimizedDiskCrossCloudTransfer ä¼˜åŒ–çš„ç£ç›˜è·¨äº‘ä¼ è¾“ï¼ˆåŸæœ‰é€»è¾‘ä¼˜åŒ–ç‰ˆï¼‰
func (f *Fs) optimizedDiskCrossCloudTransfer(ctx context.Context, srcReader io.ReadCloser, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	startTime := time.Now()

	// æ­¥éª¤1: æ£€æŸ¥MD5ç¼“å­˜
	var md5Hash string
	var downloadDuration, md5Duration time.Duration

	if cachedMD5, found := f.getCachedMD5(src); found {
		fs.Infof(f, "ğŸ¯ ä½¿ç”¨ç¼“å­˜çš„MD5: %s", cachedMD5)
		md5Hash = cachedMD5
		md5Duration = 0 // ç¼“å­˜å‘½ä¸­ï¼Œæ— éœ€è®¡ç®—æ—¶é—´

		// ä»éœ€è¦ä¸‹è½½æ–‡ä»¶ç”¨äºä¸Šä¼ ï¼Œä½†å¯ä»¥è·³è¿‡MD5è®¡ç®—
		tempFile, err := f.resourcePool.GetOptimizedTempFile("cross_cloud_cached_", fileSize)
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer f.resourcePool.PutTempFile(tempFile)

		downloadStartTime := time.Now()
		written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, false, fileName) // ä¸éœ€è¦è®¡ç®—MD5
		downloadDuration = time.Since(downloadStartTime)

		if err != nil {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
		}

		if written != fileSize {
			return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", fileSize, written)
		}

		fs.Infof(f, "ğŸ“¥ ä¸‹è½½å®Œæˆ (ä½¿ç”¨ç¼“å­˜MD5): %s, ç”¨æ—¶: %v, é€Ÿåº¦: %s/s",
			fs.SizeSuffix(fileSize), downloadDuration,
			fs.SizeSuffix(int64(float64(fileSize)/downloadDuration.Seconds())))

		// ç›´æ¥è·³è½¬åˆ°ä¸Šä¼ æ­¥éª¤
		return f.continueWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash, downloadDuration, md5Duration, startTime)
	}

	// æ­¥éª¤2: ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸ä¸‹è½½å¹¶è®¡ç®—MD5
	fs.Infof(f, "ğŸ’¾ MD5ç¼“å­˜æœªå‘½ä¸­ï¼Œå¼€å§‹ä¸‹è½½å¹¶è®¡ç®—MD5")

	// åˆ›å»ºæœ¬åœ°ä¸´æ—¶æ–‡ä»¶
	tempFile, err := f.resourcePool.GetOptimizedTempFile("cross_cloud_optimized_", fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.resourcePool.PutTempFile(tempFile)

	// å®Œæ•´ä¸‹è½½æ–‡ä»¶å¹¶è®¡ç®—MD5
	downloadStartTime := time.Now()
	written, err := f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, true, fileName)
	downloadDuration = time.Since(downloadStartTime)

	if err != nil {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}

	downloadSpeed := float64(written) / downloadDuration.Seconds() / (1024 * 1024) // MB/s
	fs.Infof(f, "âœ… æ­¥éª¤1å®Œæˆ: æ–‡ä»¶ä¸‹è½½æˆåŠŸ %sï¼Œè€—æ—¶: %vï¼Œé€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), downloadDuration.Round(time.Second), downloadSpeed)

	// æ­¥éª¤2: ä»æœ¬åœ°æ–‡ä»¶è®¡ç®—MD5
	fs.Infof(f, "ğŸ”¢ æ­¥éª¤2/4: å¼€å§‹è®¡ç®—MD5å“ˆå¸Œ...")
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	hasher := md5.New()
	md5StartTime := time.Now()
	_, err = io.Copy(hasher, tempFile)
	if err != nil {
		return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
	}

	md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
	md5Duration = time.Since(md5StartTime)
	fs.Infof(f, "âœ… æ­¥éª¤2å®Œæˆ: MD5è®¡ç®—æˆåŠŸ %sï¼Œè€—æ—¶: %v", md5Hash, md5Duration.Round(time.Second))

	// æ­¥éª¤3: åˆ›å»ºä¸Šä¼ ä¼šè¯å¹¶æ£€æŸ¥ç§’ä¼ 
	fs.Infof(f, "ğŸš€ æ­¥éª¤3/4: åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ ...")
	createResp, err := f.createUploadV2(ctx, parentFileID, fileName, md5Hash, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// æ£€æŸ¥æ˜¯å¦è§¦å‘ç§’ä¼ 
	if createResp.Data.Reuse {
		totalDuration := time.Since(startTime)
		fs.Infof(f, "ğŸ‰ æ­¥éª¤3å®Œæˆ: è§¦å‘ç§’ä¼ åŠŸèƒ½ï¼æ€»è€—æ—¶: %v (ä¸‹è½½: %v + MD5: %v)",
			totalDuration.Round(time.Second), downloadDuration.Round(time.Second), md5Duration.Round(time.Second))
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        fileSize,
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ­¥éª¤4: æ‰§è¡Œå®é™…ä¸Šä¼ 
	fs.Infof(f, "ğŸ“¤ æ­¥éª¤4/4: å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°123ç½‘ç›˜...")
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆç”¨äºä¸Šä¼ å¤±è´¥: %w", err)
	}

	// å‡†å¤‡ä¸Šä¼ å‚æ•°
	chunkSize := createResp.Data.SliceSize
	if chunkSize <= 0 {
		chunkSize = f.getOptimalChunkSize(fileSize, 20*1024*1024) // é»˜è®¤20MB/sç½‘é€Ÿ
	}
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	progress, err := f.prepareUploadProgress(createResp.Data.PreuploadID, totalChunks, chunkSize, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å‡†å¤‡ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
	}

	// è®¡ç®—å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)
	maxConcurrency := concurrencyParams.optimal

	// æ‰§è¡Œä¸Šä¼ 
	uploadStartTime := time.Now()
	result, err := f.v2UploadChunksWithConcurrency(ctx, tempFile, &localFileInfo{
		name:    fileName,
		size:    fileSize,
		modTime: src.ModTime(ctx),
	}, createResp, progress, fileName, maxConcurrency)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(fileSize) / uploadDuration.Seconds() / (1024 * 1024) // MB/s
	totalDuration := time.Since(startTime)

	// ç¼“å­˜MD5ä»¥ä¾›åç»­ä½¿ç”¨
	f.cacheMD5(src, md5Hash)

	fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“å®Œæˆ: %sï¼Œæ€»è€—æ—¶: %v (ä¸‹è½½: %v + MD5: %v + ä¸Šä¼ : %v)ï¼Œä¸Šä¼ é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(fileSize), totalDuration.Round(time.Second),
		downloadDuration.Round(time.Second), md5Duration.Round(time.Second),
		uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// putWithKnownMD5 å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ 
// è¿™æ˜¯æœ€é«˜æ•ˆçš„è·¯å¾„ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨123ç½‘ç›˜çš„ç§’ä¼ åŠŸèƒ½
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) putWithKnownMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	return f.putWithKnownMD5WithOptions(ctx, in, src, parentFileID, fileName, md5Hash, false)
}

// putWithKnownMD5WithOptions å¤„ç†å·²çŸ¥MD5å“ˆå¸Œçš„æ–‡ä»¶ä¸Šä¼ ï¼Œæ”¯æŒè·³è¿‡å•æ­¥ä¸Šä¼ é€‰é¡¹
func (f *Fs) putWithKnownMD5WithOptions(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName, md5Hash string, skipSingleStep bool) (*Object, error) {
	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆå°è¯•å•æ­¥ä¸Šä¼ APIï¼ˆé™¤éæ˜ç¡®è·³è¿‡ï¼‰
	fileSize := src.Size()
	if !skipSingleStep && fileSize < singleStepUploadLimit && fileSize > 0 && fileSize <= maxMemoryBufferSize {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", fileSize, singleStepUploadLimit)

		// è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜
		data, err := io.ReadAll(in)
		if err != nil {
			fs.Debugf(f, "è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
		} else if int64(len(data)) == fileSize {
			// å°è¯•å•æ­¥ä¸Šä¼ 
			result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
			if err != nil {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ : %v", err)
				// é‡æ–°åˆ›å»ºreaderç”¨äºä¼ ç»Ÿä¸Šä¼ 
				in = bytes.NewReader(data)
			} else {
				fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
				return result, nil
			}
		}
	}

	// ä¼ ç»Ÿä¸Šä¼ æµç¨‹ï¼šä½¿ç”¨å·²çŸ¥çš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å·²æœ‰ç›¸åŒMD5çš„æ–‡ä»¶
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, src.Size())
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ /å»é‡ï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™æ˜¯æœ€ç†æƒ³çš„æƒ…å†µï¼šæ— éœ€å®é™…ä¼ è¾“æ•°æ®
	if createResp.Data.Reuse {
		fs.Debugf(f, "æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return &Object{
			fs:          f,                                             // æ–‡ä»¶ç³»ç»Ÿå¼•ç”¨
			remote:      fileName,                                      // ä½¿ç”¨å®é™…æ–‡ä»¶åï¼Œä¸åŒ…å«.partialåç¼€
			hasMetaData: true,                                          // å·²æœ‰å…ƒæ•°æ®
			id:          strconv.FormatInt(createResp.Data.FileID, 10), // 123ç½‘ç›˜æ–‡ä»¶ID
			size:        src.Size(),                                    // æ–‡ä»¶å¤§å°
			md5sum:      md5Hash,                                       // MD5å“ˆå¸Œå€¼
			modTime:     time.Now(),                                    // ä¿®æ”¹æ—¶é—´
			isDir:       false,                                         // ä¸æ˜¯ç›®å½•
		}, nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œéœ€è¦å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®
	fs.Debugf(f, "ç§’ä¼ å¤±è´¥ï¼Œå¼€å§‹å®é™…ä¸Šä¼ æ–‡ä»¶æ•°æ®")
	err = f.uploadFile(ctx, in, createResp, src.Size())
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	// æ³¨æ„ï¼šä½¿ç”¨å®é™…çš„æ–‡ä»¶åè€Œä¸æ˜¯src.Remote()ï¼Œå› ä¸ºsrc.Remote()å¯èƒ½åŒ…å«.partialåç¼€
	// è€Œå®é™…ä¸Šä¼ åˆ°123ç½‘ç›˜çš„æ–‡ä»¶åæ˜¯fileNameï¼ˆå·²å»é™¤.partialåç¼€ï¼‰
	return &Object{
		fs:          f,
		remote:      fileName, // ä½¿ç”¨å®é™…çš„æ–‡ä»¶åï¼Œä¸åŒ…å«.partialåç¼€
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        src.Size(),
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// putSmallFileWithMD5 å¤„ç†å°æ–‡ä»¶ï¼ˆâ‰¤10MBï¼‰çš„ä¸Šä¼ 
// é€šè¿‡ç¼“å­˜åˆ°å†…å­˜å…ˆè®¡ç®—MD5ï¼Œæœ€å¤§åŒ–åˆ©ç”¨ç§’ä¼ åŠŸèƒ½ï¼ŒåŒæ—¶æ§åˆ¶å†…å­˜ä½¿ç”¨
func (f *Fs) putSmallFileWithMD5(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fileSize := src.Size()
	fs.Debugf(f, "ç¼“å­˜å°æ–‡ä»¶ï¼ˆ%då­—èŠ‚ï¼‰åˆ°å†…å­˜ä»¥è®¡ç®—MD5ï¼Œå°è¯•ç§’ä¼ ", fileSize)

	// éªŒè¯æ–‡ä»¶å¤§å°çš„åˆç†æ€§
	if fileSize < 0 {
		return nil, fmt.Errorf("æ— æ•ˆçš„æ–‡ä»¶å¤§å°: %d", fileSize)
	}
	if fileSize > 50*1024*1024 { // 50MBä¸Šé™
		fs.Debugf(f, "æ–‡ä»¶è¿‡å¤§ï¼ˆ%då­—èŠ‚ï¼‰ï¼Œä¸é€‚åˆå†…å­˜ç¼“å­˜ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ", fileSize)
		return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
	}

	// æ£€æŸ¥å†…å­˜ç®¡ç†å™¨æ˜¯å¦å…è®¸åˆ†é…
	fileID := fmt.Sprintf("%s_%d_%d", fileName, parentFileID, time.Now().UnixNano())
	if !f.memoryManager.CanAllocate(fileSize) {
		fs.Debugf(f, "å†…å­˜ä¸è¶³ï¼Œæ— æ³•ç¼“å­˜æ–‡ä»¶ï¼ˆ%då­—èŠ‚ï¼‰ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ", fileSize)
		return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
	}

	// é¢„å…ˆåˆ†é…å†…å­˜è·Ÿè¸ª
	if !f.memoryManager.Allocate(fileID, fileSize) {
		fs.Debugf(f, "å†…å­˜åˆ†é…å¤±è´¥ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ")
		return f.streamingPutWithBuffer(ctx, in, src, parentFileID, fileName)
	}

	// ç¡®ä¿åœ¨å‡½æ•°ç»“æŸæ—¶é‡Šæ”¾å†…å­˜
	defer f.memoryManager.Release(fileID)

	// ä½¿ç”¨é™åˆ¶è¯»å–å™¨é˜²æ­¢è¯»å–è¶…è¿‡é¢„æœŸå¤§å°çš„æ•°æ®
	// è¿™å¯ä»¥é˜²æ­¢æ¶æ„æˆ–æŸåçš„æ–‡ä»¶å¯¼è‡´å†…å­˜è€—å°½
	var limitedReader io.Reader
	if fileSize > 0 {
		// å…è®¸è¯»å–æ¯”é¢„æœŸç¨å¤šä¸€ç‚¹çš„æ•°æ®æ¥æ£€æµ‹å¤§å°ä¸åŒ¹é…
		limitedReader = io.LimitReader(in, fileSize+1024) // é¢å¤–1KBç”¨äºæ£€æµ‹
	} else {
		// å¯¹äºæœªçŸ¥å¤§å°çš„æ–‡ä»¶ï¼Œè®¾ç½®åˆç†çš„ä¸Šé™
		limitedReader = io.LimitReader(in, 50*1024*1024) // 50MBä¸Šé™
	}

	// å°†æ–‡ä»¶è¯»å–åˆ°å†…å­˜ï¼Œä½¿ç”¨é™åˆ¶è¯»å–å™¨ä¿æŠ¤
	data, err := io.ReadAll(limitedReader)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–å°æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// ç«‹å³æ£€æŸ¥è¯»å–çš„æ•°æ®å¤§å°ï¼Œé˜²æ­¢å†…å­˜æµªè´¹
	actualSize := int64(len(data))
	if fileSize > 0 {
		if actualSize > fileSize {
			return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡é¢„æœŸ: å®é™… %d å­—èŠ‚ > é¢„æœŸ %d å­—èŠ‚ï¼Œå¯èƒ½æ–‡ä»¶å·²æŸå", actualSize, fileSize)
		}
		if actualSize != fileSize {
			fs.Debugf(f, "æ–‡ä»¶å¤§å°ä¸é¢„æœŸä¸ç¬¦: å®é™… %d å­—èŠ‚ï¼Œé¢„æœŸ %d å­—èŠ‚", actualSize, fileSize)
		}
	}

	// æ›´æ–°å†…å­˜ä½¿ç”¨ç›‘æ§
	f.performanceMetrics.UpdateMemoryUsage(actualSize)

	// è®¡ç®—MD5å“ˆå¸Œå€¼
	// è¿™æ˜¯å…³é”®æ­¥éª¤ï¼šå…ˆè®¡ç®—MD5ï¼Œç„¶åå°è¯•ç§’ä¼ 
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "å°æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %s", md5Hash)

	// ä½¿ç”¨è®¡ç®—å‡ºçš„MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	// 123ç½‘ç›˜ä¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ 
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ˆç§’ä¼ æˆåŠŸï¼‰ï¼Œç›´æ¥è¿”å›å¯¹è±¡
	// è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬é¿å…äº†å®é™…çš„æ•°æ®ä¼ è¾“
	if createResp.Data.Reuse {
		fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ æˆåŠŸï¼ŒMD5: %s", md5Hash)
		return &Object{
			fs:          f,                                             // æ–‡ä»¶ç³»ç»Ÿå¼•ç”¨
			remote:      src.Remote(),                                  // è¿œç¨‹è·¯å¾„
			hasMetaData: true,                                          // å·²æœ‰å…ƒæ•°æ®
			id:          strconv.FormatInt(createResp.Data.FileID, 10), // 123ç½‘ç›˜æ–‡ä»¶ID
			size:        int64(len(data)),                              // æ–‡ä»¶å¤§å°
			md5sum:      md5Hash,                                       // MD5å“ˆå¸Œå€¼
			modTime:     time.Now(),                                    // ä¿®æ”¹æ—¶é—´
			isDir:       false,                                         // ä¸æ˜¯ç›®å½•
		}, nil
	}

	// ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®
	// ä½¿ç”¨bytes.NewReaderä»å†…å­˜ä¸­çš„æ•°æ®åˆ›å»ºreader
	fs.Debugf(f, "å°æ–‡ä»¶ç§’ä¼ å¤±è´¥ï¼Œä¸Šä¼ ç¼“å­˜çš„æ•°æ®")
	err = f.uploadFile(ctx, bytes.NewReader(data), createResp, int64(len(data)))
	if err != nil {
		return nil, err
	}

	// è¿”å›ä¸Šä¼ æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡
	return &Object{
		fs:          f,
		remote:      src.Remote(),
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        int64(len(data)),
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// streamingPutWithBuffer ä½¿ç”¨ç¼“å†²ç­–ç•¥å¤„ç†è·¨äº‘ç›˜ä¼ è¾“
// è§£å†³"æºæ–‡ä»¶ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é—®é¢˜
func (f *Fs) streamingPutWithBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ä½¿ç”¨ç¼“å†²ç­–ç•¥è¿›è¡Œè·¨äº‘ç›˜ä¼ è¾“")

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†ä»¥é¿å…å†…å­˜æº¢å‡º
	fileSize := src.Size()

	// å¦‚æœæ–‡ä»¶å°äº100MBï¼Œç›´æ¥è¯»å…¥å†…å­˜
	if fileSize <= 100*1024*1024 {
		fs.Debugf(f, "å°æ–‡ä»¶(%s)ï¼Œä½¿ç”¨å†…å­˜ç¼“å†²", fs.SizeSuffix(fileSize))
		return f.streamingPutWithMemoryBuffer(ctx, in, src, parentFileID, fileName)
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²", fs.SizeSuffix(fileSize))
	return f.streamingPutWithTempFile(ctx, in, src, parentFileID, fileName)
}

// streamingPutWithMemoryBuffer ä½¿ç”¨å†…å­˜ç¼“å†²å¤„ç†å°æ–‡ä»¶
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
func (f *Fs) streamingPutWithMemoryBuffer(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// è®¡ç®—MD5
	hasher := md5.New()
	hasher.Write(data)
	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))

	fs.Debugf(f, "å†…å­˜ç¼“å†²å®Œæˆï¼Œæ–‡ä»¶å¤§å°: %d, MD5: %s", len(data), md5Hash)

	// å¯¹äºå°æ–‡ä»¶ï¼ˆ<1GBï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨å•æ­¥ä¸Šä¼ API
	if len(data) < singleStepUploadLimit {
		fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes < %dï¼Œå°è¯•ä½¿ç”¨å•æ­¥ä¸Šä¼ API", len(data), singleStepUploadLimit)

		result, err := f.singleStepUpload(ctx, data, parentFileID, fileName, md5Hash)
		if err != nil {
			fs.Debugf(f, "å•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿä¸Šä¼ æ–¹å¼: %v", err)
			// å¦‚æœå•æ­¥ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿçš„åˆ†ç‰‡ä¸Šä¼ æ–¹å¼
			// ä½¿ç”¨skipSingleStepæ ‡å¿—é˜²æ­¢æ— é™é€’å½’
			return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
		}

		fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸ")
		return result, nil
	}

	// å¤§æ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼
	fs.Debugf(f, "æ–‡ä»¶å¤§å° %d bytes >= %dï¼Œä½¿ç”¨ä¼ ç»Ÿä¸Šä¼ æ–¹å¼", len(data), singleStepUploadLimit)
	return f.putWithKnownMD5WithOptions(ctx, bytes.NewReader(data), src, parentFileID, fileName, md5Hash, true)
}

// streamingPutWithTempFile ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²å¤„ç†å¤§æ–‡ä»¶
func (f *Fs) streamingPutWithTempFile(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¯¹äºè¶…å¤§æ–‡ä»¶ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥
	fileSize := src.Size()
	if fileSize > singleStepUploadLimit { // å¤§äºå•æ­¥ä¸Šä¼ é™åˆ¶çš„æ–‡ä»¶ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“
		fs.Debugf(f, "è¶…å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“ç­–ç•¥ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰", fs.SizeSuffix(fileSize))
		return f.streamingPutWithChunkedResume(ctx, in, src, parentFileID, fileName)
	}

	// å¯¹äºè¾ƒå°çš„å¤§æ–‡ä»¶ï¼Œç»§ç»­ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	fs.Debugf(f, "å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å†²ç­–ç•¥", fs.SizeSuffix(fileSize))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - ä½¿ç”¨å®‰å…¨çš„èµ„æºç®¡ç†
	tempFile, err := os.CreateTemp("", "rclone-123pan-*")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		// å®‰å…¨çš„èµ„æºæ¸…ç†ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æ‰§è¡Œ
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// è¾¹è¯»è¾¹å†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆï¼Œå†™å…¥: %d å­—èŠ‚, MD5: %s", written, md5Hash)

	// é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å·²çŸ¥MD5ä¸Šä¼ 
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// streamingPutWithTwoPhase ä½¿ç”¨ä¸¤é˜¶æ®µæµå¼ä¼ è¾“å¤„ç†è¶…å¤§æ–‡ä»¶
// ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿè®¡ç®—MD5ï¼ˆåªè¯»å–ï¼Œä¸å­˜å‚¨ï¼‰
// ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨å·²çŸ¥MD5è¿›è¡ŒçœŸæ­£çš„æµå¼ä¸Šä¼ 
// è¿™ç§æ–¹æ³•èµ„æºå ç”¨æœ€å°‘ï¼Œæœ€é€‚åˆè¶…å¤§æ–‡ä»¶ä¼ è¾“
func (f *Fs) streamingPutWithTwoPhase(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "å¼€å§‹ä¸¤é˜¶æ®µæµå¼ä¼ è¾“ï¼šç¬¬ä¸€é˜¶æ®µè®¡ç®—MD5")

	// ä¼˜åŒ–çš„MD5è®¡ç®—ç­–ç•¥ï¼šé¿å…é‡å¤è¯»å–æºæ–‡ä»¶
	// é¦–å…ˆå°è¯•ä»æºå¯¹è±¡è·å–å·²çŸ¥çš„MD5å“ˆå¸Œ
	if srcObj, ok := src.(fs.Object); ok {
		// å°è¯•è·å–å·²çŸ¥çš„MD5å“ˆå¸Œï¼Œé¿å…é‡å¤è®¡ç®—
		if md5Hash, err := srcObj.Hash(ctx, fshash.MD5); err == nil && md5Hash != "" {
			fs.Debugf(f, "ä½¿ç”¨æºå¯¹è±¡å·²çŸ¥MD5: %s", md5Hash)
			return f.putWithKnownMD5(ctx, in, src, parentFileID, fileName, md5Hash)
		}

		// å¦‚æœæ²¡æœ‰å·²çŸ¥MD5ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
		fs.Debugf(f, "æºå¯¹è±¡æ— å·²çŸ¥MD5ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
		return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
	}

	// å¦‚æœæ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	fs.Debugf(f, "æ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
	return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
}

// streamingPutWithTempFileForced å¼ºåˆ¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šå…ˆå°è¯•æµå¼ä¼ è¾“ï¼Œå¤±è´¥æ—¶æ‰ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
func (f *Fs) streamingPutWithTempFileForced(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ä½¿ç”¨ä¼˜åŒ–çš„å›é€€ç­–ç•¥ï¼šå…ˆå°è¯•æµå¼ä¼ è¾“")

	// å°è¯•è·å–æºå¯¹è±¡è¿›è¡Œæµå¼ä¼ è¾“
	if srcObj, ok := src.(fs.Object); ok {
		// å°è¯•æµå¼ä¼ è¾“ä»¥é¿å…åŒå€æµé‡
		result, err := f.attemptStreamingUpload(ctx, srcObj, parentFileID, fileName)
		if err == nil {
			fs.Debugf(f, "æµå¼ä¼ è¾“æˆåŠŸï¼Œé¿å…äº†ä¸´æ—¶æ–‡ä»¶")
			return result, nil
		}
		fs.Debugf(f, "æµå¼ä¼ è¾“å¤±è´¥ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥: %v", err)
	}

	fs.Debugf(f, "ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç­–ç•¥ä½œä¸ºæœ€ç»ˆå›é€€æ–¹æ¡ˆ")

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ - ä½¿ç”¨å®‰å…¨çš„èµ„æºç®¡ç†
	tempFile, err := os.CreateTemp("", "rclone-123pan-fallback-*")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºå›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer func() {
		// å®‰å…¨çš„èµ„æºæ¸…ç†ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½èƒ½æ‰§è¡Œ
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(f, "å…³é—­å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", closeErr)
			}
			if removeErr := os.Remove(tempFile.Name()); removeErr != nil {
				fs.Debugf(f, "åˆ é™¤å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %v", removeErr)
			}
		}
	}()

	// è¾¹è¯»è¾¹å†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ŒåŒæ—¶è®¡ç®—MD5
	hasher := md5.New()
	multiWriter := io.MultiWriter(tempFile, hasher)

	written, err := io.Copy(multiWriter, in)
	if err != nil {
		return nil, fmt.Errorf("å†™å…¥å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	fs.Debugf(f, "å›é€€ä¸´æ—¶æ–‡ä»¶ç¼“å†²å®Œæˆï¼Œå†™å…¥: %d å­—èŠ‚, MD5: %s", written, md5Hash)

	// é‡æ–°å®šä½åˆ°æ–‡ä»¶å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡æ–°å®šä½å›é€€ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	// ä½¿ç”¨å·²çŸ¥MD5ä¸Šä¼ 
	return f.putWithKnownMD5(ctx, tempFile, src, parentFileID, fileName, md5Hash)
}

// streamingPutWithChunkedResume ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“å¤„ç†è¶…å¤§æ–‡ä»¶
// æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œèµ„æºå ç”¨æœ€å°‘ï¼ˆåªéœ€è¦å•ä¸ªåˆ†ç‰‡çš„ä¸´æ—¶å­˜å‚¨ï¼‰
// è¿™æ˜¯è¶…å¤§æ–‡ä»¶ä¼ è¾“çš„æœ€ä½³ç­–ç•¥ï¼šæ—¢èŠ‚çœèµ„æºåˆæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) streamingPutWithChunkedResume(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// ä¸ºå¤§æ–‡ä»¶æ·»åŠ ç”¨æˆ·å‹å¥½çš„åˆå§‹åŒ–æç¤º
	fileSize := src.Size()
	if fileSize > 1024*1024*1024 { // å¤§äº1GBçš„æ–‡ä»¶
		fs.Infof(f, "ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤§æ–‡ä»¶ä¸Šä¼  (%s) - å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°ï¼Œè¯·ç¨å€™...", fs.SizeSuffix(fileSize))
	}

	// å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°
	params, err := f.prepareChunkedUploadParams(ctx, src)
	if err != nil {
		return nil, err
	}

	f.debugf(LogLevelDebug, "å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“ï¼šæ–‡ä»¶å¤§å° %sï¼ŒåŠ¨æ€åˆ†ç‰‡å¤§å° %sï¼ˆç½‘ç»œé€Ÿåº¦: %s/sï¼‰",
		fs.SizeSuffix(params.fileSize), fs.SizeSuffix(params.chunkSize), fs.SizeSuffix(params.networkSpeed))

	// éªŒè¯æºå¯¹è±¡
	srcObj, ok := src.(fs.Object)
	if !ok {
		f.debugf(LogLevelDebug, "æ— æ³•è·å–æºå¯¹è±¡ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
	}

	// åˆ›å»ºä¸Šä¼ ä¼šè¯
	createResp, err := f.createChunkedUploadSession(ctx, parentFileID, fileName, params.fileSize)
	if err != nil {
		return nil, err
	}

	// å¦‚æœæ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥
	if createResp.Data.Reuse {
		f.debugf(LogLevelDebug, "æ„å¤–è§¦å‘ç§’ä¼ ï¼Œå›é€€åˆ°ä¸´æ—¶æ–‡ä»¶ç­–ç•¥")
		return f.streamingPutWithTempFileForced(ctx, in, src, parentFileID, fileName)
	}

	// å¼€å§‹åˆ†ç‰‡æµå¼ä¼ è¾“
	return f.uploadFileInChunksWithResume(ctx, srcObj, createResp, params.fileSize, params.chunkSize, fileName)
}

// ChunkedUploadParams åˆ†ç‰‡ä¸Šä¼ å‚æ•°
type ChunkedUploadParams struct {
	fileSize     int64
	chunkSize    int64
	networkSpeed int64
	totalChunks  int64
}

// prepareChunkedUploadParams å‡†å¤‡åˆ†ç‰‡ä¸Šä¼ å‚æ•°
func (f *Fs) prepareChunkedUploadParams(ctx context.Context, src fs.ObjectInfo) (*ChunkedUploadParams, error) {
	fileSize := src.Size()

	// åŠ¨æ€è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
	networkSpeed := f.detectNetworkSpeed(ctx)
	chunkSize := f.getOptimalChunkSize(fileSize, networkSpeed)

	// è®¡ç®—åˆ†ç‰‡æ•°é‡
	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	return &ChunkedUploadParams{
		fileSize:     fileSize,
		chunkSize:    chunkSize,
		networkSpeed: networkSpeed,
		totalChunks:  totalChunks,
	}, nil
}

// createChunkedUploadSession åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯
func (f *Fs) createChunkedUploadSession(ctx context.Context, parentFileID int64, fileName string, fileSize int64) (*UploadCreateResp, error) {
	// åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆä½¿ç”¨ä¸´æ—¶MD5ï¼Œåç»­ä¼šæ›´æ–°ï¼‰
	tempMD5 := fmt.Sprintf("%032x", time.Now().UnixNano())
	createResp, err := f.createUpload(ctx, parentFileID, fileName, tempMD5, fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	return createResp, nil
}

// uploadFileInChunksWithResume æ‰§è¡Œåˆ†ç‰‡æµå¼ä¸Šä¼ ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
func (f *Fs) uploadFileInChunksWithResume(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	// å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
	progress, err := f.prepareUploadProgress(preuploadID, totalChunks, chunkSize, fileSize)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§
	if err := f.validateUploadedChunks(ctx, progress); err != nil {
		f.debugf(LogLevelDebug, "åˆ†ç‰‡å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: %v", err)
		// ç»§ç»­æ‰§è¡Œï¼Œä½†å·²æŸåçš„åˆ†ç‰‡ä¼šè¢«é‡æ–°ä¸Šä¼ 
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)

	f.debugf(LogLevelDebug, "åŠ¨æ€ä¼˜åŒ–å‚æ•° - ç½‘ç»œé€Ÿåº¦: %s/s, æœ€ä¼˜å¹¶å‘æ•°: %d, å®é™…å¹¶å‘æ•°: %d",
		fs.SizeSuffix(concurrencyParams.networkSpeed), concurrencyParams.optimal, concurrencyParams.actual)

	// é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šå¤šçº¿ç¨‹æˆ–å•çº¿ç¨‹
	if totalChunks > 2 && concurrencyParams.actual > 1 {
		f.debugf(LogLevelDebug, "ä½¿ç”¨å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶å‘æ•°: %d", concurrencyParams.actual)
		return f.uploadChunksWithConcurrency(ctx, srcObj, createResp, progress, md5.New(), fileName, concurrencyParams.actual)
	}

	// å•çº¿ç¨‹ä¸Šä¼ 
	return f.uploadChunksSingleThreaded(ctx, srcObj, createResp, progress, fileSize, chunkSize, fileName)
}

// ConcurrencyParams å¹¶å‘å‚æ•°
type ConcurrencyParams struct {
	networkSpeed int64
	optimal      int
	actual       int
}

// prepareUploadProgress å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
func (f *Fs) prepareUploadProgress(preuploadID string, totalChunks, chunkSize, fileSize int64) (*UploadProgress, error) {
	progress, err := f.loadUploadProgress(preuploadID)
	if err != nil || progress == nil {
		f.debugf(LogLevelDebug, "åŠ è½½ä¸Šä¼ è¿›åº¦å¤±è´¥ï¼Œåˆ›å»ºæ–°è¿›åº¦: %v", err)
		progress = &UploadProgress{
			PreuploadID:   preuploadID,
			TotalParts:    totalChunks,
			ChunkSize:     chunkSize,
			FileSize:      fileSize,
			UploadedParts: make(map[int64]bool),
			CreatedAt:     time.Now(),
		}
	} else {
		f.debugf(LogLevelDebug, "æ¢å¤åˆ†ç‰‡ä¸Šä¼ è¿›åº¦: %d/%d ä¸ªåˆ†ç‰‡å·²å®Œæˆ",
			progress.GetUploadedCount(), totalChunks)
	}
	return progress, nil
}

// validateUploadedChunks éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§
func (f *Fs) validateUploadedChunks(ctx context.Context, progress *UploadProgress) error {
	if progress.GetUploadedCount() > 0 {
		f.debugf(LogLevelDebug, "éªŒè¯å·²ä¸Šä¼ åˆ†ç‰‡çš„å®Œæ•´æ€§")
		return f.resumeUploadWithIntegrityCheck(ctx, progress)
	}
	return nil
}

// calculateConcurrencyParams è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
func (f *Fs) calculateConcurrencyParams(fileSize int64) *ConcurrencyParams {
	networkSpeed := f.detectNetworkSpeed(context.Background())
	optimalConcurrency := f.getOptimalConcurrency(fileSize, networkSpeed)

	// æ£€æŸ¥ç”¨æˆ·è®¾ç½®çš„æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	maxConcurrency := optimalConcurrency
	if f.opt.MaxConcurrentUploads > 0 && f.opt.MaxConcurrentUploads < maxConcurrency {
		maxConcurrency = f.opt.MaxConcurrentUploads
	}

	return &ConcurrencyParams{
		networkSpeed: networkSpeed,
		optimal:      optimalConcurrency,
		actual:       maxConcurrency,
	}
}

// uploadChunksSingleThreaded å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) uploadChunksSingleThreaded(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, progress *UploadProgress, fileSize, chunkSize int64, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := (fileSize + chunkSize - 1) / chunkSize
	overallHasher := md5.New()

	f.debugf(LogLevelDebug, "ä½¿ç”¨å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")

	// é€ä¸ªåˆ†ç‰‡å¤„ç†ï¼ˆå•çº¿ç¨‹æ¨¡å¼ï¼‰
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1

		// æ£€æŸ¥æ­¤åˆ†ç‰‡æ˜¯å¦å·²ä¸Šä¼  - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
		if progress.IsUploaded(partNumber) {
			f.debugf(LogLevelVerbose, "è·³è¿‡å·²ä¸Šä¼ çš„åˆ†ç‰‡ %d/%d", partNumber, totalChunks)

			// ä¸ºäº†è®¡ç®—æ•´ä½“MD5ï¼Œéœ€è¦è¯»å–å¹¶ä¸¢å¼ƒè¿™éƒ¨åˆ†æ•°æ®
			if err := f.processSkippedChunkForMD5(ctx, srcObj, overallHasher, chunkIndex, chunkSize, fileSize, partNumber); err != nil {
				return nil, err
			}
			continue
		}

		// ä¸Šä¼ æ–°åˆ†ç‰‡
		if err := f.uploadSingleChunkWithStream(ctx, srcObj, overallHasher, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks); err != nil {
			// ä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
			f.saveUploadProgress(progress) // å¿½ç•¥ä¿å­˜é”™è¯¯
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		// æ ‡è®°åˆ†ç‰‡å·²å®Œæˆå¹¶ä¿å­˜è¿›åº¦
		progress.SetUploaded(partNumber)
		if err := f.saveUploadProgress(progress); err != nil {
			f.debugf(LogLevelDebug, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
		}

		f.debugf(LogLevelVerbose, "âœ… åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ å¹¶è¿”å›ç»“æœ
	return f.finalizeChunkedUpload(ctx, createResp, overallHasher, fileSize, fileName, preuploadID)
}

// processSkippedChunkForMD5 å¤„ç†è·³è¿‡çš„åˆ†ç‰‡ä»¥è®¡ç®—MD5
func (f *Fs) processSkippedChunkForMD5(ctx context.Context, srcObj fs.Object, hasher hash.Hash, chunkIndex, chunkSize, fileSize int64, partNumber int64) error {
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	if chunkEnd > fileSize {
		chunkEnd = fileSize
	}
	actualChunkSize := chunkEnd - chunkStart

	// è¯»å–åˆ†ç‰‡æ•°æ®ç”¨äºMD5è®¡ç®—
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	_, err = io.CopyN(hasher, chunkReader, actualChunkSize)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", partNumber, err)
	}

	return nil
}

// finalizeChunkedUpload å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¹¶è¿”å›ç»“æœ
func (f *Fs) finalizeChunkedUpload(ctx context.Context, createResp *UploadCreateResp, hasher hash.Hash, fileSize int64, fileName, preuploadID string) (*Object, error) {
	// è®¡ç®—æœ€ç»ˆMD5
	finalMD5 := fmt.Sprintf("%x", hasher.Sum(nil))

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆåˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	f.debugf(LogLevelDebug, "åˆ†ç‰‡æµå¼ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// æ¸…ç†è¿›åº¦æ–‡ä»¶
	f.removeUploadProgress(preuploadID)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(result.FileID, 10), // ä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
		size:        fileSize,
		md5sum:      finalMD5, // ä½¿ç”¨è®¡ç®—çš„MD5
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// attemptStreamingUpload å°è¯•æµå¼ä¸Šä¼ ä»¥é¿å…åŒå€æµé‡æ¶ˆè€—
// è¿™ä¸ªæ–¹æ³•å®ç°çœŸæ­£çš„è¾¹ä¸‹è¾¹ä¸Šä¼ è¾“ï¼Œæœ€å°åŒ–æµé‡ä½¿ç”¨
func (f *Fs) attemptStreamingUpload(ctx context.Context, srcObj fs.Object, parentFileID int64, fileName string) (*Object, error) {
	fs.Debugf(f, "å°è¯•æµå¼ä¸Šä¼ ï¼Œé¿å…ä¸´æ—¶æ–‡ä»¶å’ŒåŒå€æµé‡")

	// ä¼˜åŒ–çš„MD5è·å–ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å·²çŸ¥å“ˆå¸Œï¼Œé¿å…é‡å¤è®¡ç®—
	var md5Hash string
	if hash, err := srcObj.Hash(ctx, fshash.MD5); err == nil && hash != "" {
		md5Hash = hash
		fs.Debugf(f, "ä½¿ç”¨æºå¯¹è±¡å·²çŸ¥MD5: %s", md5Hash)
	} else {
		// å¦‚æœæ²¡æœ‰å·²çŸ¥MD5ï¼Œè®¡ç®—MD5ï¼ˆåªè¯»å–ä¸€æ¬¡ï¼‰
		md5Reader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•æ‰“å¼€æºæ–‡ä»¶è®¡ç®—MD5: %w", err)
		}
		defer md5Reader.Close()

		hasher := md5.New()
		_, err = io.Copy(hasher, md5Reader)
		if err != nil {
			return nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
		}

		md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
		fs.Debugf(f, "æµå¼ä¼ è¾“MD5è®¡ç®—å®Œæˆ: %s", md5Hash)
	}

	// ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å·²çŸ¥MD5åˆ›å»ºä¸Šä¼ ä¼šè¯
	createResp, err := f.createUpload(ctx, parentFileID, fileName, md5Hash, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºæµå¼ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
	}

	// å¦‚æœè§¦å‘ç§’ä¼ ï¼Œç›´æ¥è¿”å›æˆåŠŸ
	if createResp.Data.Reuse {
		fs.Debugf(f, "æµå¼ä¼ è¾“è§¦å‘ç§’ä¼ ï¼ŒMD5: %s", md5Hash)
		return &Object{
			fs:          f,
			remote:      fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        srcObj.Size(),
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// ç¬¬ä¸‰æ­¥ï¼šé‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œå®é™…ä¸Šä¼ 
	uploadReader, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("æ— æ³•é‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸Šä¼ : %w", err)
	}
	defer uploadReader.Close()

	// æ‰§è¡Œå®é™…ä¸Šä¼ 
	err = f.uploadFile(ctx, uploadReader, createResp, srcObj.Size())
	if err != nil {
		return nil, fmt.Errorf("æµå¼ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "æµå¼ä¸Šä¼ æˆåŠŸå®Œæˆï¼Œæ–‡ä»¶ID: %d", createResp.Data.FileID)

	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(createResp.Data.FileID, 10),
		size:        srcObj.Size(),
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// uploadSingleChunkWithStream ä½¿ç”¨æµå¼æ–¹å¼ä¸Šä¼ å•ä¸ªåˆ†ç‰‡
// è¿™æ˜¯åˆ†ç‰‡æµå¼ä¼ è¾“çš„æ ¸å¿ƒï¼šè¾¹ä¸‹è½½è¾¹ä¸Šä¼ ï¼Œåªéœ€è¦åˆ†ç‰‡å¤§å°çš„ä¸´æ—¶å­˜å‚¨
func (f *Fs) uploadSingleChunkWithStream(ctx context.Context, srcObj fs.Object, overallHasher io.Writer, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) error {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	if chunkEnd > fileSize {
		chunkEnd = fileSize
	}
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¼€å§‹æµå¼ä¸Šä¼ åˆ†ç‰‡ %d/%d (åç§»: %d, å¤§å°: %d)",
		partNumber, totalChunks, chunkStart, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡èŒƒå›´çš„æºæ–‡ä»¶æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æºæ–‡ä»¶æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºæ­¤åˆ†ç‰‡ï¼ˆåªéœ€è¦åˆ†ç‰‡å¤§å°çš„å­˜å‚¨ï¼‰
	tempFile, err := os.CreateTemp("", fmt.Sprintf("rclone-123pan-chunk-%d-*", partNumber))
	if err != nil {
		return fmt.Errorf("åˆ›å»ºåˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}
	defer func() {
		tempFile.Close()
		os.Remove(tempFile.Name())
	}()

	// åˆ›å»ºåˆ†ç‰‡MD5è®¡ç®—å™¨
	chunkHasher := md5.New()

	// è¾¹è¯»è¾¹å†™ï¼šåŒæ—¶å†™å…¥ä¸´æ—¶æ–‡ä»¶ã€åˆ†ç‰‡MD5å’Œæ•´ä½“MD5
	multiWriter := io.MultiWriter(tempFile, chunkHasher, overallHasher)

	// æµå¼ä¼ è¾“åˆ†ç‰‡æ•°æ®
	written, err := io.Copy(multiWriter, chunkReader)
	if err != nil {
		return fmt.Errorf("æµå¼ä¼ è¾“åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
	}

	if written != actualChunkSize {
		return fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, written)
	}

	// é‡æ–°å®šä½ä¸´æ—¶æ–‡ä»¶åˆ°å¼€å¤´
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return fmt.Errorf("é‡æ–°å®šä½åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// æ³¨æ„ï¼šæ–°çš„uploadPartWithMultipartæ–¹æ³•å†…éƒ¨ä¼šè·å–ä¸Šä¼ åŸŸåï¼Œè¿™é‡Œä¸å†éœ€è¦

	// è¯»å–ä¸´æ—¶æ–‡ä»¶æ•°æ®ç”¨äºä¸Šä¼ 
	chunkData, err := io.ReadAll(tempFile)
	if err != nil {
		return fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", partNumber, err)
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	chunkMD5 := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// ä½¿ç”¨æ­£ç¡®çš„multipartä¸Šä¼ æ–¹æ³•
	err = f.uploadPartWithMultipart(ctx, preuploadID, partNumber, chunkMD5, chunkData)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}
	fs.Debugf(f, "åˆ†ç‰‡ %d æµå¼ä¸Šä¼ æˆåŠŸï¼Œå¤§å°: %d, MD5: %s", partNumber, written, chunkMD5)

	return nil
}

// uploadChunksWithConcurrency ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ï¼Œé›†æˆæµå¼å“ˆå¸Œç´¯ç§¯å™¨
func (f *Fs) uploadChunksWithConcurrency(ctx context.Context, srcObj fs.Object, createResp *UploadCreateResp, progress *UploadProgress, _ io.Writer, fileName string, maxConcurrency int) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	// åˆ›å»ºæµå¼å“ˆå¸Œç´¯ç§¯å™¨ï¼Œæ¶ˆé™¤MD5é‡å¤è®¡ç®—
	hashAccumulator := NewStreamingHashAccumulator(fileSize, chunkSize)
	defer func() {
		// ç¡®ä¿èµ„æºæ¸…ç†
		hashAccumulator.Reset()
	}()

	fs.Debugf(f, "å¼€å§‹å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡ï¼Œæœ€å¤§å¹¶å‘æ•°ï¼š%d", totalChunks, maxConcurrency)

	// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæŸ¥æ‰¾Accountå¯¹è±¡è¿›è¡Œé›†æˆ
	var currentAccount *accounting.Account
	if stats := accounting.GlobalStats(); stats != nil {
		// å°è¯•ç²¾ç¡®åŒ¹é…
		currentAccount = stats.GetInProgressAccount(fileName)
		if currentAccount == nil {
			// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
			allAccounts := stats.ListInProgressAccounts()
			for _, accountName := range allAccounts {
				if strings.Contains(accountName, fileName) || strings.Contains(fileName, accountName) {
					currentAccount = stats.GetInProgressAccount(accountName)
					if currentAccount != nil {
						break
					}
				}
			}
		}
	}

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢goroutineé•¿æ—¶é—´è¿è¡Œ
	uploadTimeout := time.Duration(totalChunks) * 10 * time.Minute // æ¯ä¸ªåˆ†ç‰‡æœ€å¤š10åˆ†é’Ÿ
	if uploadTimeout > 2*time.Hour {
		uploadTimeout = 2 * time.Hour // æœ€å¤§2å°æ—¶è¶…æ—¶
	}
	workerCtx, cancelWorkers := context.WithTimeout(ctx, uploadTimeout)
	defer cancelWorkers() // ç¡®ä¿å‡½æ•°é€€å‡ºæ—¶å–æ¶ˆæ‰€æœ‰worker

	// åˆ›å»ºæœ‰ç•Œå·¥ä½œé˜Ÿåˆ—å’Œç»“æœé€šé“ï¼Œé˜²æ­¢å†…å­˜çˆ†ç‚¸
	// é˜Ÿåˆ—å¤§å°é™åˆ¶ä¸ºå¹¶å‘æ•°çš„2å€ï¼Œé¿å…è¿‡åº¦ç¼“å†²
	queueSize := maxConcurrency * 2
	if queueSize > int(totalChunks) {
		queueSize = int(totalChunks)
	}
	chunkJobs := make(chan int64, queueSize)
	results := make(chan chunkResult, maxConcurrency) // ç»“æœé€šé“åªéœ€è¦å¹¶å‘æ•°å¤§å°

	// å¯åŠ¨å·¥ä½œåç¨‹
	for i := 0; i < maxConcurrency; i++ {
		go f.chunkUploadWorker(workerCtx, srcObj, preuploadID, chunkSize, fileSize, totalChunks, chunkJobs, results)
	}

	// å‘é€éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ä»»åŠ¡ï¼ˆä½¿ç”¨goroutineé¿å…é˜»å¡ï¼‰
	pendingChunks := int64(0)
	fs.Debugf(f, "ğŸ“¤ å¼€å§‹åˆ†å‘åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡...")
	go func() {
		defer close(chunkJobs)
		tasksSent := 0
		for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
			partNumber := chunkIndex + 1
			if !progress.IsUploaded(partNumber) {
				select {
				case chunkJobs <- chunkIndex:
					tasksSent++
					if tasksSent%10 == 0 || tasksSent <= 5 {
						fs.Debugf(f, "ğŸ“‹ å·²åˆ†å‘ %d ä¸ªåˆ†ç‰‡ä»»åŠ¡ï¼Œå½“å‰åˆ†ç‰‡: %d/%d", tasksSent, partNumber, totalChunks)
					}
				case <-workerCtx.Done():
					fs.Debugf(f, "âš ï¸  ä¸Šä¸‹æ–‡å–æ¶ˆï¼Œåœæ­¢å‘é€åˆ†ç‰‡ä»»åŠ¡ (å·²å‘é€%dä¸ªä»»åŠ¡)", tasksSent)
					return
				}
			}
		}
		fs.Debugf(f, "âœ… åˆ†ç‰‡ä»»åŠ¡åˆ†å‘å®Œæˆï¼Œå…±å‘é€ %d ä¸ªä»»åŠ¡", tasksSent)
	}()

	// è®¡ç®—éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡æ•°é‡ - ä½¿ç”¨çº¿ç¨‹å®‰å…¨æ–¹æ³•
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1
		if !progress.IsUploaded(partNumber) {
			pendingChunks++
		}
	}

	fs.Debugf(f, "éœ€è¦ä¸Šä¼  %d ä¸ªåˆ†ç‰‡", pendingChunks)

	// å¦‚æœæ²¡æœ‰éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ï¼Œç›´æ¥è·³è¿‡æ”¶é›†ç»“æœ
	if pendingChunks == 0 {
		fs.Debugf(f, "æ‰€æœ‰åˆ†ç‰‡å·²å®Œæˆï¼Œè·³è¿‡ä¸Šä¼ é˜¶æ®µ")
	} else {
		// æ”¶é›†ç»“æœ
		completedChunks := int64(0)
		for completedChunks < pendingChunks {
			select {
			case result := <-results:
				if result.err != nil {
					fs.Errorf(f, "åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %v", result.chunkIndex+1, result.err)
					// ä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
					saveErr := f.saveUploadProgress(progress)
					if saveErr != nil {
						fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
					}
					// è®°å½•partialæ–‡ä»¶ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
					f.performanceMetrics.RecordPartialFile(true)
					return nil, fmt.Errorf("åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %w", result.chunkIndex+1, result.err)
				}

				// å°†åˆ†ç‰‡æ•°æ®å†™å…¥å“ˆå¸Œç´¯ç§¯å™¨
				if len(result.chunkData) > 0 {
					_, err := hashAccumulator.WriteChunk(result.chunkIndex, result.chunkData)
					if err != nil {
						fs.Debugf(f, "å“ˆå¸Œç´¯ç§¯å™¨å†™å…¥åˆ†ç‰‡ %d å¤±è´¥: %v", result.chunkIndex+1, err)
					} else {
						fs.Debugf(f, "åˆ†ç‰‡ %d å“ˆå¸Œç´¯ç§¯æˆåŠŸ: %s", result.chunkIndex+1, result.chunkHash)
					}
				}

				// æ ‡è®°åˆ†ç‰‡å®Œæˆï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰- ä½¿ç”¨å†…ç½®çš„çº¿ç¨‹å®‰å…¨æ–¹æ³•
				partNumber := result.chunkIndex + 1
				progress.SetUploaded(partNumber)
				completedChunks++

				// ä¿å­˜è¿›åº¦
				err := f.saveUploadProgress(progress)
				if err != nil {
					fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
				}

				fs.Debugf(f, "âœ… åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ (å¹¶å‘)", partNumber, totalChunks)

				// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯
				if currentAccount != nil {
					extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completedChunks, totalChunks)
					currentAccount.SetExtraInfo(extraInfo)
				}

			case <-workerCtx.Done():
				// ä¿å­˜å½“å‰è¿›åº¦
				saveErr := f.saveUploadProgress(progress)
				if saveErr != nil {
					fs.Debugf(f, "è¶…æ—¶æ—¶ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
				}
				// è®°å½•partialæ–‡ä»¶ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
				f.performanceMetrics.RecordPartialFile(true)

				if workerCtx.Err() == context.DeadlineExceeded {
					return nil, fmt.Errorf("ä¸Šä¼ è¶…æ—¶ï¼Œå·²å®Œæˆ %d/%d åˆ†ç‰‡", completedChunks, pendingChunks)
				}
				return nil, workerCtx.Err()
			case <-ctx.Done():
				// ä¿å­˜å½“å‰è¿›åº¦
				saveErr := f.saveUploadProgress(progress)
				if saveErr != nil {
					fs.Debugf(f, "å–æ¶ˆæ—¶ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
				}
				return nil, ctx.Err()
			}
		}
	}

	// ä½¿ç”¨å“ˆå¸Œç´¯ç§¯å™¨è·å–æœ€ç»ˆMD5ï¼Œæ— éœ€é‡æ–°è¯»å–æ–‡ä»¶
	fs.Debugf(f, "æ‰€æœ‰åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œä»å“ˆå¸Œç´¯ç§¯å™¨è·å–æœ€ç»ˆMD5")

	// æ£€æŸ¥å“ˆå¸Œç´¯ç§¯å™¨çš„å®Œæ•´æ€§
	processed, total, percentage := hashAccumulator.GetProgress()
	fs.Debugf(f, "å“ˆå¸Œç´¯ç§¯å™¨è¿›åº¦: %d/%d bytes (%.2f%%)", processed, total, percentage)

	// å®Œæˆå“ˆå¸Œè®¡ç®—
	finalMD5 := hashAccumulator.Finalize()

	// éªŒè¯åˆ†ç‰‡æ•°é‡
	chunkCount := hashAccumulator.GetChunkCount()
	if int64(chunkCount) != totalChunks {
		fs.Debugf(f, "è­¦å‘Šï¼šå“ˆå¸Œç´¯ç§¯å™¨åˆ†ç‰‡æ•°é‡ (%d) ä¸é¢„æœŸ (%d) ä¸åŒ¹é…", chunkCount, totalChunks)
		// å¦‚æœå“ˆå¸Œç´¯ç§¯å™¨ä¸å®Œæ•´ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
		fs.Debugf(f, "å›é€€åˆ°ä¼ ç»ŸMD5è®¡ç®—æ–¹æ³•")
		finalHasher := md5.New()
		for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
			chunkStart := chunkIndex * chunkSize
			chunkEnd := chunkStart + chunkSize
			if chunkEnd > fileSize {
				chunkEnd = fileSize
			}
			actualChunkSize := chunkEnd - chunkStart

			chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
			if err != nil {
				return nil, fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", chunkIndex+1, err)
			}

			// ä½¿ç”¨deferç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
			func() {
				defer chunkReader.Close()
				_, err = io.CopyN(finalHasher, chunkReader, actualChunkSize)
			}()

			if err != nil {
				return nil, fmt.Errorf("è¯»å–åˆ†ç‰‡ %d ç”¨äºMD5è®¡ç®—å¤±è´¥: %w", chunkIndex+1, err)
			}
		}
		finalMD5 = fmt.Sprintf("%x", finalHasher.Sum(nil))
	}

	// å®Œæˆä¸Šä¼ å¹¶è·å–ç»“æœï¼Œä¼ å…¥æ–‡ä»¶å¤§å°ä»¥ä¼˜åŒ–è½®è¯¢ç­–ç•¥
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆå¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å¤±è´¥: %w", err)
	}

	fs.Debugf(f, "å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼Œè®¡ç®—MD5: %s, æœåŠ¡å™¨MD5: %s", finalMD5, result.Etag)

	// æ¸…ç†è¿›åº¦æ–‡ä»¶
	f.removeUploadProgress(preuploadID)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// è¿”å›æˆåŠŸçš„æ–‡ä»¶å¯¹è±¡ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(result.FileID, 10), // ä½¿ç”¨æœåŠ¡å™¨è¿”å›çš„æ–‡ä»¶ID
		size:        fileSize,
		md5sum:      finalMD5, // ä½¿ç”¨è®¡ç®—çš„MD5
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// chunkResult åˆ†ç‰‡ä¸Šä¼ ç»“æœï¼ŒåŒ…å«å“ˆå¸Œä¿¡æ¯
type chunkResult struct {
	chunkIndex int64
	chunkHash  string // åˆ†ç‰‡MD5å“ˆå¸Œ
	chunkData  []byte // åˆ†ç‰‡æ•°æ®ï¼ˆç”¨äºå“ˆå¸Œç´¯ç§¯ï¼‰
	err        error
}

// chunkUploadWorker åˆ†ç‰‡ä¸Šä¼ å·¥ä½œåç¨‹ï¼Œæ”¯æŒå“ˆå¸Œç´¯ç§¯
func (f *Fs) chunkUploadWorker(ctx context.Context, srcObj fs.Object, preuploadID string, chunkSize, fileSize, totalChunks int64, jobs <-chan int64, results chan<- chunkResult) {
	for chunkIndex := range jobs {
		chunkHash, chunkData, err := f.uploadSingleChunkWithHash(ctx, srcObj, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks)
		results <- chunkResult{
			chunkIndex: chunkIndex,
			chunkHash:  chunkHash,
			chunkData:  chunkData,
			err:        err,
		}
	}
}

// uploadSingleChunkConcurrent å¹¶å‘ç‰ˆæœ¬çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼ˆä¸éœ€è¦MD5è®¡ç®—ï¼‰
func (f *Fs) uploadSingleChunkConcurrent(ctx context.Context, srcObj fs.Object, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) error {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	if chunkEnd > fileSize {
		chunkEnd = fileSize
	}
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ %d/%dï¼ŒèŒƒå›´: %d-%dï¼Œå¤§å°: %d", partNumber, totalChunks, chunkStart, chunkEnd-1, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡æ•°æ®æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æ•°æ®æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥ åˆ†ç‰‡ %d: %w", partNumber, err)
	}

	// æ„é€ åˆ†ç‰‡ä¸Šä¼ URL
	uploadURL := fmt.Sprintf("%s/upload/v2/file/slice", uploadDomain)

	// ä½¿ç”¨æµå¼ä¸Šä¼ é¿å…å¤§å†…å­˜å ç”¨
	// å¯¹äºå¤§åˆ†ç‰‡ï¼Œç›´æ¥æµå¼ä¼ è¾“è€Œä¸æ˜¯å…¨éƒ¨åŠ è½½åˆ°å†…å­˜
	err = f.uploadPartStream(ctx, uploadURL, chunkReader, actualChunkSize)
	if err != nil {
		return fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d/%d å¹¶å‘ä¸Šä¼ æˆåŠŸ", partNumber, totalChunks)
	return nil
}

// uploadSingleChunkWithHash å¹¶å‘ç‰ˆæœ¬çš„å•ä¸ªåˆ†ç‰‡ä¸Šä¼ ï¼Œè¿”å›å“ˆå¸Œå’Œæ•°æ®ç”¨äºç´¯ç§¯
func (f *Fs) uploadSingleChunkWithHash(ctx context.Context, srcObj fs.Object, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64) (string, []byte, error) {
	partNumber := chunkIndex + 1
	chunkStart := chunkIndex * chunkSize
	chunkEnd := chunkStart + chunkSize
	if chunkEnd > fileSize {
		chunkEnd = fileSize
	}
	actualChunkSize := chunkEnd - chunkStart

	fs.Debugf(f, "å¹¶å‘ä¸Šä¼ åˆ†ç‰‡ %d/%dï¼ˆå«å“ˆå¸Œï¼‰ï¼ŒèŒƒå›´: %d-%dï¼Œå¤§å°: %d", partNumber, totalChunks, chunkStart, chunkEnd-1, actualChunkSize)

	// æ‰“å¼€åˆ†ç‰‡æ•°æ®æµ
	chunkReader, err := srcObj.Open(ctx, &fs.RangeOption{Start: chunkStart, End: chunkEnd - 1})
	if err != nil {
		return "", nil, fmt.Errorf("æ‰“å¼€åˆ†ç‰‡ %d æ•°æ®æµå¤±è´¥: %w", partNumber, err)
	}
	defer chunkReader.Close()

	// è¯»å–åˆ†ç‰‡æ•°æ®åˆ°å†…å­˜ï¼ˆç”¨äºå“ˆå¸Œè®¡ç®—å’Œä¸Šä¼ ï¼‰
	chunkData, err := io.ReadAll(chunkReader)
	if err != nil {
		return "", nil, fmt.Errorf("è¯»å–åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	if int64(len(chunkData)) != actualChunkSize {
		return "", nil, fmt.Errorf("åˆ†ç‰‡ %d å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", partNumber, actualChunkSize, len(chunkData))
	}

	// è®¡ç®—åˆ†ç‰‡MD5å“ˆå¸Œ
	chunkHasher := md5.New()
	chunkHasher.Write(chunkData)
	chunkHash := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// è·å–ä¸Šä¼ åŸŸå
	uploadDomain, err := f.getUploadDomain(ctx)
	if err != nil {
		return "", nil, fmt.Errorf("è·å–ä¸Šä¼ åŸŸåå¤±è´¥ åˆ†ç‰‡ %d: %w", partNumber, err)
	}

	// æ„é€ åˆ†ç‰‡ä¸Šä¼ URL
	uploadURL := fmt.Sprintf("%s/upload/v2/file/slice", uploadDomain)

	// ä½¿ç”¨æµå¼ä¸Šä¼ é¿å…å¤§å†…å­˜å ç”¨
	err = f.uploadPartStream(ctx, uploadURL, bytes.NewReader(chunkData), actualChunkSize)
	if err != nil {
		return "", nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d æ•°æ®å¤±è´¥: %w", partNumber, err)
	}

	fs.Debugf(f, "åˆ†ç‰‡ %d/%d å¹¶å‘ä¸Šä¼ æˆåŠŸï¼ŒMD5: %s", partNumber, totalChunks, chunkHash)
	return chunkHash, chunkData, nil
}

// singleStepUpload ä½¿ç”¨123äº‘ç›˜çš„å•æ­¥ä¸Šä¼ APIä¸Šä¼ å°æ–‡ä»¶ï¼ˆ<1GBï¼‰
// è¿™ä¸ªAPIä¸“é—¨ä¸ºå°æ–‡ä»¶è®¾è®¡ï¼Œä¸€æ¬¡HTTPè¯·æ±‚å³å¯å®Œæˆä¸Šä¼ ï¼Œæ•ˆç‡æ›´é«˜
func (f *Fs) singleStepUpload(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string) (*Object, error) {
	startTime := time.Now()
	fs.Debugf(f, "ä½¿ç”¨å•æ­¥ä¸Šä¼ APIï¼Œæ–‡ä»¶: %s, å¤§å°: %d bytes, MD5: %s", fileName, len(data), md5Hash)

	// é¦–å…ˆéªŒè¯parentFileIDæ˜¯å¦å­˜åœ¨
	fs.Debugf(f, "singleStepUpload: éªŒè¯çˆ¶ç›®å½•ID %d", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		return nil, fmt.Errorf("éªŒè¯çˆ¶ç›®å½•ID %d å¤±è´¥: %w", parentFileID, err)
	}
	if !exists {
		return nil, fmt.Errorf("çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨", parentFileID)
	}

	// éªŒè¯æ–‡ä»¶å¤§å°é™åˆ¶
	if len(data) > singleStepUploadLimit {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°è¶…è¿‡å•æ­¥ä¸Šä¼ é™åˆ¶%d bytes: %d bytes", singleStepUploadLimit, len(data))
	}

	// ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶åéªŒè¯å’Œæ¸…ç†
	cleanedFileName, warning := f.validateAndCleanFileNameUnified(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// ä½¿ç”¨ä¸“é—¨çš„multipartä¸Šä¼ æ–¹æ³•
	var uploadResp struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Data    struct {
			FileID    int64 `json:"fileID"`
			Completed bool  `json:"completed"`
		} `json:"data"`
	}

	err = f.singleStepUploadMultipart(ctx, data, parentFileID, fileName, md5Hash, &uploadResp)
	if err != nil {
		return nil, err
	}

	// æ£€æŸ¥APIå“åº”ç 
	if uploadResp.Code != 0 {
		// æ£€æŸ¥æ˜¯å¦æ˜¯parentFileIDä¸å­˜åœ¨çš„é”™è¯¯
		if uploadResp.Code == 1 && strings.Contains(uploadResp.Message, "parentFileIDä¸å­˜åœ¨") {
			fs.Errorf(f, "âš ï¸  å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå¯èƒ½ç¼“å­˜è¿‡æœŸï¼Œéœ€è¦æ¸…ç†ç¼“å­˜é‡è¯•", parentFileID)
			// æ¸…ç†ç›¸å…³ç¼“å­˜
			f.clearParentIDCache(parentFileID)
			f.clearDirListCache(parentFileID)
			// é‡ç½®ç›®å½•ç¼“å­˜
			if f.dirCache != nil {
				fs.Debugf(f, "é‡ç½®ç›®å½•ç¼“å­˜ä»¥æ¸…ç†è¿‡æœŸçš„çˆ¶ç›®å½•ID: %d", parentFileID)
				f.dirCache.ResetRoot()
			}
			return nil, fmt.Errorf("çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•: å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
		}
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ APIé”™è¯¯: code=%d, message=%s", uploadResp.Code, uploadResp.Message)
	}

	// æ£€æŸ¥æ˜¯å¦ä¸Šä¼ å®Œæˆ
	if !uploadResp.Data.Completed {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ æœªå®Œæˆï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ")
	}

	duration := time.Since(startTime)
	speed := float64(len(data)) / duration.Seconds() / 1024 / 1024 // MB/s
	fs.Debugf(f, "å•æ­¥ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: %d, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s", uploadResp.Data.FileID, duration, speed)

	// è¿”å›Object
	return &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,
		id:          strconv.FormatInt(uploadResp.Data.FileID, 10),
		size:        int64(len(data)),
		md5sum:      md5Hash,
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// validateFileNameUnified ç»Ÿä¸€çš„æ–‡ä»¶åéªŒè¯å…¥å£ç‚¹
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨é€šç”¨å·¥å…·å‡½æ•°ï¼Œå‡å°‘ä»£ç é‡å¤
func (f *Fs) validateFileNameUnified(fileName string) error {
	return ValidationUtil.ValidateFileName(fileName)
}

// validateFileName ä¿æŒå‘åå…¼å®¹çš„éªŒè¯æ–¹æ³•
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨ç»Ÿä¸€éªŒè¯å‡½æ•°ï¼Œå‡å°‘å‡½æ•°è°ƒç”¨å±‚æ¬¡
func (f *Fs) validateFileName(fileName string) error {
	return ValidationUtil.ValidateFileName(fileName)
}

// validateFileNameWithWarnings éªŒè¯æ–‡ä»¶åå¹¶æä¾›è­¦å‘Šä¿¡æ¯
func (f *Fs) validateFileNameWithWarnings(fileName string) error {
	// é¦–å…ˆè¿›è¡Œæ ‡å‡†éªŒè¯
	if err := f.validateFileNameUnified(fileName); err != nil {
		return err
	}

	// æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥ç‚¹å¼€å¤´æˆ–ç»“å°¾ï¼ˆæŸäº›ç³»ç»Ÿä¸æ”¯æŒï¼‰
	if strings.HasPrefix(fileName, ".") || strings.HasSuffix(fileName, ".") {
		fs.Debugf(f, "è­¦å‘Šï¼šæ–‡ä»¶åä»¥ç‚¹å¼€å¤´æˆ–ç»“å°¾å¯èƒ½åœ¨æŸäº›ç³»ç»Ÿä¸­ä¸è¢«æ”¯æŒ: %s", fileName)
	}

	return nil
}

// validateAndCleanFileNameUnified ç»Ÿä¸€çš„éªŒè¯å’Œæ¸…ç†å…¥å£ç‚¹
// è¿™æ˜¯æ¨èçš„æ–‡ä»¶åå¤„ç†æ–¹æ³•ï¼Œç¡®ä¿æ‰€æœ‰ä¸Šä¼ è·¯å¾„çš„ä¸€è‡´æ€§
func (f *Fs) validateAndCleanFileNameUnified(fileName string) (cleanedName string, warning error) {
	// é¦–å…ˆå°è¯•éªŒè¯åŸå§‹æ–‡ä»¶å
	if err := f.validateFileNameUnified(fileName); err == nil {
		return fileName, nil
	}

	// å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ¸…ç†æ–‡ä»¶å
	cleanedName = cleanFileName(fileName)

	// éªŒè¯æ¸…ç†åçš„æ–‡ä»¶å
	if err := f.validateFileNameUnified(cleanedName); err != nil {
		// å¦‚æœæ¸…ç†åä»ç„¶æ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åç§°
		cleanedName = "cleaned_file"
		warning = fmt.Errorf("åŸå§‹æ–‡ä»¶å '%s' æ— æ³•æ¸…ç†ä¸ºæœ‰æ•ˆåç§°ï¼Œä½¿ç”¨é»˜è®¤åç§° '%s'", fileName, cleanedName)
	} else {
		warning = fmt.Errorf("æ–‡ä»¶å '%s' åŒ…å«æ— æ•ˆå­—ç¬¦ï¼Œå·²æ¸…ç†ä¸º '%s'", fileName, cleanedName)
	}

	return cleanedName, warning
}

// getUploadDomain è·å–ä¸Šä¼ åŸŸåï¼Œæ”¯æŒåŠ¨æ€è·å–å’Œç¼“å­˜
func (f *Fs) getUploadDomain(ctx context.Context) (string, error) {
	// ä¸´æ—¶è°ƒè¯•ï¼šå¼ºåˆ¶é‡æ–°è·å–ä¸Šä¼ åŸŸåï¼Œè·³è¿‡ç¼“å­˜
	f.debugf(LogLevelDebug, "ğŸ” DEBUG: å¼ºåˆ¶é‡æ–°è·å–ä¸Šä¼ åŸŸåï¼Œè·³è¿‡ç¼“å­˜")

	// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜å·²åˆ é™¤ï¼Œä¸éœ€è¦æ¸…é™¤ç¼“å­˜
	f.debugf(LogLevelDebug, "ğŸ” DEBUG: å¼ºåˆ¶é‡æ–°è·å–ä¸Šä¼ åŸŸå")

	// å°è¯•åŠ¨æ€è·å–ä¸Šä¼ åŸŸå - ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„
	var response struct {
		Code    int      `json:"code"`
		Data    []string `json:"data"` // ä¿®æ­£ï¼šdataæ˜¯å­—ç¬¦ä¸²æ•°ç»„
		Message string   `json:"message"`
		TraceID string   `json:"x-traceID"`
	}

	// ä½¿ç”¨æ­£ç¡®çš„APIè·¯å¾„è°ƒç”¨ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
	f.debugf(LogLevelDebug, "ğŸ” DEBUG: è°ƒç”¨ /upload/v2/file/domain è·å–ä¸Šä¼ åŸŸå")
	err := f.makeAPICallWithRest(ctx, "/upload/v2/file/domain", "GET", nil, &response)
	f.debugf(LogLevelDebug, "ğŸ” DEBUG: åŸŸåAPIå“åº”: code=%d, data=%v, err=%v", response.Code, response.Data, err)
	if err == nil && response.Code == 0 && len(response.Data) > 0 {
		domain := response.Data[0] // å–ç¬¬ä¸€ä¸ªåŸŸå
		f.debugf(LogLevelDebug, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåæˆåŠŸ: %s", domain)

		// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜å·²åˆ é™¤ï¼Œä¸å†ç¼“å­˜åŸŸå
		f.debugf(LogLevelDebug, "ä¸Šä¼ åŸŸåè·å–æˆåŠŸï¼Œä¸ä½¿ç”¨ç¼“å­˜: %s", domain)

		return domain, nil
	}

	// å¦‚æœåŠ¨æ€è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå
	f.debugf(LogLevelInfo, "åŠ¨æ€è·å–ä¸Šä¼ åŸŸåå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åŸŸå: %v", err)
	defaultDomains := []string{
		"https://openapi-upload.123242.com",
		"https://openapi-upload.123pan.com", // å¤‡ç”¨åŸŸå
	}

	// ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„åŸŸå
	for _, domain := range defaultDomains {
		// è¿™é‡Œå¯ä»¥æ·»åŠ ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
		f.debugf(LogLevelDebug, "ä½¿ç”¨é»˜è®¤ä¸Šä¼ åŸŸå: %s", domain)
		return domain, nil
	}

	return defaultDomains[0], nil
}

// deleteFile é€šè¿‡ç§»åŠ¨åˆ°å›æ”¶ç«™æ¥åˆ é™¤æ–‡ä»¶
func (f *Fs) deleteFile(ctx context.Context, fileID int64) error {
	fs.Debugf(f, "åˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)

	reqBody := map[string]any{
		"fileIDs": []int64{fileID},
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/trash", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("åˆ é™¤æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("åˆ é™¤å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸåˆ é™¤æ–‡ä»¶ï¼ŒID: %d", fileID)
	return nil
}

// moveFile å°†æ–‡ä»¶ç§»åŠ¨åˆ°ä¸åŒçš„ç›®å½•
func (f *Fs) moveFile(ctx context.Context, fileID, toParentFileID int64) error {
	fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)

	reqBody := map[string]any{
		"fileIDs":        []int64{fileID},
		"toParentFileID": toParentFileID,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	err := f.makeAPICallWithRest(ctx, "/api/v1/file/move", "POST", reqBody, &response)
	if err != nil {
		return fmt.Errorf("ç§»åŠ¨æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return fmt.Errorf("ç§»åŠ¨å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	fs.Debugf(f, "æˆåŠŸç§»åŠ¨æ–‡ä»¶%dåˆ°çˆ¶ç›®å½•%d", fileID, toParentFileID)
	return nil
}

// renameFile é‡å‘½åæ–‡ä»¶ - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒå»¶è¿Ÿå’Œé‡è¯•
func (f *Fs) renameFile(ctx context.Context, fileID int64, fileName string) error {
	fs.Debugf(f, "é‡å‘½åæ–‡ä»¶%dä¸º: %s", fileID, fileName)

	// éªŒè¯æ–°æ–‡ä»¶å
	if err := validateFileName(fileName); err != nil {
		return fmt.Errorf("é‡å‘½åæ–‡ä»¶åéªŒè¯å¤±è´¥: %w", err)
	}

	// æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œè®©123ç½‘ç›˜ç³»ç»Ÿæœ‰æ—¶é—´å¤„ç†æ–‡ä»¶
	time.Sleep(2 * time.Second)

	reqBody := map[string]any{
		"fileId":   fileID,
		"fileName": fileName,
	}

	var response struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
	}

	// å®ç°é‡è¯•æœºåˆ¶ï¼Œæœ€å¤šé‡è¯•3æ¬¡
	maxRetries := 3
	var err error
	for attempt := 1; attempt <= maxRetries; attempt++ {
		fs.Debugf(f, "é‡å‘½åæ–‡ä»¶å°è¯• %d/%d", attempt, maxRetries)

		err = f.makeAPICallWithRest(ctx, "/api/v1/file/name", "PUT", reqBody, &response)
		if err == nil {
			fs.Debugf(f, "é‡å‘½åæ–‡ä»¶æˆåŠŸ: %d -> %s", fileID, fileName)
			return nil
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯"æ–‡ä»¶æœªæ‰¾åˆ°"é”™è¯¯
		if strings.Contains(err.Error(), "æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶") || strings.Contains(err.Error(), "APIé”™è¯¯ 1") {
			if attempt < maxRetries {
				waitTime := time.Duration(attempt*2) * time.Second
				fs.Debugf(f, "æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œç­‰å¾…%våé‡è¯• (å°è¯• %d/%d)", waitTime, attempt, maxRetries)
				time.Sleep(waitTime)
				continue
			}
		}

		// å…¶ä»–é”™è¯¯æˆ–æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
		break
	}

	return fmt.Errorf("é‡å‘½åæ–‡ä»¶å¤±è´¥ (å°è¯•%dæ¬¡): %w", maxRetries, err)
}

// handlePartialFileConflict å¤„ç†partialæ–‡ä»¶é‡å‘½åå†²çª
// å½“partialæ–‡ä»¶å°è¯•é‡å‘½åä¸ºæœ€ç»ˆæ–‡ä»¶åæ—¶å‘ç”Ÿå†²çªæ—¶è°ƒç”¨
func (f *Fs) handlePartialFileConflict(ctx context.Context, fileID int64, partialName string, parentID int64, targetName string) (*Object, error) {
	fs.Debugf(f, "å¤„ç†partialæ–‡ä»¶å†²çª: %s -> %s (çˆ¶ç›®å½•: %d)", partialName, targetName, parentID)

	// ç§»é™¤.partialåç¼€è·å–åŸå§‹æ–‡ä»¶å
	originalName := strings.TrimSuffix(partialName, ".partial")
	if targetName == "" {
		targetName = originalName
	}

	// æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
	exists, existingFileID, err := f.fileExistsInDirectory(ctx, parentID, targetName)
	if err != nil {
		fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å­˜åœ¨æ€§å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if exists {
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼ŒID: %d", existingFileID)

		// å¦‚æœå­˜åœ¨çš„æ–‡ä»¶å°±æ˜¯å½“å‰æ–‡ä»¶ï¼ˆå¯èƒ½å·²ç»é‡å‘½åæˆåŠŸï¼‰
		if existingFileID == fileID {
			fs.Debugf(f, "æ–‡ä»¶å·²æˆåŠŸé‡å‘½åï¼Œè¿”å›æˆåŠŸå¯¹è±¡")
			return &Object{
				fs:          f,
				remote:      targetName,
				hasMetaData: true,
				id:          strconv.FormatInt(fileID, 10),
				size:        -1, // éœ€è¦é‡æ–°è·å–
				modTime:     time.Now(),
				isDir:       false,
			}, nil
		}

		// å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°
		fs.Debugf(f, "ç›®æ ‡æ–‡ä»¶åå†²çªï¼Œç”Ÿæˆå”¯ä¸€åç§°")
		uniqueName, err := f.generateUniqueFileName(ctx, parentID, targetName)
		if err != nil {
			fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
			return nil, fmt.Errorf("ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %w", err)
		}

		fs.Logf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", targetName, uniqueName)
		targetName = uniqueName
	}

	// å°è¯•é‡å‘½åpartialæ–‡ä»¶ä¸ºç›®æ ‡åç§°
	fs.Debugf(f, "é‡å‘½åpartialæ–‡ä»¶: %d -> %s", fileID, targetName)
	err = f.renameFile(ctx, fileID, targetName)
	if err != nil {
		// å¦‚æœé‡å‘½åä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²ç»æœ‰æ­£ç¡®åç§°
		if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
			strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
			fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œæ£€æŸ¥æ–‡ä»¶å½“å‰çŠ¶æ€")

			// å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰æ­£ç¡®åç§°
			exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, parentID, targetName)
			if checkErr == nil && exists && existingFileID == fileID {
				fs.Debugf(f, "æ–‡ä»¶å®é™…å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæˆåŠŸ")
				return &Object{
					fs:          f,
					remote:      targetName,
					hasMetaData: true,
					id:          strconv.FormatInt(fileID, 10),
					size:        -1, // éœ€è¦é‡æ–°è·å–
					modTime:     time.Now(),
					isDir:       false,
				}, nil
			}
		}

		return nil, fmt.Errorf("partialæ–‡ä»¶é‡å‘½åå¤±è´¥: %w", err)
	}

	fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åæˆåŠŸ: %s", targetName)
	return &Object{
		fs:          f,
		remote:      targetName,
		hasMetaData: true,
		id:          strconv.FormatInt(fileID, 10),
		size:        -1, // éœ€è¦é‡æ–°è·å–
		modTime:     time.Now(),
		isDir:       false,
	}, nil
}

// getUserInfo è·å–ç”¨æˆ·ä¿¡æ¯åŒ…æ‹¬å­˜å‚¨é…é¢ - å·²è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
func (f *Fs) getUserInfo(ctx context.Context) (*UserInfoResp, error) {
	var response UserInfoResp

	// ä½¿ç”¨rcloneæ ‡å‡†restå®¢æˆ·ç«¯æ–¹æ³•ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶å’Œé‡è¯•æœºåˆ¶
	err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
	if err != nil {
		return nil, err
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	return &response, nil
}

// GetUserInfoWithRest è·å–ç”¨æˆ·ä¿¡æ¯åŒ…æ‹¬å­˜å‚¨é…é¢ - ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•çš„ç¤ºä¾‹
// è¿™å±•ç¤ºäº†å¦‚ä½•å°†ç°æœ‰APIè°ƒç”¨è¿ç§»åˆ°rcloneæ ‡å‡†æ–¹æ³•
func (f *Fs) GetUserInfoWithRest(ctx context.Context) (*UserInfoResp, error) {
	var response UserInfoResp

	// ä½¿ç”¨æ–°çš„restå®¢æˆ·ç«¯æ–¹æ³•ï¼Œè‡ªåŠ¨é›†æˆQPSé™åˆ¶å’Œé‡è¯•æœºåˆ¶
	err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
	if err != nil {
		return nil, err
	}

	if response.Code != 0 {
		return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	return &response, nil
}

// createDirectoryWithRest åˆ›å»ºç›®å½• - ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•çš„POSTè¯·æ±‚ç¤ºä¾‹
// å±•ç¤ºå¦‚ä½•å¤„ç†å¸¦è¯·æ±‚ä½“çš„APIè°ƒç”¨
func (f *Fs) createDirectoryWithRest(ctx context.Context, parentFileID int64, name string) (string, error) {
	// å‡†å¤‡è¯·æ±‚ä½“
	reqBody := map[string]any{
		"parentID": strconv.FormatInt(parentFileID, 10),
		"name":     name,
	}

	// ä½¿ç”¨ä¸ç°æœ‰æ–¹æ³•ç›¸åŒçš„å“åº”ç»“æ„
	var response struct {
		Code    int         `json:"code"`
		Message string      `json:"message"`
		Data    interface{} `json:"data"`
	}

	// ä½¿ç”¨restå®¢æˆ·ç«¯è¿›è¡ŒPOSTè¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç†QPSé™åˆ¶
	err := f.makeAPICallWithRest(ctx, "/upload/v1/file/mkdir", "POST", reqBody, &response)
	if err != nil {
		return "", err
	}

	if response.Code != 0 {
		return "", fmt.Errorf("åˆ›å»ºç›®å½•å¤±è´¥: APIé”™è¯¯ %d: %s", response.Code, response.Message)
	}

	// è¿™é‡Œåº”è¯¥ä»response.Dataä¸­æå–ç›®å½•IDï¼Œç®€åŒ–ç¤ºä¾‹è¿”å›ç©ºå­—ç¬¦ä¸²
	return "", nil
}

/*
=== rcloneæ ‡å‡†HTTPè¯·æ±‚æ–¹æ³•ä½¿ç”¨æŒ‡å— ===

å½“å‰123ç½‘ç›˜backendå·²æ”¯æŒrcloneæ ‡å‡†çš„HTTPè¯·æ±‚æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **è‡ªåŠ¨QPSé™åˆ¶**: é€šè¿‡paceræœºåˆ¶è‡ªåŠ¨æ§åˆ¶è¯·æ±‚é¢‘ç‡
2. **ç»Ÿä¸€é”™è¯¯å¤„ç†**: ä½¿ç”¨rcloneæ ‡å‡†çš„é‡è¯•å’Œé”™è¯¯å¤„ç†é€»è¾‘
3. **æ›´å¥½çš„æ—¥å¿—è®°å½•**: é›†æˆrcloneçš„æ—¥å¿—ç³»ç»Ÿ
4. **æ¡†æ¶ä¸€è‡´æ€§**: ç¬¦åˆrclone backendå¼€å‘æ ‡å‡†

## ä½¿ç”¨æ–¹æ³•ï¼š

### 1. GETè¯·æ±‚ç¤ºä¾‹ï¼ˆæ— è¯·æ±‚ä½“ï¼‰ï¼š
```go
var response SomeResponse
err := f.makeAPICallWithRest(ctx, "/api/v1/user/info", "GET", nil, &response)
```

### 2. POSTè¯·æ±‚ç¤ºä¾‹ï¼ˆå¸¦è¯·æ±‚ä½“ï¼‰ï¼š
```go
reqBody := map[string]any{
    "param1": "value1",
    "param2": "value2",
}
var response SomeResponse
err := f.makeAPICallWithRest(ctx, "/api/v1/some/endpoint", "POST", reqBody, &response)
```

## è¿ç§»å»ºè®®ï¼š

1. **ä¿æŒå‘åå…¼å®¹**: ç°æœ‰çš„apiCallæ–¹æ³•ç»§ç»­å·¥ä½œ
2. **é€æ­¥è¿ç§»**: æ–°åŠŸèƒ½ä¼˜å…ˆä½¿ç”¨makeAPICallWithRest
3. **æµ‹è¯•éªŒè¯**: è¿ç§»åå……åˆ†æµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£å¸¸

## QPSæ§åˆ¶ï¼š

makeAPICallWithRestä¼šè‡ªåŠ¨æ ¹æ®APIç«¯ç‚¹é€‰æ‹©åˆé€‚çš„pacerï¼š
- /api/v2/file/list: ~14 QPS
- /upload/v2/file/*: 4-16 QPSï¼ˆæ ¹æ®å…·ä½“ç«¯ç‚¹ï¼‰
- /api/v1/file/*: 1-10 QPSï¼ˆæ ¹æ®å…·ä½“ç«¯ç‚¹ï¼‰

æ‰€æœ‰QPSé™åˆ¶åŸºäº123ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£è®¾å®šã€‚
*/

// newFs ä»è·¯å¾„æ„é€ Fsï¼Œæ ¼å¼ä¸º container:path
func newFs(ctx context.Context, name, root string, m configmap.Mapper) (fs.Fs, error) {
	// è§£æé€‰é¡¹
	opt := new(Options)
	err := configstruct.Set(m, opt)
	if err != nil {
		return nil, err
	}

	// éªŒè¯é…ç½®é€‰é¡¹
	if err := validateOptions(opt); err != nil {
		return nil, fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	// æ ‡å‡†åŒ–å’Œè®¾ç½®é»˜è®¤å€¼
	normalizeOptions(opt)

	// Store the original name before any modifications for config operations
	originalName := name
	if idx := strings.IndexRune(name, '{'); idx > 0 {
		originalName = name[:idx]
	}

	// è§„èŒƒåŒ–rootè·¯å¾„ï¼Œå¤„ç†ç»å¯¹è·¯å¾„æ ¼å¼
	normalizedRoot := normalizePath(root)
	fs.Debugf(nil, "NewFsè·¯å¾„è§„èŒƒåŒ–: %q -> %q", root, normalizedRoot)

	// åˆ›å»ºHTTPå®¢æˆ·ç«¯
	client := fshttp.NewClient(ctx)

	f := &Fs{
		name:         name,
		originalName: originalName,
		root:         normalizedRoot,
		opt:          *opt,
		m:            m,
		rootFolderID: opt.RootFolderID,
		client:       client,                                         // ä¿ç•™ç”¨äºç‰¹æ®Šæƒ…å†µ
		rst:          rest.NewClient(client).SetRoot(openAPIRootURL), // rcloneæ ‡å‡†restå®¢æˆ·ç«¯
	}

	// Initialize multiple pacers for different API rate limits based on official documentation
	// ä½¿ç”¨pacerå·¥å‚å‡½æ•°å‡å°‘é‡å¤ä»£ç 

	// v2 API pacer - é«˜é¢‘ç‡ (~14 QPS for api/v2/file/list)
	listPacerSleep := listV2APIMinSleep
	if opt.ListPacerMinSleep > 0 {
		listPacerSleep = time.Duration(opt.ListPacerMinSleep)
	}
	f.listPacer = createPacer(ctx, listPacerSleep, maxSleep, decayConstant)

	// ä¸¥æ ¼API pacer - ä¸­ç­‰é¢‘ç‡ (~4 QPS for create, async_result, etc.)
	strictPacerSleep := fileMoveMinSleep
	if opt.StrictPacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.StrictPacerMinSleep)
	} else if opt.PacerMinSleep > 0 {
		strictPacerSleep = time.Duration(opt.PacerMinSleep) // å‘åå…¼å®¹
	}
	f.strictPacer = createPacer(ctx, strictPacerSleep, maxSleep, decayConstant)

	// v2åˆ†ç‰‡ä¸Šä¼ API pacer (~5 QPS for upload/v2/file/slice)
	uploadPacerSleep := uploadV2SliceMinSleep
	if opt.UploadPacerMinSleep > 0 {
		uploadPacerSleep = time.Duration(opt.UploadPacerMinSleep)
	}
	f.uploadPacer = createPacer(ctx, uploadPacerSleep, maxSleep, decayConstant)

	// ä¸‹è½½å’Œé«˜é¢‘ç‡API pacer (~16 QPS for v2 upload APIs)
	downloadPacerSleep := downloadInfoMinSleep
	if opt.DownloadPacerMinSleep > 0 {
		downloadPacerSleep = time.Duration(opt.DownloadPacerMinSleep)
	}
	f.downloadPacer = createPacer(ctx, downloadPacerSleep, maxSleep, decayConstant)

	fs.Infof(f, "ğŸ“Š QPSé™åˆ¶é…ç½® - åˆ—è¡¨API: %.1f QPS, ä¸¥æ ¼API: %.1f QPS, ä¸Šä¼ API: %.1f QPS, ä¸‹è½½API: %.1f QPS",
		1000.0/float64(listPacerSleep.Milliseconds()),
		1000.0/float64(strictPacerSleep.Milliseconds()),
		1000.0/float64(uploadPacerSleep.Milliseconds()),
		1000.0/float64(downloadPacerSleep.Milliseconds()))

	// åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–ç»„ä»¶ - ä½¿ç”¨ç»Ÿä¸€çš„å¹¶å‘æ§åˆ¶ç®¡ç†å™¨
	f.concurrencyManager = NewConcurrencyManager(opt.MaxConcurrentUploads, opt.MaxConcurrentDownloads)

	// åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ - ä¼˜åŒ–å†…å­˜é™åˆ¶
	// æ€»å†…å­˜é™åˆ¶150MBï¼Œå•æ–‡ä»¶é™åˆ¶30MBï¼Œå‡å°‘å†…å­˜å‹åŠ›
	f.memoryManager = NewMemoryManager(150*1024*1024, 30*1024*1024)

	// åˆå§‹åŒ–æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
	f.performanceMetrics = NewPerformanceMetrics()

	// åˆå§‹åŒ–å¢å¼ºæ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
	f.enhancedMetrics = NewEnhancedPerformanceMetrics()

	// åˆå§‹åŒ–èµ„æºæ± ç®¡ç†å™¨
	f.resourcePool = NewResourcePool()

	// å¯åŠ¨æ€§èƒ½ç›‘æ§å®šæ—¶å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
	if opt.EnablePerformanceLog {
		f.startPerformanceMonitoring(ctx)
	}

	// åˆå§‹åŒ–ç¼“å­˜é…ç½®
	f.cacheConfig = DefaultCacheConfig()

	// åˆå§‹åŒ–BadgerDBæŒä¹…åŒ–ç¼“å­˜
	cacheDir := cache.GetCacheDir("123drive")

	// åˆå§‹åŒ–å¤šä¸ªç¼“å­˜å®ä¾‹
	caches := map[string]**cache.BadgerCache{
		"parent_ids": &f.parentIDCache,
		"dir_list":   &f.dirListCache,
		"path_to_id": &f.pathToIDCache,
	}

	for name, cachePtr := range caches {
		cache, cacheErr := cache.NewBadgerCache(name, cacheDir)
		if cacheErr != nil {
			fs.Errorf(f, "åˆå§‹åŒ–%sç¼“å­˜å¤±è´¥: %v", name, cacheErr)
			// ç¼“å­˜åˆå§‹åŒ–å¤±è´¥ä¸åº”è¯¥é˜»æ­¢æ–‡ä»¶ç³»ç»Ÿå·¥ä½œï¼Œç»§ç»­æ‰§è¡Œ
		} else {
			*cachePtr = cache
			fs.Debugf(f, "%sç¼“å­˜åˆå§‹åŒ–æˆåŠŸ", name)
		}
	}

	fs.Debugf(f, "BadgerDBç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ: %s", cacheDir)

	// ğŸ—‘ï¸ ä¸‹è½½URLç¼“å­˜ç®¡ç†å™¨å·²åˆ é™¤

	// åˆå§‹åŒ–APIç‰ˆæœ¬ç®¡ç†å™¨
	f.apiVersionManager = NewAPIVersionManager()
	fs.Debugf(f, "APIç‰ˆæœ¬ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

	// åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨
	if resumeManager, err := NewResumeManager(f, cacheDir); err != nil {
		fs.Errorf(f, "åˆå§‹åŒ–æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨å¤±è´¥: %v", err)
		// æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ä¸åº”è¯¥é˜»æ­¢æ–‡ä»¶ç³»ç»Ÿå·¥ä½œï¼Œç»§ç»­æ‰§è¡Œ
	} else {
		f.resumeManager = resumeManager
		fs.Debugf(f, "æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
	}

	// åˆå§‹åŒ–åŠ¨æ€å‚æ•°è°ƒæ•´å™¨
	f.dynamicAdjuster = NewDynamicParameterAdjuster()
	fs.Debugf(f, "åŠ¨æ€å‚æ•°è°ƒæ•´å™¨åˆå§‹åŒ–æˆåŠŸ")

	// ğŸ›¡ï¸ æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„èµ„æºæ¸…ç†é’©å­
	// ç¡®ä¿å³ä½¿ç¨‹åºå¼‚å¸¸é€€å‡ºä¹Ÿèƒ½æ¸…ç†ä¸´æ—¶æ–‡ä»¶
	if f.resourcePool != nil {
		atexit.Register(func() {
			fs.Debugf(f, "ğŸš¨ ç¨‹åºé€€å‡ºä¿¡å·æ£€æµ‹åˆ°ï¼Œå¼€å§‹æ¸…ç†123ç½‘ç›˜èµ„æº...")

			// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
			f.resourcePool.CleanupAllTempFiles()

			// å…³é—­æ‰€æœ‰ç¼“å­˜
			caches := []*cache.BadgerCache{
				f.parentIDCache,
				f.dirListCache,
				f.pathToIDCache,
			}

			for i, c := range caches {
				if c != nil {
					if err := c.Close(); err != nil {
						fs.Debugf(f, "ç¨‹åºé€€å‡ºæ—¶å…³é—­BadgerDBç¼“å­˜%då¤±è´¥: %v", i, err)
					}
				}
			}

			fs.Debugf(f, "âœ… 123ç½‘ç›˜èµ„æºæ¸…ç†å®Œæˆ")
		})
		fs.Debugf(f, "ğŸ›¡ï¸ ç¨‹åºé€€å‡ºæ¸…ç†é’©å­æ³¨å†ŒæˆåŠŸ")
	}

	fs.Infof(f, "123ç½‘ç›˜æ€§èƒ½ä¼˜åŒ–åˆå§‹åŒ–å®Œæˆ - æœ€å¤§å¹¶å‘ä¸Šä¼ : %d, æœ€å¤§å¹¶å‘ä¸‹è½½: %d, è¿›åº¦æ˜¾ç¤º: %v, åŠ¨æ€è°ƒæ•´: å¯ç”¨",
		opt.MaxConcurrentUploads, opt.MaxConcurrentDownloads, opt.EnableProgressDisplay)

	f.features = (&fs.Features{
		ReadMimeType:            true,
		CanHaveEmptyDirectories: true,
		BucketBased:             false,
		SetTier:                 false,
		GetTier:                 false,
		DuplicateFiles:          false, // 123Pan handles duplicates
		ReadMetadata:            true,
		WriteMetadata:           false, // Not supported
		UserMetadata:            false, // Not supported
		PartialUploads:          true,  // Supports multipart uploads
		NoMultiThreading:        false, // Supports concurrent operations
		SlowModTime:             true,  // ModTime operations are slow
		SlowHash:                false, // Hash is available from API
	}).Fill(ctx, f)

	// Initialize directory cache
	// Note: We use the configured rootFolderID as the base, and let dircache
	// resolve the actual root path during FindRoot()
	// Use the normalized root path to ensure consistency
	f.dirCache = dircache.New(normalizedRoot, f.rootFolderID, f)

	// Check if we have saved token in config file
	tokenLoaded := loadTokenFromConfig(f, m)
	fs.Debugf(f, "ä»é…ç½®åŠ è½½ä»¤ç‰Œ: %vï¼ˆè¿‡æœŸæ—¶é—´ %vï¼‰", tokenLoaded, f.tokenExpiry)

	var tokenRefreshNeeded bool
	if tokenLoaded {
		// Check if token is expired or will expire soon
		if time.Now().After(f.tokenExpiry.Add(-tokenRefreshWindow)) {
			fs.Debugf(f, "ä»¤ç‰Œå·²è¿‡æœŸæˆ–å³å°†è¿‡æœŸï¼Œç«‹å³åˆ·æ–°")
			tokenRefreshNeeded = true
		}
	} else {
		// No token, so login
		fs.Debugf(f, "æœªæ‰¾åˆ°ä»¤ç‰Œï¼Œå°è¯•ç™»å½•")
		err = f.GetAccessToken(ctx)
		if err != nil {
			return nil, fmt.Errorf("initial login failed: %w", err)
		}
		// Save token to config after successful login
		f.saveToken(ctx, m)
		fs.Debugf(f, "ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Try to refresh the token if needed
	if tokenRefreshNeeded {
		err = f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°å¤±è´¥ï¼Œå°è¯•å®Œæ•´ç™»å½•: %v", err)
			// If refresh fails, try full login
			err = f.GetAccessToken(ctx)
			if err != nil {
				return nil, fmt.Errorf("login failed after token refresh failure: %w", err)
			}
		}
		// Save the refreshed/new token
		f.saveToken(ctx, m)
		fs.Debugf(f, "ä»¤ç‰Œåˆ·æ–°/ç™»å½•æˆåŠŸï¼Œä»¤ç‰Œå·²ä¿å­˜")
	}

	// Setup token renewer for automatic refresh
	fs.Debugf(f, "è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨")
	f.setupTokenRenewer(ctx, m)

	// Find the root directory
	err = f.dirCache.FindRoot(ctx, false)
	if err != nil {
		// Assume it is a file
		newRoot, remote := dircache.SplitPath(root)
		// åˆ›å»ºæ–°çš„Fså®ä¾‹ï¼Œé¿å…å¤åˆ¶é”å€¼
		tempF := &Fs{
			name:               f.name,
			root:               newRoot,
			opt:                f.opt,
			m:                  f.m,
			rootFolderID:       f.rootFolderID,
			client:             f.client,
			listPacer:          f.listPacer,
			strictPacer:        f.strictPacer,
			uploadPacer:        f.uploadPacer,
			downloadPacer:      f.downloadPacer,
			parentIDCache:      f.parentIDCache,
			dirListCache:       f.dirListCache,
			pathToIDCache:      f.pathToIDCache,
			concurrencyManager: f.concurrencyManager,
			memoryManager:      f.memoryManager,
			performanceMetrics: f.performanceMetrics,
			resourcePool:       f.resourcePool,
			apiVersionManager:  f.apiVersionManager,
			resumeManager:      f.resumeManager,
			cacheConfig:        f.cacheConfig,
		}
		tempF.dirCache = dircache.New(newRoot, f.rootFolderID, tempF)
		// Make new Fs which is the parent
		err = tempF.dirCache.FindRoot(ctx, false)
		if err != nil {
			// No root so return old f
			return f, nil
		}
		_, err := tempF.NewObject(ctx, remote)
		if err != nil {
			// unable to list folder so return old f
			return f, nil
		}
		// return an error with an fs which points to the parent
		return tempF, fs.ErrorIsFile
	}

	return f, nil
}

// NewObject finds the Object at remote.
func (f *Fs) NewObject(ctx context.Context, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨NewObject: %s", remote)

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(remote)
	fs.Debugf(f, "è§„èŒƒåŒ–åçš„è¿œç¨‹è·¯å¾„: %s -> %s", remote, normalizedRemote)

	// Get the full path
	var fullPath string
	if f.root == "" {
		fullPath = normalizedRemote
	} else if normalizedRemote == "" {
		// å½“remoteä¸ºç©ºæ—¶ï¼ŒfullPathå°±æ˜¯rootæœ¬èº«
		fullPath = f.root
	} else {
		fullPath = normalizePath(f.root + "/" + normalizedRemote)
	}

	if fullPath == "" {
		fullPath = "/"
	}

	// Get file ID
	fileID, err := f.pathToFileID(ctx, fullPath)
	if err != nil {
		if err == fs.ErrorObjectNotFound {
			return nil, fs.ErrorObjectNotFound
		}
		return nil, err
	}

	fs.Debugf(f, "ğŸ” NewObject pathToFileIDç»“æœ: fullPath=%s -> fileID=%s", fullPath, fileID)

	// Get file details
	fileInfo, err := f.getFileInfo(ctx, fileID)
	if err != nil {
		return nil, err
	}

	// Check if it's a file (not directory)
	// Type: 0-æ–‡ä»¶  1-æ–‡ä»¶å¤¹
	if fileInfo.Type == 1 {
		return nil, fs.ErrorNotAFile
	}

	// ç¡®å®šæ­£ç¡®çš„remoteè·¯å¾„
	objectRemote := remote
	if objectRemote == "" && f.root != "" {
		// å½“remoteä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶æ—¶ï¼Œä½¿ç”¨æ–‡ä»¶åä½œä¸ºremote
		objectRemote = fileInfo.Filename
		fs.Debugf(f, "ğŸ” NewObjectè®¾ç½®remote: %s -> %s", remote, objectRemote)
	}

	o := &Object{
		fs:          f,
		remote:      objectRemote,
		hasMetaData: true,
		id:          fileID,
		size:        fileInfo.Size,
		md5sum:      fileInfo.Etag,
		modTime:     time.Now(), // We'll parse this properly later
		isDir:       false,
	}

	return o, nil
}

// Precision of the ModTimes in this Fs
func (f *Fs) Precision() time.Duration {
	return fs.ModTimeNotSupported
}

// Put uploads the object.
func (f *Fs) Put(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨Put: %s", src.Remote())

	// è®°å½•ä¸Šä¼ å¼€å§‹
	startTime := time.Now()
	f.performanceMetrics.RecordUploadStart(src.Size())

	// å¹¶å‘æ§åˆ¶ï¼šè·å–ä¸Šä¼ ä¿¡å·é‡
	if err := f.concurrencyManager.AcquireUpload(ctx); err != nil {
		return nil, fmt.Errorf("è·å–ä¸Šä¼ ä¿¡å·é‡å¤±è´¥: %w", err)
	}
	// ä½¿ç”¨å‘½åè¿”å›å€¼ç¡®ä¿åœ¨panicæ—¶ä¹Ÿèƒ½é‡Šæ”¾ä¿¡å·é‡
	defer f.concurrencyManager.ReleaseUpload()

	// è§„èŒƒåŒ–è¿œç¨‹è·¯å¾„
	normalizedRemote := normalizePath(src.Remote())
	fs.Debugf(f, "Putæ“ä½œè§„èŒƒåŒ–è·¯å¾„: %s -> %s", src.Remote(), normalizedRemote)

	// Use dircache to find parent directory
	fs.Debugf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•è·¯å¾„: %s", normalizedRemote)
	leaf, parentID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		fs.Errorf(f, "æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %v", err)
		return nil, err
	}
	fileName := leaf
	fs.Debugf(f, "æ‰¾åˆ°çˆ¶ç›®å½•ID: %s, æ–‡ä»¶å: %s", parentID, fileName)

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(fileName)
	if warning != nil {
		fs.Logf(f, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		fileName = cleanedFileName
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return nil, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯parentFileIDæ˜¯å¦çœŸçš„å­˜åœ¨
	fs.Debugf(f, "éªŒè¯çˆ¶ç›®å½•ID %d æ˜¯å¦å­˜åœ¨", parentFileID)
	exists, err := f.verifyParentFileID(ctx, parentFileID)
	if err != nil {
		fs.Errorf(f, "éªŒè¯çˆ¶ç›®å½•IDå¤±è´¥: %v", err)
		// å°è¯•æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		fs.Debugf(f, "æ¸…ç†ç›®å½•ç¼“å­˜å¹¶é‡è¯•")
		f.dirCache.ResetRoot()
		leaf, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else if !exists {
		fs.Errorf(f, "çˆ¶ç›®å½•ID %d ä¸å­˜åœ¨ï¼Œå°è¯•é‡å»ºç›®å½•ç¼“å­˜", parentFileID)
		// æ¸…ç†ç¼“å­˜å¹¶é‡æ–°æŸ¥æ‰¾
		f.dirCache.ResetRoot()
		leaf, parentID, err = f.dirCache.FindPath(ctx, normalizedRemote, true)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºç›®å½•ç¼“å­˜åæŸ¥æ‰¾å¤±è´¥: %w", err)
		}
		parentFileID, err = parseParentID(parentID)
		if err != nil {
			return nil, fmt.Errorf("é‡å»ºåè§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
		}
		fs.Debugf(f, "é‡å»ºåæ‰¾åˆ°çˆ¶ç›®å½•ID: %d", parentFileID)
	} else {
		fs.Debugf(f, "çˆ¶ç›®å½•ID %d éªŒè¯æˆåŠŸ", parentFileID)
	}

	// Use streaming optimization for cross-cloud transfers
	obj, err := f.streamingPut(ctx, in, src, parentFileID, fileName)

	// è®°å½•ä¸Šä¼ å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(startTime)
	if err != nil {
		f.performanceMetrics.RecordUploadError()
	} else {
		f.performanceMetrics.RecordUploadComplete(src.Size(), duration)
		// ä¸Šä¼ æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		f.invalidateRelatedCaches(normalizedRemote, "upload")
	}

	return obj, err
}

// Fs returns the parent Fs
func (o *Object) Fs() fs.Info {
	return o.fs
}

// String returns a description of the Object
func (o *Object) String() string {
	if o == nil {
		return "<nil>"
	}
	return o.remote
}

// Remote returns the remote path
func (o *Object) Remote() string {
	return o.remote
}

// ModTime returns the modification time
func (o *Object) ModTime(ctx context.Context) time.Time {
	// In a real implementation, you would fetch metadata if not already available.
	return o.modTime
}

// Size returns the size of the file
func (o *Object) Size() int64 {
	// In a real implementation, you would fetch metadata if not already available.
	return o.size
}

// Hash returns the MD5 checksum
func (o *Object) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t != fshash.MD5 {
		return "", fshash.ErrUnsupported
	}
	// In a real implementation, you would fetch metadata if not already available.
	return o.md5sum, nil
}

// ID returns the ID of the Object
func (o *Object) ID() string {
	// In a real implementation, you would fetch metadata if not already available.
	return o.id
}

// Storable indicates this object can be stored
func (o *Object) Storable() bool {
	return true
}

// Open the file for reading.
func (o *Object) Open(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Debugf(o.fs, "ğŸ” DEBUG: 123ç½‘ç›˜Openæ–¹æ³•è¢«è°ƒç”¨: %s, å¤§å°: %s", o.remote, fs.SizeSuffix(o.size))

	// ğŸš€ è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šæ£€æµ‹å¤§æ–‡ä»¶å¹¶å¯ç”¨å¤šçº¿ç¨‹ä¸‹è½½ï¼ˆå‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
	if o.shouldUseConcurrentDownload(ctx, options) {

		return o.openWithConcurrency(ctx, options...)
	}

	fs.Debugf(o.fs, "ğŸ” DEBUG: 123ç½‘ç›˜ä½¿ç”¨æ™®é€šOpenæ–¹æ³•: %s", o.remote)

	// è®°å½•ä¸‹è½½å¼€å§‹
	o.fs.performanceMetrics.RecordDownloadStart(o.size)

	// å¹¶å‘æ§åˆ¶ï¼šè·å–ä¸‹è½½ä¿¡å·é‡
	if err := o.fs.concurrencyManager.AcquireDownload(ctx); err != nil {
		return nil, err
	}
	// ä¿¡å·é‡å°†åœ¨ReadCloserå…³é—­æ—¶é‡Šæ”¾

	var resp *http.Response
	var err error

	// Use pacer for download with retry (use strict pacer for download URL requests)
	err = o.fs.strictPacer.Call(func() (bool, error) {
		// Get download URL (may need refresh)
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, nil, err)
		}

		// Create HTTP request
		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, fmt.Errorf("failed to create download request: %w", err)
		}

		// Handle range requests
		var start, end int64 = 0, -1
		for _, option := range options {
			switch x := option.(type) {
			case *fs.RangeOption:
				start, end = x.Start, x.End
			case *fs.SeekOption:
				start = x.Offset
			}
		}

		if start > 0 || end >= 0 {
			rangeHeader := fmt.Sprintf("bytes=%d-", start)
			if end >= 0 {
				rangeHeader = fmt.Sprintf("bytes=%d-%d", start, end)
			}
			req.Header.Set("Range", rangeHeader)
			fs.Debugf(o.fs, "è¯·æ±‚èŒƒå›´: %s", rangeHeader)
		}

		// Set user agent
		req.Header.Set("User-Agent", o.fs.opt.UserAgent)
		// Note: Don't set timeout here as it will be applied to the entire download

		// Make the request
		resp, err = o.fs.client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, err)
		}

		// Check status code
		switch resp.StatusCode {
		case http.StatusOK, http.StatusPartialContent:
			return false, nil // Success
		case http.StatusFound, http.StatusMovedPermanently, http.StatusTemporaryRedirect, http.StatusPermanentRedirect:
			// Let the HTTP client handle redirects automatically
			// This should not happen if the client is configured correctly
			resp.Body.Close()
			return false, fmt.Errorf("unexpected redirect response %d - client should handle this automatically", resp.StatusCode)
		case http.StatusRequestedRangeNotSatisfiable:
			resp.Body.Close()
			return false, fmt.Errorf("range not satisfiable")
		case http.StatusNotFound:
			resp.Body.Close()
			return false, fs.ErrorObjectNotFound
		default:
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("download failed with status %d: %s", resp.StatusCode, string(body)))
		}
	})

	if err != nil {
		// é‡Šæ”¾ä¸‹è½½ä¿¡å·é‡
		o.fs.concurrencyManager.ReleaseDownload()
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// shouldUseConcurrentDownload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼ˆå‚è€ƒ115ç½‘ç›˜å®ç°ï¼‰
func (o *Object) shouldUseConcurrentDownload(ctx context.Context, options []fs.OpenOption) bool {
	// æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œåªå¯¹å¤§æ–‡ä»¶ä½¿ç”¨å¹¶å‘ä¸‹è½½

	if o.size < 1*1024*1024 { // é™ä½åˆ°1MBé˜ˆå€¼ï¼Œå‡ ä¹æ‰€æœ‰æ–‡ä»¶éƒ½è§¦å‘

		return false
	}

	// æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·æŒ‡å®šçš„Rangeé€‰é¡¹
	// æ™ºèƒ½Rangeæ£€æµ‹ï¼šåŒºåˆ†å°Rangeè¯·æ±‚å’Œå¤§Rangeè¯·æ±‚ï¼ˆè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
	for _, option := range options {
		if rangeOpt, ok := option.(*fs.RangeOption); ok {
			// å¦‚æœRangeè¦†ç›–äº†æ•´ä¸ªæ–‡ä»¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨å¹¶å‘ä¸‹è½½
			if rangeOpt.Start == 0 && (rangeOpt.End == -1 || rangeOpt.End >= o.size-1) {
				fs.Debugf(o, "æ£€æµ‹åˆ°å…¨æ–‡ä»¶Rangeé€‰é¡¹ï¼Œå…è®¸å¹¶å‘ä¸‹è½½")
				break
			} else {
				// è®¡ç®—Rangeå¤§å°
				rangeSize := rangeOpt.End - rangeOpt.Start + 1

				// ğŸš€ è·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼šå¦‚æœRangeè¶³å¤Ÿå¤§ï¼ˆ>=32MBï¼‰ï¼Œå¯ç”¨å¹¶å‘ä¸‹è½½
				// è¿™ä¸»è¦é’ˆå¯¹rcloneå¤šçº¿ç¨‹ä¼ è¾“åœºæ™¯ï¼Œæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„æ•°æ®å—é€šå¸¸è¾ƒå¤§
				if rangeSize >= 32*1024*1024 { // 32MBé˜ˆå€¼
					fs.Infof(o, "ğŸš€ æ£€æµ‹åˆ°å¤§Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œå¯ç”¨å¹¶å‘ä¸‹è½½ä¼˜åŒ–ï¼ˆè·¨äº‘ä¼ è¾“åœºæ™¯ï¼‰",
						rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
					break
				} else {
					fs.Debugf(o, "æ£€æµ‹åˆ°å°Rangeé€‰é¡¹ (%d-%d, %s)ï¼Œè·³è¿‡å¹¶å‘ä¸‹è½½",
						rangeOpt.Start, rangeOpt.End, fs.SizeSuffix(rangeSize))
					return false
				}
			}
		}
	}

	return true
}

// openWithConcurrency ä½¿ç”¨å¹¶å‘ä¸‹è½½æ‰“å¼€æ–‡ä»¶ï¼ˆç”¨äºè·¨äº‘ä¼ è¾“ä¼˜åŒ–ï¼‰
func (o *Object) openWithConcurrency(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	fs.Infof(o, "ğŸš€ 123ç½‘ç›˜å¯åŠ¨å¹¶å‘ä¸‹è½½: %s", fs.SizeSuffix(o.size))

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºå¹¶å‘ä¸‹è½½
	tempFile, err := os.CreateTemp("", "123_concurrent_download_*.tmp")
	if err != nil {
		fs.Debugf(o, "åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// ä½¿ç”¨å¹¶å‘ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
	downloadedSize, err := o.fs.downloadWithConcurrency(ctx, o, tempFile, o.size, o.remote)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "å¹¶å‘ä¸‹è½½å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: %v", err)
		return o.openNormal(ctx, options...)
	}

	// éªŒè¯ä¸‹è½½å¤§å°
	if downloadedSize != o.size {
		tempFile.Close()
		os.Remove(tempFile.Name())
		fs.Debugf(o, "ä¸‹è½½å¤§å°ä¸åŒ¹é…ï¼Œå›é€€åˆ°æ™®é€šä¸‹è½½: æœŸæœ›%dï¼Œå®é™…%d", o.size, downloadedSize)
		return o.openNormal(ctx, options...)
	}

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		tempFile.Close()
		os.Remove(tempFile.Name())
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(o, "âœ… 123ç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s", fs.SizeSuffix(downloadedSize))

	// è¿”å›ä¸€ä¸ªåŒ…è£…çš„ReadCloserï¼Œåœ¨å…³é—­æ—¶åˆ é™¤ä¸´æ—¶æ–‡ä»¶
	return &ConcurrentDownloadReader{
		file:     tempFile,
		tempPath: tempFile.Name(),
	}, nil
}

// openNormal æ™®é€šçš„æ‰“å¼€æ–¹æ³•ï¼ˆåŸæ¥çš„é€»è¾‘ï¼‰
func (o *Object) openNormal(ctx context.Context, options ...fs.OpenOption) (io.ReadCloser, error) {
	// è®°å½•ä¸‹è½½å¼€å§‹
	o.fs.performanceMetrics.RecordDownloadStart(o.size)

	// å¹¶å‘æ§åˆ¶ï¼šè·å–ä¸‹è½½ä¿¡å·é‡
	if err := o.fs.concurrencyManager.AcquireDownload(ctx); err != nil {
		return nil, err
	}
	// ä¿¡å·é‡å°†åœ¨ReadCloserå…³é—­æ—¶é‡Šæ”¾

	var resp *http.Response
	var err error

	// ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–ä¸‹è½½é“¾æ¥å¹¶ä¸‹è½½
	err = o.fs.downloadPacer.Call(func() (bool, error) {
		downloadURL, err := o.fs.getDownloadURL(ctx, o.id)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: %w", err))
		}

		req, err := http.NewRequestWithContext(ctx, "GET", downloadURL, nil)
		if err != nil {
			return false, err
		}

		// å¤„ç†Rangeé€‰é¡¹
		fs.FixRangeOption(options, o.size)
		fs.OpenOptionAddHTTPHeaders(req.Header, options)

		resp, err = o.fs.client.Do(req)
		if err != nil {
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½è¯·æ±‚å¤±è´¥: %w", err))
		}

		if resp.StatusCode < 200 || resp.StatusCode >= 300 {
			body, _ := io.ReadAll(resp.Body)
			resp.Body.Close()
			return shouldRetry(ctx, resp, fmt.Errorf("ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : %d, å“åº”: %s", resp.StatusCode, string(body)))
		}

		return false, nil
	})

	if err != nil {
		// é‡Šæ”¾ä¸‹è½½ä¿¡å·é‡
		o.fs.concurrencyManager.ReleaseDownload()
		return nil, err
	}

	// åˆ›å»ºåŒ…è£…çš„ReadCloseræ¥å¤„ç†è¿›åº¦æ›´æ–°å’Œèµ„æºæ¸…ç†
	return &ProgressReadCloser{
		ReadCloser:      resp.Body,
		fs:              o.fs,
		totalSize:       o.size,
		transferredSize: 0,
		startTime:       time.Now(),
	}, nil
}

// Update the object with new content.
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿï¼Œæ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œå¤šçº¿ç¨‹ä¸Šä¼ 
func (o *Object) Update(ctx context.Context, in io.Reader, src fs.ObjectInfo, options ...fs.OpenOption) error {
	fs.Debugf(o.fs, "è°ƒç”¨Update: %s", o.remote)

	// è®°å½•ä¸Šä¼ å¼€å§‹
	startTime := time.Now()
	o.fs.performanceMetrics.RecordUploadStart(src.Size())

	// å¹¶å‘æ§åˆ¶ï¼šè·å–ä¸Šä¼ ä¿¡å·é‡
	if err := o.fs.concurrencyManager.AcquireUpload(ctx); err != nil {
		return fmt.Errorf("è·å–ä¸Šä¼ ä¿¡å·é‡å¤±è´¥: %w", err)
	}
	defer o.fs.concurrencyManager.ReleaseUpload()

	// Use dircache to find parent directory
	leaf, parentID, err := o.fs.dirCache.FindPath(ctx, o.remote, false)
	if err != nil {
		return err
	}

	// Convert parentID to int64
	parentFileID, err := parseParentID(parentID)
	if err != nil {
		return fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// éªŒè¯å¹¶æ¸…ç†æ–‡ä»¶å
	cleanedFileName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(o.fs, "æ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedFileName
	}

	// ä½¿ç”¨æ–°çš„ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡Œæ›´æ–°
	fs.Debugf(o.fs, "ä½¿ç”¨ç»Ÿä¸€ä¸Šä¼ ç³»ç»Ÿè¿›è¡ŒUpdateæ“ä½œ")
	newObj, err := o.fs.streamingPut(ctx, in, src, parentFileID, leaf)

	// è®°å½•ä¸Šä¼ å®Œæˆæˆ–é”™è¯¯
	duration := time.Since(startTime)
	if err != nil {
		o.fs.performanceMetrics.RecordUploadError()
		fs.Errorf(o.fs, "Updateæ“ä½œå¤±è´¥: %v", err)
		return err
	}

	// æ›´æ–°å½“å‰å¯¹è±¡çš„å…ƒæ•°æ®
	o.size = newObj.size
	o.md5sum = newObj.md5sum
	o.modTime = newObj.modTime
	o.id = newObj.id

	// è®°å½•ä¸Šä¼ æˆåŠŸ
	o.fs.performanceMetrics.RecordUploadComplete(src.Size(), duration)
	fs.Debugf(o.fs, "Updateæ“ä½œæˆåŠŸï¼Œè€—æ—¶: %v", duration)

	return nil
}

// Remove the object.
func (o *Object) Remove(ctx context.Context) error {
	fs.Debugf(o.fs, "è°ƒç”¨Remove: %s", o.remote)

	// æ£€æŸ¥æ–‡ä»¶IDæ˜¯å¦æœ‰æ•ˆ
	if o.id == "" {
		fs.Debugf(o.fs, "æ–‡ä»¶IDä¸ºç©ºï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶: %s", o.remote)
		// å¯¹äºpartialæ–‡ä»¶æˆ–ä¸Šä¼ å¤±è´¥çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬æ— æ³•åˆ é™¤ï¼Œä½†ä¹Ÿä¸åº”è¯¥æŠ¥é”™
		// å› ä¸ºè¿™äº›æ–‡ä»¶å¯èƒ½æ ¹æœ¬ä¸å­˜åœ¨äºæœåŠ¡å™¨ä¸Š
		return nil
	}

	// Convert file ID to int64
	fileID, err := parseFileID(o.id)
	if err != nil {
		fs.Debugf(o.fs, "è§£ææ–‡ä»¶IDå¤±è´¥ï¼Œå¯èƒ½æ˜¯partialæ–‡ä»¶: %s, é”™è¯¯: %v", o.remote, err)
		// å¯¹äºæ— æ³•è§£æçš„æ–‡ä»¶IDï¼Œæˆ‘ä»¬ä¹Ÿä¸æŠ¥é”™ï¼Œå› ä¸ºè¿™é€šå¸¸æ˜¯partialæ–‡ä»¶
		return nil
	}

	err = o.fs.deleteFile(ctx, fileID)
	if err == nil {
		// åˆ é™¤æ–‡ä»¶æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		o.fs.invalidateRelatedCaches(o.remote, "remove")
	}
	return err
}

// SetModTime is not supported
func (o *Object) SetModTime(ctx context.Context, modTime time.Time) error {
	return fs.ErrorCantSetModTime
}

// DirCacher interface implementation for dircache

// FindLeaf finds a directory or file leaf in the parent folder pathID.
// Used by dircache.
func (f *Fs) FindLeaf(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the parent directory
	response, err := f.ListFile(ctx, int(parentFileID), f.opt.ListChunk, "", "", 0)
	if err != nil {
		return "", false, err
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true
			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}
			return foundID, found, nil
		}
	}

	return "", false, nil
}

// findLeafWithForceRefresh å¼ºåˆ¶åˆ·æ–°ç¼“å­˜åæŸ¥æ‰¾å¶èŠ‚ç‚¹
// ç”¨äºè§£å†³ç¼“å­˜ä¸ä¸€è‡´å¯¼è‡´çš„ç›®å½•æŸ¥æ‰¾å¤±è´¥é—®é¢˜
func (f *Fs) findLeafWithForceRefresh(ctx context.Context, pathID, leaf string) (foundID string, found bool, err error) {
	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æŸ¥æ‰¾å¶èŠ‚ç‚¹: pathID=%s, leaf=%s", pathID, leaf)

	// Convert pathID to int64
	parentFileID, err := parseParentID(pathID)
	if err != nil {
		return "", false, fmt.Errorf("è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", err)
	}

	// å¼ºåˆ¶æ¸…ç†ç¼“å­˜
	f.clearDirListCache(parentFileID)

	// ç›´æ¥è°ƒç”¨APIè·å–æœ€æ–°çš„ç›®å½•åˆ—è¡¨ï¼Œè·³è¿‡ç¼“å­˜
	params := url.Values{}
	params.Add("parentFileId", fmt.Sprintf("%d", parentFileID))
	params.Add("limit", fmt.Sprintf("%d", f.opt.ListChunk))

	endpoint := "/api/v2/file/list?" + params.Encode()

	var response ListResponse
	err = f.makeAPICallWithRest(ctx, endpoint, "GET", nil, &response)
	if err != nil {
		return "", false, fmt.Errorf("å¼ºåˆ¶åˆ·æ–°APIè°ƒç”¨å¤±è´¥: %w", err)
	}

	if response.Code != 0 {
		return "", false, fmt.Errorf("API error %d: %s", response.Code, response.Message)
	}

	// Search for the leaf in fresh data
	for _, file := range response.Data.FileList {
		if file.Filename == leaf {
			foundID = strconv.FormatInt(file.FileID, 10)
			found = true

			// æ›´æ–°ç¼“å­˜ä»¥ä¿æŒä¸€è‡´æ€§
			f.saveDirListToCache(parentFileID, 0, response.Data.FileList, response.Data.LastFileId)

			// Cache the found item's path/ID mapping
			parentPath, ok := f.dirCache.GetInv(pathID)
			if ok {
				var itemPath string
				if parentPath == "" {
					itemPath = leaf
				} else {
					itemPath = parentPath + "/" + leaf
				}
				f.dirCache.Put(itemPath, foundID)
			}

			fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æ‰¾åˆ°å¶èŠ‚ç‚¹: %s -> %s", leaf, foundID)
			return foundID, found, nil
		}
	}

	fs.Debugf(f, "å¼ºåˆ¶åˆ·æ–°æœªæ‰¾åˆ°å¶èŠ‚ç‚¹: %s", leaf)
	return "", false, nil
}

// CreateDir creates a directory with the given name in the parent folder pathID.
// Used by dircache.
func (f *Fs) CreateDir(ctx context.Context, pathID, leaf string) (newID string, err error) {
	fs.Debugf(f, "åˆ›å»ºç›®å½•: pathID=%s, leaf=%s", pathID, leaf)

	// éªŒè¯å‚æ•°
	if leaf == "" {
		return "", fmt.Errorf("ç›®å½•åä¸èƒ½ä¸ºç©º")
	}
	if pathID == "" {
		return "", fmt.Errorf("çˆ¶ç›®å½•IDä¸èƒ½ä¸ºç©º")
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®å½•å
	cleanedDirName, warning := validateAndCleanFileName(leaf)
	if warning != nil {
		fs.Logf(f, "ç›®å½•åéªŒè¯è­¦å‘Š: %v", warning)
		leaf = cleanedDirName
	}

	// Create the directory using the existing createDirectory method
	dirID, err := f.createDirectory(ctx, pathID, leaf)
	if err != nil {
		return "", err
	}

	return dirID, nil
}

// Check the interfaces are satisfied
var (
	_ fs.Fs          = (*Fs)(nil)
	_ fs.Object      = (*Object)(nil)
	_ fs.Mover       = (*Fs)(nil)
	_ fs.DirMover    = (*Fs)(nil)
	_ fs.Copier      = (*Fs)(nil)
	_ fs.Abouter     = (*Fs)(nil)
	_ fs.Purger      = (*Fs)(nil)
	_ fs.Shutdowner  = (*Fs)(nil)
	_ fs.ObjectInfo  = (*Object)(nil)
	_ fs.IDer        = (*Object)(nil)
	_ fs.SetModTimer = (*Object)(nil)
)

// loadTokenFromConfig attempts to load and parse tokens from the config file
func loadTokenFromConfig(f *Fs, _ configmap.Mapper) bool {
	if f.opt.Token == "" {
		fs.Debugf(f, "é…ç½®ä¸­æœªæ‰¾åˆ°ä»¤ç‰Œï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰")
		return false
	}

	var tokenData TokenJSON                                                // Use the new TokenJSON struct
	var err error                                                          // Declare err here
	if err = json.Unmarshal([]byte(f.opt.Token), &tokenData); err != nil { // Unmarshal unobscured token
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œå¤±è´¥ï¼ˆæ¥è‡ªé€‰é¡¹ï¼‰: %v", err)
		return false
	}

	f.token = tokenData.AccessToken
	f.tokenExpiry, err = time.Parse(time.RFC3339, tokenData.Expiry) // Use tokenData.Expiry
	if err != nil {
		fs.Debugf(f, "ä»é…ç½®è§£æä»¤ç‰Œè¿‡æœŸæ—¶é—´å¤±è´¥: %v", err)
		return false
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "é…ç½®ä¸­çš„ä»¤ç‰Œä¸å®Œæ•´")
		return false
	}

	fs.Debugf(f, "ä»é…ç½®æ–‡ä»¶åŠ è½½ä»¤ç‰Œï¼Œè¿‡æœŸæ—¶é—´: %v", f.tokenExpiry)
	return true
}

// saveToken saves the current token to the config
func (f *Fs) saveToken(_ context.Context, m configmap.Mapper) {
	if m == nil {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - æä¾›çš„æ˜ å°„å™¨ä¸ºç©º")
		return
	}

	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸ä¿å­˜ä»¤ç‰Œ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	tokenData := TokenJSON{ // Use the new TokenJSON struct
		AccessToken: f.token,
		Expiry:      f.tokenExpiry.Format(time.RFC3339), // Use Expiry
	}
	tokenJSON, err := json.Marshal(tokenData)
	if err != nil {
		fs.Errorf(f, "ä¿å­˜ä»¤ç‰Œæ—¶åºåˆ—åŒ–å¤±è´¥: %v", err)
		return
	}

	m.Set("token", string(tokenJSON))
	// m.Set does not return an error, so we assume it succeeds.
	// Any errors related to config saving would be handled by the underlying config system.

	fs.Debugf(f, "ä½¿ç”¨åŸå§‹åç§°%qä¿å­˜ä»¤ç‰Œåˆ°é…ç½®æ–‡ä»¶", f.originalName)
}

// setupTokenRenewer initializes the token renewer to automatically refresh tokens
func (f *Fs) setupTokenRenewer(ctx context.Context, m configmap.Mapper) {
	// Only set up renewer if we have valid tokens
	if f.token == "" || f.tokenExpiry.IsZero() {
		fs.Debugf(f, "ä¸è®¾ç½®ä»¤ç‰Œæ›´æ–°å™¨ - ä»¤ç‰Œä¿¡æ¯ä¸å®Œæ•´")
		return
	}

	// Create a renewal transaction function
	transaction := func() error {
		fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨è§¦å‘ï¼Œåˆ·æ–°ä»¤ç‰Œ")
		// Use non-global function to avoid deadlocks
		err := f.refreshTokenIfNecessary(ctx, false, true)
		if err != nil {
			fs.Errorf(f, "æ›´æ–°å™¨ä¸­åˆ·æ–°ä»¤ç‰Œå¤±è´¥: %v", err)
			return err
		}

		return nil // saveToken is already called in refreshTokenIfNecessary
	}

	// Create minimal OAuth config
	config := &oauthutil.Config{
		TokenURL: openAPIRootURL + "/api/v1/access_token", // Use the correct token URL for 123Pan
	}

	// Create a token source using the existing token
	token := &oauth2.Token{
		AccessToken: f.token,
		// 123Pan API does not seem to return a refresh token in the initial access token response,
		// so we rely on re-logging in if the access token expires.
		// RefreshToken: f.refreshToken, // Not available for 123Pan
		Expiry:    f.tokenExpiry,
		TokenType: "Bearer",
	}

	// Save token to config so it can be accessed by TokenSource
	err := oauthutil.PutToken(f.originalName, m, token, false)
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨ä¿å­˜ä»¤ç‰Œå¤±è´¥: %v", err)
		return
	}

	// Create a client with the token source
	_, ts, err := oauthutil.NewClientWithBaseClient(ctx, f.originalName, m, config, getHTTPClient(ctx, f.originalName, m))
	if err != nil {
		fs.Logf(f, "ä¸ºæ›´æ–°å™¨åˆ›å»ºä»¤ç‰Œæºå¤±è´¥: %v", err)
		return
	}

	// Create token renewer that will trigger when the token is about to expire
	f.tokenRenewer = oauthutil.NewRenew(f.originalName, ts, transaction)
	f.tokenRenewer.Start() // Start the renewer immediately
	fs.Debugf(f, "ä»¤ç‰Œæ›´æ–°å™¨å·²åˆå§‹åŒ–å¹¶å¯åŠ¨ï¼Œä½¿ç”¨åŸå§‹åç§°%q", f.originalName)
}

// refreshTokenIfNecessary refreshes the token if necessary
func (f *Fs) refreshTokenIfNecessary(ctx context.Context, refreshTokenExpired bool, forceRefresh bool) error {
	f.tokenMu.Lock()

	// Check if another thread has successfully refreshed while we were waiting
	if !refreshTokenExpired && !forceRefresh && f.token != "" && time.Now().Before(f.tokenExpiry.Add(-tokenRefreshWindow)) {
		fs.Debugf(f, "è·å–é”åä»¤ç‰Œä»ç„¶æœ‰æ•ˆï¼Œè·³è¿‡åˆ·æ–°")
		f.tokenMu.Unlock()
		return nil
	}
	defer f.tokenMu.Unlock()

	// Perform the actual token refresh
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token during refresh: %w", err)
	}

	f.token = accessToken
	f.tokenExpiry = expiredAt

	// Save the refreshed token to config
	f.saveToken(ctx, f.m)

	return nil
}

// GetAccessToken fetches a new access token from the 123Pan API.
func (f *Fs) GetAccessToken(ctx context.Context) error {
	accessToken, expiredAt, err := GetAccessToken(f.opt.ClientID, f.opt.ClientSecret)
	if err != nil {
		return fmt.Errorf("failed to get new access token: %w", err)
	}
	f.token = accessToken
	f.tokenExpiry = expiredAt
	return nil
}

type ClientCredentials struct {
	ClientID     string `json:"clientID"`
	ClientSecret string `json:"clientSecret"`
}

type TokenJSON struct {
	AccessToken string `json:"access_token"`
	Expiry      string `json:"expiry"`
}

type AccessTokenData struct {
	AccessToken string `json:"accessToken"`
	ExpiredAt   string `json:"expiredAt"`
}

type Response struct {
	Code     int    `json:"code"`
	Message  string `json:"message"`
	Data     AccessTokenData
	XTraceID string `json:"x-traceID"`
}

func GetAccessToken(clientID, clientSecret string) (string, time.Time, error) {
	credentials := ClientCredentials{
		ClientID:     clientID,
		ClientSecret: clientSecret,
	}
	payload, err := json.Marshal(credentials)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to marshal payload: %v", err)
	}
	req, err := http.NewRequest("POST", "https://open-api.123pan.com/api/v1/access_token", bytes.NewBuffer(payload))
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to create request: %v", err)
	}
	req.Header.Set("Platform", "open_platform")
	req.Header.Set("Content-Type", "application/json")
	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to send request: %v", err)
	}
	defer resp.Body.Close()
	if resp.StatusCode != http.StatusOK {
		return "", time.Time{}, fmt.Errorf("unexpected status code: %d", resp.StatusCode)
	}
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to read response body: %v", err)
	}
	var result Response
	if err = json.Unmarshal(body, &result); err != nil {
		return "", time.Time{}, fmt.Errorf("failed to unmarshal response: %v", err)
	}
	expiredAt, err := time.Parse(time.RFC3339, result.Data.ExpiredAt)
	if err != nil {
		return "", time.Time{}, fmt.Errorf("failed to parse expiration time: %v", err)
	}
	return result.Data.AccessToken, expiredAt, nil
}

// Name of the remote (e.g. "123")
func (f *Fs) Name() string {
	return f.name
}

// Root of the remote (e.g. "bucket" or "bucket/dir")
func (f *Fs) Root() string {
	return f.root
}

// String returns a description of the FS
func (f *Fs) String() string {
	return fmt.Sprintf("123 root '%s'", f.root)
}

// Features returns the optional features of this Fs
func (f *Fs) Features() *fs.Features {
	return f.features
}

// Hashes returns the supported hash sets.
func (f *Fs) Hashes() fshash.Set {
	return fshash.Set(fshash.MD5)
}

// About gets quota information
func (f *Fs) About(ctx context.Context) (*fs.Usage, error) {
	fs.Debugf(f, "è°ƒç”¨About")

	userInfo, err := f.getUserInfo(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get user info: %w", err)
	}

	usage := &fs.Usage{}
	if userInfo.Data.SpacePermanent > 0 {
		usage.Total = fs.NewUsageValue(userInfo.Data.SpacePermanent)
	}
	if userInfo.Data.SpaceUsed > 0 {
		usage.Used = fs.NewUsageValue(userInfo.Data.SpaceUsed)
	}
	if userInfo.Data.SpacePermanent > 0 && userInfo.Data.SpaceUsed > 0 {
		free := userInfo.Data.SpacePermanent - userInfo.Data.SpaceUsed
		if free > 0 {
			usage.Free = fs.NewUsageValue(free)
		}
	}

	return usage, nil
}

// Purge deletes all the files in the directory
func (f *Fs) Purge(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨Purgeåˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List all files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)

	for {
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return err
		}

		if response.Code != 0 {
			return fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Collect all files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Delete all files
	for _, file := range allFiles {
		err := f.deleteFile(ctx, file.FileID)
		if err != nil {
			fs.Errorf(f, "åˆ é™¤æ–‡ä»¶å¤±è´¥ %s: %v", file.Filename, err)
			// Continue with other files
		}
	}

	return nil
}

// Shutdown the backend
func (f *Fs) Shutdown(ctx context.Context) error {
	fs.Debugf(f, "è°ƒç”¨å…³é—­")

	// Stop token renewer if it exists
	if f.tokenRenewer != nil {
		f.tokenRenewer.Stop()
		f.tokenRenewer = nil
	}

	// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
	if f.resourcePool != nil {
		f.resourcePool.CleanupAllTempFiles()
		f.resourcePool.Close()
	}

	// å…³é—­æ‰€æœ‰BadgerDBç¼“å­˜
	caches := []*cache.BadgerCache{
		f.parentIDCache,
		f.dirListCache,
		f.pathToIDCache,
	}

	for i, c := range caches {
		if c != nil {
			if err := c.Close(); err != nil {
				fs.Debugf(f, "å…³é—­BadgerDBç¼“å­˜%då¤±è´¥: %v", i, err)
			}
		}
	}

	return nil
}

// List the objects and directories in dir into entries.
func (f *Fs) List(ctx context.Context, dir string) (entries fs.DirEntries, err error) {
	fs.Debugf(f, "è°ƒç”¨åˆ—è¡¨ï¼Œç›®å½•: %s", dir)

	// ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœdirä¸ºç©ºä¸”rootæŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
	if dir == "" && f.root != "" {
		// æ£€æŸ¥rootæ˜¯å¦æŒ‡å‘ä¸€ä¸ªæ–‡ä»¶
		rootObj, err := f.NewObject(ctx, "")
		if err == nil {
			// rootæŒ‡å‘ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨
			fs.Debugf(f, "rootè·¯å¾„æŒ‡å‘æ–‡ä»¶ï¼Œè¿”å›åŒ…å«è¯¥æ–‡ä»¶çš„åˆ—è¡¨: %s", f.root)
			return fs.DirEntries{rootObj}, nil
		}
		// å¦‚æœNewObjectå¤±è´¥ï¼Œç»§ç»­æ­£å¸¸çš„ç›®å½•åˆ—è¡¨é€»è¾‘
		fs.Debugf(f, "rootè·¯å¾„ä¸æ˜¯æ–‡ä»¶ï¼Œç»§ç»­ç›®å½•åˆ—è¡¨é€»è¾‘: %s", f.root)
	}

	// ä½¿ç”¨ç›®å½•ç¼“å­˜æŸ¥æ‰¾ç›®å½•ID
	fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•ID: %s", dir)
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		fs.Debugf(f, "æŸ¥æ‰¾ç›®å½•å¤±è´¥ %s: %v", dir, err)
		return nil, err
	}
	fs.Debugf(f, "æ‰¾åˆ°ç›®å½•ID: %sï¼Œç›®å½•: %s", dirID, dir)

	// Convert dirID to int64
	parentFileID, err := parseDirID(dirID)
	if err != nil {
		return nil, fmt.Errorf("è§£æç›®å½•IDå¤±è´¥: %w", err)
	}

	// List files in the directory
	var allFiles []FileListInfoRespDataV2
	lastFileID := int64(0)

	for {
		response, err := f.ListFile(ctx, int(parentFileID), 100, "", "", int(lastFileID))
		if err != nil {
			return nil, err
		}

		if response.Code != 0 {
			return nil, fmt.Errorf("API error %d: %s", response.Code, response.Message)
		}

		// Filter out trashed files
		for _, file := range response.Data.FileList {
			if file.Status < 100 { // Not trashed
				allFiles = append(allFiles, file)
			}
		}

		// Check if we have more files
		if response.Data.LastFileId == -1 {
			break
		}
		lastFileID = response.Data.LastFileId
	}

	// Convert to fs.DirEntries
	for _, file := range allFiles {
		remote := file.Filename
		if dir != "" {
			remote = strings.Trim(dir+"/"+file.Filename, "/")
		}

		if file.Type == 1 { // Directory
			// Cache the directory ID for future use
			f.dirCache.Put(remote, strconv.FormatInt(file.FileID, 10))
			d := fs.NewDir(remote, time.Time{})
			entries = append(entries, d)
		} else { // File
			o := &Object{
				fs:          f,
				remote:      remote,
				hasMetaData: true,
				id:          strconv.FormatInt(file.FileID, 10),
				size:        file.Size,
				md5sum:      file.Etag,
				modTime:     time.Now(), // We'll parse this properly later
				isDir:       false,
			}
			entries = append(entries, o)
		}
	}

	return entries, nil
}

// Mkdir makes the directory.
func (f *Fs) Mkdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ›å»ºç›®å½•: %s", dir)

	// Use dircache to create the directory
	_, err := f.dirCache.FindDir(ctx, dir, true)
	if err == nil {
		// åˆ›å»ºç›®å½•æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
		f.invalidateRelatedCaches(dir, "mkdir")
	}
	return err
}

// Rmdir removes an empty directory.
func (f *Fs) Rmdir(ctx context.Context, dir string) error {
	fs.Debugf(f, "è°ƒç”¨åˆ é™¤ç›®å½•: %s", dir)

	// Use dircache to find the directory ID
	dirID, err := f.dirCache.FindDir(ctx, dir, false)
	if err != nil {
		return err
	}

	// Convert dirID to int64
	fileID, err := strconv.ParseInt(dirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid directory ID: %s", dirID)
	}

	// Delete the directory
	err = f.deleteFile(ctx, fileID)
	if err != nil {
		return err
	}

	// Remove from cache
	f.dirCache.FlushDir(dir)

	// åˆ é™¤ç›®å½•æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches(dir, "rmdir")

	return nil
}

// Move server-side moves a file.
func (f *Fs) Move(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨ç§»åŠ¨ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„ - ä½¿ç”¨é€šç”¨å·¥å…·
	normalizedRemote := PathUtil.NormalizePath(remote)
	fs.Debugf(f, "Moveæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		return nil, fs.ErrorCantMove
	}

	// Use dircache to find destination directory
	dstName, dstDirID, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	cleanedDstName, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "ç§»åŠ¨æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		dstName = cleanedDstName
	}

	// è½¬æ¢ç›®æ ‡ç›®å½•ID
	dstParentID, err := strconv.ParseInt(dstDirID, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid destination directory ID: %s", dstDirID)
	}

	// ç”Ÿæˆå”¯ä¸€çš„ç›®æ ‡æ–‡ä»¶åï¼Œé¿å…å†²çª
	uniqueDstName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
	if err != nil {
		fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åç§°: %v", err)
		uniqueDstName = dstName
	}

	if uniqueDstName != dstName {
		fs.Logf(f, "æ£€æµ‹åˆ°æ–‡ä»¶åå†²çªï¼Œä½¿ç”¨å”¯ä¸€åç§°: %s -> %s", dstName, uniqueDstName)
		dstName = uniqueDstName
	}

	// Convert IDs to int64
	fileID, err := strconv.ParseInt(srcObj.id, 10, 64)
	if err != nil {
		return nil, fmt.Errorf("invalid file ID: %s", srcObj.id)
	}

	// Get source directory ID for comparison
	_, srcDirID, err := f.dirCache.FindPath(ctx, srcObj.Remote(), false)
	if err != nil {
		return nil, fmt.Errorf("failed to find source directory: %w", err)
	}

	// è·å–æºæ–‡ä»¶çš„åŸºç¡€åç§°ï¼ˆä¸åŒ…å«è·¯å¾„ï¼‰
	srcBaseName := filepath.Base(srcObj.Remote())
	fs.Debugf(f, "æºæ–‡ä»¶åŸºç¡€åç§°: %s, ç›®æ ‡æ–‡ä»¶å: %s", srcBaseName, dstName)

	// Check if we're moving to the same directory
	if srcDirID == dstDirID {
		// Same directory - only rename if name changed
		if dstName != srcBaseName {
			fs.Debugf(f, "åŒç›®å½•ç§»åŠ¨ï¼Œä»…é‡å‘½å %s ä¸º %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				return nil, err
			}
		} else {
			fs.Debugf(f, "åŒç›®å½•åŒå - æ— éœ€æ“ä½œ")
		}
	} else {
		// ä¸åŒç›®å½• - å…ˆç§»åŠ¨ï¼Œç„¶åæ ¹æ®éœ€è¦é‡å‘½å
		fs.Debugf(f, "ç§»åŠ¨æ–‡ä»¶ä»ç›®å½• %s åˆ° %s", srcDirID, dstDirID)

		// åœ¨è·¨ç›®å½•ç§»åŠ¨æ—¶ï¼Œå…ˆæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨åŒåæ–‡ä»¶
		if dstName == srcBaseName {
			// å¦‚æœç›®æ ‡æ–‡ä»¶åä¸æºæ–‡ä»¶åç›¸åŒï¼Œæ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡ä»¶
			exists, existingFileID, err := f.fileExistsInDirectory(ctx, dstParentID, dstName)
			if err != nil {
				fs.Debugf(f, "æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¤±è´¥: %v", err)
			} else if exists {
				fs.Debugf(f, "ç›®æ ‡ç›®å½•å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ŒID: %d", existingFileID)
				if existingFileID == fileID {
					fs.Debugf(f, "æ–‡ä»¶å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
					// æ–‡ä»¶å·²ç»åœ¨æ­£ç¡®ä½ç½®ï¼Œç›´æ¥è¿”å›æˆåŠŸ
					return &Object{
						fs:          f,
						remote:      remote,
						hasMetaData: srcObj.hasMetaData,
						id:          srcObj.id,
						size:        srcObj.size,
						md5sum:      srcObj.md5sum,
						modTime:     srcObj.modTime,
						isDir:       srcObj.isDir,
					}, nil
				} else {
					fs.Debugf(f, "ç›®æ ‡ç›®å½•å­˜åœ¨ä¸åŒçš„åŒåæ–‡ä»¶ï¼Œç”Ÿæˆå”¯ä¸€åç§°")
					uniqueName, err := f.generateUniqueFileName(ctx, dstParentID, dstName)
					if err != nil {
						fs.Debugf(f, "ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå¤±è´¥: %v", err)
					} else {
						dstName = uniqueName
						fs.Logf(f, "ä½¿ç”¨å”¯ä¸€æ–‡ä»¶åé¿å…å†²çª: %s", dstName)
					}
				}
			}
		}

		err = f.moveFile(ctx, fileID, dstParentID)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯å› ä¸ºæ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½• - ä¿®å¤å­—ç¬¦ä¸²åŒ¹é…é—®é¢˜
			if strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸­") ||
				strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
				fs.Debugf(f, "æ–‡ä»¶å¯èƒ½å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œæ£€æŸ¥çŠ¶æ€")
				exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, srcBaseName)
				if checkErr == nil && exists && existingFileID == fileID {
					fs.Debugf(f, "ç¡®è®¤æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œç»§ç»­é‡å‘½åæ­¥éª¤")
					// æ–‡ä»¶å·²åœ¨ç›®æ ‡ç›®å½•ï¼Œè·³è¿‡ç§»åŠ¨æ­¥éª¤
				} else {
					// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
					if strings.HasSuffix(srcBaseName, ".partial") {
						fs.Debugf(f, "Partialæ–‡ä»¶ç§»åŠ¨å†²çªï¼Œå°è¯•å¤„ç†æ–‡ä»¶åå†²çª")
						return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
					}
					return nil, err
				}
			} else {
				return nil, err
			}
		}

		// If the name changed, rename the file
		if dstName != srcBaseName {
			fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åæ–‡ä»¶: %s -> %s", srcBaseName, dstName)
			err = f.renameFile(ctx, fileID, dstName)
			if err != nil {
				// æ£€æŸ¥é‡å‘½åå¤±è´¥çš„åŸå›  - å¢å¼ºé”™è¯¯åŒ¹é…
				if strings.Contains(err.Error(), "å½“å‰ç›®å½•æœ‰é‡åæ–‡ä»¶") ||
					strings.Contains(err.Error(), "æ–‡ä»¶å·²åœ¨å½“å‰æ–‡ä»¶å¤¹") {
					fs.Debugf(f, "é‡å‘½åå¤±è´¥ï¼Œç›®å½•ä¸­å·²æœ‰é‡åæ–‡ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡æ–‡ä»¶")
					exists, existingFileID, checkErr := f.fileExistsInDirectory(ctx, dstParentID, dstName)
					if checkErr == nil && exists && existingFileID == fileID {
						fs.Debugf(f, "æ–‡ä»¶å·²æœ‰æ­£ç¡®åç§°ï¼Œé‡å‘½åæ“ä½œå®é™…å·²æˆåŠŸ")
						// æ–‡ä»¶å·²ç»æœ‰æ­£ç¡®çš„åç§°ï¼Œè®¤ä¸ºæ“ä½œæˆåŠŸ
					} else {
						// æ£€æŸ¥æ˜¯å¦æ˜¯partialæ–‡ä»¶é‡å‘½åå†²çª
						if strings.HasSuffix(srcBaseName, ".partial") {
							fs.Debugf(f, "Partialæ–‡ä»¶é‡å‘½åå†²çªï¼Œå°è¯•ç‰¹æ®Šå¤„ç†")
							return f.handlePartialFileConflict(ctx, fileID, srcBaseName, dstParentID, dstName)
						}
						return nil, err
					}
				} else {
					return nil, err
				}
			}
		}
	}

	// ç§»åŠ¨æˆåŠŸåï¼Œæ¸…ç†ç›¸å…³ç¼“å­˜
	f.invalidateRelatedCaches(srcObj.Remote(), "move")
	f.invalidateRelatedCaches(remote, "move")

	// Return the moved object
	return &Object{
		fs:          f,
		remote:      remote,
		hasMetaData: srcObj.hasMetaData,
		id:          srcObj.id,
		size:        srcObj.size,
		md5sum:      srcObj.md5sum,
		modTime:     srcObj.modTime,
		isDir:       srcObj.isDir,
	}, nil
}

// DirMove server-side moves a directory.
func (f *Fs) DirMove(ctx context.Context, src fs.Fs, srcRemote, dstRemote string) error {
	fs.Debugf(f, "è°ƒç”¨ç›®å½•ç§»åŠ¨ %s åˆ° %s", srcRemote, dstRemote)

	srcFs, ok := src.(*Fs)
	if !ok {
		fs.Debugf(f, "æ— æ³•ç§»åŠ¨ç›®å½• - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return fs.ErrorCantDirMove
	}

	// Find source directory ID
	srcDirID, err := srcFs.dirCache.FindDir(ctx, srcRemote, false)
	if err != nil {
		return err
	}

	// Find destination parent directory
	dstName, dstParentID, err := f.dirCache.FindPath(ctx, dstRemote, true)
	if err != nil {
		return err
	}

	// Convert IDs to int64
	srcFileID, err := strconv.ParseInt(srcDirID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid source directory ID: %s", srcDirID)
	}

	dstParentFileID, err := strconv.ParseInt(dstParentID, 10, 64)
	if err != nil {
		return fmt.Errorf("invalid destination parent ID: %s", dstParentID)
	}

	// è·å–æºç›®å½•çš„åŸºç¡€åç§°
	srcBaseName := filepath.Base(srcRemote)
	fs.Debugf(f, "æºç›®å½•åŸºç¡€åç§°: %s, ç›®æ ‡ç›®å½•å: %s", srcBaseName, dstName)
	fs.Debugf(f, "æºç›®å½•ID: %d, ç›®æ ‡çˆ¶ç›®å½•ID: %d", srcFileID, dstParentFileID)

	// æ£€æŸ¥æ˜¯å¦å°è¯•ç§»åŠ¨åˆ°åŒä¸€ä¸ªçˆ¶ç›®å½•
	srcParentID, err := f.getParentID(ctx, srcFileID)
	if err != nil {
		fs.Debugf(f, "è·å–æºç›®å½•çˆ¶IDå¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "æºç›®å½•å½“å‰çˆ¶ID: %d", srcParentID)
		if srcParentID == dstParentFileID && dstName == srcBaseName {
			fs.Debugf(f, "ç›®å½•å·²åœ¨ç›®æ ‡ä½ç½®ï¼Œæ— éœ€ç§»åŠ¨")
			return nil
		}
	}

	// Move the directory
	err = f.moveFile(ctx, srcFileID, dstParentFileID)
	if err != nil {
		return err
	}

	// If the name changed, rename the directory
	if dstName != srcBaseName {
		fs.Debugf(f, "ç§»åŠ¨åé‡å‘½åç›®å½•: %s -> %s", srcBaseName, dstName)
		err = f.renameFile(ctx, srcFileID, dstName)
		if err != nil {
			return err
		}
	}

	// Update cache
	srcFs.dirCache.FlushDir(srcRemote)
	f.dirCache.FlushDir(dstRemote)

	return nil
}

// Copy server-side copies a file.
func (f *Fs) Copy(ctx context.Context, src fs.Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "è°ƒç”¨å¤åˆ¶ %s åˆ° %s", src.Remote(), remote)

	// è§„èŒƒåŒ–ç›®æ ‡è·¯å¾„ - ä½¿ç”¨é€šç”¨å·¥å…·
	normalizedRemote := PathUtil.NormalizePath(remote)
	fs.Debugf(f, "Copyæ“ä½œè§„èŒƒåŒ–ç›®æ ‡è·¯å¾„: %s -> %s", remote, normalizedRemote)

	srcObj, ok := src.(*Object)
	if !ok {
		fs.Debugf(f, "æ— æ³•å¤åˆ¶ - ä¸æ˜¯ç›¸åŒçš„è¿œç¨‹ç±»å‹")
		return nil, fs.ErrorCantCopy
	}

	// Use dircache to find destination directory
	dstName, _, err := f.dirCache.FindPath(ctx, normalizedRemote, true)
	if err != nil {
		return nil, err
	}

	// éªŒè¯å¹¶æ¸…ç†ç›®æ ‡æ–‡ä»¶å
	cleanedDstName, warning := validateAndCleanFileName(dstName)
	if warning != nil {
		fs.Logf(f, "å¤åˆ¶æ“ä½œæ–‡ä»¶åéªŒè¯è­¦å‘Š: %v", warning)
		dstName = cleanedDstName
	}

	// 123Pan doesn't have a direct copy API, use download+upload
	fs.Debugf(f, "123ç½‘ç›˜ä¸æ”¯æŒæœåŠ¡å™¨ç«¯å¤åˆ¶ï¼Œä½¿ç”¨ä¸‹è½½+ä¸Šä¼ æ–¹å¼")
	return f.copyViaDownloadUpload(ctx, srcObj, remote)
}

// copyViaDownloadUpload copies a file by downloading and re-uploading
func (f *Fs) copyViaDownloadUpload(ctx context.Context, srcObj *Object, remote string) (fs.Object, error) {
	fs.Debugf(f, "é€šè¿‡ä¸‹è½½+ä¸Šä¼ å¤åˆ¶ %s åˆ° %s", srcObj.remote, remote)

	// Open source file for reading
	src, err := srcObj.Open(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to open source file: %w", err)
	}
	defer src.Close()

	// Create a new object info for the destination
	dstInfo := &ObjectInfo{
		fs:      f,
		remote:  remote,
		size:    srcObj.size,
		md5sum:  srcObj.md5sum,
		modTime: srcObj.modTime,
	}

	// Upload to destination
	return f.Put(ctx, src, dstInfo)
}

// ObjectInfo implements fs.ObjectInfo for copy operations
type ObjectInfo struct {
	fs      *Fs
	remote  string
	size    int64
	md5sum  string
	modTime time.Time
}

func (oi *ObjectInfo) Fs() fs.Info                           { return oi.fs }
func (oi *ObjectInfo) Remote() string                        { return oi.remote }
func (oi *ObjectInfo) Size() int64                           { return oi.size }
func (oi *ObjectInfo) ModTime(ctx context.Context) time.Time { return oi.modTime }
func (oi *ObjectInfo) Storable() bool                        { return true }
func (oi *ObjectInfo) String() string                        { return oi.remote }
func (oi *ObjectInfo) Hash(ctx context.Context, t fshash.Type) (string, error) {
	if t == fshash.MD5 {
		return oi.md5sum, nil
	}
	return "", fshash.ErrUnsupported
}

// getHTTPClient makes an http client according to the options
// ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨rcloneçš„æ ‡å‡†HTTPå®¢æˆ·ç«¯é…ç½®
func getHTTPClient(ctx context.Context, name string, m configmap.Mapper) *http.Client {
	// rcloneçš„fshttp.NewClientå·²ç»æä¾›äº†åˆé€‚çš„é»˜è®¤é…ç½®
	// åŒ…æ‹¬è¶…æ—¶ã€è¿æ¥æ± ã€TLSè®¾ç½®ç­‰ï¼Œæ— éœ€é¢å¤–è‡ªå®šä¹‰
	return fshttp.NewClient(ctx)
}

// getNetworkQuality è¯„ä¼°å½“å‰ç½‘ç»œè´¨é‡ï¼Œè¿”å›0.0-1.0çš„è´¨é‡åˆ†æ•°
func (f *Fs) getNetworkQuality() float64 {
	// åŸºäºæœ€è¿‘çš„ä¼ è¾“ç»Ÿè®¡è¯„ä¼°ç½‘ç»œè´¨é‡
	if f.performanceMetrics == nil {
		return 0.8 // é»˜è®¤å‡è®¾ç½‘ç»œè´¨é‡è‰¯å¥½
	}

	// ä»æ€§èƒ½æŒ‡æ ‡è·å–å®é™…çš„é”™è¯¯ç‡å’Œä¼ è¾“ç»Ÿè®¡
	stats := f.performanceMetrics.GetStats()

	// è·å–é”™è¯¯ç‡
	var errorRate float64 = 0.02 // é»˜è®¤2%é”™è¯¯ç‡
	if apiErrorRate, ok := stats["api_error_rate"].(float64); ok {
		errorRate = apiErrorRate
	}

	// è·å–å¹³å‡ä¼ è¾“é€Ÿåº¦ä½œä¸ºç½‘ç»œè´¨é‡æŒ‡æ ‡
	var speedQuality float64 = 0.8 // é»˜è®¤è´¨é‡åˆ†æ•°
	if avgSpeed, ok := stats["average_upload_speed"].(float64); ok {
		// åŸºäºä¼ è¾“é€Ÿåº¦è¯„ä¼°ç½‘ç»œè´¨é‡
		if avgSpeed > 50 { // >50MB/s - ä¼˜ç§€ç½‘ç»œ
			speedQuality = 0.95
		} else if avgSpeed > 20 { // >20MB/s - è‰¯å¥½ç½‘ç»œ
			speedQuality = 0.85
		} else if avgSpeed > 10 { // >10MB/s - ä¸€èˆ¬ç½‘ç»œ
			speedQuality = 0.7
		} else if avgSpeed > 5 { // >5MB/s - è¾ƒå·®ç½‘ç»œ
			speedQuality = 0.5
		} else { // <5MB/s - å¾ˆå·®ç½‘ç»œ
			speedQuality = 0.3
		}
	}

	// åŸºäºé”™è¯¯ç‡è°ƒæ•´è´¨é‡åˆ†æ•°
	var errorQuality float64
	if errorRate > 0.15 { // é”™è¯¯ç‡>15%
		errorQuality = 0.2 // ç½‘ç»œè´¨é‡å¾ˆå·®
	} else if errorRate > 0.10 { // é”™è¯¯ç‡>10%
		errorQuality = 0.4 // ç½‘ç»œè´¨é‡å·®
	} else if errorRate > 0.05 { // é”™è¯¯ç‡>5%
		errorQuality = 0.6 // ç½‘ç»œè´¨é‡ä¸€èˆ¬
	} else if errorRate > 0.02 { // é”™è¯¯ç‡>2%
		errorQuality = 0.8 // ç½‘ç»œè´¨é‡è‰¯å¥½
	} else {
		errorQuality = 0.95 // ç½‘ç»œè´¨é‡ä¼˜ç§€
	}

	// ç»¼åˆé€Ÿåº¦å’Œé”™è¯¯ç‡è¯„ä¼°ï¼ˆæƒé‡ï¼šé€Ÿåº¦60%ï¼Œé”™è¯¯ç‡40%ï¼‰
	finalQuality := speedQuality*0.6 + errorQuality*0.4

	f.debugf(LogLevelVerbose, "ç½‘ç»œè´¨é‡è¯„ä¼°: é”™è¯¯ç‡=%.3f, é€Ÿåº¦è´¨é‡=%.2f, é”™è¯¯è´¨é‡=%.2f, æœ€ç»ˆè´¨é‡=%.2f",
		errorRate, speedQuality, errorQuality, finalQuality)

	return finalQuality
}

// DynamicParameterAdjuster åŠ¨æ€å‚æ•°è°ƒæ•´å™¨
type DynamicParameterAdjuster struct {
	mu                    sync.RWMutex
	lastAdjustment        time.Time
	adjustmentInterval    time.Duration
	networkQualityHistory []float64
	maxHistorySize        int
	currentConcurrency    int
	currentChunkSize      int64
	currentTimeout        time.Duration
}

// NewDynamicParameterAdjuster åˆ›å»ºåŠ¨æ€å‚æ•°è°ƒæ•´å™¨
func NewDynamicParameterAdjuster() *DynamicParameterAdjuster {
	return &DynamicParameterAdjuster{
		adjustmentInterval:    30 * time.Second, // æ¯30ç§’è°ƒæ•´ä¸€æ¬¡
		networkQualityHistory: make([]float64, 0),
		maxHistorySize:        10,                // ä¿ç•™æœ€è¿‘10æ¬¡è´¨é‡è®°å½•
		currentConcurrency:    4,                 // é»˜è®¤å¹¶å‘æ•°
		currentChunkSize:      100 * 1024 * 1024, // é»˜è®¤100MBåˆ†ç‰‡
		currentTimeout:        5 * time.Minute,   // é»˜è®¤5åˆ†é’Ÿè¶…æ—¶
	}
}

// ShouldAdjust æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´å‚æ•°
func (dpa *DynamicParameterAdjuster) ShouldAdjust() bool {
	dpa.mu.RLock()
	defer dpa.mu.RUnlock()

	return time.Since(dpa.lastAdjustment) >= dpa.adjustmentInterval
}

// RecordNetworkQuality è®°å½•ç½‘ç»œè´¨é‡
func (dpa *DynamicParameterAdjuster) RecordNetworkQuality(quality float64) {
	dpa.mu.Lock()
	defer dpa.mu.Unlock()

	dpa.networkQualityHistory = append(dpa.networkQualityHistory, quality)

	// ä¿æŒå†å²è®°å½•å¤§å°é™åˆ¶
	if len(dpa.networkQualityHistory) > dpa.maxHistorySize {
		dpa.networkQualityHistory = dpa.networkQualityHistory[1:]
	}
}

// GetAverageNetworkQuality è·å–å¹³å‡ç½‘ç»œè´¨é‡
func (dpa *DynamicParameterAdjuster) GetAverageNetworkQuality() float64 {
	dpa.mu.RLock()
	defer dpa.mu.RUnlock()

	if len(dpa.networkQualityHistory) == 0 {
		return 0.8 // é»˜è®¤è´¨é‡
	}

	var total float64
	for _, quality := range dpa.networkQualityHistory {
		total += quality
	}

	return total / float64(len(dpa.networkQualityHistory))
}

// AdjustParameters æ ¹æ®ç½‘ç»œè´¨é‡è°ƒæ•´å‚æ•°
func (dpa *DynamicParameterAdjuster) AdjustParameters(fileSize int64, networkSpeed int64, networkQuality float64) (concurrency int, chunkSize int64, timeout time.Duration) {
	dpa.mu.Lock()
	defer dpa.mu.Unlock()

	dpa.lastAdjustment = time.Now()

	// ç›´æ¥è®°å½•ç½‘ç»œè´¨é‡ï¼Œé¿å…é‡å…¥é”é—®é¢˜
	dpa.networkQualityHistory = append(dpa.networkQualityHistory, networkQuality)
	if len(dpa.networkQualityHistory) > dpa.maxHistorySize {
		dpa.networkQualityHistory = dpa.networkQualityHistory[1:]
	}

	// ç›´æ¥è®¡ç®—å¹³å‡ç½‘ç»œè´¨é‡ï¼Œé¿å…é‡å…¥é”é—®é¢˜
	var avgQuality float64
	if len(dpa.networkQualityHistory) == 0 {
		avgQuality = 0.8 // é»˜è®¤è´¨é‡
	} else {
		var total float64
		for _, quality := range dpa.networkQualityHistory {
			total += quality
		}
		avgQuality = total / float64(len(dpa.networkQualityHistory))
	}

	// åŸºäºå¹³å‡ç½‘ç»œè´¨é‡è°ƒæ•´å¹¶å‘æ•°
	baseConcurrency := 4
	if avgQuality > 0.9 { // ä¼˜ç§€ç½‘ç»œ
		baseConcurrency = 8
	} else if avgQuality > 0.7 { // è‰¯å¥½ç½‘ç»œ
		baseConcurrency = 6
	} else if avgQuality > 0.5 { // ä¸€èˆ¬ç½‘ç»œ
		baseConcurrency = 4
	} else { // è¾ƒå·®ç½‘ç»œ
		baseConcurrency = 2
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´
	if fileSize > 5*1024*1024*1024 { // >5GB
		baseConcurrency = int(float64(baseConcurrency) * 1.5)
	} else if fileSize < 500*1024*1024 { // <500MB
		baseConcurrency = int(float64(baseConcurrency) * 0.7)
	}

	// é™åˆ¶å¹¶å‘æ•°èŒƒå›´
	if baseConcurrency < 1 {
		baseConcurrency = 1
	}
	if baseConcurrency > 20 {
		baseConcurrency = 20
	}

	dpa.currentConcurrency = baseConcurrency

	// åŸºäºç½‘ç»œè´¨é‡è°ƒæ•´åˆ†ç‰‡å¤§å°
	baseChunkSize := int64(100 * 1024 * 1024) // 100MB
	if avgQuality > 0.8 {                     // é«˜è´¨é‡ç½‘ç»œä½¿ç”¨å¤§åˆ†ç‰‡
		baseChunkSize = int64(200 * 1024 * 1024) // 200MB
	} else if avgQuality < 0.5 { // ä½è´¨é‡ç½‘ç»œä½¿ç”¨å°åˆ†ç‰‡
		baseChunkSize = int64(50 * 1024 * 1024) // 50MB
	}

	dpa.currentChunkSize = baseChunkSize

	// åŸºäºç½‘ç»œè´¨é‡è°ƒæ•´è¶…æ—¶æ—¶é—´
	baseTimeout := 5 * time.Minute
	if avgQuality < 0.5 { // ç½‘ç»œè´¨é‡å·®ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
		baseTimeout = time.Duration(float64(baseTimeout) * 2.0)
	} else if avgQuality > 0.8 { // ç½‘ç»œè´¨é‡å¥½ï¼Œå¯ä»¥å‡å°‘è¶…æ—¶æ—¶é—´
		baseTimeout = time.Duration(float64(baseTimeout) * 0.8)
	}

	dpa.currentTimeout = baseTimeout

	return dpa.currentConcurrency, dpa.currentChunkSize, dpa.currentTimeout
}

// getAdaptiveTimeout æ ¹æ®æ–‡ä»¶å¤§å°ã€ä¼ è¾“ç±»å‹å’Œç½‘ç»œè´¨é‡è®¡ç®—è‡ªé€‚åº”è¶…æ—¶æ—¶é—´
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šé›†æˆåŠ¨æ€å‚æ•°è°ƒæ•´å™¨ï¼Œå®ç°æ™ºèƒ½è‡ªé€‚åº”è¶…æ—¶æ§åˆ¶
func (f *Fs) getAdaptiveTimeout(fileSize int64, transferType string) time.Duration {
	// æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒåŠ¨æ€è°ƒæ•´
	if f.dynamicAdjuster != nil && f.dynamicAdjuster.ShouldAdjust() {
		networkSpeed := f.detectNetworkSpeed(context.Background())
		networkQuality := f.getNetworkQuality()
		_, _, timeout := f.dynamicAdjuster.AdjustParameters(fileSize, networkSpeed, networkQuality)

		// æ ¹æ®ä¼ è¾“ç±»å‹è°ƒæ•´è¶…æ—¶æ—¶é—´
		var typeMultiplier float64 = 1.0
		switch transferType {
		case "chunked_upload":
			typeMultiplier = 1.5 // åˆ†ç‰‡ä¸Šä¼ éœ€è¦é€‚ä¸­æ—¶é—´
		case "stream_download":
			typeMultiplier = 1.2 // æµå¼ä¸‹è½½éœ€è¦é€‚ä¸­æ—¶é—´
		case "single_step":
			typeMultiplier = 0.8 // å•æ­¥ä¸Šä¼ æ—¶é—´è¾ƒçŸ­
		case "concurrent_upload":
			typeMultiplier = 2.0 // å¹¶å‘ä¸Šä¼ éœ€è¦æ›´é•¿æ—¶é—´
		default:
			typeMultiplier = 1.0
		}

		adjustedTimeout := time.Duration(float64(timeout) * typeMultiplier)

		f.debugf(LogLevelVerbose, "åŠ¨æ€è¶…æ—¶è°ƒæ•´: %s -> %v (è´¨é‡=%.2f)",
			fs.SizeSuffix(fileSize), adjustedTimeout, networkQuality)

		return adjustedTimeout
	}

	// å›é€€åˆ°ä¼ ç»Ÿçš„é™æ€è®¡ç®—æ–¹æ³•
	baseTimeout := time.Duration(f.opt.Timeout)
	if baseTimeout <= 0 {
		baseTimeout = defaultTimeout
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆæ›´ç²¾ç»†çš„è®¡ç®—ï¼‰
	// æ¯50MBå¢åŠ 30ç§’è¶…æ—¶æ—¶é—´ï¼Œå¯¹å¤§æ–‡ä»¶æ›´å‹å¥½
	sizeBasedTimeout := time.Duration(fileSize/50/1024/1024) * 30 * time.Second

	// æ ¹æ®ä¼ è¾“ç±»å‹å’Œç½‘ç»œæ¡ä»¶è°ƒæ•´
	var typeMultiplier float64 = 1.0
	switch transferType {
	case "chunked_upload":
		typeMultiplier = 1.5 // åˆ†ç‰‡ä¸Šä¼ éœ€è¦é€‚ä¸­æ—¶é—´
	case "stream_download":
		typeMultiplier = 1.2 // æµå¼ä¸‹è½½éœ€è¦é€‚ä¸­æ—¶é—´
	case "single_step":
		typeMultiplier = 0.8 // å•æ­¥ä¸Šä¼ æ—¶é—´è¾ƒçŸ­
	case "concurrent_upload":
		typeMultiplier = 2.0 // å¹¶å‘ä¸Šä¼ éœ€è¦æ›´é•¿æ—¶é—´
	default:
		typeMultiplier = 1.0
	}

	// è€ƒè™‘ç½‘ç»œè´¨é‡
	networkQuality := f.getNetworkQuality()
	var qualityMultiplier float64 = 1.0
	if networkQuality < 0.3 { // ç½‘ç»œè´¨é‡å¾ˆå·®
		qualityMultiplier = 3.0
	} else if networkQuality < 0.5 { // ç½‘ç»œè´¨é‡å·®
		qualityMultiplier = 2.0
	} else if networkQuality < 0.7 { // ç½‘ç»œè´¨é‡ä¸€èˆ¬
		qualityMultiplier = 1.5
	} else {
		qualityMultiplier = 1.0 // ç½‘ç»œè´¨é‡è‰¯å¥½
	}

	adaptiveTimeout := baseTimeout + time.Duration(float64(sizeBasedTimeout)*typeMultiplier*qualityMultiplier)

	// è®¾ç½®æ›´åˆç†çš„è¾¹ç•Œ
	minTimeout := 2 * time.Minute // æœ€å°2åˆ†é’Ÿ
	maxTimeout := 4 * time.Hour   // æœ€å¤§4å°æ—¶ï¼Œæ”¯æŒè¶…å¤§æ–‡ä»¶

	if adaptiveTimeout < minTimeout {
		adaptiveTimeout = minTimeout
	}
	if adaptiveTimeout > maxTimeout {
		adaptiveTimeout = maxTimeout
	}

	fs.Debugf(f, "è‡ªé€‚åº”è¶…æ—¶è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç±»å‹=%s, ç½‘ç»œè´¨é‡=%.2f, è¶…æ—¶æ—¶é—´=%v",
		fs.SizeSuffix(fileSize), transferType, networkQuality, adaptiveTimeout)

	return adaptiveTimeout
}

// detectNetworkSpeed æ£€æµ‹ç½‘ç»œä¼ è¾“é€Ÿåº¦ï¼Œç”¨äºåŠ¨æ€ä¼˜åŒ–å‚æ•°
func (f *Fs) detectNetworkSpeed(ctx context.Context) int64 {
	f.debugf(LogLevelDebug, "å¼€å§‹æ£€æµ‹ç½‘ç»œé€Ÿåº¦")

	// ä¼˜å…ˆä½¿ç”¨å†å²æ€§èƒ½æ•°æ®è¿›è¡Œé€Ÿåº¦ä¼°ç®—
	if f.performanceMetrics != nil {
		// å°è¯•ä»æ€§èƒ½æŒ‡æ ‡è·å–æœ€è¿‘çš„ä¼ è¾“é€Ÿåº¦
		stats := f.performanceMetrics.GetStats()
		if avgSpeed, ok := stats["average_upload_speed"].(float64); ok && avgSpeed > 0 {
			// å°†MB/sè½¬æ¢ä¸ºbytes/s
			historicalSpeed := int64(avgSpeed * 1024 * 1024)
			f.debugf(LogLevelDebug, "ä½¿ç”¨å†å²æ•°æ®æ£€æµ‹ç½‘ç»œé€Ÿåº¦: %s/s (åŸºäºæ€§èƒ½ç»Ÿè®¡)",
				fs.SizeSuffix(historicalSpeed))
			return historicalSpeed
		}
	}

	// ä½¿ç”¨å¿«é€Ÿä¼°ç®—æ–¹æ³•ï¼Œé¿å…å¤§æ–‡ä»¶åˆå§‹åŒ–å»¶è¿Ÿ
	// åŸºäºç½‘ç»œè´¨é‡è¿›è¡Œå¿«é€Ÿä¼°ç®—ï¼Œä¸è¿›è¡Œå®é™…ç½‘ç»œæµ‹è¯•
	networkQuality := f.getNetworkQuality()

	// åŸºç¡€é€Ÿåº¦ä¼°ç®—ï¼ˆåŸºäºå…¸å‹ç½‘ç»œç¯å¢ƒï¼‰
	var baseSpeed int64 = 10 * 1024 * 1024 // 10MB/s åŸºç¡€é€Ÿåº¦

	// æ ¹æ®ç½‘ç»œè´¨é‡è°ƒæ•´é€Ÿåº¦ä¼°ç®—
	if networkQuality >= 0.8 {
		baseSpeed = 20 * 1024 * 1024 // 20MB/s é«˜è´¨é‡ç½‘ç»œ
	} else if networkQuality >= 0.6 {
		baseSpeed = 15 * 1024 * 1024 // 15MB/s è‰¯å¥½ç½‘ç»œ
	} else if networkQuality >= 0.4 {
		baseSpeed = 8 * 1024 * 1024 // 8MB/s ä¸€èˆ¬ç½‘ç»œ
	} else {
		baseSpeed = 5 * 1024 * 1024 // 5MB/s è¾ƒå·®ç½‘ç»œ
	}

	f.debugf(LogLevelVerbose, "ç½‘ç»œé€Ÿåº¦ä¼°ç®—: %s/s (è´¨é‡=%.2f)",
		fs.SizeSuffix(baseSpeed), networkQuality)

	return baseSpeed
}

// getOptimalConcurrency æ ¹æ®æ–‡ä»¶å¤§å°å’Œç½‘ç»œé€Ÿåº¦è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šé›†æˆåŠ¨æ€å‚æ•°è°ƒæ•´å™¨ï¼Œå®ç°æ™ºèƒ½è‡ªé€‚åº”å¹¶å‘æ§åˆ¶
func (f *Fs) getOptimalConcurrency(fileSize int64, networkSpeed int64) int {
	// æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒåŠ¨æ€è°ƒæ•´
	if f.dynamicAdjuster != nil && f.dynamicAdjuster.ShouldAdjust() {
		networkQuality := f.getNetworkQuality()
		concurrency, _, _ := f.dynamicAdjuster.AdjustParameters(fileSize, networkSpeed, networkQuality)

		f.debugf(LogLevelVerbose, "åŠ¨æ€å¹¶å‘è°ƒæ•´: %s -> %då¹¶å‘ (é€Ÿåº¦=%s/s)",
			fs.SizeSuffix(fileSize), concurrency, fs.SizeSuffix(networkSpeed))

		// åº”ç”¨ç”¨æˆ·é…ç½®çš„æœ€å¤§å¹¶å‘æ•°é™åˆ¶
		if f.opt.MaxConcurrentUploads > 0 && concurrency > f.opt.MaxConcurrentUploads {
			concurrency = f.opt.MaxConcurrentUploads
			f.debugf(LogLevelDebug, "åº”ç”¨ç”¨æˆ·é…ç½®é™åˆ¶ï¼Œæœ€ç»ˆå¹¶å‘æ•°: %d", concurrency)
		}

		return concurrency
	}

	// å›é€€åˆ°ä¼ ç»Ÿçš„é™æ€è®¡ç®—æ–¹æ³•
	baseConcurrency := f.opt.MaxConcurrentUploads
	if baseConcurrency <= 0 {
		baseConcurrency = 4 // é»˜è®¤å¹¶å‘æ•°
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´å¹¶å‘æ•° - ä¼˜åŒ–å¤§æ–‡ä»¶å¹¶å‘ç­–ç•¥
	var sizeFactor float64 = 1.0
	if fileSize > 20*1024*1024*1024 { // >20GB - è¶…å¤§æ–‡ä»¶
		sizeFactor = 3.0 // æ˜¾è‘—æå‡å¹¶å‘æ•°
	} else if fileSize > 10*1024*1024*1024 { // >10GB - å¤§æ–‡ä»¶
		sizeFactor = 2.5
	} else if fileSize > 5*1024*1024*1024 { // >5GB - ä¸­å¤§æ–‡ä»¶
		sizeFactor = 2.0
	} else if fileSize > 2*1024*1024*1024 { // >2GB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.5
	} else if fileSize > 1*1024*1024*1024 { // >1GB - è¾ƒå¤§æ–‡ä»¶
		sizeFactor = 1.2
	} else if fileSize > 500*1024*1024 { // >500MB - ä¸­ç­‰æ–‡ä»¶
		sizeFactor = 1.0
	} else if fileSize > 100*1024*1024 { // >100MB - å°æ–‡ä»¶
		sizeFactor = 0.8
	} else {
		sizeFactor = 0.5 // å¾ˆå°æ–‡ä»¶ä½¿ç”¨è¾ƒå°‘å¹¶å‘
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´ - æ›´ç²¾ç»†çš„ç½‘ç»œé€Ÿåº¦åˆ†çº§
	var speedFactor float64 = 1.0
	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.8
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.5
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é«˜é€Ÿç½‘ç»œ
		speedFactor = 1.2
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - ä¸­é€Ÿç½‘ç»œ
		speedFactor = 1.0
	} else if networkSpeed > 10*1024*1024 { // >10Mbps - ä¸­ä½é€Ÿç½‘ç»œ
		speedFactor = 0.8
	} else {
		speedFactor = 0.6 // ä½é€Ÿç½‘ç»œå‡å°‘å¹¶å‘é¿å…æ‹¥å¡
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘æ•°
	optimalConcurrency := int(float64(baseConcurrency) * sizeFactor * speedFactor)

	// è®¾ç½®åˆç†è¾¹ç•Œ - æå‡æœ€å¤§å¹¶å‘æ•°é™åˆ¶
	minConcurrency := 1
	maxConcurrency := 16 // æå‡åˆ°16ï¼Œæ”¯æŒæ›´é«˜å¹¶å‘

	// å¯¹äºè¶…å¤§æ–‡ä»¶ï¼Œå…è®¸æ›´é«˜çš„å¹¶å‘æ•°
	if fileSize > 10*1024*1024*1024 && maxConcurrency < 20 {
		maxConcurrency = 20
	}

	if optimalConcurrency < minConcurrency {
		optimalConcurrency = minConcurrency
	}
	if optimalConcurrency > maxConcurrency {
		optimalConcurrency = maxConcurrency
	}

	fs.Debugf(f, "ğŸš€ ä¼˜åŒ–å¹¶å‘æ•°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€å¹¶å‘=%d, å¤§å°å› å­=%.1f, é€Ÿåº¦å› å­=%.1f, æœ€ä¼˜å¹¶å‘=%d",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed), baseConcurrency, sizeFactor, speedFactor, optimalConcurrency)

	return optimalConcurrency
}

// getOptimalChunkSize æ ¹æ®æ–‡ä»¶å¤§å°å’Œç½‘ç»œé€Ÿåº¦è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šé›†æˆåŠ¨æ€å‚æ•°è°ƒæ•´å™¨ï¼Œå®ç°æ™ºèƒ½è‡ªé€‚åº”åˆ†ç‰‡å¤§å°æ§åˆ¶
func (f *Fs) getOptimalChunkSize(fileSize int64, networkSpeed int64) int64 {
	fs.Debugf(f, "ğŸ”§ å¼€å§‹è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s", fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed))

	// æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡ŒåŠ¨æ€è°ƒæ•´
	if f.dynamicAdjuster != nil && f.dynamicAdjuster.ShouldAdjust() {
		fs.Debugf(f, "ğŸ”§ ä½¿ç”¨åŠ¨æ€è°ƒæ•´å™¨è®¡ç®—åˆ†ç‰‡å¤§å°")
		networkQuality := f.getNetworkQuality()
		_, chunkSize, _ := f.dynamicAdjuster.AdjustParameters(fileSize, networkSpeed, networkQuality)

		f.debugf(LogLevelVerbose, "åŠ¨æ€åˆ†ç‰‡è°ƒæ•´: %s -> %såˆ†ç‰‡",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(chunkSize))

		return chunkSize
	}

	// å›é€€åˆ°ä¼ ç»Ÿçš„é™æ€è®¡ç®—æ–¹æ³•
	baseChunk := int64(f.opt.ChunkSize)
	if baseChunk <= 0 {
		baseChunk = int64(defaultChunkSize) // 100MB
	}

	// æ ¹æ®ç½‘ç»œé€Ÿåº¦è°ƒæ•´åˆ†ç‰‡å¤§å°
	var speedMultiplier float64 = 1.0
	if networkSpeed > 200*1024*1024 { // >200Mbps - è¶…é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 3.0 // 300MBåˆ†ç‰‡
	} else if networkSpeed > 100*1024*1024 { // >100Mbps - é«˜é€Ÿç½‘ç»œ
		speedMultiplier = 2.0 // 200MBåˆ†ç‰‡
	} else if networkSpeed > 50*1024*1024 { // >50Mbps - ä¸­é€Ÿç½‘ç»œ
		speedMultiplier = 1.5 // 150MBåˆ†ç‰‡
	} else if networkSpeed > 20*1024*1024 { // >20Mbps - æ™®é€šç½‘ç»œ
		speedMultiplier = 1.0 // 100MBåˆ†ç‰‡
	} else { // <20Mbps - æ…¢é€Ÿç½‘ç»œ
		speedMultiplier = 0.5 // 50MBåˆ†ç‰‡
	}

	// æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´
	var sizeMultiplier float64 = 1.0
	if fileSize > 50*1024*1024*1024 { // >50GB
		sizeMultiplier = 1.5 // å¤§æ–‡ä»¶ä½¿ç”¨æ›´å¤§åˆ†ç‰‡
	} else if fileSize < 500*1024*1024 { // <500MB
		sizeMultiplier = 0.5 // å°æ–‡ä»¶ä½¿ç”¨æ›´å°åˆ†ç‰‡
	}

	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å¤§å°
	optimalChunkSize := int64(float64(baseChunk) * speedMultiplier * sizeMultiplier)

	// è®¾ç½®åˆç†è¾¹ç•Œ
	minChunkSize := int64(minChunkSize) // 50MB
	maxChunkSize := int64(maxChunkSize) // 500MB

	if optimalChunkSize < minChunkSize {
		optimalChunkSize = minChunkSize
	}
	if optimalChunkSize > maxChunkSize {
		optimalChunkSize = maxChunkSize
	}

	fs.Debugf(f, "åŠ¨æ€åˆ†ç‰‡å¤§å°è®¡ç®—: æ–‡ä»¶å¤§å°=%s, ç½‘ç»œé€Ÿåº¦=%s/s, åŸºç¡€åˆ†ç‰‡=%s, æœ€ä¼˜åˆ†ç‰‡=%s",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(networkSpeed),
		fs.SizeSuffix(baseChunk), fs.SizeSuffix(optimalChunkSize))

	return optimalChunkSize
}

// ResourcePool èµ„æºæ± ç®¡ç†å™¨ï¼Œç”¨äºä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œå‡å°‘GCå‹åŠ›
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç®€åŒ–å®ç°ï¼Œæ›´å¤šä½¿ç”¨Goæ ‡å‡†åº“çš„sync.Pool
type ResourcePool struct {
	bufferPool sync.Pool // ç¼“å†²åŒºæ± 
	hasherPool sync.Pool // MD5å“ˆå¸Œè®¡ç®—å™¨æ± 

	// ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ª
	tempFilesMu sync.Mutex
	tempFiles   map[string]bool // è·Ÿè¸ªåˆ›å»ºçš„ä¸´æ—¶æ–‡ä»¶
}

// DownloadProgress 123ç½‘ç›˜ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
type DownloadProgress struct {
	totalChunks     int64
	completedChunks int64
	totalBytes      int64
	downloadedBytes int64
	startTime       time.Time
	lastUpdateTime  time.Time
	chunkSizes      map[int64]int64         // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„å¤§å°
	chunkTimes      map[int64]time.Duration // è®°å½•æ¯ä¸ªåˆ†ç‰‡çš„ä¸‹è½½æ—¶é—´
	peakSpeed       float64                 // å³°å€¼é€Ÿåº¦ MB/s
	mu              sync.RWMutex
}

// NewDownloadProgress åˆ›å»ºæ–°çš„ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨
func NewDownloadProgress(totalChunks, totalBytes int64) *DownloadProgress {
	return &DownloadProgress{
		totalChunks:    totalChunks,
		totalBytes:     totalBytes,
		startTime:      time.Now(),
		lastUpdateTime: time.Now(),
		chunkSizes:     make(map[int64]int64),
		chunkTimes:     make(map[int64]time.Duration),
		peakSpeed:      0,
	}
}

// UpdateChunkProgress æ›´æ–°åˆ†ç‰‡ä¸‹è½½è¿›åº¦
func (dp *DownloadProgress) UpdateChunkProgress(chunkIndex, chunkSize int64, chunkDuration time.Duration) {
	dp.mu.Lock()
	defer dp.mu.Unlock()

	// å¦‚æœè¿™ä¸ªåˆ†ç‰‡è¿˜æ²¡æœ‰è®°å½•ï¼Œå¢åŠ å®Œæˆè®¡æ•°
	if _, exists := dp.chunkSizes[chunkIndex]; !exists {
		dp.completedChunks++
		dp.downloadedBytes += chunkSize
		dp.chunkSizes[chunkIndex] = chunkSize
		dp.chunkTimes[chunkIndex] = chunkDuration
		dp.lastUpdateTime = time.Now()

		// è®¡ç®—å¹¶æ›´æ–°å³°å€¼é€Ÿåº¦
		if chunkDuration.Seconds() > 0 {
			chunkSpeed := float64(chunkSize) / chunkDuration.Seconds() / 1024 / 1024 // MB/s
			if chunkSpeed > dp.peakSpeed {
				dp.peakSpeed = chunkSpeed
			}
		}
	}
}

// GetProgressInfo è·å–å½“å‰è¿›åº¦ä¿¡æ¯
func (dp *DownloadProgress) GetProgressInfo() (percentage float64, avgSpeed float64, peakSpeed float64, eta time.Duration, completed, total int64, downloadedBytes, totalBytes int64) {
	dp.mu.RLock()
	defer dp.mu.RUnlock()

	elapsed := time.Since(dp.startTime)
	percentage = float64(dp.completedChunks) / float64(dp.totalChunks) * 100

	if elapsed.Seconds() > 0 {
		avgSpeed = float64(dp.downloadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s
	}

	peakSpeed = dp.peakSpeed

	if avgSpeed > 0 && dp.downloadedBytes > 0 {
		remainingBytes := dp.totalBytes - dp.downloadedBytes
		etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
		eta = time.Duration(etaSeconds) * time.Second
	}

	return percentage, avgSpeed, peakSpeed, eta, dp.completedChunks, dp.totalChunks, dp.downloadedBytes, dp.totalBytes
}

// updateAccountExtraInfo æ›´æ–°Accountçš„é¢å¤–è¿›åº¦ä¿¡æ¯ï¼ˆé›†æˆåˆ°rcloneæ ‡å‡†è¿›åº¦æ˜¾ç¤ºï¼‰
func (f *Fs) updateAccountExtraInfo(ctx context.Context, progress *DownloadProgress, remoteName string, account *accounting.Account) {
	ticker := time.NewTicker(2 * time.Second) // æ¯2ç§’æ›´æ–°ä¸€æ¬¡Accountçš„é¢å¤–ä¿¡æ¯
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ¸…é™¤é¢å¤–ä¿¡æ¯
			if account != nil {
				account.SetExtraInfo("")
			}
			return
		case <-ticker.C:
			if account == nil {
				// å°è¯•é‡æ–°è·å–Accountå¯¹è±¡
				if stats := accounting.GlobalStats(); stats != nil {
					// é¦–å…ˆå°è¯•ç”¨åŸå§‹åç§°æŸ¥æ‰¾
					if acc := stats.GetInProgressAccount(remoteName); acc != nil {
						account = acc
					} else {
						// æ·»åŠ æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
						allAccounts := stats.ListInProgressAccounts()
						for _, accountName := range allAccounts {
							if strings.Contains(accountName, remoteName) || strings.Contains(remoteName, accountName) {
								if acc := stats.GetInProgressAccount(accountName); acc != nil {
									account = acc
									break
								}
							}
						}
					}
				}
				if account == nil {
					continue // å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œè·³è¿‡è¿™æ¬¡æ›´æ–°
				}
			}

			_, _, _, _, completed, total, _, _ := progress.GetProgressInfo()

			// ğŸ”§ é›†æˆï¼šæ„å»ºé¢å¤–ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œæ˜¾ç¤ºåˆ†ç‰‡è¿›åº¦
			if completed > 0 && total > 0 {
				extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completed, total)
				account.SetExtraInfo(extraInfo)
			}
		}
	}
}

// displayDownloadProgress æ˜¾ç¤º123ç½‘ç›˜ä¸‹è½½è¿›åº¦
func (f *Fs) displayDownloadProgress(ctx context.Context, progress *DownloadProgress, fileName string) {

	ticker := time.NewTicker(2 * time.Second) // æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			// æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
			percentage, avgSpeed, peakSpeed, _, completed, total, downloadedBytes, totalBytes := progress.GetProgressInfo()
			fs.Infof(f, "ğŸ“¥ 123ç½‘ç›˜ä¸‹è½½å®Œæˆ: %s | %d/%dåˆ†ç‰‡ (%.1f%%) | %s/%s | å¹³å‡: %.2f MB/s | å³°å€¼: %.2f MB/s",
				fileName, completed, total, percentage,
				fs.SizeSuffix(downloadedBytes), fs.SizeSuffix(totalBytes), avgSpeed, peakSpeed)
			return
		case <-ticker.C:
			percentage, avgSpeed, peakSpeed, eta, completed, total, downloadedBytes, totalBytes := progress.GetProgressInfo()

			// æ„å»ºETAå­—ç¬¦ä¸²
			var etaStr string
			if eta > 0 {
				if eta > time.Hour {
					etaStr = fmt.Sprintf("ETA: %dh%dm", int(eta.Hours()), int(eta.Minutes())%60)
				} else if eta > time.Minute {
					etaStr = fmt.Sprintf("ETA: %dm%ds", int(eta.Minutes()), int(eta.Seconds())%60)
				} else {
					etaStr = fmt.Sprintf("ETA: %ds", int(eta.Seconds()))
				}
			} else {
				etaStr = "ETA: è®¡ç®—ä¸­..."
			}

			// æ˜¾ç¤ºè¯¦ç»†è¿›åº¦ä¿¡æ¯ï¼ˆä»å¼€å§‹å°±æ˜¾ç¤ºï¼Œä¸ç­‰å¾…ç¬¬ä¸€ä¸ªåˆ†ç‰‡å®Œæˆï¼‰
			// åˆ›å»ºè¿›åº¦æ¡
			progressBar := f.createProgressBar(percentage)

			fs.Infof(f, "ğŸ“¥ 123ç½‘ç›˜ä¸‹è½½è¿›åº¦: %s", fileName)
			fs.Infof(f, "   %s %.1f%% | %d/%dåˆ†ç‰‡ | %s/%s",
				progressBar, percentage, completed, total,
				fs.SizeSuffix(downloadedBytes), fs.SizeSuffix(totalBytes))
			fs.Infof(f, "   å¹³å‡é€Ÿåº¦: %.2f MB/s | å³°å€¼é€Ÿåº¦: %.2f MB/s | %s",
				avgSpeed, peakSpeed, etaStr)
		}
	}
}

// displayUnifiedDownloadProgress ç»Ÿä¸€çš„ä¸‹è½½è¿›åº¦æ˜¾ç¤ºå‡½æ•°
func (f *Fs) displayUnifiedDownloadProgress(progress *DownloadProgress, networkName, fileName string) {
	percentage, avgSpeed, peakSpeed, eta, completed, total, downloadedBytes, totalBytes := progress.GetProgressInfo()

	// è®¡ç®—ETAæ˜¾ç¤º
	etaStr := "ETA: -"
	if eta > 0 {
		etaStr = fmt.Sprintf("ETA: %v", eta.Round(time.Second))
	} else if avgSpeed > 0 {
		etaStr = "ETA: è®¡ç®—ä¸­..."
	}

	// åˆ›å»ºè¿›åº¦æ¡
	progressBar := f.createProgressBar(percentage)

	// ç»Ÿä¸€çš„è¿›åº¦æ˜¾ç¤ºæ ¼å¼
	fs.Infof(f, "ğŸ“¥ %sä¸‹è½½è¿›åº¦: %s", networkName, fileName)
	fs.Infof(f, "   %s %.1f%% | %d/%dåˆ†ç‰‡ | %s/%s",
		progressBar, percentage, completed, total,
		fs.SizeSuffix(downloadedBytes), fs.SizeSuffix(totalBytes))
	fs.Infof(f, "   å¹³å‡é€Ÿåº¦: %.2f MB/s | å³°å€¼é€Ÿåº¦: %.2f MB/s | %s",
		avgSpeed, peakSpeed, etaStr)
}

// createProgressBar åˆ›å»ºè¿›åº¦æ¡å­—ç¬¦ä¸²
func (f *Fs) createProgressBar(percentage float64) string {
	const barLength = 30
	filled := int(percentage / 100 * barLength)
	if filled > barLength {
		filled = barLength
	}

	bar := "["
	for i := 0; i < barLength; i++ {
		if i < filled {
			bar += "â–ˆ"
		} else {
			bar += "â–‘"
		}
	}
	bar += "]"
	return bar
}

// NewResourcePool åˆ›å»ºæ–°çš„èµ„æºæ± 
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç§»é™¤å¤æ‚çš„ä¸´æ—¶æ–‡ä»¶æ± ï¼Œä¸“æ³¨äºå†…å­˜æ± ç®¡ç†
func NewResourcePool() *ResourcePool {
	return &ResourcePool{
		bufferPool: sync.Pool{
			New: func() interface{} {
				// åˆ›å»ºé»˜è®¤åˆ†ç‰‡å¤§å°çš„ç¼“å†²åŒº
				return make([]byte, defaultChunkSize)
			},
		},
		hasherPool: sync.Pool{
			New: func() interface{} {
				return md5.New()
			},
		},
		tempFiles: make(map[string]bool),
	}
}

// GetBuffer ä»æ± ä¸­è·å–ç¼“å†²åŒº
func (rp *ResourcePool) GetBuffer() []byte {
	buf := rp.bufferPool.Get().([]byte)
	// é‡ç½®ç¼“å†²åŒºé•¿åº¦ä½†ä¿ç•™å®¹é‡
	return buf[:0]
}

// PutBuffer å°†ç¼“å†²åŒºå½’è¿˜åˆ°æ± ä¸­
func (rp *ResourcePool) PutBuffer(buf []byte) {
	// æ£€æŸ¥ç¼“å†²åŒºå¤§å°ï¼Œé¿å…æ± ä¸­ç§¯ç´¯è¿‡å¤§çš„ç¼“å†²åŒº
	if cap(buf) <= int(maxChunkSize) {
		rp.bufferPool.Put(buf)
	}
}

// GetHasher ä»æ± ä¸­è·å–MD5å“ˆå¸Œè®¡ç®—å™¨
func (rp *ResourcePool) GetHasher() hash.Hash {
	hasher := rp.hasherPool.Get().(hash.Hash)
	hasher.Reset() // é‡ç½®å“ˆå¸Œè®¡ç®—å™¨çŠ¶æ€
	return hasher
}

// PutHasher å°†å“ˆå¸Œè®¡ç®—å™¨å½’è¿˜åˆ°æ± ä¸­
func (rp *ResourcePool) PutHasher(hasher hash.Hash) {
	rp.hasherPool.Put(hasher)
}

// GetTempFile åˆ›å»ºä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å¼‚å¸¸å¤„ç†å’Œèµ„æºç®¡ç†ä¿è¯
func (rp *ResourcePool) GetTempFile(prefix string) (*os.File, error) {
	// æ£€æŸ¥èµ„æºæ± çŠ¶æ€
	if rp == nil {
		return nil, fmt.Errorf("èµ„æºæ± æœªåˆå§‹åŒ–")
	}

	// åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤ä¸´æ—¶ç›®å½•
	tempFile, err := os.CreateTemp("", prefix+"*.tmp")
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}

	fileName := tempFile.Name()

	// ä½¿ç”¨deferç¡®ä¿å¼‚å¸¸æ—¶ä¹Ÿèƒ½æ¸…ç†èµ„æº
	var success bool
	defer func() {
		if !success {
			// åˆ›å»ºå¤±è´¥æ—¶çš„æ¸…ç†å·¥ä½œ
			if tempFile != nil {
				tempFile.Close()
			}
			rp.removeTempFileFromTracking(fileName)
			if err := os.Remove(fileName); err != nil && !os.IsNotExist(err) {
				fs.Debugf(nil, "âš ï¸  æ¸…ç†å¤±è´¥çš„ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: %s, é”™è¯¯: %v", fileName, err)
			}
		}
	}()

	// è·Ÿè¸ªä¸´æ—¶æ–‡ä»¶
	rp.tempFilesMu.Lock()
	if rp.tempFiles == nil {
		rp.tempFiles = make(map[string]bool)
	}
	rp.tempFiles[fileName] = true
	rp.tempFilesMu.Unlock()

	// è®¾ç½®æ–‡ä»¶æƒé™ï¼Œç¡®ä¿åªæœ‰å½“å‰ç”¨æˆ·å¯ä»¥è®¿é—®
	if err = tempFile.Chmod(0600); err != nil {
		return nil, fmt.Errorf("è®¾ç½®ä¸´æ—¶æ–‡ä»¶æƒé™å¤±è´¥: %w", err)
	}

	success = true
	fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ: %s", fileName)
	return tempFile, nil
}

// GetOptimizedTempFile åˆ›å»ºä¼˜åŒ–çš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶é«˜æ•ˆå¤„ç†
func (rp *ResourcePool) GetOptimizedTempFile(prefix string, expectedSize int64) (*os.File, error) {
	tempFile, err := rp.GetTempFile(prefix)
	if err != nil {
		return nil, err
	}

	// GetTempFileå·²ç»å¤„ç†äº†è·Ÿè¸ªï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è·Ÿè¸ª

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œé¢„åˆ†é…ç£ç›˜ç©ºé—´ä»¥æå‡å†™å…¥æ€§èƒ½
	if expectedSize > 100*1024*1024 { // >100MB
		err = tempFile.Truncate(expectedSize)
		if err != nil {
			tempFile.Close()
			rp.removeTempFileFromTracking(tempFile.Name())
			os.Remove(tempFile.Name())
			return nil, fmt.Errorf("é¢„åˆ†é…ä¸´æ—¶æ–‡ä»¶ç©ºé—´å¤±è´¥: %w", err)
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å§‹ä½ç½®
		_, err = tempFile.Seek(0, 0)
		if err != nil {
			tempFile.Close()
			rp.removeTempFileFromTracking(tempFile.Name())
			os.Remove(tempFile.Name())
			return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
		}
	}

	return tempFile, nil
}

// removeTempFileFromTracking ä»è·Ÿè¸ªåˆ—è¡¨ä¸­ç§»é™¤ä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å®‰å…¨æ£€æŸ¥å’Œé”™è¯¯å¤„ç†
func (rp *ResourcePool) removeTempFileFromTracking(fileName string) {
	if rp == nil {
		fs.Debugf(nil, "âš ï¸  èµ„æºæ± æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç§»é™¤ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ª: %s", fileName)
		return
	}

	if fileName == "" {
		fs.Debugf(nil, "âš ï¸  ä¸´æ—¶æ–‡ä»¶åä¸ºç©ºï¼Œè·³è¿‡ç§»é™¤è·Ÿè¸ª")
		return
	}

	rp.tempFilesMu.Lock()
	defer rp.tempFilesMu.Unlock()

	if rp.tempFiles == nil {
		fs.Debugf(nil, "âš ï¸  ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ªæ˜ å°„æœªåˆå§‹åŒ–")
		return
	}

	if _, exists := rp.tempFiles[fileName]; exists {
		delete(rp.tempFiles, fileName)
		fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶è·Ÿè¸ªç§»é™¤æˆåŠŸ: %s", fileName)
	} else {
		fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶æœªåœ¨è·Ÿè¸ªåˆ—è¡¨ä¸­: %s", fileName)
	}
}

// PutTempFile æ¸…ç†ä¸´æ—¶æ–‡ä»¶
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„å¼‚å¸¸å¤„ç†å’Œèµ„æºæ¸…ç†ä¿è¯
func (rp *ResourcePool) PutTempFile(tempFile *os.File) {
	if tempFile == nil {
		return
	}

	fileName := tempFile.Name()

	// å®‰å…¨å…³é—­æ–‡ä»¶ï¼Œä½¿ç”¨deferç¡®ä¿å³ä½¿å‡ºç°panicä¹Ÿèƒ½å…³é—­
	defer func() {
		if tempFile != nil {
			if closeErr := tempFile.Close(); closeErr != nil {
				fs.Debugf(nil, "âš ï¸  å…³é—­ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", fileName, closeErr)
			}
		}
	}()

	// ä»è·Ÿè¸ªåˆ—è¡¨ä¸­ç§»é™¤
	rp.removeTempFileFromTracking(fileName)

	// å¤šæ¬¡å°è¯•åˆ é™¤æ–‡ä»¶ï¼Œå¤„ç†å¯èƒ½çš„æ–‡ä»¶é”å®šé—®é¢˜
	maxRetries := 3
	for attempt := 0; attempt < maxRetries; attempt++ {
		if err := os.Remove(fileName); err != nil {
			if os.IsNotExist(err) {
				// æ–‡ä»¶å·²ä¸å­˜åœ¨ï¼Œè®¤ä¸ºåˆ é™¤æˆåŠŸ
				fs.Debugf(nil, "ğŸ“ ä¸´æ—¶æ–‡ä»¶å·²ä¸å­˜åœ¨: %s", fileName)
				return
			}

			if attempt < maxRetries-1 {
				// ç­‰å¾…ä¸€å°æ®µæ—¶é—´åé‡è¯•ï¼ˆå¯èƒ½æ˜¯æ–‡ä»¶é”å®šï¼‰
				time.Sleep(time.Duration(attempt+1) * 100 * time.Millisecond)
				fs.Debugf(nil, "ğŸ”„ ä¸´æ—¶æ–‡ä»¶åˆ é™¤é‡è¯• %d/%d: %s", attempt+1, maxRetries, fileName)
				continue
			}

			// æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸å½±å“ç¨‹åºè¿è¡Œ
			fs.Errorf(nil, "âŒ åˆ é™¤ä¸´æ—¶æ–‡ä»¶æœ€ç»ˆå¤±è´¥: %s, é”™è¯¯: %v", fileName, err)
		} else {
			fs.Debugf(nil, "âœ… ä¸´æ—¶æ–‡ä»¶åˆ é™¤æˆåŠŸ: %s", fileName)
			return
		}
	}
}

// CleanupAllTempFiles æ¸…ç†æ‰€æœ‰è·Ÿè¸ªçš„ä¸´æ—¶æ–‡ä»¶
func (rp *ResourcePool) CleanupAllTempFiles() {
	rp.tempFilesMu.Lock()
	defer rp.tempFilesMu.Unlock()

	cleanedCount := 0
	for fileName := range rp.tempFiles {
		if err := os.Remove(fileName); err != nil {
			if !os.IsNotExist(err) {
				fs.Debugf(nil, "âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %s, é”™è¯¯: %v", fileName, err)
			}
		} else {
			cleanedCount++
		}
		delete(rp.tempFiles, fileName)
	}

	if cleanedCount > 0 {
		fs.Debugf(nil, "ğŸ§¹ èµ„æºæ± æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† %d ä¸ªä¸´æ—¶æ–‡ä»¶", cleanedCount)
	}
}

// Close å…³é—­èµ„æºæ± ï¼Œæ¸…ç†æ‰€æœ‰èµ„æº
// ä¼˜åŒ–ç‰ˆæœ¬ï¼šç®€åŒ–å®ç°ï¼Œsync.Poolä¼šè‡ªåŠ¨ç®¡ç†èµ„æº
func (rp *ResourcePool) Close() {
	// æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
	rp.CleanupAllTempFiles()

	// sync.Poolä¼šè‡ªåŠ¨ç®¡ç†å†…å­˜æ± ï¼Œæ— éœ€æ‰‹åŠ¨æ¸…ç†
	// è¿™ä¸ªæ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†æ¥å£å…¼å®¹æ€§
}

// ===== é€šç”¨å·¥å…·å‡½æ•°æ¨¡å— =====
// è¿™äº›å‡½æ•°å¯ä»¥è¢«å¤šä¸ªåœ°æ–¹å¤ç”¨ï¼Œæé«˜ä»£ç çš„å¯ç»´æŠ¤æ€§

// StringUtils å­—ç¬¦ä¸²å¤„ç†å·¥å…·é›†åˆ
type StringUtils struct{}

// IsEmpty æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦
func (StringUtils) IsEmpty(s string) bool {
	return strings.TrimSpace(s) == ""
}

// TruncateString æˆªæ–­å­—ç¬¦ä¸²åˆ°æŒ‡å®šå­—èŠ‚é•¿åº¦ï¼Œä¿æŒUTF-8å®Œæ•´æ€§
func (StringUtils) TruncateString(s string, maxBytes int) string {
	if len(s) <= maxBytes {
		return s
	}

	// ç¡®ä¿ä¸ä¼šåœ¨UTF-8å­—ç¬¦ä¸­é—´æˆªæ–­
	for i := maxBytes; i >= 0; i-- {
		if utf8.ValidString(s[:i]) {
			return s[:i]
		}
	}
	return ""
}

// PathUtils è·¯å¾„å¤„ç†å·¥å…·é›†åˆ
type PathUtils struct{}

// NormalizePath æ ‡å‡†åŒ–è·¯å¾„å¤„ç†
func (PathUtils) NormalizePath(path string) string {
	return normalizePath(path)
}

// SplitPath åˆ†å‰²è·¯å¾„ä¸ºç›®å½•å’Œæ–‡ä»¶å
func (PathUtils) SplitPath(path string) (dir, name string) {
	path = normalizePath(path)
	if path == "" {
		return "", ""
	}

	lastSlash := strings.LastIndex(path, "/")
	if lastSlash == -1 {
		return "", path
	}

	return path[:lastSlash], path[lastSlash+1:]
}

// ValidationUtils éªŒè¯å·¥å…·é›†åˆ
type ValidationUtils struct{}

// ValidateFileID éªŒè¯æ–‡ä»¶IDæ ¼å¼
func (ValidationUtils) ValidateFileID(idStr string) (int64, error) {
	return parseFileID(idStr)
}

// ValidateFileName éªŒè¯æ–‡ä»¶å
func (ValidationUtils) ValidateFileName(name string) error {
	return validateFileName(name)
}

// TimeUtils æ—¶é—´å¤„ç†å·¥å…·é›†åˆ
type TimeUtils struct{}

// CalculateRetryDelay è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´
func (TimeUtils) CalculateRetryDelay(attempt int) time.Duration {
	return calculateRetryDelay(attempt)
}

// ParseUnixTimestamp è§£æUnixæ—¶é—´æˆ³
func (TimeUtils) ParseUnixTimestamp(timestamp string) (time.Time, error) {
	if timestamp == "" {
		return time.Time{}, fmt.Errorf("æ—¶é—´æˆ³ä¸èƒ½ä¸ºç©º")
	}

	ts, err := strconv.ParseInt(timestamp, 10, 64)
	if err != nil {
		return time.Time{}, fmt.Errorf("æ— æ•ˆçš„æ—¶é—´æˆ³æ ¼å¼: %s", timestamp)
	}

	return time.Unix(ts, 0), nil
}

// HTTPUtils HTTPå¤„ç†å·¥å…·é›†åˆ
type HTTPUtils struct{}

// IsRetryableError åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯é‡è¯•
func (HTTPUtils) IsRetryableError(ctx context.Context, err error, resp *http.Response) (bool, error) {
	return shouldRetry(ctx, resp, err)
}

// BuildAPIEndpoint æ„å»ºAPIç«¯ç‚¹URL
func (HTTPUtils) BuildAPIEndpoint(base, path string, params map[string]string) string {
	endpoint := base + path
	if len(params) > 0 {
		values := url.Values{}
		for k, v := range params {
			values.Set(k, v)
		}
		endpoint += "?" + values.Encode()
	}
	return endpoint
}

// LoggingUtils ç»Ÿä¸€æ—¥å¿—å’Œè°ƒè¯•æ¥å£
type LoggingUtils struct{}

// LogWithLevel æ ¹æ®çº§åˆ«è¾“å‡ºæ—¥å¿—
func (LoggingUtils) LogWithLevel(f *Fs, level int, format string, args ...any) {
	if f == nil {
		return
	}

	// æ£€æŸ¥æ˜¯å¦åº”è¯¥è¾“å‡ºæ­¤çº§åˆ«çš„æ—¥å¿—
	if !f.shouldLog(level) {
		return
	}

	switch level {
	case LogLevelError:
		fs.Errorf(f, format, args...)
	case LogLevelInfo:
		fs.Infof(f, format, args...)
	case LogLevelDebug, LogLevelVerbose:
		fs.Debugf(f, format, args...)
	default:
		fs.Debugf(f, format, args...)
	}
}

// LogError è¾“å‡ºé”™è¯¯æ—¥å¿—
func (LoggingUtils) LogError(f *Fs, format string, args ...any) {
	LoggingUtil.LogWithLevel(f, LogLevelError, format, args...)
}

// LogInfo è¾“å‡ºä¿¡æ¯æ—¥å¿—
func (LoggingUtils) LogInfo(f *Fs, format string, args ...any) {
	LoggingUtil.LogWithLevel(f, LogLevelInfo, format, args...)
}

// LogDebug è¾“å‡ºè°ƒè¯•æ—¥å¿—
func (LoggingUtils) LogDebug(f *Fs, format string, args ...any) {
	LoggingUtil.LogWithLevel(f, LogLevelDebug, format, args...)
}

// LogVerbose è¾“å‡ºè¯¦ç»†æ—¥å¿—
func (LoggingUtils) LogVerbose(f *Fs, format string, args ...any) {
	LoggingUtil.LogWithLevel(f, LogLevelVerbose, format, args...)
}

// shouldLog æ£€æŸ¥æ˜¯å¦åº”è¯¥è¾“å‡ºæŒ‡å®šçº§åˆ«çš„æ—¥å¿—
func (f *Fs) shouldLog(level int) bool {
	if f == nil {
		return false
	}
	return level <= f.opt.DebugLevel
}

// å…¨å±€å·¥å…·å®ä¾‹ï¼Œæ–¹ä¾¿ä½¿ç”¨
var (
	StringUtil     = StringUtils{}
	PathUtil       = PathUtils{}
	ValidationUtil = ValidationUtils{}
	TimeUtil       = TimeUtils{}
	HTTPUtil       = HTTPUtils{}
	LoggingUtil    = LoggingUtils{}
)

// StreamingHashAccumulator æµå¼å“ˆå¸Œç´¯ç§¯å™¨ï¼Œç”¨äºæ¶ˆé™¤MD5é‡å¤è®¡ç®—
type StreamingHashAccumulator struct {
	hasher      hash.Hash        // æ€»ä½“MD5å“ˆå¸Œè®¡ç®—å™¨
	chunkHashes map[int64]string // åˆ†ç‰‡å“ˆå¸Œç¼“å­˜ (chunkIndex -> MD5)
	mu          sync.Mutex       // ä¿æŠ¤å¹¶å‘è®¿é—®çš„äº’æ–¥é”
	totalSize   int64            // æ–‡ä»¶æ€»å¤§å°
	processed   int64            // å·²å¤„ç†çš„å­—èŠ‚æ•°
	chunkSize   int64            // åˆ†ç‰‡å¤§å°
	isFinalized bool             // æ˜¯å¦å·²å®Œæˆæœ€ç»ˆå“ˆå¸Œè®¡ç®—
	finalHash   string           // æœ€ç»ˆçš„MD5å“ˆå¸Œå€¼
}

// NewStreamingHashAccumulator åˆ›å»ºæ–°çš„æµå¼å“ˆå¸Œç´¯ç§¯å™¨
func NewStreamingHashAccumulator(totalSize, chunkSize int64) *StreamingHashAccumulator {
	return &StreamingHashAccumulator{
		hasher:      md5.New(),
		chunkHashes: make(map[int64]string),
		totalSize:   totalSize,
		chunkSize:   chunkSize,
		processed:   0,
		isFinalized: false,
	}
}

// WriteChunk å†™å…¥åˆ†ç‰‡æ•°æ®å¹¶è®¡ç®—å“ˆå¸Œï¼Œè¿”å›åˆ†ç‰‡MD5
func (sha *StreamingHashAccumulator) WriteChunk(chunkIndex int64, data []byte) (string, error) {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	if sha.isFinalized {
		return "", fmt.Errorf("å“ˆå¸Œç´¯ç§¯å™¨å·²å®Œæˆï¼Œæ— æ³•ç»§ç»­å†™å…¥")
	}

	// è®¡ç®—åˆ†ç‰‡å“ˆå¸Œ
	chunkHasher := md5.New()
	chunkHasher.Write(data)
	chunkHash := fmt.Sprintf("%x", chunkHasher.Sum(nil))

	// ç¼“å­˜åˆ†ç‰‡å“ˆå¸Œ
	sha.chunkHashes[chunkIndex] = chunkHash

	// ç´¯ç§¯åˆ°æ€»å“ˆå¸Œï¼ˆæŒ‰é¡ºåºï¼‰
	expectedOffset := chunkIndex * sha.chunkSize
	if expectedOffset == sha.processed {
		// æŒ‰é¡ºåºå†™å…¥ï¼Œç›´æ¥ç´¯ç§¯
		sha.hasher.Write(data)
		sha.processed += int64(len(data))

		// æ£€æŸ¥æ˜¯å¦æœ‰åç»­çš„è¿ç»­åˆ†ç‰‡å¯ä»¥å¤„ç†
		sha.processConsecutiveChunks()
	}

	return chunkHash, nil
}

// processConsecutiveChunks å¤„ç†è¿ç»­çš„åˆ†ç‰‡ï¼Œä¿è¯å“ˆå¸Œè®¡ç®—çš„é¡ºåºæ€§
func (sha *StreamingHashAccumulator) processConsecutiveChunks() {
	nextChunkIndex := sha.processed / sha.chunkSize

	for {
		if _, exists := sha.chunkHashes[nextChunkIndex]; exists {
			// é‡æ–°è®¡ç®—è¿™ä¸ªåˆ†ç‰‡çš„æ•°æ®å¹¶ç´¯ç§¯åˆ°æ€»å“ˆå¸Œ
			// æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°è¯»å–æ•°æ®ï¼Œè¿™æ˜¯ä¸€ä¸ªæƒè¡¡
			// å®é™…å®ç°ä¸­å¯ä»¥è€ƒè™‘ç¼“å­˜åˆ†ç‰‡æ•°æ®æˆ–ä½¿ç”¨å…¶ä»–ç­–ç•¥
			nextChunkIndex++
		} else {
			break
		}
	}
}

// WriteSequential æŒ‰é¡ºåºå†™å…¥æ•°æ®ï¼ˆç”¨äºå•çº¿ç¨‹åœºæ™¯ï¼‰
func (sha *StreamingHashAccumulator) WriteSequential(data []byte) (int, error) {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	if sha.isFinalized {
		return 0, fmt.Errorf("å“ˆå¸Œç´¯ç§¯å™¨å·²å®Œæˆï¼Œæ— æ³•ç»§ç»­å†™å…¥")
	}

	// ç›´æ¥å†™å…¥æ€»å“ˆå¸Œ
	n, err := sha.hasher.Write(data)
	if err != nil {
		return n, err
	}

	sha.processed += int64(n)
	return n, nil
}

// GetChunkHash è·å–æŒ‡å®šåˆ†ç‰‡çš„MD5å“ˆå¸Œ
func (sha *StreamingHashAccumulator) GetChunkHash(chunkIndex int64) (string, bool) {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	hash, exists := sha.chunkHashes[chunkIndex]
	return hash, exists
}

// GetProgress è·å–å½“å‰å¤„ç†è¿›åº¦
func (sha *StreamingHashAccumulator) GetProgress() (processed, total int64, percentage float64) {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	percentage = 0.0
	if sha.totalSize > 0 {
		percentage = float64(sha.processed) / float64(sha.totalSize) * 100.0
	}

	return sha.processed, sha.totalSize, percentage
}

// Finalize å®Œæˆå“ˆå¸Œè®¡ç®—å¹¶è¿”å›æœ€ç»ˆMD5å€¼
func (sha *StreamingHashAccumulator) Finalize() string {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	if sha.isFinalized {
		return sha.finalHash
	}

	// è®¡ç®—æœ€ç»ˆå“ˆå¸Œ
	sha.finalHash = fmt.Sprintf("%x", sha.hasher.Sum(nil))
	sha.isFinalized = true

	return sha.finalHash
}

// GetFinalHash è·å–æœ€ç»ˆçš„MD5å“ˆå¸Œå€¼ï¼ˆå¦‚æœå·²å®Œæˆï¼‰
func (sha *StreamingHashAccumulator) GetFinalHash() (string, bool) {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	return sha.finalHash, sha.isFinalized
}

// Reset é‡ç½®ç´¯ç§¯å™¨çŠ¶æ€
func (sha *StreamingHashAccumulator) Reset() {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	sha.hasher.Reset()
	sha.chunkHashes = make(map[int64]string)
	sha.processed = 0
	sha.isFinalized = false
	sha.finalHash = ""
}

// GetChunkCount è·å–å·²å¤„ç†çš„åˆ†ç‰‡æ•°é‡
func (sha *StreamingHashAccumulator) GetChunkCount() int {
	sha.mu.Lock()
	defer sha.mu.Unlock()

	return len(sha.chunkHashes)
}

// EnhancedPerformanceMetrics å¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
type EnhancedPerformanceMetrics struct {
	mu                    sync.RWMutex
	uploadMetrics         []UploadMetric         // ä¸Šä¼ æŒ‡æ ‡å†å²
	downloadMetrics       []DownloadMetric       // ä¸‹è½½æŒ‡æ ‡å†å²
	networkQualityHistory []NetworkQualityMetric // ç½‘ç»œè´¨é‡å†å²
	maxHistorySize        int                    // æœ€å¤§å†å²è®°å½•æ•°
	startTime             time.Time              // å¼€å§‹æ—¶é—´
}

// UploadMetric ä¸Šä¼ æ€§èƒ½æŒ‡æ ‡
type UploadMetric struct {
	Timestamp       time.Time     // æ—¶é—´æˆ³
	FileSize        int64         // æ–‡ä»¶å¤§å°
	Duration        time.Duration // ä¼ è¾“æ—¶é—´
	ThroughputMBps  float64       // ååé‡ (MB/s)
	ChunkSize       int64         // åˆ†ç‰‡å¤§å°
	ConcurrencyUsed int           // ä½¿ç”¨çš„å¹¶å‘æ•°
	ErrorCount      int           // é”™è¯¯æ¬¡æ•°
	RetryCount      int           // é‡è¯•æ¬¡æ•°
	NetworkSpeed    int64         // æ£€æµ‹åˆ°çš„ç½‘ç»œé€Ÿåº¦
	Success         bool          // æ˜¯å¦æˆåŠŸ
}

// DownloadMetric ä¸‹è½½æ€§èƒ½æŒ‡æ ‡
type DownloadMetric struct {
	Timestamp      time.Time     // æ—¶é—´æˆ³
	FileSize       int64         // æ–‡ä»¶å¤§å°
	Duration       time.Duration // ä¼ è¾“æ—¶é—´
	ThroughputMBps float64       // ååé‡ (MB/s)
	Success        bool          // æ˜¯å¦æˆåŠŸ
}

// NetworkQualityMetric ç½‘ç»œè´¨é‡æŒ‡æ ‡
type NetworkQualityMetric struct {
	Timestamp  time.Time     // æ—¶é—´æˆ³
	Quality    float64       // è´¨é‡åˆ†æ•° (0.0-1.0)
	ErrorRate  float64       // é”™è¯¯ç‡
	AvgLatency time.Duration // å¹³å‡å»¶è¿Ÿ
	PacketLoss float64       // ä¸¢åŒ…ç‡
	Bandwidth  int64         // å¸¦å®½ (bytes/s)
}

// RecentMetrics æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
type RecentMetrics struct {
	AverageSpeed       float64       // å¹³å‡ä¼ è¾“é€Ÿåº¦ (MB/s)
	ErrorRate          float64       // é”™è¯¯ç‡
	SuccessRate        float64       // æˆåŠŸç‡
	AverageConcurrency float64       // å¹³å‡å¹¶å‘æ•°
	OptimalChunkSize   int64         // æœ€ä¼˜åˆ†ç‰‡å¤§å°
	NetworkQuality     float64       // ç½‘ç»œè´¨é‡
	TotalTransfers     int           // æ€»ä¼ è¾“æ¬¡æ•°
	TimeWindow         time.Duration // ç»Ÿè®¡æ—¶é—´çª—å£
}

// NewEnhancedPerformanceMetrics åˆ›å»ºå¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
func NewEnhancedPerformanceMetrics() *EnhancedPerformanceMetrics {
	return &EnhancedPerformanceMetrics{
		uploadMetrics:         make([]UploadMetric, 0),
		downloadMetrics:       make([]DownloadMetric, 0),
		networkQualityHistory: make([]NetworkQualityMetric, 0),
		maxHistorySize:        100, // ä¿ç•™æœ€è¿‘100æ¡è®°å½•
		startTime:             time.Now(),
	}
}

// RecordUpload è®°å½•ä¸Šä¼ æ€§èƒ½æŒ‡æ ‡
func (epm *EnhancedPerformanceMetrics) RecordUpload(metric UploadMetric) {
	epm.mu.Lock()
	defer epm.mu.Unlock()

	// è®¡ç®—ååé‡
	if metric.Duration > 0 {
		metric.ThroughputMBps = float64(metric.FileSize) / (1024 * 1024) / metric.Duration.Seconds()
	}

	epm.uploadMetrics = append(epm.uploadMetrics, metric)

	// ä¿æŒå†å²è®°å½•å¤§å°é™åˆ¶
	if len(epm.uploadMetrics) > epm.maxHistorySize {
		epm.uploadMetrics = epm.uploadMetrics[1:]
	}
}

// RecordDownload è®°å½•ä¸‹è½½æ€§èƒ½æŒ‡æ ‡
func (epm *EnhancedPerformanceMetrics) RecordDownload(metric DownloadMetric) {
	epm.mu.Lock()
	defer epm.mu.Unlock()

	// è®¡ç®—ååé‡
	if metric.Duration > 0 {
		metric.ThroughputMBps = float64(metric.FileSize) / (1024 * 1024) / metric.Duration.Seconds()
	}

	epm.downloadMetrics = append(epm.downloadMetrics, metric)

	// ä¿æŒå†å²è®°å½•å¤§å°é™åˆ¶
	if len(epm.downloadMetrics) > epm.maxHistorySize {
		epm.downloadMetrics = epm.downloadMetrics[1:]
	}
}

// RecordNetworkQuality è®°å½•ç½‘ç»œè´¨é‡æŒ‡æ ‡
func (epm *EnhancedPerformanceMetrics) RecordNetworkQuality(metric NetworkQualityMetric) {
	epm.mu.Lock()
	defer epm.mu.Unlock()

	epm.networkQualityHistory = append(epm.networkQualityHistory, metric)

	// ä¿æŒå†å²è®°å½•å¤§å°é™åˆ¶
	if len(epm.networkQualityHistory) > epm.maxHistorySize {
		epm.networkQualityHistory = epm.networkQualityHistory[1:]
	}
}

// PerformanceStats æ€§èƒ½ç»Ÿè®¡æ‘˜è¦
type PerformanceStats struct {
	TotalRuntime         time.Duration // æ€»è¿è¡Œæ—¶é—´
	TotalUploads         int           // æ€»ä¸Šä¼ æ•°
	SuccessfulUploads    int           // æˆåŠŸä¸Šä¼ æ•°
	FailedUploads        int           // å¤±è´¥ä¸Šä¼ æ•°
	AverageUploadSpeed   float64       // å¹³å‡ä¸Šä¼ é€Ÿåº¦ MB/s
	TotalDownloads       int           // æ€»ä¸‹è½½æ•°
	SuccessfulDownloads  int           // æˆåŠŸä¸‹è½½æ•°
	FailedDownloads      int           // å¤±è´¥ä¸‹è½½æ•°
	AverageDownloadSpeed float64       // å¹³å‡ä¸‹è½½é€Ÿåº¦ MB/s
	TotalAPICalls        int           // æ€»APIè°ƒç”¨æ•°
	APISuccessRate       float64       // APIæˆåŠŸç‡
	AverageAPILatency    time.Duration // å¹³å‡APIå»¶è¿Ÿ
	CurrentMemoryUsage   int64         // å½“å‰å†…å­˜ä½¿ç”¨
	PeakMemoryUsage      int64         // å³°å€¼å†…å­˜ä½¿ç”¨
}

// GetStats è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦
func (epm *EnhancedPerformanceMetrics) GetStats() PerformanceStats {
	epm.mu.RLock()
	defer epm.mu.RUnlock()

	stats := PerformanceStats{
		TotalRuntime: time.Since(epm.startTime),
	}

	// è®¡ç®—ä¸Šä¼ ç»Ÿè®¡
	stats.TotalUploads = len(epm.uploadMetrics)
	var totalUploadSpeed float64
	for _, metric := range epm.uploadMetrics {
		if metric.Success {
			stats.SuccessfulUploads++
		} else {
			stats.FailedUploads++
		}
		totalUploadSpeed += metric.ThroughputMBps
	}
	if stats.TotalUploads > 0 {
		stats.AverageUploadSpeed = totalUploadSpeed / float64(stats.TotalUploads)
	}

	// è®¡ç®—ä¸‹è½½ç»Ÿè®¡
	stats.TotalDownloads = len(epm.downloadMetrics)
	var totalDownloadSpeed float64
	for _, metric := range epm.downloadMetrics {
		if metric.Success {
			stats.SuccessfulDownloads++
		} else {
			stats.FailedDownloads++
		}
		totalDownloadSpeed += metric.ThroughputMBps
	}
	if stats.TotalDownloads > 0 {
		stats.AverageDownloadSpeed = totalDownloadSpeed / float64(stats.TotalDownloads)
	}

	// è®¡ç®—ç½‘ç»œè´¨é‡ç»Ÿè®¡
	if len(epm.networkQualityHistory) > 0 {
		var totalLatency time.Duration
		successCount := 0
		for _, metric := range epm.networkQualityHistory {
			totalLatency += metric.AvgLatency
			if metric.ErrorRate < 0.1 { // é”™è¯¯ç‡å°äº10%è§†ä¸ºæˆåŠŸ
				successCount++
			}
		}
		stats.TotalAPICalls = len(epm.networkQualityHistory)
		stats.APISuccessRate = float64(successCount) / float64(stats.TotalAPICalls)
		stats.AverageAPILatency = totalLatency / time.Duration(stats.TotalAPICalls)
	}

	return stats
}

// GetRecentMetrics è·å–æœ€è¿‘çš„æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
func (epm *EnhancedPerformanceMetrics) GetRecentMetrics(timeWindow time.Duration) RecentMetrics {
	epm.mu.RLock()
	defer epm.mu.RUnlock()

	cutoff := time.Now().Add(-timeWindow)

	// ç»Ÿè®¡ä¸Šä¼ æŒ‡æ ‡
	var totalSpeed, totalConcurrency float64
	var totalChunkSize int64
	var successCount, totalCount, errorCount int

	for _, metric := range epm.uploadMetrics {
		if metric.Timestamp.After(cutoff) {
			totalCount++
			totalSpeed += metric.ThroughputMBps
			totalConcurrency += float64(metric.ConcurrencyUsed)
			totalChunkSize += metric.ChunkSize
			errorCount += metric.ErrorCount

			if metric.Success {
				successCount++
			}
		}
	}

	// ç»Ÿè®¡ç½‘ç»œè´¨é‡
	var totalQuality float64
	var qualityCount int
	for _, metric := range epm.networkQualityHistory {
		if metric.Timestamp.After(cutoff) {
			totalQuality += metric.Quality
			qualityCount++
		}
	}

	result := RecentMetrics{
		TotalTransfers: totalCount,
		TimeWindow:     timeWindow,
	}

	if totalCount > 0 {
		result.AverageSpeed = totalSpeed / float64(totalCount)
		result.AverageConcurrency = totalConcurrency / float64(totalCount)
		result.OptimalChunkSize = totalChunkSize / int64(totalCount)
		result.SuccessRate = float64(successCount) / float64(totalCount)
		result.ErrorRate = float64(errorCount) / float64(totalCount)
	}

	if qualityCount > 0 {
		result.NetworkQuality = totalQuality / float64(qualityCount)
	}

	return result
}

// GetOptimalParameters åŸºäºå†å²æ•°æ®è·å–æœ€ä¼˜å‚æ•°å»ºè®®
func (epm *EnhancedPerformanceMetrics) GetOptimalParameters(fileSize int64) (optimalConcurrency int, optimalChunkSize int64) {
	epm.mu.RLock()
	defer epm.mu.RUnlock()

	// æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶å¤§å°çš„æœ€ä½³æ€§èƒ½è®°å½•
	var bestThroughput float64
	var bestConcurrency int = 4                 // é»˜è®¤å€¼
	var bestChunkSize int64 = 100 * 1024 * 1024 // é»˜è®¤100MB

	for _, metric := range epm.uploadMetrics {
		// æŸ¥æ‰¾æ–‡ä»¶å¤§å°ç›¸è¿‘çš„è®°å½•ï¼ˆÂ±50%èŒƒå›´å†…ï¼‰
		if metric.Success &&
			metric.FileSize >= fileSize/2 &&
			metric.FileSize <= fileSize*2 &&
			metric.ThroughputMBps > bestThroughput {
			bestThroughput = metric.ThroughputMBps
			bestConcurrency = metric.ConcurrencyUsed
			bestChunkSize = metric.ChunkSize
		}
	}

	return bestConcurrency, bestChunkSize
}

// GetPerformanceReport ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
func (epm *EnhancedPerformanceMetrics) GetPerformanceReport() string {
	epm.mu.RLock()
	defer epm.mu.RUnlock()

	recent := epm.GetRecentMetrics(time.Hour) // æœ€è¿‘1å°æ—¶çš„æ•°æ®

	report := fmt.Sprintf("=== 123ç½‘ç›˜æ€§èƒ½æŠ¥å‘Š ===\n")
	report += fmt.Sprintf("è¿è¡Œæ—¶é—´: %v\n", time.Since(epm.startTime))
	report += fmt.Sprintf("ç»Ÿè®¡æ—¶é—´çª—å£: %v\n", recent.TimeWindow)
	report += fmt.Sprintf("æ€»ä¼ è¾“æ¬¡æ•°: %d\n", recent.TotalTransfers)
	report += fmt.Sprintf("å¹³å‡ä¼ è¾“é€Ÿåº¦: %.2f MB/s\n", recent.AverageSpeed)
	report += fmt.Sprintf("æˆåŠŸç‡: %.2f%%\n", recent.SuccessRate*100)
	report += fmt.Sprintf("é”™è¯¯ç‡: %.2f%%\n", recent.ErrorRate*100)
	report += fmt.Sprintf("å¹³å‡å¹¶å‘æ•°: %.1f\n", recent.AverageConcurrency)
	report += fmt.Sprintf("æœ€ä¼˜åˆ†ç‰‡å¤§å°: %s\n", fs.SizeSuffix(recent.OptimalChunkSize))
	report += fmt.Sprintf("ç½‘ç»œè´¨é‡: %.2f/1.0\n", recent.NetworkQuality)

	return report
}

// UploadStrategy ä¸Šä¼ ç­–ç•¥æšä¸¾
type UploadStrategy int

const (
	StrategyAuto       UploadStrategy = iota // è‡ªåŠ¨é€‰æ‹©ç­–ç•¥
	StrategySingleStep                       // å•æ­¥ä¸Šä¼ API
	StrategyChunked                          // åˆ†ç‰‡ä¸Šä¼ 
	StrategyStreaming                        // æµå¼ä¸Šä¼ 
)

// String è¿”å›ç­–ç•¥çš„å­—ç¬¦ä¸²è¡¨ç¤º
func (us UploadStrategy) String() string {
	switch us {
	case StrategyAuto:
		return "è‡ªåŠ¨é€‰æ‹©"
	case StrategySingleStep:
		return "å•æ­¥ä¸Šä¼ "
	case StrategyChunked:
		return "åˆ†ç‰‡ä¸Šä¼ "
	case StrategyStreaming:
		return "æµå¼ä¸Šä¼ "
	default:
		return "æœªçŸ¥ç­–ç•¥"
	}
}

// UploadStrategySelector ä¸Šä¼ ç­–ç•¥é€‰æ‹©å™¨
type UploadStrategySelector struct {
	fileSize       int64   // æ–‡ä»¶å¤§å°
	hasKnownMD5    bool    // æ˜¯å¦æœ‰å·²çŸ¥MD5
	isRemoteSource bool    // æ˜¯å¦æ˜¯è¿œç¨‹æº
	networkSpeed   int64   // ç½‘ç»œé€Ÿåº¦
	networkQuality float64 // ç½‘ç»œè´¨é‡
}

// SelectStrategy é€‰æ‹©æœ€ä¼˜ä¸Šä¼ ç­–ç•¥
func (uss *UploadStrategySelector) SelectStrategy() UploadStrategy {
	// ç­–ç•¥é€‰æ‹©é€»è¾‘ç®€åŒ–å’Œä¼˜åŒ–

	// 1. å°æ–‡ä»¶ä¸”æœ‰å·²çŸ¥MD5 -> å•æ­¥ä¸Šä¼ ï¼ˆåˆ©ç”¨ç§’ä¼ ï¼‰
	if uss.fileSize < singleStepUploadLimit && uss.hasKnownMD5 && uss.fileSize > 0 {
		return StrategySingleStep
	}

	// 2. å¤§æ–‡ä»¶ -> åˆ†ç‰‡ä¸Šä¼ ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œå¹¶å‘ï¼‰
	if uss.fileSize > int64(defaultUploadCutoff) {
		return StrategyChunked
	}

	// 3. ä¸­ç­‰æ–‡ä»¶ï¼Œç½‘ç»œè´¨é‡å·® -> åˆ†ç‰‡ä¸Šä¼ ï¼ˆæ›´ç¨³å®šï¼‰
	if uss.fileSize > 50*1024*1024 && uss.networkQuality < 0.5 {
		return StrategyChunked
	}

	// 4. å…¶ä»–æƒ…å†µ -> æµå¼ä¸Šä¼ ï¼ˆå¹³è¡¡æ€§èƒ½å’Œèµ„æºä½¿ç”¨ï¼‰
	return StrategyStreaming
}

// GetStrategyReason è·å–ç­–ç•¥é€‰æ‹©çš„åŸå› è¯´æ˜
func (uss *UploadStrategySelector) GetStrategyReason(strategy UploadStrategy) string {
	switch strategy {
	case StrategySingleStep:
		return fmt.Sprintf("å°æ–‡ä»¶(%s)ä¸”æœ‰MD5ï¼Œä½¿ç”¨å•æ­¥ä¸Šä¼ åˆ©ç”¨ç§’ä¼ åŠŸèƒ½", fs.SizeSuffix(uss.fileSize))
	case StrategyChunked:
		if uss.fileSize > int64(defaultUploadCutoff) {
			return fmt.Sprintf("å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ æ”¯æŒæ–­ç‚¹ç»­ä¼ ", fs.SizeSuffix(uss.fileSize))
		}
		return fmt.Sprintf("ç½‘ç»œè´¨é‡å·®(%.2f)ï¼Œä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ æé«˜ç¨³å®šæ€§", uss.networkQuality)
	case StrategyStreaming:
		return fmt.Sprintf("ä¸­ç­‰æ–‡ä»¶(%s)ï¼Œä½¿ç”¨æµå¼ä¸Šä¼ å¹³è¡¡æ€§èƒ½", fs.SizeSuffix(uss.fileSize))
	default:
		return "è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥"
	}
}

// UnifiedUploadContext ç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
type UnifiedUploadContext struct {
	ctx            context.Context
	in             io.Reader
	src            fs.ObjectInfo
	parentFileID   int64
	fileName       string
	strategy       UploadStrategy
	selector       *UploadStrategySelector
	startTime      time.Time
	networkSpeed   int64
	networkQuality float64
}

// NewUnifiedUploadContext åˆ›å»ºç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
func (f *Fs) NewUnifiedUploadContext(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) *UnifiedUploadContext {
	// æ£€æµ‹ç½‘ç»œæ¡ä»¶
	networkSpeed := f.detectNetworkSpeed(ctx)
	networkQuality := f.getNetworkQuality()

	// å°è¯•è·å–å·²çŸ¥MD5
	var hasKnownMD5 bool
	if hashValue, err := src.Hash(ctx, fshash.MD5); err == nil && hashValue != "" {
		hasKnownMD5 = true
	}

	// åˆ›å»ºç­–ç•¥é€‰æ‹©å™¨
	selector := &UploadStrategySelector{
		fileSize:       src.Size(),
		hasKnownMD5:    hasKnownMD5,
		isRemoteSource: f.isRemoteSource(src),
		networkSpeed:   networkSpeed,
		networkQuality: networkQuality,
	}

	// é€‰æ‹©ç­–ç•¥
	strategy := selector.SelectStrategy()

	return &UnifiedUploadContext{
		ctx:            ctx,
		in:             in,
		src:            src,
		parentFileID:   parentFileID,
		fileName:       fileName,
		strategy:       strategy,
		selector:       selector,
		startTime:      time.Now(),
		networkSpeed:   networkSpeed,
		networkQuality: networkQuality,
	}
}

// unifiedUpload ç»Ÿä¸€ä¸Šä¼ å…¥å£ï¼Œç®€åŒ–å¤æ‚çš„å›é€€ç­–ç•¥
func (f *Fs) unifiedUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, parentFileID int64, fileName string) (*Object, error) {
	// åˆ›å»ºç»Ÿä¸€ä¸Šä¼ ä¸Šä¸‹æ–‡
	uploadCtx := f.NewUnifiedUploadContext(ctx, in, src, parentFileID, fileName)

	// è®°å½•ç­–ç•¥é€‰æ‹©
	reason := uploadCtx.selector.GetStrategyReason(uploadCtx.strategy)
	fs.Debugf(f, "ç»Ÿä¸€ä¸Šä¼ ç­–ç•¥: %s - %s", uploadCtx.strategy.String(), reason)

	// è®°å½•ä¸Šä¼ å¼€å§‹æŒ‡æ ‡
	startTime := time.Now()

	// æ‰§è¡Œå¯¹åº”çš„ä¸Šä¼ ç­–ç•¥
	var result *Object
	var err error

	switch uploadCtx.strategy {
	case StrategySingleStep:
		result, err = f.executeSingleStepUpload(uploadCtx)
	case StrategyChunked:
		result, err = f.executeChunkedUpload(uploadCtx)
	case StrategyStreaming:
		result, err = f.executeStreamingUpload(uploadCtx)
	default:
		// é»˜è®¤å›é€€åˆ°æµå¼ä¸Šä¼ 
		fs.Debugf(f, "æœªçŸ¥ç­–ç•¥ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ")
		result, err = f.executeStreamingUpload(uploadCtx)
	}

	// è®°å½•æ€§èƒ½æŒ‡æ ‡
	duration := time.Since(startTime)
	uploadMetric := UploadMetric{
		Timestamp:       startTime,
		FileSize:        src.Size(),
		Duration:        duration,
		ChunkSize:       int64(f.opt.ChunkSize),
		ConcurrencyUsed: f.opt.MaxConcurrentUploads,
		NetworkSpeed:    uploadCtx.networkSpeed,
		Success:         err == nil,
	}

	if err != nil {
		uploadMetric.ErrorCount = 1
		fs.Errorf(f, "ç»Ÿä¸€ä¸Šä¼ å¤±è´¥: %v", err)
	} else {
		fs.Debugf(f, "ç»Ÿä¸€ä¸Šä¼ æˆåŠŸï¼Œè€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s",
			duration, uploadMetric.ThroughputMBps)
	}

	// è®°å½•åˆ°æ€§èƒ½æŒ‡æ ‡
	if f.performanceMetrics != nil {
		// ä½¿ç”¨åŸºç¡€æ€§èƒ½æŒ‡æ ‡è®°å½•ä¸Šä¼ ç»“æœ
		if err != nil {
			f.performanceMetrics.RecordUploadError()
		} else {
			f.performanceMetrics.RecordUploadComplete(uploadMetric.FileSize, duration)
		}
	}

	return result, err
}

// executeSingleStepUpload æ‰§è¡Œå•æ­¥ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeSingleStepUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	fs.Debugf(f, "æ‰§è¡Œå•æ­¥ä¸Šä¼ ç­–ç•¥")

	// è·å–å·²çŸ¥MD5
	md5Hash, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5)
	if err != nil || md5Hash == "" {
		return nil, fmt.Errorf("å•æ­¥ä¸Šä¼ éœ€è¦å·²çŸ¥MD5ï¼Œä½†è·å–å¤±è´¥: %w", err)
	}

	// è¯»å–æ–‡ä»¶æ•°æ®
	data, err := io.ReadAll(uploadCtx.in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	if int64(len(data)) != uploadCtx.src.Size() {
		return nil, fmt.Errorf("æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ› %d, å®é™… %d", uploadCtx.src.Size(), len(data))
	}

	// æ‰§è¡Œå•æ­¥ä¸Šä¼ 
	result, err := f.singleStepUpload(uploadCtx.ctx, data, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash)
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ç›®å½•IDä¸å­˜åœ¨çš„é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡è¯•ä¸€æ¬¡
		if strings.Contains(err.Error(), "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•") {
			fs.Debugf(f, "å•æ­¥ä¸Šä¼ : çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œé‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¹¶é‡è¯•")

			// é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•ID
			leaf, parentID, findErr := f.dirCache.FindPath(uploadCtx.ctx, uploadCtx.fileName, true)
			if findErr != nil {
				return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", findErr)
			}

			newParentFileID, parseErr := parseParentID(parentID)
			if parseErr != nil {
				return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", parseErr)
			}

			fs.Debugf(f, "å•æ­¥ä¸Šä¼ : é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d (åŸID: %d)", newParentFileID, uploadCtx.parentFileID)
			uploadCtx.parentFileID = newParentFileID
			uploadCtx.fileName = leaf

			// é‡è¯•å•æ­¥ä¸Šä¼ 
			result, err = f.singleStepUpload(uploadCtx.ctx, data, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash)
			if err != nil {
				return nil, fmt.Errorf("é‡è¯•å•æ­¥ä¸Šä¼ å¤±è´¥: %w", err)
			}
		} else {
			return nil, err
		}
	}

	return result, nil
}

// executeChunkedUpload æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeChunkedUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è¿›å…¥executeChunkedUploadå‡½æ•°ï¼Œæ–‡ä»¶: %s", uploadCtx.fileName)
	fs.Debugf(f, "æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ ç­–ç•¥")

	// ğŸŒ è·¨äº‘ä¼ è¾“æ£€æµ‹å’Œæ™ºèƒ½ç§’ä¼ ç­–ç•¥
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘å¼€å§‹è·¨äº‘ä¼ è¾“æ£€æµ‹...")
	fs.Debugf(f, "ğŸ” å¼€å§‹æ£€æµ‹æ˜¯å¦ä¸ºè·¨äº‘ä¼ è¾“...")
	isRemoteSource := f.isRemoteSource(uploadCtx.src)
	fs.Infof(f, "ğŸš¨ ã€é‡è¦è°ƒè¯•ã€‘è·¨äº‘ä¼ è¾“æ£€æµ‹ç»“æœ: %v", isRemoteSource)
	fs.Debugf(f, "ğŸ” è·¨äº‘ä¼ è¾“æ£€æµ‹ç»“æœ: %v", isRemoteSource)

	// ğŸš€ æ™ºèƒ½ç§’ä¼ æ£€æµ‹ï¼šæ— è®ºæœ¬åœ°è¿˜æ˜¯è·¨äº‘ä¼ è¾“ï¼Œéƒ½å…ˆå°è¯•è·å–MD5è¿›è¡Œç§’ä¼ æ£€æµ‹
	var md5Hash string

	if isRemoteSource {
		fs.Infof(f, "ğŸŒ æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“ï¼Œå…ˆå°è¯•è·å–æºæ–‡ä»¶MD5è¿›è¡Œç§’ä¼ æ£€æµ‹")

		// å°è¯•ä»æºå¯¹è±¡è·å–å·²çŸ¥MD5ï¼ˆæ— éœ€é¢å¤–è®¡ç®—ï¼‰
		if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
			md5Hash = hashValue
			fs.Infof(f, "ğŸš€ è·¨äº‘ä¼ è¾“è·å–åˆ°æºæ–‡ä»¶MD5: %sï¼Œå°†å°è¯•ç§’ä¼ ", md5Hash)
		} else {
			fs.Debugf(f, "ğŸ” è·¨äº‘ä¼ è¾“æºæ–‡ä»¶æ— å¯ç”¨MD5ï¼Œå°†è·³è¿‡ç§’ä¼ æ£€æµ‹: %v", err)
		}
	}

	// æœ¬åœ°æ–‡ä»¶æˆ–è·¨äº‘ä¼ è¾“æ— MD5æ—¶ï¼šè·å–æˆ–è®¡ç®—MD5
	if !isRemoteSource || md5Hash == "" {
		if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
			md5Hash = hashValue
			fs.Debugf(f, "ä½¿ç”¨å·²çŸ¥MD5: %s", md5Hash)
		} else if !isRemoteSource {
			// å¯¹äºåˆ†ç‰‡ä¸Šä¼ ï¼Œå¿…é¡»è®¡ç®—å‡†ç¡®çš„MD5
			fs.Debugf(f, "æºå¯¹è±¡æ— MD5å“ˆå¸Œï¼Œå°è¯•ä»è¾“å…¥æµè®¡ç®—MD5")

			// ä¼˜å…ˆå°è¯•ä»fs.Objectæ¥å£è®¡ç®—MD5
			if srcObj, ok := uploadCtx.src.(fs.Object); ok {
				fs.Debugf(f, "ä½¿ç”¨fs.Objectæ¥å£è®¡ç®—MD5")
				md5Reader, err := srcObj.Open(uploadCtx.ctx)
				if err != nil {
					fs.Debugf(f, "æ— æ³•æ‰“å¼€æºå¯¹è±¡ï¼Œå›é€€åˆ°è¾“å…¥æµè®¡ç®—: %v", err)
				} else {
					defer md5Reader.Close()
					hasher := md5.New()
					_, err = io.Copy(hasher, md5Reader)
					if err != nil {
						fs.Debugf(f, "ä»æºå¯¹è±¡è®¡ç®—MD5å¤±è´¥ï¼Œå›é€€åˆ°è¾“å…¥æµè®¡ç®—: %v", err)
					} else {
						md5Hash = fmt.Sprintf("%x", hasher.Sum(nil))
						fs.Debugf(f, "ä»æºå¯¹è±¡è®¡ç®—MD5å®Œæˆ: %s", md5Hash)
					}
				}
			}
		}

		// å¦‚æœä»æºå¯¹è±¡è®¡ç®—MD5å¤±è´¥ï¼Œå°è¯•é€šç”¨æ–¹æ³•è®¡ç®—MD5
		if md5Hash == "" {
			fs.Debugf(f, "ä½¿ç”¨é€šç”¨æ–¹æ³•è®¡ç®—MD5ï¼ˆè·¨äº‘ç›˜ä¼ è¾“å…¼å®¹æ¨¡å¼ï¼‰")

			// å°è¯•é€šè¿‡fs.ObjectInfoæ¥å£é‡æ–°æ‰“å¼€æºå¯¹è±¡
			if opener, ok := uploadCtx.src.(fs.ObjectInfo); ok {
				fs.Debugf(f, "é€šè¿‡fs.ObjectInfoæ¥å£è®¡ç®—MD5")

				// æ£€æŸ¥æ˜¯å¦æœ‰Openæ–¹æ³•ï¼ˆé€šè¿‡æ¥å£æ–­è¨€ï¼‰
				if openable, hasOpen := opener.(interface {
					Open(context.Context, ...fs.OpenOption) (io.ReadCloser, error)
				}); hasOpen {
					md5Reader, err := openable.Open(uploadCtx.ctx)
					if err != nil {
						fs.Debugf(f, "æ— æ³•é€šè¿‡Openæ–¹æ³•æ‰“å¼€æºå¯¹è±¡: %v", err)
					} else {
						defer md5Reader.Close()
						md5Hash, err = f.calculateMD5FromReader(uploadCtx.ctx, md5Reader, uploadCtx.src.Size())
						if err != nil {
							fs.Debugf(f, "é€šè¿‡Openæ–¹æ³•è®¡ç®—MD5å¤±è´¥: %v", err)
						} else {
							fs.Debugf(f, "é€šè¿‡Openæ–¹æ³•è®¡ç®—MD5æˆåŠŸ: %s", md5Hash)
						}
					}
				}
			}

			// å¦‚æœä»ç„¶æ²¡æœ‰MD5ï¼Œä½¿ç”¨è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5ï¼ˆæ­¤æ—¶å·²ç¡®ä¿æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
			if md5Hash == "" {
				fs.Debugf(f, "ä½¿ç”¨è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5")
				var err error
				md5Hash, uploadCtx.in, err = f.calculateMD5WithStreamCache(uploadCtx.ctx, uploadCtx.in, uploadCtx.src.Size())
				if err != nil {
					return nil, fmt.Errorf("æ— æ³•è®¡ç®—æ–‡ä»¶MD5: %w", err)
				}
				fs.Debugf(f, "è¾“å…¥æµç¼“å­˜æ–¹æ³•è®¡ç®—MD5å®Œæˆ: %s", md5Hash)
			}
		}
	}

	// ğŸŒ è·¨äº‘ä¼ è¾“æ— MD5æ—¶çš„ç‰¹æ®Šå¤„ç†
	if isRemoteSource && md5Hash == "" {
		fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“æ— å¯ç”¨MD5ï¼Œä½¿ç”¨ä¸‹è½½åä¸Šä¼ ç­–ç•¥")

		// ä¸ºè·¨äº‘ä¼ è¾“åˆ›å»ºä¸Šä¼ ä¼šè¯ï¼ˆä¸éœ€è¦MD5ï¼‰
		createResp, err := f.createUploadV2(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, "", uploadCtx.src.Size())
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
		}

		// å‡†å¤‡ä¸Šä¼ è¿›åº¦ä¿¡æ¯
		fileSize := uploadCtx.src.Size()
		chunkSize := createResp.Data.SliceSize
		if chunkSize <= 0 {
			// ä½¿ç”¨é»˜è®¤ç½‘ç»œé€Ÿåº¦è®¡ç®—åˆ†ç‰‡å¤§å°
			defaultNetworkSpeed := int64(20 * 1024 * 1024) // 20MB/s
			chunkSize = f.getOptimalChunkSize(fileSize, defaultNetworkSpeed)
		}
		totalChunks := (fileSize + chunkSize - 1) / chunkSize
		progress, err := f.prepareUploadProgress(createResp.Data.PreuploadID, totalChunks, chunkSize, fileSize)
		if err != nil {
			return nil, fmt.Errorf("å‡†å¤‡è·¨äº‘ä¼ è¾“ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
		}

		// è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
		concurrencyParams := f.calculateConcurrencyParams(fileSize)
		maxConcurrency := concurrencyParams.optimal

		return f.downloadThenUpload(uploadCtx.ctx, uploadCtx.src, createResp, progress, uploadCtx.fileName, maxConcurrency)
	}

	// ä¸ºv2å¤šçº¿ç¨‹ä¸Šä¼ åˆ›å»ºä¸“ç”¨ä¼šè¯ï¼Œå¼ºåˆ¶ä½¿ç”¨v2 API
	createResp, err := f.createUploadV2(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash, uploadCtx.src.Size())
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯çˆ¶ç›®å½•IDä¸å­˜åœ¨çš„é”™è¯¯ï¼Œå¦‚æœæ˜¯åˆ™é‡è¯•ä¸€æ¬¡
		if strings.Contains(err.Error(), "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œç¼“å­˜å·²æ¸…ç†ï¼Œè¯·é‡è¯•") {
			fs.Debugf(f, "çˆ¶ç›®å½•IDä¸å­˜åœ¨ï¼Œé‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¹¶é‡è¯•")

			// é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•ID
			leaf, parentID, findErr := f.dirCache.FindPath(uploadCtx.ctx, uploadCtx.fileName, true)
			if findErr != nil {
				return nil, fmt.Errorf("é‡æ–°æŸ¥æ‰¾çˆ¶ç›®å½•å¤±è´¥: %w", findErr)
			}

			newParentFileID, parseErr := parseParentID(parentID)
			if parseErr != nil {
				return nil, fmt.Errorf("é‡æ–°è§£æçˆ¶ç›®å½•IDå¤±è´¥: %w", parseErr)
			}

			fs.Debugf(f, "é‡æ–°æ‰¾åˆ°çˆ¶ç›®å½•ID: %d (åŸID: %d)", newParentFileID, uploadCtx.parentFileID)
			uploadCtx.parentFileID = newParentFileID
			uploadCtx.fileName = leaf

			// é‡è¯•åˆ›å»ºä¸Šä¼ ä¼šè¯
			createResp, err = f.createUploadV2(uploadCtx.ctx, uploadCtx.parentFileID, uploadCtx.fileName, md5Hash, uploadCtx.src.Size())
			if err != nil {
				return nil, fmt.Errorf("é‡è¯•åˆ›å»ºv2åˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
			}
		} else {
			return nil, fmt.Errorf("åˆ›å»ºv2åˆ†ç‰‡ä¸Šä¼ ä¼šè¯å¤±è´¥: %w", err)
		}
	}

	// å¦‚æœè§¦å‘ç§’ä¼ ï¼Œç›´æ¥è¿”å›
	if createResp.Data.Reuse {
		fs.Debugf(f, "è§¦å‘ç§’ä¼ åŠŸèƒ½")
		return &Object{
			fs:          f,
			remote:      uploadCtx.fileName,
			hasMetaData: true,
			id:          strconv.FormatInt(createResp.Data.FileID, 10),
			size:        uploadCtx.src.Size(),
			md5sum:      md5Hash,
			modTime:     time.Now(),
			isDir:       false,
		}, nil
	}

	// æ‰§è¡Œåˆ†ç‰‡ä¸Šä¼ 
	return f.executeChunkedUploadWithResume(uploadCtx, createResp)
}

// executeStreamingUpload æ‰§è¡Œæµå¼ä¸Šä¼ ç­–ç•¥
func (f *Fs) executeStreamingUpload(uploadCtx *UnifiedUploadContext) (*Object, error) {
	fs.Debugf(f, "æ‰§è¡Œæµå¼ä¸Šä¼ ç­–ç•¥")

	// ä½¿ç”¨ç°æœ‰çš„æµå¼ä¸Šä¼ é€»è¾‘ï¼Œä½†ç®€åŒ–å›é€€ç­–ç•¥
	return f.streamingPutSimplified(uploadCtx)
}

// executeChunkedUploadWithResume æ‰§è¡Œå¸¦æ–­ç‚¹ç»­ä¼ çš„åˆ†ç‰‡ä¸Šä¼ 
func (f *Fs) executeChunkedUploadWithResume(uploadCtx *UnifiedUploadContext, createResp *UploadCreateResp) (*Object, error) {
	// ä¼˜å…ˆå°è¯•v2å¤šçº¿ç¨‹ä¸Šä¼ ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹å¯¹è±¡
	fs.Debugf(f, "ğŸš€ å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ æµç¨‹ï¼Œæ–‡ä»¶å¤§å°: %d bytes", uploadCtx.src.Size())
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ1/3ã€‘åˆ›å»ºæ–‡ä»¶ - è°ƒç”¨åˆ›å»ºæ–‡ä»¶æ¥å£ï¼Œæ£€æŸ¥æ˜¯å¦ç§’ä¼ ")

	// ğŸ”§ å…³é”®ä¿®å¤ï¼šå¿…é¡»ä½¿ç”¨APIè¿”å›çš„SliceSizeï¼Œè€Œä¸æ˜¯è‡ªå®šä¹‰è®¡ç®—çš„åˆ†ç‰‡å¤§å°
	// æ ¹æ®123ç½‘ç›˜å®˜æ–¹APIæ–‡æ¡£ï¼Œåˆ†ç‰‡å¤§å°å¿…é¡»ä¸¥æ ¼éµå¾ªåˆ›å»ºæ–‡ä»¶æ¥å£è¿”å›çš„sliceSizeå‚æ•°
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize <= 0 {
		// å¦‚æœAPIæ²¡æœ‰è¿”å›æœ‰æ•ˆçš„SliceSizeï¼Œå›é€€åˆ°åŠ¨æ€è®¡ç®—
		fs.Debugf(f, "âš ï¸ APIæœªè¿”å›æœ‰æ•ˆSliceSize(%d)ï¼Œå›é€€åˆ°åŠ¨æ€è®¡ç®—", apiSliceSize)
		apiSliceSize = f.getOptimalChunkSize(uploadCtx.src.Size(), uploadCtx.networkSpeed)
	}

	fs.Debugf(f, "ğŸ”§ ä½¿ç”¨APIè§„èŒƒåˆ†ç‰‡å¤§å°: %s (APIè¿”å›SliceSize=%d)", fs.SizeSuffix(apiSliceSize), createResp.Data.SliceSize)

	// ä½¿ç”¨æ–°çš„v2å¤šçº¿ç¨‹ä¸Šä¼ å®ç°ï¼Œä¸¥æ ¼éµå¾ªAPIè¿”å›çš„åˆ†ç‰‡å¤§å°
	result, err := f.v2MultiThreadUpload(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, createResp, apiSliceSize, uploadCtx.fileName)
	if err != nil {
		// æ£€æŸ¥æ˜¯å¦æ˜¯å¯ä»¥é€šè¿‡fallbackè§£å†³çš„é”™è¯¯
		if f.shouldFallbackFromV2Upload(err) {
			fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ é‡åˆ°å¯æ¢å¤é”™è¯¯ï¼Œå°è¯•ä¼ ç»Ÿæ–¹å¼: %v", err)

			// å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼ï¼šå°è¯•è½¬æ¢ä¸ºfs.Object
			if srcObj, ok := uploadCtx.src.(fs.Object); ok {
				fs.Debugf(f, "å›é€€åˆ°ä¼ ç»Ÿåˆ†ç‰‡ä¸Šä¼ ")
				return f.uploadFileInChunksWithResume(uploadCtx.ctx, srcObj, createResp, uploadCtx.src.Size(), apiSliceSize, uploadCtx.fileName)
			}

			// æœ€åå›é€€åˆ°æµå¼ä¸Šä¼ 
			fs.Debugf(f, "æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€ï¼Œå›é€€åˆ°æµå¼ä¸Šä¼ ç­–ç•¥")
			return f.executeStreamingUpload(uploadCtx)
		} else {
			// å¯¹äºä¸å¯æ¢å¤çš„é”™è¯¯ï¼ˆå¦‚upload_completeå¤±è´¥ï¼‰ï¼Œç›´æ¥è¿”å›é”™è¯¯
			fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ é‡åˆ°ä¸å¯æ¢å¤é”™è¯¯ï¼Œç›´æ¥è¿”å›: %v", err)
			return nil, err
		}
	}

	return result, nil
}

// shouldFallbackFromV2Upload åˆ¤æ–­æ˜¯å¦åº”è¯¥ä»v2ä¸Šä¼ fallbackåˆ°ä¼ ç»Ÿæ–¹å¼
func (f *Fs) shouldFallbackFromV2Upload(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// ä¸åº”è¯¥fallbackçš„é”™è¯¯ç±»å‹ï¼ˆè¿™äº›æ˜¯æœ€ç»ˆé”™è¯¯ï¼Œfallbackä¹Ÿæ— æ³•è§£å†³ï¼‰
	if strings.Contains(errStr, "upload completion failed") ||
		strings.Contains(errStr, "æ–‡ä»¶æ ¡éªŒè¶…æ—¶") ||
		strings.Contains(errStr, "å®Œæˆä¸Šä¼ å¤±è´¥") {
		return false
	}

	// åº”è¯¥fallbackçš„é”™è¯¯ç±»å‹ï¼ˆè¿™äº›å¯èƒ½é€šè¿‡ä¼ ç»Ÿæ–¹å¼è§£å†³ï¼‰
	if strings.Contains(errStr, "ç½‘ç»œè¿æ¥") ||
		strings.Contains(errStr, "è¶…æ—¶") ||
		strings.Contains(errStr, "åˆ†ç‰‡ä¸Šä¼ å¤±è´¥") ||
		strings.Contains(errStr, "å¹¶å‘") {
		return true
	}

	// é»˜è®¤ä¸fallbackï¼Œé¿å…ä¸å¿…è¦çš„é‡è¯•
	return false
}

// v2MultiThreadUpload åŸºäºv2 APIçš„å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®ç°
// è§£å†³"æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€"çš„é™åˆ¶ï¼Œæ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œè¿œç¨‹å¯¹è±¡çš„çœŸæ­£å¹¶å‘ä¸Šä¼ 
func (f *Fs) v2MultiThreadUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, chunkSize int64, fileName string) (*Object, error) {
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ2/3ã€‘ä¸Šä¼ åˆ†ç‰‡ - å¼€å§‹å¤šçº¿ç¨‹å¹¶å‘ä¸Šä¼ ï¼Œåˆ†ç‰‡å¤§å°=%s", fs.SizeSuffix(chunkSize))

	// æ³¨æ„ï¼šè·¨äº‘ä¼ è¾“æ£€æµ‹å·²åœ¨æ›´æ—©é˜¶æ®µå¤„ç†ï¼Œæ­¤å‡½æ•°åªå¤„ç†æœ¬åœ°æ–‡ä»¶

	preuploadID := createResp.Data.PreuploadID
	fileSize := src.Size()

	// ğŸ”§ å…³é”®éªŒè¯ï¼šç¡®ä¿ä½¿ç”¨çš„åˆ†ç‰‡å¤§å°ä¸APIè¿”å›çš„SliceSizeä¸€è‡´
	apiSliceSize := createResp.Data.SliceSize
	if apiSliceSize > 0 && chunkSize != apiSliceSize {
		fs.Debugf(f, "âš ï¸ åˆ†ç‰‡å¤§å°ä¸åŒ¹é…è­¦å‘Š: ä¼ å…¥å¤§å°=%s, APIè¦æ±‚å¤§å°=%s, å¼ºåˆ¶ä½¿ç”¨APIè¦æ±‚å¤§å°",
			fs.SizeSuffix(chunkSize), fs.SizeSuffix(apiSliceSize))
		chunkSize = apiSliceSize
	}

	totalChunks := (fileSize + chunkSize - 1) / chunkSize

	fs.Debugf(f, "å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ : æ–‡ä»¶å¤§å°=%s, åˆ†ç‰‡å¤§å°=%s, æ€»åˆ†ç‰‡æ•°=%d, APIè¦æ±‚SliceSize=%d",
		fs.SizeSuffix(fileSize), fs.SizeSuffix(chunkSize), totalChunks, apiSliceSize)

	// å‡†å¤‡æˆ–æ¢å¤ä¸Šä¼ è¿›åº¦
	progress, err := f.prepareUploadProgress(preuploadID, totalChunks, chunkSize, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å‡†å¤‡ä¸Šä¼ è¿›åº¦å¤±è´¥: %w", err)
	}

	// è®¡ç®—æœ€ä¼˜å¹¶å‘å‚æ•°
	concurrencyParams := f.calculateConcurrencyParams(fileSize)

	// æ™ºèƒ½å¤šçº¿ç¨‹å¯ç”¨ç­–ç•¥ - æ ¹æ®æ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´æœ€å°å¹¶å‘æ•°
	minConcurrency := 1
	if fileSize > 2*1024*1024*1024 { // >2GB - å¼ºåˆ¶è‡³å°‘4ä¸ªå¹¶å‘
		minConcurrency = 4
	} else if fileSize > 1*1024*1024*1024 { // >1GB - å¼ºåˆ¶è‡³å°‘3ä¸ªå¹¶å‘
		minConcurrency = 3
	} else if fileSize > 500*1024*1024 { // >500MB - å¼ºåˆ¶è‡³å°‘2ä¸ªå¹¶å‘
		minConcurrency = 2
	} else if fileSize > 100*1024*1024 { // >100MB - å¼ºåˆ¶è‡³å°‘2ä¸ªå¹¶å‘
		minConcurrency = 2
	}

	if concurrencyParams.actual < minConcurrency {
		concurrencyParams.actual = minConcurrency
		fs.Debugf(f, "ğŸ”§ æ™ºèƒ½å¤šçº¿ç¨‹è°ƒæ•´: æ–‡ä»¶å¤§å°=%sï¼Œæœ€å°å¹¶å‘æ•°=%dï¼Œè°ƒæ•´åå¹¶å‘æ•°=%d",
			fs.SizeSuffix(fileSize), minConcurrency, concurrencyParams.actual)
	}

	fs.Debugf(f, "ğŸš€ v2å¤šçº¿ç¨‹å‚æ•° - ç½‘ç»œé€Ÿåº¦: %s/s, æœ€ä¼˜å¹¶å‘æ•°: %d, å®é™…å¹¶å‘æ•°: %d",
		fs.SizeSuffix(concurrencyParams.networkSpeed), concurrencyParams.optimal, concurrencyParams.actual)

	// æ³¨æ„ï¼šè·¨äº‘ä¼ è¾“æµç¼“å­˜ç­–ç•¥å·²åœ¨v2UploadChunksWithConcurrencyå‡½æ•°ä¸­å®ç°

	// é€‰æ‹©ä¸Šä¼ ç­–ç•¥ï¼šå¤šçº¿ç¨‹æˆ–å•çº¿ç¨‹
	if totalChunks > 1 && concurrencyParams.actual > 1 {
		fs.Debugf(f, "ä½¿ç”¨v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶å‘æ•°: %d", concurrencyParams.actual)
		return f.v2UploadChunksWithConcurrency(ctx, in, src, createResp, progress, fileName, concurrencyParams.actual)
	}

	// å•çº¿ç¨‹ä¸Šä¼ ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
	fs.Debugf(f, "ä½¿ç”¨v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ")
	return f.v2UploadChunksSingleThreaded(ctx, in, src, createResp, progress, fileName)
}

// downloadThenUpload è·¨äº‘ä¼ è¾“ç­–ç•¥ï¼šå…ˆå®Œæ•´ä¸‹è½½åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ï¼Œå†è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ 
// ç¡®ä¿è·¨äº‘ä¼ è¾“çš„å¯é æ€§ï¼Œé¿å…å¤æ‚çš„æµç®¡ç†é—®é¢˜
func (f *Fs) downloadThenUpload(ctx context.Context, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, maxConcurrency int) (*Object, error) {
	fileSize := src.Size()
	fs.Infof(f, "ğŸ“¥ è·¨äº‘ä¼ è¾“ç­–ç•¥ï¼šå¼€å§‹ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶ (%s)", fs.SizeSuffix(fileSize))

	// é‡æ–°æ‰“å¼€æºå¯¹è±¡è·å–å…¨æ–°çš„è¾“å…¥æµ
	srcObj, ok := src.(fs.Object)
	if !ok {
		return nil, fmt.Errorf("æºå¯¹è±¡ä¸æ”¯æŒé‡æ–°æ‰“å¼€ï¼Œæ— æ³•è¿›è¡Œè·¨äº‘ä¼ è¾“")
	}

	// åˆ›å»ºæœ¬åœ°ä¸´æ—¶æ–‡ä»¶ç”¨äºå®Œæ•´ä¸‹è½½
	tempFile, err := f.resourcePool.GetOptimizedTempFile("cross_cloud_download_", fileSize)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè·¨äº‘ä¼ è¾“ä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
	}
	defer f.resourcePool.PutTempFile(tempFile)

	// å®Œæ•´ä¸‹è½½æ–‡ä»¶å†…å®¹åˆ°æœ¬åœ°ï¼Œæ˜¾ç¤ºè¯¦ç»†è¿›åº¦
	fs.Infof(f, "ğŸ“¥ æ­£åœ¨ä»æºäº‘ç›˜ä¸‹è½½æ–‡ä»¶å†…å®¹ï¼Œé¢„è®¡å¤§å°: %s", fs.SizeSuffix(fileSize))

	// æ™ºèƒ½é€‰æ‹©ä¸‹è½½ç­–ç•¥ï¼šå¤§æ–‡ä»¶ä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼Œå°æ–‡ä»¶ä½¿ç”¨å•çº¿ç¨‹
	startTime := time.Now()
	var written int64

	if fileSize > 50*1024*1024 { // å¤§äº50MBä½¿ç”¨å¹¶å‘ä¸‹è½½ï¼Œé™ä½è§¦å‘é˜ˆå€¼
		fs.Infof(f, "ğŸš€ 123ç½‘ç›˜å¯ç”¨å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½ä¼˜åŒ– (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		written, err = f.downloadWithConcurrency(ctx, srcObj, tempFile, fileSize, fileName)
	} else {
		fs.Infof(f, "ğŸ“¥ 123ç½‘ç›˜ä½¿ç”¨å•çº¿ç¨‹ä¸‹è½½ (æ–‡ä»¶å¤§å°: %s)", fs.SizeSuffix(fileSize))
		srcReader, err := srcObj.Open(ctx)
		if err != nil {
			return nil, fmt.Errorf("æ— æ³•é‡æ–°æ‰“å¼€æºæ–‡ä»¶è¿›è¡Œä¸‹è½½: %w", err)
		}
		defer srcReader.Close()
		written, err = f.copyWithProgressAndValidation(ctx, tempFile, srcReader, fileSize, true, fileName)
	}
	downloadDuration := time.Since(startTime)

	if err != nil {
		return nil, fmt.Errorf("ä»æºäº‘ç›˜ä¸‹è½½æ–‡ä»¶å¤±è´¥: %w", err)
	}

	if written != fileSize {
		return nil, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}

	// è®¡ç®—ä¸‹è½½é€Ÿåº¦
	downloadSpeed := float64(written) / downloadDuration.Seconds() / (1024 * 1024) // MB/s
	fs.Infof(f, "âœ… æ–‡ä»¶ä¸‹è½½å®Œæˆ: %sï¼Œè€—æ—¶: %vï¼Œå¹³å‡é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), downloadDuration.Round(time.Second), downloadSpeed)

	// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´å‡†å¤‡ä¸Šä¼ 
	_, err = tempFile.Seek(0, io.SeekStart)
	if err != nil {
		return nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
	}

	fs.Infof(f, "ğŸš€ å¼€å§‹ä¸Šä¼ åˆ°123ç½‘ç›˜: %s", fs.SizeSuffix(written))

	// ä½¿ç”¨æœ¬åœ°æ–‡ä»¶è¿›è¡Œv2å¤šçº¿ç¨‹ä¸Šä¼ ï¼ˆé€’å½’è°ƒç”¨ï¼Œä½†æ­¤æ—¶isRemoteSourceä¸ºfalseï¼‰
	uploadStartTime := time.Now()
	result, err := f.v2UploadChunksWithConcurrency(ctx, tempFile, &localFileInfo{
		name:    fileName,
		size:    written,
		modTime: src.ModTime(ctx),
	}, createResp, progress, fileName, maxConcurrency)

	if err != nil {
		return nil, fmt.Errorf("ä¸Šä¼ åˆ°123ç½‘ç›˜å¤±è´¥: %w", err)
	}

	uploadDuration := time.Since(uploadStartTime)
	uploadSpeed := float64(written) / uploadDuration.Seconds() / (1024 * 1024) // MB/s
	totalDuration := time.Since(startTime)

	fs.Infof(f, "ğŸ‰ è·¨äº‘ä¼ è¾“å®Œæˆ: %sï¼Œæ€»è€—æ—¶: %v (ä¸‹è½½: %v + ä¸Šä¼ : %v)ï¼Œä¸Šä¼ é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(written), totalDuration.Round(time.Second),
		downloadDuration.Round(time.Second), uploadDuration.Round(time.Second), uploadSpeed)

	return result, nil
}

// localFileInfo æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ç»“æ„ï¼Œç”¨äºdownloadThenUploadç­–ç•¥
type localFileInfo struct {
	name    string
	size    int64
	modTime time.Time
}

func (lfi *localFileInfo) Fs() fs.Info                           { return nil }
func (lfi *localFileInfo) String() string                        { return lfi.name }
func (lfi *localFileInfo) Remote() string                        { return lfi.name }
func (lfi *localFileInfo) ModTime(ctx context.Context) time.Time { return lfi.modTime }
func (lfi *localFileInfo) Size() int64                           { return lfi.size }
func (lfi *localFileInfo) Storable() bool                        { return true }
func (lfi *localFileInfo) Hash(ctx context.Context, ty fshash.Type) (string, error) {
	return "", fshash.ErrUnsupported
}

// streamingPutSimplified ç®€åŒ–çš„æµå¼ä¸Šä¼ å®ç°
func (f *Fs) streamingPutSimplified(uploadCtx *UnifiedUploadContext) (*Object, error) {
	// æ£€æŸ¥æ˜¯å¦æœ‰å·²çŸ¥MD5
	if hashValue, err := uploadCtx.src.Hash(uploadCtx.ctx, fshash.MD5); err == nil && hashValue != "" {
		fs.Debugf(f, "ä½¿ç”¨å·²çŸ¥MD5è¿›è¡Œæµå¼ä¸Šä¼ : %s", hashValue)
		return f.putWithKnownMD5(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName, hashValue)
	}

	// å¯¹äºä¸­ç­‰å¤§å°æ–‡ä»¶ï¼Œä½¿ç”¨å†…å­˜ç¼“å†²ç­–ç•¥
	if uploadCtx.src.Size() > 0 && uploadCtx.src.Size() <= 50*1024*1024 { // 50MBä»¥ä¸‹
		fs.Debugf(f, "ä¸­ç­‰æ–‡ä»¶ä½¿ç”¨å†…å­˜ç¼“å†²ç­–ç•¥")
		return f.putSmallFileWithMD5(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName)
	}

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“
	fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨åˆ†ç‰‡æµå¼ä¼ è¾“")
	return f.streamingPutWithChunkedResume(uploadCtx.ctx, uploadCtx.in, uploadCtx.src, uploadCtx.parentFileID, uploadCtx.fileName)
}

// v2ChunkResult v2åˆ†ç‰‡ä¸Šä¼ ç»“æœ
type v2ChunkResult struct {
	chunkIndex int64
	err        error
	chunkHash  string
	duration   time.Duration // ä¸Šä¼ è€—æ—¶
	size       int64         // åˆ†ç‰‡å¤§å°
}

// v2UploadChunksWithConcurrency åŸºäºv2 APIçš„å¤šçº¿ç¨‹å¹¶å‘åˆ†ç‰‡ä¸Šä¼ 
// ä½¿ç”¨io.SectionReaderå®ç°çœŸæ­£çš„å¹¶å‘ä¸Šä¼ ï¼Œä¸ä¾èµ–æºå¯¹è±¡ç±»å‹
func (f *Fs) v2UploadChunksWithConcurrency(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, maxConcurrency int) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	// è°ƒè¯•preuploadID
	fs.Debugf(f, "v2å¤šçº¿ç¨‹ä¸Šä¼ å‚æ•°æ£€æŸ¥: preuploadID='%s', totalChunks=%d, maxConcurrency=%d", preuploadID, totalChunks, maxConcurrency)
	if preuploadID == "" {
		return nil, fmt.Errorf("é¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	fs.Debugf(f, "å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡ï¼Œæœ€å¤§å¹¶å‘æ•°ï¼š%d", totalChunks, maxConcurrency)

	// æ£€æŸ¥å½“å‰uploadPacerçš„é…ç½®ï¼Œç»™å‡ºQPSé™åˆ¶æé†’
	uploadQPS := 5.0 // v2åˆ†ç‰‡ä¸Šä¼ APIçš„ä¿å®ˆä¼°è®¡QPS
	if maxConcurrency > int(uploadQPS) {
		fs.Infof(f, "âš ï¸  QPSæé†’: å½“å‰å¹¶å‘æ•°ä¸º%dï¼Œv2åˆ†ç‰‡ä¸Šä¼ APIé™åˆ¶çº¦%.1f QPSï¼Œå¦‚é‡429é”™è¯¯è¯·è€ƒè™‘é™ä½å¹¶å‘æ•°", maxConcurrency, uploadQPS)
	} else {
		fs.Infof(f, "âœ… QPSé…ç½®: å¹¶å‘æ•°%dé€‚é…v2åˆ†ç‰‡ä¸Šä¼ APIé™åˆ¶%.1f QPS", maxConcurrency, uploadQPS)
	}

	// è·å–é¢„æœŸçš„æ•´ä½“æ–‡ä»¶MD5ç”¨äºæœ€ç»ˆéªŒè¯
	var expectedMD5 string
	if hashValue, err := src.Hash(ctx, fshash.MD5); err == nil && hashValue != "" {
		expectedMD5 = strings.ToLower(hashValue)
		fs.Debugf(f, "é¢„æœŸæ•´ä½“æ–‡ä»¶MD5: %s", expectedMD5)
	}

	// å°†è¾“å…¥æµè¯»å–åˆ°å†…å­˜æˆ–ä¸´æ—¶æ–‡ä»¶ï¼Œä»¥æ”¯æŒå¹¶å‘è®¿é—®ï¼ˆæ­¤æ—¶å·²ç¡®ä¿æ˜¯æœ¬åœ°æ–‡ä»¶ï¼‰
	var dataSource io.ReaderAt
	var cleanup func()

	if fileSize <= maxMemoryBufferSize {
		// å°æ–‡ä»¶ï¼šè¯»å–åˆ°å†…å­˜è¿›è¡Œå¹¶å‘ä¸Šä¼ 
		fs.Debugf(f, "ğŸ“ å°æ–‡ä»¶(%s)ï¼Œè¯»å–åˆ°å†…å­˜è¿›è¡Œå¹¶å‘ä¸Šä¼ ", fs.SizeSuffix(fileSize))
		data, err := f.readToMemoryWithRetry(ctx, in, fileSize, false)
		if err != nil {
			return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®åˆ°å†…å­˜å¤±è´¥: %w", err)
		}
		dataSource = bytes.NewReader(data)
		cleanup = func() {} // å†…å­˜æ•°æ®æ— éœ€æ¸…ç†
		fs.Debugf(f, "âœ… å°æ–‡ä»¶å†…å­˜ç¼“å­˜å®Œæˆï¼Œå®é™…å¤§å°: %s", fs.SizeSuffix(int64(len(data))))
	} else {
		// å¤§æ–‡ä»¶ï¼šä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¹¶å‘ä¸Šä¼ 
		fs.Debugf(f, "ğŸ—‚ï¸  å¤§æ–‡ä»¶(%s)ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¹¶å‘ä¸Šä¼ ", fs.SizeSuffix(fileSize))
		tempFile, written, err := f.createTempFileWithRetry(ctx, in, fileSize, false, fileName)
		if err != nil {
			return nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}

		dataSource = tempFile
		cleanup = func() {
			f.resourcePool.PutTempFile(tempFile) // ä½¿ç”¨èµ„æºæ± çš„æ¸…ç†æ–¹æ³•
		}

		fs.Debugf(f, "âœ… å¤§æ–‡ä»¶ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå®Œæˆï¼ŒæœŸæœ›å¤§å°: %sï¼Œå®é™…å¤§å°: %s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(written))
	}
	defer cleanup()

	// éªŒè¯æ•°æ®æºå®Œæ•´æ€§
	fs.Debugf(f, "ğŸ” éªŒè¯æ•°æ®æºå®Œæ•´æ€§...")
	if dataSource == nil {
		return nil, fmt.Errorf("æ•°æ®æºåˆ›å»ºå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¤šçº¿ç¨‹ä¸Šä¼ ")
	}

	// æµ‹è¯•æ•°æ®æºçš„éšæœºè®¿é—®èƒ½åŠ›
	testBuffer := make([]byte, 1024)
	if _, err := dataSource.ReadAt(testBuffer, 0); err != nil && err != io.EOF {
		fs.Debugf(f, "âš ï¸  æ•°æ®æºéšæœºè®¿é—®æµ‹è¯•å¤±è´¥: %v", err)
		return nil, fmt.Errorf("æ•°æ®æºä¸æ”¯æŒéšæœºè®¿é—®ï¼Œæ— æ³•è¿›è¡Œå¤šçº¿ç¨‹ä¸Šä¼ : %w", err)
	}
	fs.Debugf(f, "âœ… æ•°æ®æºéªŒè¯é€šè¿‡ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘è®¿é—®")

	// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡
	uploadTimeout := time.Duration(totalChunks) * 10 * time.Minute
	if uploadTimeout > 2*time.Hour {
		uploadTimeout = 2 * time.Hour
	}
	workerCtx, cancelWorkers := context.WithTimeout(ctx, uploadTimeout)
	defer cancelWorkers()

	// åˆ›å»ºå·¥ä½œé˜Ÿåˆ—å’Œç»“æœé€šé“
	queueSize := maxConcurrency * 2
	if queueSize > int(totalChunks) {
		queueSize = int(totalChunks)
	}
	chunkJobs := make(chan int64, queueSize)
	results := make(chan v2ChunkResult, maxConcurrency)

	// å¯åŠ¨å·¥ä½œåç¨‹
	fs.Debugf(f, "ğŸš€ å¯åŠ¨ %d ä¸ªå·¥ä½œåç¨‹è¿›è¡Œå¹¶å‘ä¸Šä¼ ", maxConcurrency)
	for i := 0; i < maxConcurrency; i++ {
		workerID := i + 1
		fs.Debugf(f, "ğŸ“‹ å¯åŠ¨å·¥ä½œåç¨‹ #%d", workerID)
		go f.v2ChunkUploadWorker(workerCtx, dataSource, preuploadID, chunkSize, fileSize, totalChunks, chunkJobs, results)
	}
	fs.Debugf(f, "âœ… æ‰€æœ‰å·¥ä½œåç¨‹å·²å¯åŠ¨ï¼Œå¼€å§‹åˆ†ç‰‡ä¸Šä¼ ä»»åŠ¡åˆ†å‘")

	// å‘é€éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡ä»»åŠ¡
	pendingChunks := int64(0)
	go func() {
		defer close(chunkJobs)
		for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
			partNumber := chunkIndex + 1
			if !progress.IsUploaded(partNumber) {
				select {
				case chunkJobs <- chunkIndex:
					// ä»»åŠ¡å‘é€æˆåŠŸ
				case <-workerCtx.Done():
					fs.Debugf(f, "ä¸Šä¸‹æ–‡å–æ¶ˆï¼Œåœæ­¢å‘é€åˆ†ç‰‡ä»»åŠ¡")
					return
				}
			}
		}
	}()

	// è®¡ç®—éœ€è¦ä¸Šä¼ çš„åˆ†ç‰‡æ•°é‡
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1
		if !progress.IsUploaded(partNumber) {
			pendingChunks++
		}
	}

	fs.Debugf(f, "éœ€è¦ä¸Šä¼  %d ä¸ªåˆ†ç‰‡", pendingChunks)

	// æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
	if pendingChunks > 0 {
		completedChunks := int64(0)
		totalUploadedBytes := int64(0)
		uploadStartTime := time.Now()

		fs.Infof(f, "ğŸš€ å¼€å§‹v2å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ : %dä¸ªåˆ†ç‰‡, æ€»å¤§å°: %s, å¹¶å‘æ•°: %d",
			pendingChunks, fs.SizeSuffix(fileSize), maxConcurrency)

		// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæŸ¥æ‰¾Accountå¯¹è±¡è¿›è¡Œé›†æˆ
		var currentAccount *accounting.Account
		if stats := accounting.GlobalStats(); stats != nil {
			// å°è¯•ç²¾ç¡®åŒ¹é…
			currentAccount = stats.GetInProgressAccount(fileName)
			if currentAccount == nil {
				// å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«æ–‡ä»¶åçš„Account
				allAccounts := stats.ListInProgressAccounts()
				for _, accountName := range allAccounts {
					if strings.Contains(accountName, fileName) || strings.Contains(fileName, accountName) {
						currentAccount = stats.GetInProgressAccount(accountName)
						if currentAccount != nil {
							break
						}
					}
				}
			}
		}

		for completedChunks < pendingChunks {
			select {
			case result := <-results:
				if result.err != nil {
					fs.Errorf(f, "âŒ åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %v", result.chunkIndex+1, result.err)

					// æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ™ºèƒ½å›é€€
					shouldFallback := f.shouldFallbackToSingleThread(result.err, src)
					if shouldFallback {
						fs.Infof(f, "ğŸ”„ æ£€æµ‹åˆ°å¤šçº¿ç¨‹ä¸Šä¼ é—®é¢˜ï¼Œå¯åŠ¨æ™ºèƒ½å›é€€æœºåˆ¶...")

						// ä¿å­˜å½“å‰è¿›åº¦
						saveErr := f.saveUploadProgress(progress)
						if saveErr != nil {
							fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
						}

						// å°è¯•å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
						return f.fallbackToSingleThreadUpload(ctx, in, src, createResp, progress, fileName, result.err)
					}

					// å¦‚æœä¸éœ€è¦å›é€€ï¼Œä¿å­˜è¿›åº¦åè¿”å›é”™è¯¯
					saveErr := f.saveUploadProgress(progress)
					if saveErr != nil {
						fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", saveErr)
					}
					return nil, fmt.Errorf("åˆ†ç‰‡ %d ä¸Šä¼ å¤±è´¥: %w", result.chunkIndex+1, result.err)
				}

				// æ ‡è®°åˆ†ç‰‡å®Œæˆ
				partNumber := result.chunkIndex + 1
				progress.SetUploaded(partNumber)
				completedChunks++

				// è®¡ç®—å·²ä¸Šä¼ çš„å­—èŠ‚æ•°
				chunkStart := (result.chunkIndex) * chunkSize
				actualChunkSize := chunkSize
				if chunkStart+chunkSize > fileSize {
					actualChunkSize = fileSize - chunkStart
				}
				totalUploadedBytes += actualChunkSize

				// ä¿å­˜è¿›åº¦
				err := f.saveUploadProgress(progress)
				if err != nil {
					fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
				}

				// è®¡ç®—æ•´ä½“è¿›åº¦ç»Ÿè®¡
				elapsed := time.Since(uploadStartTime)
				percentage := float64(completedChunks) / float64(pendingChunks) * 100
				avgSpeed := float64(totalUploadedBytes) / elapsed.Seconds() / 1024 / 1024 // MB/s

				// ä¼°ç®—å‰©ä½™æ—¶é—´
				var eta string
				if avgSpeed > 0 {
					remainingBytes := fileSize - totalUploadedBytes
					etaSeconds := float64(remainingBytes) / (avgSpeed * 1024 * 1024)
					eta = fmt.Sprintf("ETA: %v", time.Duration(etaSeconds)*time.Second)
				} else {
					eta = "ETA: è®¡ç®—ä¸­..."
				}

				fs.Infof(f, "ğŸ“Š ä¸Šä¼ è¿›åº¦: %d/%dåˆ†ç‰‡ (%.1f%%) | å·²ä¼ è¾“: %s/%s | é€Ÿåº¦: %.2f MB/s | %s",
					completedChunks, pendingChunks, percentage,
					fs.SizeSuffix(totalUploadedBytes), fs.SizeSuffix(fileSize), avgSpeed, eta)

				// ğŸ”§ ç»Ÿä¸€è¿›åº¦æ˜¾ç¤ºï¼šæ›´æ–°Accountçš„é¢å¤–ä¿¡æ¯
				if currentAccount != nil {
					extraInfo := fmt.Sprintf("[%d/%dåˆ†ç‰‡]", completedChunks, pendingChunks)
					currentAccount.SetExtraInfo(extraInfo)
				}

			case <-workerCtx.Done():
				return nil, fmt.Errorf("ä¸Šä¼ è¶…æ—¶æˆ–è¢«å–æ¶ˆ")
			}
		}

		// æœ€ç»ˆç»Ÿè®¡
		totalElapsed := time.Since(uploadStartTime)
		finalSpeed := float64(fileSize) / totalElapsed.Seconds() / 1024 / 1024
		fs.Infof(f, "ğŸ‰ å¤šçº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å®Œæˆï¼æ–‡ä»¶å¤§å°: %s | æ€»è€—æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s",
			fs.SizeSuffix(fileSize), totalElapsed.Round(time.Second), finalSpeed)
	}

	// å®Œæˆä¸Šä¼ ï¼Œä¼ é€’é¢„æœŸMD5ç”¨äºéªŒè¯
	return f.completeV2Upload(ctx, preuploadID, fileName, fileSize, expectedMD5)
}

// v2ChunkUploadWorker v2åˆ†ç‰‡ä¸Šä¼ å·¥ä½œåç¨‹ï¼Œå¸¦è¶…æ—¶å’Œæ­»é”æ£€æµ‹
func (f *Fs) v2ChunkUploadWorker(ctx context.Context, dataSource io.ReaderAt, preuploadID string, chunkSize, fileSize, totalChunks int64, jobs <-chan int64, results chan<- v2ChunkResult) {
	workerID := fmt.Sprintf("Worker-%d", time.Now().UnixNano()%1000) // ç®€å•çš„å·¥ä½œåç¨‹ID
	fs.Debugf(f, "ğŸ”§ %s å¯åŠ¨ï¼Œç­‰å¾…åˆ†ç‰‡ä»»åŠ¡...", workerID)

	processedCount := 0
	lastActivityTime := time.Now()

	// å¯åŠ¨æ­»é”æ£€æµ‹åç¨‹
	deadlockCtx, cancelDeadlock := context.WithCancel(ctx)
	defer cancelDeadlock()

	go f.workerDeadlockDetector(deadlockCtx, workerID, &lastActivityTime, 5*time.Minute)

	for chunkIndex := range jobs {
		select {
		case <-ctx.Done():
			fs.Debugf(f, "âš ï¸  %s æ”¶åˆ°å–æ¶ˆä¿¡å·ï¼Œå·²å¤„ç† %d ä¸ªåˆ†ç‰‡", workerID, processedCount)
			return
		default:
			// æ›´æ–°æ´»åŠ¨æ—¶é—´
			lastActivityTime = time.Now()
			processedCount++
			partNumber := chunkIndex + 1
			fs.Debugf(f, "ğŸ“‹ %s å¼€å§‹å¤„ç†åˆ†ç‰‡ %d/%d (ç¬¬%dä¸ªä»»åŠ¡)", workerID, partNumber, totalChunks, processedCount)

			// ä¸ºå•ä¸ªåˆ†ç‰‡å¤„ç†è®¾ç½®è¶…æ—¶
			chunkCtx, cancelChunk := context.WithTimeout(ctx, 10*time.Minute)
			result := f.processChunkWithTimeout(chunkCtx, dataSource, preuploadID, chunkIndex, chunkSize, fileSize, totalChunks, workerID)
			cancelChunk()

			// å‘é€ç»“æœæ—¶ä¹Ÿè¦æ£€æµ‹è¶…æ—¶
			select {
			case results <- result:
				lastActivityTime = time.Now() // æ›´æ–°æ´»åŠ¨æ—¶é—´
			case <-ctx.Done():
				fs.Debugf(f, "âš ï¸  %s å‘é€ç»“æœæ—¶æ”¶åˆ°å–æ¶ˆä¿¡å·", workerID)
				return
			case <-time.After(30 * time.Second):
				fs.Errorf(f, "âš ï¸  %s å‘é€ç»“æœè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ­»é”", workerID)
				// å°è¯•å‘é€è¶…æ—¶é”™è¯¯ç»“æœ
				timeoutResult := v2ChunkResult{
					chunkIndex: chunkIndex,
					err:        fmt.Errorf("å·¥ä½œåç¨‹å‘é€ç»“æœè¶…æ—¶ï¼Œå¯èƒ½å­˜åœ¨æ­»é”"),
				}
				select {
				case results <- timeoutResult:
				default:
					fs.Errorf(f, "âŒ %s æ— æ³•å‘é€è¶…æ—¶é”™è¯¯ç»“æœï¼Œé€šé“å¯èƒ½å·²æ»¡", workerID)
				}
				return
			}
		}
	}

	fs.Debugf(f, "ğŸ %s å®Œæˆå·¥ä½œï¼Œå…±å¤„ç† %d ä¸ªåˆ†ç‰‡", workerID, processedCount)
}

// workerDeadlockDetector å·¥ä½œåç¨‹æ­»é”æ£€æµ‹å™¨
func (f *Fs) workerDeadlockDetector(ctx context.Context, workerID string, lastActivityTime *time.Time, timeout time.Duration) {
	ticker := time.NewTicker(30 * time.Second) // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			if time.Since(*lastActivityTime) > timeout {
				fs.Errorf(f, "ğŸš¨ %s å¯èƒ½å‘ç”Ÿæ­»é”ï¼è¶…è¿‡ %v æ— æ´»åŠ¨", workerID, timeout)
				// è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„æ­»é”å¤„ç†é€»è¾‘ï¼Œæ¯”å¦‚å‘é€å‘Šè­¦ç­‰
			}
		}
	}
}

// processChunkWithTimeout å¸¦è¶…æ—¶çš„åˆ†ç‰‡å¤„ç†å‡½æ•°
func (f *Fs) processChunkWithTimeout(ctx context.Context, dataSource io.ReaderAt, preuploadID string, chunkIndex, chunkSize, fileSize, totalChunks int64, workerID string) v2ChunkResult {
	partNumber := chunkIndex + 1

	// è®¡ç®—åˆ†ç‰‡çš„èµ·å§‹ä½ç½®å’Œå¤§å°
	start := chunkIndex * chunkSize
	actualChunkSize := chunkSize
	if start+chunkSize > fileSize {
		actualChunkSize = fileSize - start
	}

	// åˆ›å»ºåˆ†ç‰‡è¯»å–å™¨
	chunkReader := io.NewSectionReader(dataSource, start, actualChunkSize)

	// è¯»å–åˆ†ç‰‡æ•°æ®å¹¶éªŒè¯
	chunkData := make([]byte, actualChunkSize)
	bytesRead, err := io.ReadFull(chunkReader, chunkData)
	if err != nil {
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("è¯»å–åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err),
		}
	}

	// éªŒè¯è¯»å–çš„æ•°æ®é•¿åº¦
	if int64(bytesRead) != actualChunkSize {
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("åˆ†ç‰‡æ•°æ®é•¿åº¦ä¸åŒ¹é…: æœŸæœ›%dï¼Œå®é™…%d", actualChunkSize, bytesRead),
		}
	}

	// è®¡ç®—åˆ†ç‰‡MD5
	hasher := md5.New()
	hasher.Write(chunkData)
	chunkHash := fmt.Sprintf("%x", hasher.Sum(nil))

	// ä¸Šä¼ åˆ†ç‰‡ï¼ˆè®°å½•å¼€å§‹æ—¶é—´ç”¨äºé€Ÿåº¦è®¡ç®—ï¼‰
	startTime := time.Now()

	fs.Debugf(f, "ğŸš€ %s å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %d/%d (å¤§å°: %s, MD5: %s)",
		workerID, partNumber, totalChunks, fs.SizeSuffix(actualChunkSize), chunkHash[:8]+"...")

	// å®ç°é‡è¯•æœºåˆ¶ - æå‡ä¸Šä¼ æˆåŠŸç‡
	var uploadErr error
	maxRetries := 3
	for retry := 0; retry <= maxRetries; retry++ {
		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
		select {
		case <-ctx.Done():
			return v2ChunkResult{
				chunkIndex: chunkIndex,
				err:        fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ è¢«å–æ¶ˆ: %w", ctx.Err()),
			}
		default:
		}

		uploadErr = f.uploadChunkV2(ctx, preuploadID, partNumber, chunkData, chunkHash)
		if uploadErr == nil {
			break // ä¸Šä¼ æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
		}

		if retry < maxRetries {
			retryDelay := time.Duration(retry+1) * 2 * time.Second // é€’å¢å»¶è¿Ÿ
			fs.Debugf(f, "âš ï¸  %s åˆ†ç‰‡ %d/%d ä¸Šä¼ å¤±è´¥ï¼Œ%våé‡è¯• (%d/%d): %v",
				workerID, partNumber, totalChunks, retryDelay, retry+1, maxRetries, uploadErr)

			select {
			case <-ctx.Done():
				return v2ChunkResult{
					chunkIndex: chunkIndex,
					err:        fmt.Errorf("åˆ†ç‰‡ä¸Šä¼ é‡è¯•æ—¶è¢«å–æ¶ˆ: %w", ctx.Err()),
				}
			case <-time.After(retryDelay):
				// ç»§ç»­é‡è¯•
			}
		}
	}

	if uploadErr != nil {
		duration := time.Since(startTime)
		fs.Errorf(f, "âŒ %s åˆ†ç‰‡ %d/%d ä¸Šä¼ å¤±è´¥ (è€—æ—¶: %v, é‡è¯•: %dæ¬¡): %v",
			workerID, partNumber, totalChunks, duration, maxRetries, uploadErr)
		return v2ChunkResult{
			chunkIndex: chunkIndex,
			err:        fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡å¤±è´¥(é‡è¯•%dæ¬¡): %w", maxRetries, uploadErr),
		}
	}

	// è®¡ç®—ä¸Šä¼ é€Ÿåº¦å’Œç»Ÿè®¡ä¿¡æ¯
	duration := time.Since(startTime)
	speed := float64(actualChunkSize) / duration.Seconds() / 1024 / 1024 // MB/s

	fs.Infof(f, "âœ… %s åˆ†ç‰‡ %d/%d ä¸Šä¼ æˆåŠŸ (å¤§å°: %s, è€—æ—¶: %v, é€Ÿåº¦: %.2f MB/s)",
		workerID, partNumber, totalChunks, fs.SizeSuffix(actualChunkSize), duration, speed)

	// æˆåŠŸ
	return v2ChunkResult{
		chunkIndex: chunkIndex,
		err:        nil,
		chunkHash:  chunkHash,
		duration:   duration,
		size:       actualChunkSize,
	}
}

// shouldFallbackToSingleThread åˆ¤æ–­æ˜¯å¦åº”è¯¥å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
func (f *Fs) shouldFallbackToSingleThread(err error, src fs.ObjectInfo) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())

	// æ£€æŸ¥è·¨äº‘ä¼ è¾“ç›¸å…³é”™è¯¯
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		// è·¨äº‘ä¼ è¾“æ—¶ï¼Œä»¥ä¸‹é”™è¯¯è§¦å‘å›é€€
		fallbackKeywords := []string{
			"æ•°æ®ä¸å®Œæ•´",
			"æ•°æ®ä¸åŒ¹é…",
			"è¯»å–å¤±è´¥",
			"è¿æ¥é‡ç½®",
			"è¶…æ—¶",
			"ç½‘ç»œé”™è¯¯",
			"æºå¯¹è±¡",
			"é‡æ–°æ‰“å¼€",
			"æ­»é”",
			"å·¥ä½œåç¨‹",
		}

		for _, keyword := range fallbackKeywords {
			if strings.Contains(errStr, keyword) {
				fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°è·¨äº‘ä¼ è¾“é”™è¯¯å…³é”®è¯: %s", keyword)
				return true
			}
		}
	}

	// æ£€æŸ¥å¤šçº¿ç¨‹ç›¸å…³é”™è¯¯
	multiThreadKeywords := []string{
		"å¹¶å‘",
		"åç¨‹",
		"goroutine",
		"concurrent",
		"deadlock",
		"timeout",
		"context canceled",
		"context deadline exceeded",
	}

	for _, keyword := range multiThreadKeywords {
		if strings.Contains(errStr, keyword) {
			fs.Debugf(f, "ğŸ” æ£€æµ‹åˆ°å¤šçº¿ç¨‹é”™è¯¯å…³é”®è¯: %s", keyword)
			return true
		}
	}

	return false
}

// fallbackToSingleThreadUpload å›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ 
func (f *Fs) fallbackToSingleThreadUpload(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string, originalErr error) (*Object, error) {
	fs.Infof(f, "ğŸ”„ å¤šçº¿ç¨‹ä¸Šä¼ å¤±è´¥ï¼Œå›é€€åˆ°å•çº¿ç¨‹ä¸Šä¼ æ¨¡å¼")
	fs.Debugf(f, "åŸå§‹é”™è¯¯: %v", originalErr)

	// æ£€æŸ¥æ˜¯å¦ä¸ºè·¨äº‘ä¼ è¾“
	isRemoteSource := f.isRemoteSource(src)
	if isRemoteSource {
		fs.Infof(f, "ğŸŒ è·¨äº‘ä¼ è¾“åœºæ™¯ï¼Œä½¿ç”¨å¢å¼ºå•çº¿ç¨‹æ¨¡å¼")
	}

	// å°è¯•å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ 
	fs.Debugf(f, "ğŸ”§ å°è¯•v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ...")
	result, err := f.v2UploadChunksSingleThreaded(ctx, in, src, createResp, progress, fileName)
	if err == nil {
		fs.Infof(f, "âœ… å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ æˆåŠŸ")
		return result, nil
	}

	fs.Debugf(f, "å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ä¹Ÿå¤±è´¥: %v", err)

	// å¦‚æœå•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ä¹Ÿå¤±è´¥ï¼Œå°è¯•æµå¼ä¸Šä¼ 
	if isRemoteSource {
		fs.Infof(f, "ğŸ”„ å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ å¤±è´¥ï¼Œå°è¯•æµå¼ä¸Šä¼ ...")

		// åˆ›å»ºæµå¼ä¸Šä¼ ä¸Šä¸‹æ–‡
		// æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»å…¶ä»–åœ°æ–¹è·å–parentFileIDï¼Œæš‚æ—¶ä½¿ç”¨0ä½œä¸ºå ä½ç¬¦
		uploadCtx := &UnifiedUploadContext{
			ctx:          ctx,
			in:           in,
			src:          src,
			parentFileID: 0, // TODO: éœ€è¦ä»ä¸Šä¼ ä¸Šä¸‹æ–‡ä¸­è·å–æ­£ç¡®çš„parentFileID
			fileName:     fileName,
		}

		streamResult, streamErr := f.streamingPutSimplified(uploadCtx)
		if streamErr == nil {
			fs.Infof(f, "âœ… æµå¼ä¸Šä¼ æˆåŠŸ")
			return streamResult, nil
		}

		fs.Debugf(f, "æµå¼ä¸Šä¼ ä¹Ÿå¤±è´¥: %v", streamErr)

		// è¿”å›æœ€è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
		return nil, fmt.Errorf("æ‰€æœ‰ä¸Šä¼ æ–¹å¼éƒ½å¤±è´¥ - åŸå§‹å¤šçº¿ç¨‹é”™è¯¯: %w, å•çº¿ç¨‹é”™è¯¯: %v, æµå¼ä¸Šä¼ é”™è¯¯: %v", originalErr, err, streamErr)
	}

	// éè·¨äº‘ä¼ è¾“åœºæ™¯ï¼Œè¿”å›å•çº¿ç¨‹ä¸Šä¼ é”™è¯¯
	return nil, fmt.Errorf("å¤šçº¿ç¨‹ä¸Šä¼ å¤±è´¥åå•çº¿ç¨‹ä¸Šä¼ ä¹Ÿå¤±è´¥ - åŸå§‹é”™è¯¯: %w, å•çº¿ç¨‹é”™è¯¯: %v", originalErr, err)
}

// uploadChunkV2 ä½¿ç”¨v2 APIä¸Šä¼ å•ä¸ªåˆ†ç‰‡
func (f *Fs) uploadChunkV2(ctx context.Context, preuploadID string, partNumber int64, chunkData []byte, chunkHash string) error {
	// æ£€æŸ¥preuploadIDæ˜¯å¦ä¸ºç©º
	if preuploadID == "" {
		fs.Errorf(f, "âŒ è‡´å‘½é”™è¯¯ï¼šé¢„ä¸Šä¼ IDä¸ºç©ºï¼Œæ— æ³•ä¸Šä¼ åˆ†ç‰‡ %d", partNumber)
		return fmt.Errorf("é¢„ä¸Šä¼ IDä¸èƒ½ä¸ºç©º")
	}

	// æ£€æŸ¥åˆ†ç‰‡æ•°æ®æ˜¯å¦ä¸ºç©º
	if len(chunkData) == 0 {
		fs.Errorf(f, "âŒ è‡´å‘½é”™è¯¯ï¼šåˆ†ç‰‡ %d æ•°æ®ä¸ºç©º", partNumber)
		return fmt.Errorf("åˆ†ç‰‡æ•°æ®ä¸èƒ½ä¸ºç©º")
	}

	fs.Debugf(f, "å¼€å§‹ä¸Šä¼ åˆ†ç‰‡ %dï¼Œé¢„ä¸Šä¼ ID: %s, åˆ†ç‰‡MD5: %s, æ•°æ®å¤§å°: %d å­—èŠ‚",
		partNumber, preuploadID, chunkHash, len(chunkData))

	// ä½¿ç”¨uploadPacerè¿›è¡ŒQPSé™åˆ¶æ§åˆ¶
	return f.uploadPacer.Call(func() (bool, error) {
		// åˆ›å»ºmultipartè¡¨å•æ•°æ®
		var buf bytes.Buffer
		writer := multipart.NewWriter(&buf)

		// æ·»åŠ è¡¨å•å­—æ®µ
		writer.WriteField("preuploadID", preuploadID)
		writer.WriteField("sliceNo", fmt.Sprintf("%d", partNumber))
		writer.WriteField("sliceMD5", chunkHash)

		// æ·»åŠ æ–‡ä»¶æ•°æ®
		part, err := writer.CreateFormFile("slice", fmt.Sprintf("chunk_%d", partNumber))
		if err != nil {
			return false, fmt.Errorf("åˆ›å»ºè¡¨å•æ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
		}

		_, err = part.Write(chunkData)
		if err != nil {
			return false, fmt.Errorf("å†™å…¥åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		err = writer.Close()
		if err != nil {
			return false, fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
		}

		// ä½¿ç”¨é€šç”¨çš„APIè°ƒç”¨æ–¹æ³•ï¼Œä½†éœ€è¦ç‰¹æ®Šå¤„ç†multipartæ•°æ®
		var response struct {
			Code    int    `json:"code"`
			Message string `json:"message"`
		}

		// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è¿›è¡Œmultipartä¸Šä¼ 
		err = f.makeAPICallWithRestMultipart(ctx, "/upload/v2/file/slice", "POST", &buf, writer.FormDataContentType(), &response)
		if err != nil {
			// æ£€æŸ¥æ˜¯å¦æ˜¯QPSé™åˆ¶é”™è¯¯
			if strings.Contains(err.Error(), "429") || strings.Contains(err.Error(), "é¢‘ç‡") {
				fs.Debugf(f, "åˆ†ç‰‡%dé‡åˆ°QPSé™åˆ¶ï¼Œå°†é‡è¯•: %v", partNumber, err)
				return true, err
			}
			return false, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡æ•°æ®å¤±è´¥: %w", err)
		}

		if response.Code != 0 {
			// æ£€æŸ¥æ˜¯å¦æ˜¯QPSé™åˆ¶ç›¸å…³é”™è¯¯
			if response.Code == 429 || response.Code == 20003 || strings.Contains(response.Message, "é¢‘ç‡") {
				fs.Debugf(f, "åˆ†ç‰‡%dé‡åˆ°APIé¢‘ç‡é™åˆ¶(code=%d)ï¼Œå°†é‡è¯•", partNumber, response.Code)
				return true, fmt.Errorf("APIé¢‘ç‡é™åˆ¶: code=%d, message=%s", response.Code, response.Message)
			}
			return false, fmt.Errorf("APIè¿”å›é”™è¯¯: code=%d, message=%s", response.Code, response.Message)
		}

		return false, nil
	})
}

// v2UploadChunksSingleThreaded v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
func (f *Fs) v2UploadChunksSingleThreaded(ctx context.Context, in io.Reader, src fs.ObjectInfo, createResp *UploadCreateResp, progress *UploadProgress, fileName string) (*Object, error) {
	preuploadID := createResp.Data.PreuploadID
	totalChunks := progress.TotalParts
	chunkSize := progress.ChunkSize
	fileSize := progress.FileSize

	fs.Debugf(f, "å¼€å§‹v2å•çº¿ç¨‹åˆ†ç‰‡ä¸Šä¼ ï¼š%dä¸ªåˆ†ç‰‡", totalChunks)

	// è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜ï¼ˆå¯¹äºå•çº¿ç¨‹ï¼Œè¿™æ ·æ›´ç®€å•ï¼‰
	data, err := io.ReadAll(in)
	if err != nil {
		return nil, fmt.Errorf("è¯»å–æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	// éªŒè¯è¯»å–çš„æ•°æ®é•¿åº¦ä¸é¢„æœŸæ–‡ä»¶å¤§å°åŒ¹é…
	actualDataSize := int64(len(data))
	if actualDataSize != fileSize {
		return nil, fmt.Errorf("æ•°æ®é•¿åº¦ä¸åŒ¹é…: å®é™…è¯»å– %d å­—èŠ‚ï¼Œé¢„æœŸ %d å­—èŠ‚", actualDataSize, fileSize)
	}

	if actualDataSize == 0 {
		return nil, fmt.Errorf("è¯»å–çš„æ–‡ä»¶æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†ç‰‡ä¸Šä¼ ")
	}

	fs.Debugf(f, "æ•°æ®éªŒè¯é€šè¿‡: è¯»å– %d å­—èŠ‚ï¼Œåˆ†ç‰‡å¤§å° %dï¼Œæ€»åˆ†ç‰‡æ•° %d", actualDataSize, chunkSize, totalChunks)

	// é€ä¸ªä¸Šä¼ åˆ†ç‰‡
	for chunkIndex := int64(0); chunkIndex < totalChunks; chunkIndex++ {
		partNumber := chunkIndex + 1

		// æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
		if progress.IsUploaded(partNumber) {
			fs.Debugf(f, "åˆ†ç‰‡ %d å·²ä¸Šä¼ ï¼Œè·³è¿‡", partNumber)
			continue
		}

		// è®¡ç®—åˆ†ç‰‡æ•°æ®ï¼Œæ·»åŠ è¾¹ç•Œæ£€æŸ¥
		start := chunkIndex * chunkSize
		end := start + chunkSize
		if end > fileSize {
			end = fileSize
		}

		// é¢å¤–çš„è¾¹ç•Œæ£€æŸ¥ï¼Œé˜²æ­¢slice panic
		if start >= actualDataSize {
			return nil, fmt.Errorf("åˆ†ç‰‡èµ·å§‹ä½ç½®è¶…å‡ºæ•°æ®èŒƒå›´: start=%d, dataSize=%d", start, actualDataSize)
		}
		if end > actualDataSize {
			end = actualDataSize
		}

		chunkData := data[start:end]

		// è®¡ç®—åˆ†ç‰‡MD5
		hasher := md5.New()
		hasher.Write(chunkData)
		chunkHash := fmt.Sprintf("%x", hasher.Sum(nil))

		// æ·»åŠ è¯¦ç»†çš„MD5è°ƒè¯•ä¿¡æ¯
		fs.Debugf(f, "åˆ†ç‰‡ %d MD5è®¡ç®—: æ•°æ®é•¿åº¦=%d, MD5=%s",
			partNumber, len(chunkData), chunkHash)

		// ä¸Šä¼ åˆ†ç‰‡
		err = f.uploadChunkV2(ctx, preuploadID, partNumber, chunkData, chunkHash)
		if err != nil {
			return nil, fmt.Errorf("ä¸Šä¼ åˆ†ç‰‡ %d å¤±è´¥: %w", partNumber, err)
		}

		// æ ‡è®°åˆ†ç‰‡å®Œæˆ
		progress.SetUploaded(partNumber)

		// ä¿å­˜è¿›åº¦
		err = f.saveUploadProgress(progress)
		if err != nil {
			fs.Debugf(f, "ä¿å­˜è¿›åº¦å¤±è´¥: %v", err)
		}

		fs.Debugf(f, "âœ… åˆ†ç‰‡ %d/%d ä¸Šä¼ å®Œæˆ", partNumber, totalChunks)
	}

	// å®Œæˆä¸Šä¼ ï¼ˆå•çº¿ç¨‹ç‰ˆæœ¬ï¼Œæ²¡æœ‰é¢„æœŸMD5ï¼‰
	return f.completeV2Upload(ctx, preuploadID, fileName, fileSize, "")
}

// completeV2Upload å®Œæˆv2ä¸Šä¼ å¹¶è¿”å›Object
func (f *Fs) completeV2Upload(ctx context.Context, preuploadID, fileName string, fileSize int64, expectedMD5 string) (*Object, error) {
	fs.Debugf(f, "ğŸ“‹ ã€é˜¶æ®µ3/3ã€‘ä¸Šä¼ å®Œæ¯• - å¼€å§‹è½®è¯¢æ ¡éªŒï¼Œç­‰å¾…æœåŠ¡ç«¯æ–‡ä»¶æ ¡éªŒå®Œæˆ")
	// è°ƒç”¨å¢å¼ºçš„å®Œæˆä¸Šä¼ é€»è¾‘ï¼ŒåŒ…å«MD5éªŒè¯å’ŒåŠ¨æ€è½®è¯¢
	result, err := f.completeUploadWithMD5VerificationAndSize(ctx, preuploadID, expectedMD5, fileSize)
	if err != nil {
		return nil, fmt.Errorf("å®Œæˆä¸Šä¼ å¤±è´¥: %w", err)
	}

	// æ¸…ç†ä¸Šä¼ è¿›åº¦æ–‡ä»¶
	f.removeUploadProgress(preuploadID)

	// éªŒè¯æ–‡ä»¶ID
	if result.FileID == 0 {
		return nil, fmt.Errorf("ä¸Šä¼ å®Œæˆä½†æœªè·å–åˆ°æœ‰æ•ˆçš„æ–‡ä»¶ID")
	}

	// åˆ›å»ºå¹¶è¿”å›Objectï¼Œä½¿ç”¨ä»APIè·å–çš„å®é™…æ•°æ®
	obj := &Object{
		fs:          f,
		remote:      fileName,
		hasMetaData: true,                                 // æˆ‘ä»¬æœ‰å®Œæ•´çš„å…ƒæ•°æ®
		id:          strconv.FormatInt(result.FileID, 10), // ä½¿ç”¨å®é™…çš„æ–‡ä»¶ID
		size:        fileSize,
		md5sum:      result.Etag, // ä½¿ç”¨ä»APIè·å–çš„MD5
		modTime:     time.Now(),
		isDir:       false,
	}

	fs.Infof(f, "ğŸ‰ æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼æ–‡ä»¶å: %s | å¤§å°: %s | æ–‡ä»¶ID: %d | MD5: %s", fileName, fs.SizeSuffix(fileSize), result.FileID, result.Etag)
	return obj, nil
}

// completeUploadWithMD5Verification å®Œæˆä¸Šä¼ å¹¶è¿›è¡ŒMD5éªŒè¯ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰
func (f *Fs) completeUploadWithMD5Verification(ctx context.Context, preuploadID, expectedMD5 string) (*UploadCompleteResult, error) {
	return f.completeUploadWithMD5VerificationAndSize(ctx, preuploadID, expectedMD5, 0)
}

// completeUploadWithMD5VerificationAndSize å®Œæˆä¸Šä¼ å¹¶è¿›è¡ŒMD5éªŒè¯ï¼Œæ”¯æŒåŠ¨æ€è½®è¯¢
func (f *Fs) completeUploadWithMD5VerificationAndSize(ctx context.Context, preuploadID, expectedMD5 string, fileSize int64) (*UploadCompleteResult, error) {
	// é¦–å…ˆè°ƒç”¨æ”¯æŒåŠ¨æ€è½®è¯¢çš„å®Œæˆä¸Šä¼ é€»è¾‘å¹¶è·å–ç»“æœ
	result, err := f.completeUploadWithResultAndSize(ctx, preuploadID, fileSize)
	if err != nil {
		return nil, err
	}

	// å¦‚æœæä¾›äº†é¢„æœŸMD5ï¼Œè¿›è¡Œé¢å¤–çš„éªŒè¯
	if expectedMD5 != "" && result.Etag != "" {
		fs.Debugf(f, "å¼€å§‹MD5éªŒè¯ï¼Œé¢„æœŸå€¼: %s, å®é™…å€¼: %s", expectedMD5, result.Etag)

		if result.Etag != expectedMD5 {
			fs.Debugf(f, "MD5ä¸åŒ¹é…ï¼Œä½†æ–‡ä»¶å·²ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: %d", result.FileID)
			// ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸ºæ–‡ä»¶å·²ç»ä¸Šä¼ æˆåŠŸï¼Œåªæ˜¯MD5éªŒè¯æœ‰é—®é¢˜
		} else {
			fs.Debugf(f, "MD5éªŒè¯æˆåŠŸ")
		}
	} else if expectedMD5 != "" {
		fs.Debugf(f, "æœåŠ¡å™¨æœªè¿”å›MD5å€¼ï¼Œè·³è¿‡éªŒè¯")
	}

	return result, nil
}

// verifyUploadMD5 éªŒè¯ä¸Šä¼ æ–‡ä»¶çš„MD5æ˜¯å¦æ­£ç¡®
// å¢å¼ºç‰ˆæœ¬ï¼šæ›´å¼ºçš„é”™è¯¯å¤„ç†å’Œæ™ºèƒ½é‡è¯•æœºåˆ¶
func (f *Fs) verifyUploadMD5(ctx context.Context, preuploadID, expectedMD5 string) error {
	fs.Debugf(f, "ğŸ” å¼€å§‹MD5éªŒè¯è½®è¯¢ï¼ŒpreuploadID: %s, é¢„æœŸMD5: %s", preuploadID, expectedMD5)

	// å‚æ•°éªŒè¯
	if preuploadID == "" {
		return fmt.Errorf("preuploadIDä¸èƒ½ä¸ºç©º")
	}
	if expectedMD5 == "" {
		fs.Debugf(f, "âš ï¸  é¢„æœŸMD5ä¸ºç©ºï¼Œè·³è¿‡MD5éªŒè¯")
		return nil
	}

	// å¢å¼ºçš„é‡è¯•é…ç½®
	maxAttempts := 15 // å¢åŠ é‡è¯•æ¬¡æ•°
	networkErrorRetries := 0
	maxNetworkErrorRetries := 5
	md5MismatchRetries := 0
	maxMD5MismatchRetries := 3

	for attempt := 0; attempt < maxAttempts; attempt++ {
		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦å·²å–æ¶ˆ
		select {
		case <-ctx.Done():
			return fmt.Errorf("MD5éªŒè¯è¢«å–æ¶ˆ: %w", ctx.Err())
		default:
		}

		reqBody := map[string]any{
			"preuploadID": preuploadID,
		}

		var response struct {
			Code    int    `json:"code"`
			Message string `json:"message"`
			Data    struct {
				Completed bool   `json:"completed"`
				FileID    int64  `json:"fileID"`
				Etag      string `json:"etag"`
			} `json:"data"`
		}

		// ä½¿ç”¨v2ä¸Šä¼ å®Œæ¯•APIæ£€æŸ¥æ–‡ä»¶çŠ¶æ€å’ŒMD5
		err := f.makeAPICallWithRest(ctx, "/upload/v2/file/upload_complete", "POST", reqBody, &response)
		if err != nil {
			// ç½‘ç»œé”™è¯¯åˆ†ç±»å¤„ç†
			errorType := f.classifyNetworkError(err)
			fs.Debugf(f, "ğŸ”„ MD5éªŒè¯ç¬¬%dæ¬¡å°è¯•å¤±è´¥ [%s]: %v", attempt+1, errorType, err)

			networkErrorRetries++
			if networkErrorRetries > maxNetworkErrorRetries {
				return fmt.Errorf("ç½‘ç»œé”™è¯¯é‡è¯•æ¬¡æ•°è¶…é™ï¼ŒMD5éªŒè¯å¤±è´¥: %w", err)
			}

			// æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­‰å¾…æ—¶é—´
			waitTime := f.calculateRetryWaitTime(errorType, attempt)
			fs.Debugf(f, "â° ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…%våé‡è¯•", waitTime)
			time.Sleep(waitTime)
			continue
		}

		// APIé”™è¯¯å¤„ç†
		if response.Code != 0 {
			fs.Debugf(f, "ğŸš« MD5éªŒè¯ç¬¬%dæ¬¡å°è¯•è¿”å›é”™è¯¯: APIé”™è¯¯ %d: %s", attempt+1, response.Code, response.Message)

			// ç‰¹å®šAPIé”™è¯¯çš„å¤„ç†
			apiErr := fmt.Errorf("APIé”™è¯¯ %d: %s", response.Code, response.Message)
			if f.isRetryableAPIError(apiErr) {
				waitTime := time.Duration(attempt+1) * 2 * time.Second
				fs.Debugf(f, "â° å¯é‡è¯•çš„APIé”™è¯¯ï¼Œç­‰å¾…%våé‡è¯•", waitTime)
				time.Sleep(waitTime)
				continue
			} else {
				return fmt.Errorf("ä¸å¯é‡è¯•çš„APIé”™è¯¯ %d: %s", response.Code, response.Message)
			}
		}

		if response.Data.Completed {
			// æ£€æŸ¥MD5æ˜¯å¦åŒ¹é…
			actualMD5 := strings.ToLower(strings.TrimSpace(response.Data.Etag))
			expectedMD5Lower := strings.ToLower(strings.TrimSpace(expectedMD5))

			if actualMD5 == expectedMD5Lower {
				fs.Debugf(f, "âœ… MD5éªŒè¯æˆåŠŸ: %s", actualMD5)
				return nil
			} else {
				md5MismatchRetries++
				fs.Debugf(f, "âŒ MD5ä¸åŒ¹é… (ç¬¬%dæ¬¡): é¢„æœŸ=%s, å®é™…=%s", md5MismatchRetries, expectedMD5Lower, actualMD5)

				// MD5ä¸åŒ¹é…çš„æ™ºèƒ½é‡è¯•
				if md5MismatchRetries <= maxMD5MismatchRetries {
					waitTime := time.Duration(md5MismatchRetries) * 3 * time.Second
					fs.Debugf(f, "ğŸ”„ MD5ä¸åŒ¹é…ï¼Œç­‰å¾…%våé‡è¯•éªŒè¯ï¼ˆå¯èƒ½æ˜¯æœåŠ¡ç«¯å¤„ç†å»¶è¿Ÿï¼‰", waitTime)
					time.Sleep(waitTime)
					continue
				} else {
					return fmt.Errorf("MD5éªŒè¯å¤±è´¥ï¼Œå¤šæ¬¡é‡è¯•åä»ä¸åŒ¹é…: é¢„æœŸ=%s, å®é™…=%s", expectedMD5Lower, actualMD5)
				}
			}
		}

		// ç­‰å¾…åé‡è¯•
		waitTime := f.calculateProgressiveWaitTime(attempt)
		fs.Debugf(f, "â³ MD5éªŒè¯å°šæœªå®Œæˆï¼Œç­‰å¾…%våé‡è¯•ï¼ˆç¬¬%d/%dæ¬¡ï¼‰", waitTime, attempt+1, maxAttempts)
		time.Sleep(waitTime)
	}

	return fmt.Errorf("MD5éªŒè¯è¶…æ—¶ï¼Œç»è¿‡%dæ¬¡å°è¯•åæ— æ³•ç¡®è®¤æ–‡ä»¶MD5çŠ¶æ€", maxAttempts)
}

// classifyNetworkError åˆ†ç±»ç½‘ç»œé”™è¯¯ç±»å‹
func (f *Fs) classifyNetworkError(err error) string {
	if err == nil {
		return "æ— é”™è¯¯"
	}

	errStr := strings.ToLower(err.Error())

	// è¶…æ—¶é”™è¯¯
	if strings.Contains(errStr, "timeout") || strings.Contains(errStr, "deadline exceeded") {
		return "è¶…æ—¶é”™è¯¯"
	}

	// è¿æ¥é”™è¯¯
	if strings.Contains(errStr, "connection refused") || strings.Contains(errStr, "no route to host") {
		return "è¿æ¥é”™è¯¯"
	}

	// DNSé”™è¯¯
	if strings.Contains(errStr, "no such host") || strings.Contains(errStr, "dns") {
		return "DNSé”™è¯¯"
	}

	// ç½‘ç»œä¸å¯è¾¾
	if strings.Contains(errStr, "network unreachable") || strings.Contains(errStr, "host unreachable") {
		return "ç½‘ç»œä¸å¯è¾¾"
	}

	// TLS/SSLé”™è¯¯
	if strings.Contains(errStr, "tls") || strings.Contains(errStr, "ssl") || strings.Contains(errStr, "certificate") {
		return "TLSé”™è¯¯"
	}

	// HTTPçŠ¶æ€ç é”™è¯¯
	if strings.Contains(errStr, "status") && (strings.Contains(errStr, "5") || strings.Contains(errStr, "4")) {
		return "HTTPçŠ¶æ€é”™è¯¯"
	}

	return "å…¶ä»–ç½‘ç»œé”™è¯¯"
}

// calculateRetryWaitTime æ ¹æ®é”™è¯¯ç±»å‹è®¡ç®—é‡è¯•ç­‰å¾…æ—¶é—´
func (f *Fs) calculateRetryWaitTime(errorType string, attempt int) time.Duration {
	baseWait := time.Duration(attempt+1) * time.Second

	switch errorType {
	case "è¶…æ—¶é”™è¯¯":
		// è¶…æ—¶é”™è¯¯éœ€è¦æ›´é•¿çš„ç­‰å¾…æ—¶é—´
		return baseWait * 3
	case "è¿æ¥é”™è¯¯", "ç½‘ç»œä¸å¯è¾¾":
		// è¿æ¥é—®é¢˜éœ€è¦è¾ƒé•¿ç­‰å¾…
		return baseWait * 2
	case "DNSé”™è¯¯":
		// DNSé—®é¢˜é€šå¸¸éœ€è¦æ›´é•¿æ—¶é—´æ¢å¤
		return baseWait * 4
	case "TLSé”™è¯¯":
		// TLSé—®é¢˜å¯èƒ½éœ€è¦é‡æ–°æ¡æ‰‹
		return baseWait * 2
	case "HTTPçŠ¶æ€é”™è¯¯":
		// HTTPé”™è¯¯é€šå¸¸å¯ä»¥è¾ƒå¿«é‡è¯•
		return baseWait
	default:
		// å…¶ä»–é”™è¯¯ä½¿ç”¨æ ‡å‡†ç­‰å¾…æ—¶é—´
		return baseWait
	}
}

// calculateProgressiveWaitTime è®¡ç®—æ¸è¿›å¼ç­‰å¾…æ—¶é—´
func (f *Fs) calculateProgressiveWaitTime(attempt int) time.Duration {
	// æ¸è¿›å¼é€€é¿ç®—æ³•ï¼š1s, 2s, 4s, 8s, 16s, ç„¶åä¿æŒ16s
	waitSeconds := 1 << attempt // 2^attempt
	if waitSeconds > 16 {
		waitSeconds = 16
	}
	return time.Duration(waitSeconds) * time.Second
}

// calculateMD5WithStreamCache ä»è¾“å…¥æµè®¡ç®—MD5å¹¶ç¼“å­˜æµå†…å®¹ï¼Œè¿”å›æ–°çš„å¯è¯»æµ
// ä½¿ç”¨åˆ†å—è¯»å–å’Œè¶…æ—¶æ§åˆ¶ï¼Œé¿å…å¤§æ–‡ä»¶å¤„ç†æ—¶çš„ç½‘ç»œè¶…æ—¶é—®é¢˜
func (f *Fs) calculateMD5WithStreamCache(ctx context.Context, in io.Reader, expectedSize int64) (string, io.Reader, error) {
	fs.Debugf(f, "å¼€å§‹ä»è¾“å…¥æµè®¡ç®—MD5å¹¶ç¼“å­˜æµå†…å®¹ï¼Œé¢„æœŸå¤§å°: %s", fs.SizeSuffix(expectedSize))

	hasher := md5.New()
	startTime := time.Now()

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜
	if expectedSize > maxMemoryBufferSize {
		fs.Infof(f, "ğŸ“‹ å‡†å¤‡ä¸Šä¼ å¤§æ–‡ä»¶ (%s) - æ­£åœ¨è®¡ç®—MD5å“ˆå¸Œä»¥å¯ç”¨ç§’ä¼ åŠŸèƒ½...", fs.SizeSuffix(expectedSize))
		fs.Infof(f, "ğŸ’¡ æç¤ºï¼šMD5è®¡ç®—å®Œæˆåå°†æ£€æŸ¥æ˜¯å¦å¯ä»¥ç§’ä¼ ï¼Œå¤§æ–‡ä»¶è®¡ç®—éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
		fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ç¼“å­˜æµå†…å®¹ï¼Œé‡‡ç”¨åˆ†å—è¯»å–ç­–ç•¥")
		tempFile, err := f.resourcePool.GetTempFile("stream_cache_")
		if err != nil {
			return "", nil, fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}

		// ä½¿ç”¨åˆ†å—è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ªå¤§æ–‡ä»¶å¯¼è‡´è¶…æ—¶
		written, err := f.copyWithChunksAndTimeout(ctx, tempFile, hasher, in, expectedSize)
		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			return "", nil, fmt.Errorf("ç¼“å­˜æµå†…å®¹æ—¶å¤åˆ¶æ•°æ®å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´
		_, err = tempFile.Seek(0, io.SeekStart)
		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			return "", nil, fmt.Errorf("é‡ç½®ä¸´æ—¶æ–‡ä»¶æŒ‡é’ˆå¤±è´¥: %w", err)
		}

		md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
		elapsed := time.Since(startTime)
		speed := float64(written) / elapsed.Seconds() / 1024 / 1024 // MB/s
		fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | å¤„ç†: %s | è€—æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s",
			md5Hash, fs.SizeSuffix(written), elapsed.Round(time.Second), speed)
		fs.Debugf(f, "å¤§æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %sï¼Œç¼“å­˜äº† %s æ•°æ®", md5Hash, fs.SizeSuffix(written))

		// è¿”å›ä¸€ä¸ªè‡ªåŠ¨æ¸…ç†çš„è¯»å–å™¨
		return md5Hash, &tempFileReader{file: tempFile}, nil
	} else {
		// å°æ–‡ä»¶åœ¨å†…å­˜ä¸­ç¼“å­˜
		fs.Infof(f, "ğŸ“‹ æ­£åœ¨è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ (%s)...", fs.SizeSuffix(expectedSize))
		fs.Debugf(f, "å°æ–‡ä»¶åœ¨å†…å­˜ä¸­ç¼“å­˜æµå†…å®¹")
		var buffer bytes.Buffer
		teeReader := io.TeeReader(in, &buffer)

		written, err := io.Copy(hasher, teeReader)
		if err != nil {
			return "", nil, fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
		elapsed := time.Since(startTime)
		fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | å¤„ç†: %s | è€—æ—¶: %v",
			md5Hash, fs.SizeSuffix(written), elapsed.Round(time.Millisecond))
		fs.Debugf(f, "å°æ–‡ä»¶MD5è®¡ç®—å®Œæˆ: %sï¼Œç¼“å­˜äº† %s æ•°æ®", md5Hash, fs.SizeSuffix(written))

		return md5Hash, bytes.NewReader(buffer.Bytes()), nil
	}
}

// copyWithChunksAndTimeout åˆ†å—å¤åˆ¶æ•°æ®ï¼Œæ”¯æŒè¶…æ—¶æ§åˆ¶å’Œè¿›åº¦ç›‘æ§
// åŒæ—¶å†™å…¥åˆ°æ–‡ä»¶å’ŒMD5è®¡ç®—å™¨ï¼Œé¿å…å¤§æ–‡ä»¶ä¸€æ¬¡æ€§è¯»å–å¯¼è‡´çš„è¶…æ—¶é—®é¢˜
// ä¿®å¤ç‰ˆæœ¬ï¼šè§£å†³è·¨äº‘ä¼ è¾“å¡æ­»é—®é¢˜ï¼Œå¢å¼ºé”™è¯¯å¤„ç†å’Œè¶…æ—¶æœºåˆ¶
func (f *Fs) copyWithChunksAndTimeout(ctx context.Context, file *os.File, hasher hash.Hash, in io.Reader, expectedSize int64) (int64, error) {
	const (
		chunkSize       = 4 * 1024 * 1024  // 4MB åˆ†å—å¤§å°ï¼ˆå‡å°ä»¥æé«˜å“åº”æ€§ï¼‰
		timeoutPerChunk = 60 * time.Second // æ¯ä¸ªåˆ†å—çš„è¶…æ—¶æ—¶é—´ï¼ˆå¢åŠ åˆ°60ç§’ï¼‰
		maxRetries      = 3                // æœ€å¤§é‡è¯•æ¬¡æ•°
	)

	multiWriter := io.MultiWriter(file, hasher)
	buffer := make([]byte, chunkSize)
	var totalWritten int64

	fs.Debugf(f, "ğŸš€ å¼€å§‹åˆ†å—å¤åˆ¶ï¼Œåˆ†å—å¤§å°: %sï¼Œé¢„æœŸæ€»å¤§å°: %s", fs.SizeSuffix(chunkSize), fs.SizeSuffix(expectedSize))

	// ä¸ºå¤§æ–‡ä»¶æ·»åŠ è¿›åº¦æç¤º
	var lastProgressTime time.Time
	startTime := time.Now()
	const progressInterval = 3 * time.Second // æ¯3ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼ˆæ›´é¢‘ç¹çš„åé¦ˆï¼‰

	// æ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶ï¼Œé˜²æ­¢æ•´ä¸ªæ“ä½œæ— é™æœŸå¡ä½
	var overallTimeout time.Duration
	if expectedSize > 0 {
		// åŸºäºæ–‡ä»¶å¤§å°è®¡ç®—åˆç†çš„æ€»è¶…æ—¶æ—¶é—´ï¼šæ¯MBæœ€å¤š30ç§’ï¼Œæœ€å°‘5åˆ†é’Ÿï¼Œæœ€å¤š2å°æ—¶
		timeoutPerMB := 30 * time.Second
		overallTimeout = time.Duration(expectedSize/(1024*1024)) * timeoutPerMB
		if overallTimeout < 5*time.Minute {
			overallTimeout = 5 * time.Minute
		}
		if overallTimeout > 2*time.Hour {
			overallTimeout = 2 * time.Hour
		}
	} else {
		overallTimeout = 30 * time.Minute // æœªçŸ¥å¤§å°æ–‡ä»¶é»˜è®¤30åˆ†é’Ÿè¶…æ—¶
	}

	overallCtx, overallCancel := context.WithTimeout(ctx, overallTimeout)
	defer overallCancel()

	fs.Debugf(f, "â° è®¾ç½®æ€»ä½“è¶…æ—¶æ—¶é—´: %v", overallTimeout)

	chunkNumber := 0
	for {
		chunkNumber++

		// æ£€æŸ¥æ€»ä½“è¶…æ—¶
		select {
		case <-overallCtx.Done():
			return totalWritten, fmt.Errorf("æ•´ä½“æ“ä½œè¶…æ—¶ (%v): %w", overallTimeout, overallCtx.Err())
		default:
		}

		var n int
		var readErr error

		// ä½¿ç”¨é‡è¯•æœºåˆ¶å¤„ç†ç½‘ç»œä¸ç¨³å®šé—®é¢˜
		for retry := 0; retry < maxRetries; retry++ {
			// ä¸ºæ¯ä¸ªåˆ†å—è®¾ç½®è¶…æ—¶
			chunkCtx, cancel := context.WithTimeout(overallCtx, timeoutPerChunk)

			// åˆ›å»ºä¸€ä¸ªå¸¦è¶…æ—¶çš„è¯»å–å™¨
			readDone := make(chan struct {
				n   int
				err error
			}, 1)

			go func() {
				n, err := in.Read(buffer)
				readDone <- struct {
					n   int
					err error
				}{n, err}
			}()

			select {
			case result := <-readDone:
				n, readErr = result.n, result.err
				cancel()
				break // æˆåŠŸè¯»å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
			case <-chunkCtx.Done():
				cancel()
				if retry < maxRetries-1 {
					fs.Debugf(f, "âš ï¸ åˆ†å— %d è¯»å–è¶…æ—¶ï¼Œé‡è¯• %d/%d", chunkNumber, retry+1, maxRetries)
					time.Sleep(time.Duration(retry+1) * time.Second) // é€’å¢å»¶è¿Ÿ
					continue
				} else {
					return totalWritten, fmt.Errorf("åˆ†å— %d è¯»å–è¶…æ—¶ï¼Œå·²é‡è¯• %d æ¬¡: %w", chunkNumber, maxRetries, chunkCtx.Err())
				}
			}
		}

		if readErr != nil {
			if readErr == io.EOF {
				break
			}
			return totalWritten, fmt.Errorf("è¯»å–åˆ†å— %d æ•°æ®å¤±è´¥: %w", chunkNumber, readErr)
		}

		if n == 0 {
			break
		}

		// å†™å…¥æ•°æ®
		written, writeErr := multiWriter.Write(buffer[:n])
		if writeErr != nil {
			return totalWritten, fmt.Errorf("å†™å…¥åˆ†å— %d æ•°æ®å¤±è´¥: %w", chunkNumber, writeErr)
		}

		if written != n {
			return totalWritten, fmt.Errorf("å†™å…¥åˆ†å— %d æ•°æ®ä¸å®Œæ•´: æœŸæœ› %dï¼Œå®é™… %d", chunkNumber, n, written)
		}

		totalWritten += int64(written)

		// ä¼˜åŒ–çš„è¿›åº¦æ˜¾ç¤ºï¼šåŸºäºæ—¶é—´é—´éš”ï¼ŒåŒ…å«é€Ÿåº¦å’ŒETAä¿¡æ¯
		now := time.Now()
		if now.Sub(lastProgressTime) >= progressInterval || (expectedSize > 0 && totalWritten >= expectedSize) {
			elapsed := now.Sub(startTime)
			if expectedSize > 0 {
				progress := float64(totalWritten) / float64(expectedSize) * 100
				speed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s

				// è®¡ç®—ETA
				var etaStr string
				if speed > 0 && progress < 100 {
					remainingBytes := expectedSize - totalWritten
					etaSeconds := float64(remainingBytes) / (speed * 1024 * 1024)
					eta := time.Duration(etaSeconds) * time.Second
					etaStr = fmt.Sprintf(" | ETA: %v", eta.Round(time.Second))
				} else {
					etaStr = ""
				}

				fs.Infof(f, "ğŸ”„ MD5è®¡ç®—è¿›åº¦: %s / %s (%.1f%%) | é€Ÿåº¦: %.2f MB/s%s | åˆ†å—: %d",
					fs.SizeSuffix(totalWritten), fs.SizeSuffix(expectedSize), progress, speed, etaStr, chunkNumber)
			} else {
				// æœªçŸ¥å¤§å°çš„æ–‡ä»¶ï¼Œåªæ˜¾ç¤ºå·²ä¼ è¾“é‡å’Œé€Ÿåº¦
				speed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s
				fs.Infof(f, "ğŸ”„ MD5è®¡ç®—è¿›åº¦: %s | é€Ÿåº¦: %.2f MB/s | è€—æ—¶: %v | åˆ†å—: %d",
					fs.SizeSuffix(totalWritten), speed, elapsed.Round(time.Second), chunkNumber)
			}
			lastProgressTime = now
		}

		// æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¢«å–æ¶ˆ
		select {
		case <-ctx.Done():
			return totalWritten, fmt.Errorf("æ“ä½œè¢«å–æ¶ˆ: %w", ctx.Err())
		default:
		}
	}

	// æœ€ç»ˆè¿›åº¦æŠ¥å‘Š
	elapsed := time.Now().Sub(startTime)
	avgSpeed := float64(totalWritten) / elapsed.Seconds() / 1024 / 1024 // MB/s
	fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼æ€»å¤§å°: %s | å¹³å‡é€Ÿåº¦: %.2f MB/s | æ€»è€—æ—¶: %v | æ€»åˆ†å—æ•°: %d",
		fs.SizeSuffix(totalWritten), avgSpeed, elapsed.Round(time.Second), chunkNumber-1)

	return totalWritten, nil
}

// readToMemoryWithRetry å¢å¼ºçš„å†…å­˜è¯»å–å‡½æ•°ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“é‡è¯•
func (f *Fs) readToMemoryWithRetry(ctx context.Context, in io.Reader, expectedSize int64, isRemoteSource bool) ([]byte, error) {
	maxRetries := 1
	if isRemoteSource {
		maxRetries = 3 // è·¨äº‘ä¼ è¾“å¢åŠ é‡è¯•æ¬¡æ•°
	}

	var lastErr error
	for retry := 0; retry <= maxRetries; retry++ {
		if retry > 0 {
			fs.Debugf(f, "ğŸ”„ å†…å­˜è¯»å–é‡è¯• %d/%d", retry, maxRetries)
			time.Sleep(time.Duration(retry) * 2 * time.Second) // é€’å¢å»¶è¿Ÿ
		}

		// ä½¿ç”¨é™åˆ¶è¯»å–å™¨é˜²æ­¢è¯»å–è¿‡å¤šæ•°æ®
		var limitedReader io.Reader
		if expectedSize > 0 {
			limitedReader = io.LimitReader(in, expectedSize+1024) // é¢å¤–1KBç”¨äºæ£€æµ‹å¤§å°å¼‚å¸¸
		} else {
			limitedReader = io.LimitReader(in, maxMemoryBufferSize) // ä½¿ç”¨æœ€å¤§å†…å­˜é™åˆ¶
		}

		data, err := io.ReadAll(limitedReader)
		if err != nil {
			lastErr = fmt.Errorf("è¯»å–æ•°æ®å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, lastErr
		}

		actualSize := int64(len(data))

		// éªŒè¯æ•°æ®å¤§å°
		if expectedSize > 0 {
			if actualSize > expectedSize+1024 {
				lastErr = fmt.Errorf("è¯»å–æ•°æ®è¿‡å¤š: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
				if retry < maxRetries {
					continue
				}
				return nil, lastErr
			}

			if actualSize < expectedSize {
				fs.Debugf(f, "âš ï¸  æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
				if isRemoteSource && retry < maxRetries {
					lastErr = fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualSize)
					continue
				}
			}
		}

		fs.Debugf(f, "âœ… å†…å­˜è¯»å–æˆåŠŸ: %då­—èŠ‚", actualSize)
		return data, nil
	}

	return nil, lastErr
}

// createTempFileWithRetry å¢å¼ºçš„ä¸´æ—¶æ–‡ä»¶åˆ›å»ºå‡½æ•°ï¼Œæ”¯æŒè·¨äº‘ä¼ è¾“é‡è¯•
func (f *Fs) createTempFileWithRetry(ctx context.Context, in io.Reader, expectedSize int64, isRemoteSource bool, fileName ...string) (*os.File, int64, error) {
	maxRetries := 1
	if isRemoteSource {
		maxRetries = 3 // è·¨äº‘ä¼ è¾“å¢åŠ é‡è¯•æ¬¡æ•°
	}

	var lastErr error
	for retry := 0; retry <= maxRetries; retry++ {
		if retry > 0 {
			fs.Debugf(f, "ğŸ”„ ä¸´æ—¶æ–‡ä»¶åˆ›å»ºé‡è¯• %d/%d", retry, maxRetries)
			time.Sleep(time.Duration(retry) * 2 * time.Second) // é€’å¢å»¶è¿Ÿ
		}

		tempFile, err := f.resourcePool.GetOptimizedTempFile("v2upload_", expectedSize)
		if err != nil {
			lastErr = fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// ä½¿ç”¨ç¼“å†²å†™å…¥æå‡æ€§èƒ½
		bufWriter := bufio.NewWriterSize(tempFile, 1024*1024) // 1MBç¼“å†²åŒº
		written, err := f.copyWithProgressAndValidation(ctx, bufWriter, in, expectedSize, isRemoteSource, fileName...)

		// åˆ·æ–°ç¼“å†²åŒº
		flushErr := bufWriter.Flush()
		if flushErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("åˆ·æ–°ç¼“å†²åŒºå¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, flushErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		if err != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("å¤åˆ¶æ•°æ®å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, err)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// è·¨äº‘ä¼ è¾“é¢å¤–éªŒè¯ï¼šæ£€æŸ¥æ–‡ä»¶å®é™…å¤§å°
		if isRemoteSource && expectedSize > 0 {
			fileInfo, statErr := tempFile.Stat()
			if statErr != nil {
				tempFile.Close()
				os.Remove(tempFile.Name())
				lastErr = fmt.Errorf("è·å–ä¸´æ—¶æ–‡ä»¶ä¿¡æ¯å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, statErr)
				if retry < maxRetries {
					continue
				}
				return nil, 0, lastErr
			}

			actualFileSize := fileInfo.Size()
			if actualFileSize != written {
				tempFile.Close()
				os.Remove(tempFile.Name())
				lastErr = fmt.Errorf("æ–‡ä»¶å¤§å°éªŒè¯å¤±è´¥(é‡è¯•%d/%d): å†™å…¥%då­—èŠ‚ï¼Œæ–‡ä»¶å®é™…%då­—èŠ‚", retry, maxRetries, written, actualFileSize)
				if retry < maxRetries {
					continue
				}
				return nil, 0, lastErr
			}

			if actualFileSize != expectedSize {
				fs.Debugf(f, "âš ï¸  è·¨äº‘ä¼ è¾“å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, actualFileSize)
				if actualFileSize < expectedSize && retry < maxRetries {
					tempFile.Close()
					os.Remove(tempFile.Name())
					lastErr = fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´(é‡è¯•%d/%d): æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", retry, maxRetries, expectedSize, actualFileSize)
					continue
				}
			}
		}

		// å¼ºåˆ¶åŒæ­¥åˆ°ç£ç›˜
		if syncErr := tempFile.Sync(); syncErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("åŒæ­¥ç£ç›˜å¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, syncErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		// é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
		if _, seekErr := tempFile.Seek(0, 0); seekErr != nil {
			tempFile.Close()
			os.Remove(tempFile.Name())
			lastErr = fmt.Errorf("é‡ç½®æ–‡ä»¶æŒ‡é’ˆå¤±è´¥(é‡è¯•%d/%d): %w", retry, maxRetries, seekErr)
			if retry < maxRetries {
				continue
			}
			return nil, 0, lastErr
		}

		fs.Debugf(f, "âœ… ä¸´æ—¶æ–‡ä»¶åˆ›å»ºæˆåŠŸ: %då­—èŠ‚", written)
		return tempFile, written, nil
	}

	return nil, 0, lastErr
}

// copyWithProgressAndValidation å¸¦è¿›åº¦å’ŒéªŒè¯çš„æ•°æ®å¤åˆ¶
func (f *Fs) copyWithProgressAndValidation(ctx context.Context, dst io.Writer, src io.Reader, expectedSize int64, isRemoteSource bool, fileName ...string) (int64, error) {
	// ä¼˜åŒ–ç¼“å†²åŒºå¤§å°ä»¥æå‡ä¼ è¾“é€Ÿåº¦
	bufferSize := 1024 * 1024 // 1MB - å¤§å¹…æå‡é€Ÿåº¦
	if isRemoteSource {
		// è·¨äº‘ä¼ è¾“ä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒºä»¥æå‡é€Ÿåº¦
		if expectedSize > 1024*1024*1024 { // å¤§äº1GBçš„æ–‡ä»¶
			bufferSize = 2 * 1024 * 1024 // 2MBç¼“å†²åŒº
		} else {
			bufferSize = 1024 * 1024 // 1MBç¼“å†²åŒº
		}
	}

	buffer := make([]byte, bufferSize)
	var totalWritten int64
	startTime := time.Now()
	lastProgressTime := startTime

	for {
		select {
		case <-ctx.Done():
			return totalWritten, ctx.Err()
		default:
		}

		// è®¾ç½®è¯»å–è¶…æ—¶ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
		readTimeout := 30 * time.Second
		if isRemoteSource {
			readTimeout = 60 * time.Second // è·¨äº‘ä¼ è¾“å¢åŠ è¶…æ—¶æ—¶é—´
		}
		_ = readTimeout // æš‚æ—¶æœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºæœªæ¥è¶…æ—¶æ§åˆ¶

		// è¯»å–æ•°æ®
		n, readErr := src.Read(buffer)
		if n > 0 {
			// å†™å…¥æ•°æ®
			written, writeErr := dst.Write(buffer[:n])
			if writeErr != nil {
				return totalWritten, fmt.Errorf("å†™å…¥å¤±è´¥: %w", writeErr)
			}
			totalWritten += int64(written)

			// å®šæœŸè¾“å‡ºè¿›åº¦ - æ›´é¢‘ç¹çš„è¿›åº¦æ›´æ–°
			progressInterval := 2 * time.Second                  // æ¯2ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
			if isRemoteSource && expectedSize > 1024*1024*1024 { // å¤§æ–‡ä»¶è·¨äº‘ä¼ è¾“
				progressInterval = 1 * time.Second // æ¯1ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
			}

			if time.Since(lastProgressTime) > progressInterval {
				progress := float64(totalWritten) / float64(expectedSize) * 100
				elapsed := time.Since(startTime).Seconds()
				currentSpeed := float64(totalWritten) / elapsed / (1024 * 1024) // MB/s

				// è·å–æ–‡ä»¶åç”¨äºæ˜¾ç¤º
				displayFileName := "æ–‡ä»¶"
				if len(fileName) > 0 && fileName[0] != "" {
					displayFileName = fileName[0]
					// å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªå–æ˜¾ç¤º
					if len(displayFileName) > 30 {
						displayFileName = displayFileName[:27] + "..."
					}
				}

				if expectedSize > 0 {
					remainingBytes := expectedSize - totalWritten
					eta := time.Duration(float64(remainingBytes) / (currentSpeed * 1024 * 1024) * float64(time.Second))
					// ä½¿ç”¨INFOæ—¥å¿—è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ
					fs.Infof(f, "ğŸ“Š [%s] 123ç½‘ç›˜æ•°æ®å¤åˆ¶è¿›åº¦: %s/%s (%.1f%%) - é€Ÿåº¦: %.2f MB/s - é¢„è®¡å‰©ä½™: %v",
						displayFileName, fs.SizeSuffix(totalWritten), fs.SizeSuffix(expectedSize), progress, currentSpeed, eta.Round(time.Second))
				} else {
					fs.Infof(f, "ğŸ“Š [%s] 123ç½‘ç›˜æ•°æ®å¤åˆ¶è¿›åº¦: %s - é€Ÿåº¦: %.2f MB/s",
						displayFileName, fs.SizeSuffix(totalWritten), currentSpeed)
				}
				lastProgressTime = time.Now()
			}
		}

		if readErr != nil {
			if readErr == io.EOF {
				break // æ­£å¸¸ç»“æŸ
			}
			return totalWritten, fmt.Errorf("è¯»å–å¤±è´¥: %w", readErr)
		}
	}

	// éªŒè¯æœ€ç»ˆå¤§å°å’Œæ•°æ®å®Œæ•´æ€§
	if expectedSize > 0 && totalWritten != expectedSize {
		if isRemoteSource && totalWritten < expectedSize {
			// è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´ï¼Œå°è¯•æ¢å¤
			fs.Debugf(f, "ğŸ”„ è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´ï¼Œå°è¯•æ•°æ®æ¢å¤...")
			return totalWritten, fmt.Errorf("è·¨äº‘ä¼ è¾“æ•°æ®ä¸å®Œæ•´: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚ï¼Œå»ºè®®é‡è¯•", expectedSize, totalWritten)
		}
		fs.Debugf(f, "âš ï¸  æ•°æ®å¤§å°ä¸åŒ¹é…: æœŸæœ›%då­—èŠ‚ï¼Œå®é™…%då­—èŠ‚", expectedSize, totalWritten)
	}

	// å¯¹äºè·¨äº‘ä¼ è¾“ï¼Œæ·»åŠ é¢å¤–çš„å®Œæ•´æ€§æ£€æŸ¥
	if isRemoteSource && expectedSize > 0 {
		fs.Debugf(f, "âœ… è·¨äº‘ä¼ è¾“æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡: %då­—èŠ‚", totalWritten)
	}

	return totalWritten, nil
}

// tempFileReader åŒ…è£…ä¸´æ—¶æ–‡ä»¶ï¼Œå®ç°è‡ªåŠ¨æ¸…ç†
type tempFileReader struct {
	file *os.File
}

func (r *tempFileReader) Read(p []byte) (n int, err error) {
	return r.file.Read(p)
}

func (r *tempFileReader) Close() error {
	if r.file != nil {
		fileName := r.file.Name()
		r.file.Close()
		os.Remove(fileName)
		r.file = nil
	}
	return nil
}

// calculateMD5FromReader ä»è¾“å…¥æµè®¡ç®—MD5å“ˆå¸Œï¼Œæ”¯æŒè·¨äº‘ç›˜ä¼ è¾“
// ä½¿ç”¨åˆ†å—è¯»å–å’Œè¶…æ—¶æ§åˆ¶ï¼Œé¿å…å¤§æ–‡ä»¶å¤„ç†æ—¶çš„ç½‘ç»œè¶…æ—¶é—®é¢˜
func (f *Fs) calculateMD5FromReader(ctx context.Context, in io.Reader, expectedSize int64) (string, error) {
	fs.Debugf(f, "å¼€å§‹ä»è¾“å…¥æµè®¡ç®—MD5ï¼Œé¢„æœŸå¤§å°: %s", fs.SizeSuffix(expectedSize))

	hasher := md5.New()
	startTime := time.Now()

	// å¯¹äºå¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†å—è¯»å–é¿å…è¶…æ—¶
	if expectedSize > maxMemoryBufferSize {
		fs.Infof(f, "ğŸ“‹ æ­£åœ¨è®¡ç®—å¤§æ–‡ä»¶MD5å“ˆå¸Œ (%s) - å¯ç”¨ç§’ä¼ æ£€æµ‹...", fs.SizeSuffix(expectedSize))
		fs.Debugf(f, "å¤§æ–‡ä»¶ä½¿ç”¨åˆ†å—è¯»å–è®¡ç®—MD5")
		tempFile, err := f.resourcePool.GetTempFile("md5calc_")
		if err != nil {
			return "", fmt.Errorf("åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤±è´¥: %w", err)
		}
		defer func() {
			tempFile.Close()
			os.Remove(tempFile.Name())
		}()

		// ä½¿ç”¨åˆ†å—è¯»å–ï¼Œé¿å…ä¸€æ¬¡æ€§è¯»å–æ•´ä¸ªå¤§æ–‡ä»¶å¯¼è‡´è¶…æ—¶
		written, err := f.copyWithChunksAndTimeout(ctx, tempFile, hasher, in, expectedSize)
		if err != nil {
			return "", fmt.Errorf("è®¡ç®—MD5æ—¶å¤åˆ¶æ•°æ®å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		fs.Debugf(f, "MD5è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† %s æ•°æ®", fs.SizeSuffix(written))
	} else {
		// å°æ–‡ä»¶ç›´æ¥åœ¨å†…å­˜ä¸­è®¡ç®—
		fs.Debugf(f, "å°æ–‡ä»¶åœ¨å†…å­˜ä¸­è®¡ç®—MD5")
		written, err := io.Copy(hasher, in)
		if err != nil {
			return "", fmt.Errorf("è®¡ç®—MD5å¤±è´¥: %w", err)
		}

		if expectedSize > 0 && written != expectedSize {
			fs.Debugf(f, "è­¦å‘Šï¼šå®é™…è¯»å–å¤§å°(%d)ä¸é¢„æœŸå¤§å°(%d)ä¸åŒ¹é…", written, expectedSize)
		}

		fs.Debugf(f, "MD5è®¡ç®—å®Œæˆï¼Œå¤„ç†äº† %s æ•°æ®", fs.SizeSuffix(written))
	}

	md5Hash := fmt.Sprintf("%x", hasher.Sum(nil))
	elapsed := time.Since(startTime)
	fs.Infof(f, "âœ… MD5è®¡ç®—å®Œæˆï¼å“ˆå¸Œå€¼: %s | è€—æ—¶: %v", md5Hash, elapsed.Round(time.Millisecond))
	fs.Debugf(f, "MD5è®¡ç®—ç»“æœ: %s", md5Hash)
	return md5Hash, nil
}

// singleStepUploadMultipart ä½¿ç”¨multipart formä¸Šä¼ æ–‡ä»¶çš„ä¸“ç”¨æ–¹æ³•
func (f *Fs) singleStepUploadMultipart(ctx context.Context, data []byte, parentFileID int64, fileName, md5Hash string, result interface{}) error {
	// åˆ›å»ºmultipart formæ•°æ®
	var buf bytes.Buffer
	writer := multipart.NewWriter(&buf)

	// æ·»åŠ è¡¨å•å­—æ®µ
	if err := writer.WriteField("parentFileID", strconv.FormatInt(parentFileID, 10)); err != nil {
		return fmt.Errorf("å†™å…¥parentFileIDå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("filename", fileName); err != nil {
		return fmt.Errorf("å†™å…¥filenameå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("etag", md5Hash); err != nil {
		return fmt.Errorf("å†™å…¥etagå¤±è´¥: %w", err)
	}

	if err := writer.WriteField("size", strconv.Itoa(len(data))); err != nil {
		return fmt.Errorf("å†™å…¥sizeå¤±è´¥: %w", err)
	}

	// æ·»åŠ duplicateå­—æ®µï¼ˆä¿ç•™ä¸¤è€…ï¼Œé¿å…è¦†ç›–ï¼‰
	if err := writer.WriteField("duplicate", "1"); err != nil {
		return fmt.Errorf("å†™å…¥duplicateå¤±è´¥: %w", err)
	}

	// æ·»åŠ æ–‡ä»¶æ•°æ®
	fileWriter, err := writer.CreateFormFile("file", fileName)
	if err != nil {
		return fmt.Errorf("åˆ›å»ºæ–‡ä»¶å­—æ®µå¤±è´¥: %w", err)
	}

	if _, err := fileWriter.Write(data); err != nil {
		return fmt.Errorf("å†™å…¥æ–‡ä»¶æ•°æ®å¤±è´¥: %w", err)
	}

	if err := writer.Close(); err != nil {
		return fmt.Errorf("å…³é—­multipart writerå¤±è´¥: %w", err)
	}

	// ä½¿ç”¨rcloneæ ‡å‡†æ–¹æ³•è¿›è¡Œå•æ­¥ä¸Šä¼ multipartè°ƒç”¨
	return f.makeAPICallWithRestMultipart(ctx, "/upload/v2/file/single/create", "POST", &buf, writer.FormDataContentType(), result)
}

// downloadWithConcurrency å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶
// åŸºäºç°æœ‰çš„downloadChunksConcurrentlyå®ç°ï¼Œä¸“é—¨ç”¨äºè·¨äº‘ä¼ è¾“åœºæ™¯
func (f *Fs) downloadWithConcurrency(ctx context.Context, srcObj fs.Object, tempFile *os.File, fileSize int64, fileName string) (int64, error) {
	// è®¡ç®—æœ€ä¼˜åˆ†ç‰‡å‚æ•°
	chunkSize := f.calculateDownloadChunkSize(fileSize)
	numChunks := (fileSize + chunkSize - 1) / chunkSize
	maxConcurrency := f.calculateDownloadConcurrency(fileSize)

	fs.Infof(f, "ğŸ“Š 123ç½‘ç›˜å¹¶å‘ä¸‹è½½å‚æ•°: åˆ†ç‰‡å¤§å°=%s, åˆ†ç‰‡æ•°=%d, å¹¶å‘æ•°=%d",
		fs.SizeSuffix(chunkSize), numChunks, maxConcurrency)

	fs.Infof(f, "ğŸš€ 123ç½‘ç›˜å¼€å§‹å¹¶å‘ä¸‹è½½: %s (%s)", fileName, fs.SizeSuffix(fileSize))

	// ä½¿ç”¨å¹¶å‘ä¸‹è½½å®ç°
	downloadStartTime := time.Now()
	err := f.downloadChunksConcurrently(ctx, srcObj, tempFile, chunkSize, numChunks, int64(maxConcurrency))
	downloadDuration := time.Since(downloadStartTime)
	if err != nil {
		return 0, fmt.Errorf("123ç½‘ç›˜å¹¶å‘ä¸‹è½½å¤±è´¥: %w", err)
	}

	// éªŒè¯ä¸‹è½½å®Œæ•´æ€§
	stat, err := tempFile.Stat()
	if err != nil {
		return 0, fmt.Errorf("è·å–ä¸‹è½½æ–‡ä»¶ä¿¡æ¯å¤±è´¥: %w", err)
	}

	actualSize := stat.Size()
	if actualSize != fileSize {
		return 0, fmt.Errorf("ä¸‹è½½æ–‡ä»¶å¤§å°ä¸åŒ¹é…: æœŸæœ›%sï¼Œå®é™…%s",
			fs.SizeSuffix(fileSize), fs.SizeSuffix(actualSize))
	}

	// è®¡ç®—å¹³å‡ä¸‹è½½é€Ÿåº¦
	avgSpeed := float64(actualSize) / downloadDuration.Seconds() / 1024 / 1024 // MB/s

	fs.Infof(f, "âœ… 123ç½‘ç›˜å¹¶å‘ä¸‹è½½å®Œæˆ: %s | ç”¨æ—¶: %v | å¹³å‡é€Ÿåº¦: %.2f MB/s",
		fs.SizeSuffix(actualSize), downloadDuration, avgSpeed)
	return actualSize, nil
}

// startPerformanceMonitoring å¯åŠ¨æ€§èƒ½ç›‘æ§å®šæ—¶å™¨
func (f *Fs) startPerformanceMonitoring(ctx context.Context) {
	if f.enhancedMetrics == nil {
		return
	}

	interval := time.Duration(f.opt.PerformanceLogInterval)
	if interval <= 0 {
		interval = 60 * time.Second // é»˜è®¤60ç§’
	}

	go func() {
		ticker := time.NewTicker(interval)
		defer ticker.Stop()

		for {
			select {
			case <-ctx.Done():
				fs.Debugf(f, "ğŸ”š æ€§èƒ½ç›‘æ§å®šæ—¶å™¨åœæ­¢")
				return
			case <-ticker.C:
				f.logPerformanceStats()
			}
		}
	}()

	fs.Infof(f, "ğŸ“Š æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œé—´éš”: %v", interval)
}

// logPerformanceStats è¾“å‡ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
func (f *Fs) logPerformanceStats() {
	if f.enhancedMetrics == nil {
		return
	}

	stats := f.enhancedMetrics.GetStats()

	fs.Infof(f, "ğŸ“Š æ€§èƒ½ç»Ÿè®¡ - è¿è¡Œæ—¶é—´: %v", stats.TotalRuntime)
	fs.Infof(f, "ğŸ“¤ ä¸Šä¼ ç»Ÿè®¡ - æ€»æ•°: %d, æˆåŠŸ: %d, å¤±è´¥: %d, å¹³å‡é€Ÿåº¦: %.2f MB/s",
		stats.TotalUploads, stats.SuccessfulUploads, stats.FailedUploads, stats.AverageUploadSpeed)
	fs.Infof(f, "ğŸ“¥ ä¸‹è½½ç»Ÿè®¡ - æ€»æ•°: %d, æˆåŠŸ: %d, å¤±è´¥: %d, å¹³å‡é€Ÿåº¦: %.2f MB/s",
		stats.TotalDownloads, stats.SuccessfulDownloads, stats.FailedDownloads, stats.AverageDownloadSpeed)
	fs.Infof(f, "ğŸ”„ APIè°ƒç”¨ - æ€»æ•°: %d, æˆåŠŸç‡: %.2f%%, å¹³å‡å»¶è¿Ÿ: %v",
		stats.TotalAPICalls, stats.APISuccessRate*100, stats.AverageAPILatency)
	fs.Infof(f, "ğŸ’¾ å†…å­˜ä½¿ç”¨ - å½“å‰: %s, å³°å€¼: %s",
		fs.SizeSuffix(stats.CurrentMemoryUsage), fs.SizeSuffix(stats.PeakMemoryUsage))
}

// isRetryableAPIError åˆ¤æ–­æ˜¯å¦ä¸ºå¯é‡è¯•çš„APIé”™è¯¯
func (f *Fs) isRetryableAPIError(err error) bool {
	if err == nil {
		return false
	}

	errStr := strings.ToLower(err.Error())

	// æ£€æŸ¥å¸¸è§çš„å¯é‡è¯•é”™è¯¯
	retryableErrors := []string{
		"timeout",
		"connection reset",
		"connection refused",
		"temporary failure",
		"server error",
		"service unavailable",
		"too many requests",
		"rate limit",
		"ç½‘ç»œé”™è¯¯",
		"è¿æ¥è¶…æ—¶",
		"æœåŠ¡å™¨ç¹å¿™",
	}

	for _, retryableErr := range retryableErrors {
		if strings.Contains(errStr, retryableErr) {
			return true
		}
	}

	// æ£€æŸ¥HTTPçŠ¶æ€ç 
	if strings.Contains(errStr, "429") || // Too Many Requests
		strings.Contains(errStr, "500") || // Internal Server Error
		strings.Contains(errStr, "502") || // Bad Gateway
		strings.Contains(errStr, "503") || // Service Unavailable
		strings.Contains(errStr, "504") { // Gateway Timeout
		return true
	}

	return false
}

// calculateDownloadChunkSize è®¡ç®—ä¸‹è½½åˆ†ç‰‡å¤§å°
func (f *Fs) calculateDownloadChunkSize(fileSize int64) int64 {
	switch {
	case fileSize < 500*1024*1024: // <500MB
		return 16 * 1024 * 1024 // 16MBåˆ†ç‰‡
	case fileSize < 2*1024*1024*1024: // <2GB
		return 32 * 1024 * 1024 // 32MBåˆ†ç‰‡
	default: // >2GB
		return 64 * 1024 * 1024 // 64MBåˆ†ç‰‡
	}
}

// calculateDownloadConcurrency è®¡ç®—ä¸‹è½½å¹¶å‘æ•°
func (f *Fs) calculateDownloadConcurrency(fileSize int64) int {
	switch {
	case fileSize < 500*1024*1024: // <500MB
		return 2 // 2å¹¶å‘
	case fileSize < 2*1024*1024*1024: // <2GB
		return 4 // 4å¹¶å‘
	default: // >2GB
		return 6 // 6å¹¶å‘ï¼Œä½†ä¸è¶…è¿‡APIé™åˆ¶
	}
}
